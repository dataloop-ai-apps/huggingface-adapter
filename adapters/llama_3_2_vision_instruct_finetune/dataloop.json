{
    "name": "llama-3-2-vision-instruct-fine-tune",
    "displayName": "Llama-3.2-Vision-Instruct-Fine-Tune",
    "version": "0.1.0",
    "scope": "public",
    "description": "Llama-3.2-Vision-Instruct from Hugging Face, ready for fine-tuning with QLoRA. Supports multimodal vision-to-text capabilities.",
    "attributes": {
      "Provider": "Meta",
      "Deployed By": "Hugging Face",
      "License": "Llama 3.2",
      "Media Type": [
        "Image",
        "Text"
      ],
      "Category": "Model",
      "Gen AI": "LLM",
      "Vision": "Multimodal",
      "NLP": "Conversational",
      "Hub": [
        "Dataloop"
      ]
    },
    "codebase": {
      "type": "git",
      "gitUrl": "https://github.com/dataloop-ai-apps/huggingface-adapter.git",
      "gitTag": "0.1.0"
    },
    "components": {
      "computeConfigs": [
        {
          "name": "llama-vision-finetuned-deploy",
          "runtime": {
            "podType": "regular-s",
            "concurrency": 5,
            "runnerImage": "gcr.io/viewo-g/piper/agent/runner/apps/llm-finetune-playground:0.0.6",
            "autoscaler": {
              "type": "rabbitmq",
              "minReplicas": 0,
              "maxReplicas": 2
            }
          }
        },
        {
          "name": "llama-vision-finetuned-train",
          "versions": {
            "dtlpy": "1.106.5"
          },
          "runtime": {
            "podType": "gpu-a100-s",
            "concurrency": 1,
            "runnerImage": "gcr.io/viewo-g/piper/agent/runner/apps/llm-finetune-playground:0.0.6",
            "autoscaler": {
              "type": "rabbitmq",
              "minReplicas": 0,
              "maxReplicas": 1
            }
          }
        }
      ],
      "modules": [
        {
          "name": "llama-vision-finetuned-module",
          "entryPoint": "adapters/llama_3_2_vision_instruct_finetune/model_adapter.py",
          "className": "ModelAdapter",
          "computeConfig": "llama-vision-finetuned-deploy",
          "description": "Llama 3.2 Vision-Instruct model adapter for fine-tuning and inference",
          "integrations": [
            "dl-huggingface-api-key"
          ],
          "initInputs": [
            {
              "type": "Model",
              "name": "model_entity"
            }
          ],
          "functions": [
            {
              "name": "predict_items",
              "input": [
                {
                  "type": "Item[]",
                  "name": "items",
                  "description": "List of items to run inference on"
                }
              ],
              "output": [
                {
                  "type": "Item[]",
                  "name": "items",
                  "description": "The same input items for prediction."
                },
                {
                  "type": "Annotation[]",
                  "name": "annotations",
                  "description": "The predicted annotations."
                }
              ],
              "displayName": "Predict Items",
              "displayIcon": "",
              "description": "Run inference on a list of items with the vision model"
            },
            {
              "name": "predict_dataset",
              "input": [
                {
                  "type": "Dataset",
                  "name": "dataset",
                  "description": "Dataset to run inference on"
                },
                {
                  "type": "Json",
                  "name": "filters",
                  "description": "Dataloop Filter DQL"
                }
              ],
              "output": [],
              "displayName": "Predict Dataset",
              "displayIcon": "",
              "description": "Run inference on an entire dataset with filters"
            },
            {
              "name": "train_model",
              "computeConfig": "llama-vision-finetuned-train",
              "input": [
                {
                  "type": "Model",
                  "name": "model",
                  "description": "Dataloop Model Entity"
                }
              ],
              "output": [
                {
                  "type": "Model",
                  "name": "model",
                  "description": "Dataloop Model Entity"
                }
              ],
              "displayName": "Fine-tuned Llama Vision Model",
              "displayIcon": "",
              "description": "Fine-tune the Llama vision-language model on a custom dataset using QLoRA."
            }
          ]
        }
      ],
      "models": [
        {
          "name": "meta-llama/Llama-3.2-11B-Vision-Instruct",
          "moduleName": "llama-vision-finetuned-module",
          "scope": "project",
          "status": "pre-trained",
          "outputType": "text",
          "configuration": {
            "system_prompt": "You are a helpful visual assistant. Describe what you see in the image accurately and answer questions related to the visual content. If you don't know or can't see clearly, just say so.",
            "model_name": "meta-llama/Llama-3.2-11B-Vision-Instruct",
            "r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.05,
            "task_type": "CAUSAL_LM",
            "target_modules": [],
            "num_train_epochs": 10,
            "per_device_train_batch_size": 1,
            "gradient_accumulation_steps": 16,
            "optim": "paged_adamw_32bit",
            "save_steps": 10,
            "logging_steps": 10,
            "learning_rate": 2e-4,
            "warmup_ratio": 0.03,
            "lr_scheduler_type": "constant",
            "bf16": true,
            "group_by_length": true,
            "save_total_limit": 3,
            "max_grad_norm": 0.3,
            "remove_unused_columns": false,
            "gradient_checkpointing": true,
            "use_reentrant": false,
            "report_to": ["tensorboard"],
            "logging_first_step": true,
            "log_level": "info",
            "logging_strategy": "steps",
            "save_every_n_epochs": 2,
            "max_new_tokens": 512,
            "temperature": 0.7,
            "do_sample": true,
            "top_p": 0.95,
            "repetition_penalty": 1.1,
            "add_metadata": false
          },
          "description": "Llama-3.2-11B-Vision-Instruct from Hugging Face, ready for fine-tuning with QLoRA. Process both images and text for multimodal understanding."
        }
      ]
    }
  } 