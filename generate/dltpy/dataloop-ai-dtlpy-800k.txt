Directory structure:
└── dataloop-ai-dtlpy/
    ├── README.md
    ├── Dockerfile
    ├── Dockerfile.docs
    ├── LICENSE
    ├── MANIFEST.in
    ├── bitbucket-pipelines.yml
    ├── build-docker.sh
    ├── files_to_copy.txt
    ├── release_notes.txt
    ├── requirements-full.txt
    ├── requirements.txt
    ├── setup.py
    ├── .bumpversion.cfg
    ├── .pylintrc
    ├── .readthedocs.yml
    ├── docs/
    │   ├── 404.rst
    │   ├── Makefile
    │   ├── cli.rst
    │   ├── conf.py
    │   ├── entities.rst
    │   ├── index.rst
    │   ├── make.bat
    │   ├── repositories.rst
    │   ├── requirements.txt
    │   ├── utilities.rst
    │   └── _static/
    │       └── custom.css
    ├── dtlpy/
    │   ├── __init__.py
    │   ├── __version__.py
    │   ├── exceptions.py
    │   ├── new_instance.py
    │   ├── assets/
    │   │   ├── __init__.py
    │   │   ├── main.py
    │   │   ├── main_partial.py
    │   │   ├── mock.json
    │   │   ├── model_adapter.py
    │   │   ├── package.json
    │   │   ├── package_catalog.json
    │   │   ├── package_gitignore
    │   │   ├── voc_annotation_template.xml
    │   │   ├── code_server/
    │   │   │   ├── config.yaml
    │   │   │   ├── installation.sh
    │   │   │   ├── launch.json
    │   │   │   └── settings.json
    │   │   └── service_runners/
    │   │       ├── __init__.py
    │   │       ├── converter.py
    │   │       ├── multi_method.py
    │   │       ├── multi_method_annotation.py
    │   │       ├── multi_method_dataset.py
    │   │       ├── multi_method_item.py
    │   │       ├── multi_method_json.py
    │   │       ├── single_method.py
    │   │       ├── single_method_annotation.py
    │   │       ├── single_method_dataset.py
    │   │       ├── single_method_item.py
    │   │       ├── single_method_json.py
    │   │       └── single_method_multi_input.py
    │   ├── caches/
    │   │   ├── __init__.py
    │   │   ├── base_cache.py
    │   │   ├── cache.py
    │   │   ├── dl_cache.py
    │   │   ├── filesystem_cache.py
    │   │   └── redis_cache.py
    │   ├── dlp/
    │   │   ├── __init__.py
    │   │   ├── cli_utilities.py
    │   │   ├── command_executor.py
    │   │   ├── dlp
    │   │   ├── dlp.bat
    │   │   ├── dlp.py
    │   │   └── parser.py
    │   ├── entities/
    │   │   ├── __init__.py
    │   │   ├── analytic.py
    │   │   ├── annotation.py
    │   │   ├── annotation_collection.py
    │   │   ├── app.py
    │   │   ├── app_module.py
    │   │   ├── artifact.py
    │   │   ├── assignment.py
    │   │   ├── base_entity.py
    │   │   ├── bot.py
    │   │   ├── codebase.py
    │   │   ├── collection.py
    │   │   ├── command.py
    │   │   ├── compute.py
    │   │   ├── dataset.py
    │   │   ├── directory_tree.py
    │   │   ├── dpk.py
    │   │   ├── driver.py
    │   │   ├── execution.py
    │   │   ├── feature.py
    │   │   ├── feature_set.py
    │   │   ├── filters.py
    │   │   ├── gis_item.py
    │   │   ├── integration.py
    │   │   ├── item.py
    │   │   ├── label.py
    │   │   ├── links.py
    │   │   ├── message.py
    │   │   ├── model.py
    │   │   ├── node.py
    │   │   ├── ontology.py
    │   │   ├── organization.py
    │   │   ├── package.py
    │   │   ├── package_defaults.py
    │   │   ├── package_function.py
    │   │   ├── package_module.py
    │   │   ├── package_slot.py
    │   │   ├── paged_entities.py
    │   │   ├── pipeline.py
    │   │   ├── pipeline_execution.py
    │   │   ├── project.py
    │   │   ├── prompt_item.py
    │   │   ├── recipe.py
    │   │   ├── reflect_dict.py
    │   │   ├── resource_execution.py
    │   │   ├── service.py
    │   │   ├── setting.py
    │   │   ├── task.py
    │   │   ├── time_series.py
    │   │   ├── trigger.py
    │   │   ├── user.py
    │   │   ├── webhook.py
    │   │   └── annotation_definitions/
    │   │       ├── __init__.py
    │   │       ├── base_annotation_definition.py
    │   │       ├── box.py
    │   │       ├── classification.py
    │   │       ├── comparison.py
    │   │       ├── cube.py
    │   │       ├── cube_3d.py
    │   │       ├── description.py
    │   │       ├── ellipse.py
    │   │       ├── free_text.py
    │   │       ├── gis.py
    │   │       ├── note.py
    │   │       ├── point.py
    │   │       ├── polygon.py
    │   │       ├── polyline.py
    │   │       ├── pose.py
    │   │       ├── ref_image.py
    │   │       ├── segmentation.py
    │   │       ├── subtitle.py
    │   │       ├── text.py
    │   │       └── undefined_annotation.py
    │   ├── examples/
    │   │   ├── __init__.py
    │   │   ├── add_labels.py
    │   │   ├── add_metadata_to_item.py
    │   │   ├── annotate_items_using_model.py
    │   │   ├── annotate_video_using_model_and_tracker.py
    │   │   ├── annotations_convert_to_voc.py
    │   │   ├── annotations_convert_to_yolo.py
    │   │   ├── convert_annotation_types.py
    │   │   ├── converter.py
    │   │   ├── copy_annotations.py
    │   │   ├── copy_folder.py
    │   │   ├── create_annotations.py
    │   │   ├── create_video_annotations.py
    │   │   ├── delete_annotations.py
    │   │   ├── filters.py
    │   │   ├── move_item.py
    │   │   ├── play_video_annotation.py
    │   │   ├── show_item_and_mask.py
    │   │   ├── triggers.py
    │   │   ├── upload_batch_of_items.py
    │   │   ├── upload_items_and_custom_format_annotations.py
    │   │   ├── upload_items_with_modalities.py
    │   │   ├── upload_segmentation_annotations_from_mask_image.py
    │   │   └── upload_yolo_format_annotations.py
    │   ├── miscellaneous/
    │   │   ├── __init__.py
    │   │   ├── dict_differ.py
    │   │   ├── git_utils.py
    │   │   ├── json_utils.py
    │   │   ├── list_print.py
    │   │   └── zipping.py
    │   ├── ml/
    │   │   ├── __init__.py
    │   │   ├── base_feature_extractor_adapter.py
    │   │   ├── base_model_adapter.py
    │   │   ├── metrics.py
    │   │   ├── predictions_utils.py
    │   │   ├── summary_writer.py
    │   │   └── train_utils.py
    │   ├── repositories/
    │   │   ├── __init__.py
    │   │   ├── analytics.py
    │   │   ├── annotations.py
    │   │   ├── apps.py
    │   │   ├── artifacts.py
    │   │   ├── assignments.py
    │   │   ├── bots.py
    │   │   ├── codebases.py
    │   │   ├── collections.py
    │   │   ├── commands.py
    │   │   ├── compositions.py
    │   │   ├── computes.py
    │   │   ├── datasets.py
    │   │   ├── downloader.py
    │   │   ├── dpks.py
    │   │   ├── drivers.py
    │   │   ├── executions.py
    │   │   ├── feature_sets.py
    │   │   ├── features.py
    │   │   ├── integrations.py
    │   │   ├── items.py
    │   │   ├── messages.py
    │   │   ├── models.py
    │   │   ├── nodes.py
    │   │   ├── ontologies.py
    │   │   ├── organizations.py
    │   │   ├── packages.py
    │   │   ├── pipeline_executions.py
    │   │   ├── pipelines.py
    │   │   ├── projects.py
    │   │   ├── recipes.py
    │   │   ├── resource_executions.py
    │   │   ├── schema.py
    │   │   ├── services.py
    │   │   ├── settings.py
    │   │   ├── tasks.py
    │   │   ├── times_series.py
    │   │   ├── triggers.py
    │   │   ├── upload_element.py
    │   │   ├── uploader.py
    │   │   └── webhooks.py
    │   ├── services/
    │   │   ├── __init__.py
    │   │   ├── aihttp_retry.py
    │   │   ├── api_client.py
    │   │   ├── api_reference.py
    │   │   ├── async_utils.py
    │   │   ├── calls_counter.py
    │   │   ├── check_sdk.py
    │   │   ├── cookie.py
    │   │   ├── create_logger.py
    │   │   ├── events.py
    │   │   ├── logins.py
    │   │   ├── reporter.py
    │   │   └── service_defaults.py
    │   └── utilities/
    │       ├── __init__.py
    │       ├── base_package_runner.py
    │       ├── converter.py
    │       ├── annotations/
    │       │   ├── __init__.py
    │       │   └── annotation_converters.py
    │       ├── dataset_generators/
    │       │   ├── __init__.py
    │       │   ├── dataset_generator.py
    │       │   ├── dataset_generator_tensorflow.py
    │       │   └── dataset_generator_torch.py
    │       ├── local_development/
    │       │   ├── __init__.py
    │       │   └── local_session.py
    │       ├── reports/
    │       │   ├── __init__.py
    │       │   ├── figures.py
    │       │   └── report.py
    │       └── videos/
    │           ├── __init__.py
    │           ├── video_player.py
    │           └── videos.py
    ├── tests/
    │   ├── __init__.py
    │   ├── debug.py
    │   ├── debug_config
    │   ├── env_from_git_branch.py
    │   ├── requirements.txt
    │   ├── test_examples.py
    │   ├── test_login.py
    │   ├── test_runner.py
    │   └── features/
    │       ├── __init__.py
    │       ├── environment.py
    │       ├── ann_text_object/
    │       │   └── test_annotation_text.feature
    │       ├── annotation_collection/
    │       │   └── test_annotation_collection.feature
    │       ├── annotation_entity/
    │       │   ├── test_annotation_add.feature
    │       │   ├── test_annotation_attributes.feature
    │       │   ├── test_annotation_description.feature
    │       │   ├── test_annotation_description_audio_item.feature
    │       │   ├── test_annotation_description_image_item.feature
    │       │   ├── test_annotation_description_text_item.feature
    │       │   ├── test_annotation_description_video_item.feature
    │       │   ├── test_annotation_draw.feature
    │       │   ├── test_annotation_json_to_object.feature
    │       │   ├── test_annotation_repo_methods.feature
    │       │   ├── test_create_video_annotation_from_blank.feature
    │       │   ├── test_segmentation_to_box.feature
    │       │   ├── test_segmentation_to_polygon.feature
    │       │   ├── test_upload_annotations.feature
    │       │   └── test_video_annotation_updated.feature
    │       ├── annotations_repo/
    │       │   ├── test_annotations_adding_multiple_frames.feature
    │       │   ├── test_annotations_compare_snapshots_with_platform.feature
    │       │   ├── test_annotations_context.feature
    │       │   ├── test_annotations_delete.feature
    │       │   ├── test_annotations_download.feature
    │       │   ├── test_annotations_download_video.feature
    │       │   ├── test_annotations_draw.feature
    │       │   ├── test_annotations_edit.feature
    │       │   ├── test_annotations_format_json.feature
    │       │   ├── test_annotations_format_mask.feature
    │       │   ├── test_annotations_format_vtt.feature
    │       │   ├── test_annotations_get.feature
    │       │   ├── test_annotations_gis_upload.feature
    │       │   ├── test_annotations_list.feature
    │       │   ├── test_annotations_merge.feature
    │       │   ├── test_annotations_show.feature
    │       │   ├── test_annotations_thumbnail.feature
    │       │   ├── test_annotations_upload.feature
    │       │   ├── test_note_annotation_with_messages.feature
    │       │   └── test_rotated_box_points.feature
    │       ├── app_entity/
    │       │   ├── app_update_external_entities.feature
    │       │   ├── app_with_fs_panel.feature
    │       │   ├── test_app_auto_update.feature
    │       │   ├── test_app_auto_update2.feature
    │       │   ├── test_app_bot_creation.feature
    │       │   ├── test_app_dpk_config.feature
    │       │   ├── test_app_get.feature
    │       │   ├── test_app_install.feature
    │       │   ├── test_app_integrations.feature
    │       │   ├── test_app_model_integrations.feature
    │       │   ├── test_app_status.feature
    │       │   ├── test_app_uninstall.feature
    │       │   ├── test_app_update.feature
    │       │   └── test_app_with_map_var.feature
    │       ├── app_integrations/
    │       │   ├── test_integrations_in_module.feature
    │       │   ├── test_integrations_in_module_app.feature
    │       │   ├── test_integrations_in_service.feature
    │       │   ├── test_integrations_integrations_category.feature
    │       │   ├── test_integrations_override.feature
    │       │   ├── test_model_integrations.feature
    │       │   └── test_sheare_integrations_in_module.feature
    │       ├── artifacts_repo/
    │       │   ├── test_artifacts_delete.feature
    │       │   ├── test_artifacts_download.feature
    │       │   ├── test_artifacts_get.feature
    │       │   ├── test_artifacts_list.feature
    │       │   └── test_artifacts_upload.feature
    │       ├── assignments_repo/
    │       │   ├── test_assignments_context.feature
    │       │   ├── test_assignments_create.feature
    │       │   ├── test_assignments_get.feature
    │       │   ├── test_assignments_items_operations.feature
    │       │   ├── test_assignments_list.feature
    │       │   ├── test_assignments_reassign.feature
    │       │   └── test_assignments_redistribute.feature
    │       ├── billing_repo/
    │       │   ├── billing_daily/
    │       │   │   ├── test_org_creation.feature
    │       │   │   ├── test_org_deletion.feature
    │       │   │   └── test_plan_resources.feature
    │       │   └── billing_weekly/
    │       │       └── test_faas_blocking.feature
    │       ├── bot_entity/
    │       │   └── test_bot_entity_methods.feature
    │       ├── bots_repo/
    │       │   ├── test_bots_create.feature
    │       │   ├── test_bots_delete.feature
    │       │   ├── test_bots_get.feature
    │       │   └── test_bots_list.feature
    │       ├── cache/
    │       │   └── test_entites_get.feature
    │       ├── checkout_testing/
    │       │   └── test_checkout.feature
    │       ├── cli_testing/
    │       │   ├── cli_api.feature
    │       │   ├── cli_datasets.feature
    │       │   ├── cli_items.feature
    │       │   ├── cli_others.feature
    │       │   └── cli_projects.feature
    │       ├── code_base_entity/
    │       │   └── test_code_base_repo_methods.feature
    │       ├── code_bases_repo/
    │       │   ├── test_code_bases_get.feature
    │       │   ├── test_code_bases_init.feature
    │       │   ├── test_code_bases_list.feature
    │       │   ├── test_code_bases_list_versions.feature
    │       │   ├── test_code_bases_unpack.feature
    │       │   └── test_code_basess_pack.feature
    │       ├── command_entity/
    │       │   └── test_command.feature
    │       ├── compute/
    │       │   ├── test_compute.feature
    │       │   └── test_compute_archive.feature
    │       ├── converter/
    │       │   ├── test_converter_coco.feature
    │       │   ├── test_converter_dataloop.feature
    │       │   ├── test_converter_voc.feature
    │       │   └── test_converter_yolo.feature
    │       ├── dataset_entity/
    │       │   ├── test_add_labels_methods.feature
    │       │   ├── test_dataset_repo_methods.feature
    │       │   └── test_directory_tree.feature
    │       ├── datasets_repo/
    │       │   ├── test_dataset_clone.feature
    │       │   ├── test_dataset_context.feature
    │       │   ├── test_dataset_download.feature
    │       │   ├── test_dataset_upload_annotations.feature
    │       │   ├── test_dataset_upload_csv.feature
    │       │   ├── test_datasets_create.feature
    │       │   ├── test_datasets_delete.feature
    │       │   ├── test_datasets_download_annotations.feature
    │       │   ├── test_datasets_edit.feature
    │       │   ├── test_datasets_get.feature
    │       │   ├── test_datasets_list.feature
    │       │   ├── test_datasets_lists.feature
    │       │   ├── test_datasets_storage.feature
    │       │   └── tets_dataset_upload_labels.feature
    │       ├── documentation_tests/
    │       │   ├── test_contributor_docs.feature
    │       │   ├── test_dataset_docs.feature
    │       │   ├── test_projects_docs.feature
    │       │   └── test_recipe_docs.feature
    │       ├── dpk_tests/
    │       │   ├── dpk_comuteconfig_levels.feature
    │       │   ├── dpk_flatten.feature
    │       │   ├── dpk_json_to_object.feature
    │       │   ├── dpk_pipeline_custom_node.feature
    │       │   ├── dpk_pipeline_custom_node_long_name.feature
    │       │   ├── dpk_pipeline_custom_node_scope.feature
    │       │   ├── test_dpk_attributes.feature
    │       │   ├── test_dpk_delete.feature
    │       │   ├── test_dpk_get.feature
    │       │   ├── test_dpk_include_model.feature
    │       │   ├── test_dpk_include_multiple_models.feature
    │       │   ├── test_dpk_include_trigger_cron.feature
    │       │   ├── test_dpk_include_trigger_event.feature
    │       │   ├── test_dpk_include_trigger_event_filter.feature
    │       │   ├── test_dpk_list.feature
    │       │   ├── test_dpk_operation.feature
    │       │   ├── test_dpk_pipeline_template.feature
    │       │   ├── test_dpk_pipetemp_name.feature
    │       │   ├── test_dpk_publish.feature
    │       │   ├── test_dpk_pull.feature
    │       │   ├── test_dpk_update.feature
    │       │   └── test_refs_delete.feature
    │       ├── drivers_repo/
    │       │   ├── test_aws_driver.feature
    │       │   ├── test_azure_driver.feature
    │       │   ├── test_drivers_providers.feature
    │       │   └── test_gcs_driver.feature
    │       ├── execution_monitoring/
    │       │   ├── execution_monitoring_terminate.feature
    │       │   └── execution_monitoring_timeout.feature
    │       ├── executions_repo/
    │       │   ├── test_execution_rerun.feature
    │       │   ├── test_executions_context.feature
    │       │   ├── test_executions_create.feature
    │       │   ├── test_executions_get.feature
    │       │   ├── test_executions_list.feature
    │       │   └── text_execution_creator.feature
    │       ├── features_vector_entity/
    │       │   ├── test_features_create.feature
    │       │   ├── test_features_delete.feature
    │       │   └── test_features_get.feature
    │       ├── filters_entity/
    │       │   ├── test_filters.feature
    │       │   ├── test_filters_singlestore.feature
    │       │   └── test_filters_task_browser.feature
    │       ├── integrations_repo/
    │       │   ├── test_ecr_intergration.feature
    │       │   ├── test_gcp_cross_project_intergration.feature
    │       │   └── test_integration_create.feature
    │       ├── item_collections/
    │       │   └── item_collections.feature
    │       ├── item_entity/
    │       │   ├── test_item_creator.feature
    │       │   ├── test_item_description.feature
    │       │   ├── test_item_move.feature
    │       │   ├── test_item_repo_methods.feature
    │       │   ├── test_item_src.feature
    │       │   ├── test_item_thumbnail.feature
    │       │   └── test_item_update_status.feature
    │       ├── items_repo/
    │       │   ├── test_download_and_upload_ndarray_item.feature
    │       │   ├── test_items_clone.feature
    │       │   ├── test_items_context.feature
    │       │   ├── test_items_delete.feature
    │       │   ├── test_items_download.feature
    │       │   ├── test_items_download_anotations.feature
    │       │   ├── test_items_download_batch.feature
    │       │   ├── test_items_edit.feature
    │       │   ├── test_items_get.feature
    │       │   ├── test_items_list.feature
    │       │   ├── test_items_set_items_entity.feature
    │       │   ├── test_items_upload.feature
    │       │   ├── test_items_upload_batch.feature
    │       │   ├── test_items_upload_dataframe.feature
    │       │   └── test_upload_and_download_images.feature
    │       ├── ml_subsets/
    │       │   └── test_ml_subsets.feature
    │       ├── models_repo/
    │       │   ├── test_dpk_model_genai.feature
    │       │   ├── test_model_verification.feature
    │       │   ├── test_models_clone_1.feature
    │       │   ├── test_models_clone_2.feature
    │       │   ├── test_models_context.feature
    │       │   ├── test_models_create.feature
    │       │   ├── test_models_delete.feature
    │       │   ├── test_models_dpk_updated.feature
    │       │   ├── test_models_embed_datasets.feature
    │       │   ├── test_models_flow.feature
    │       │   ├── test_models_list.feature
    │       │   ├── test_models_service_delete.feature
    │       │   ├── test_models_service_update.feature
    │       │   ├── test_models_service_update_archive.feature
    │       │   ├── test_models_services_updated.feature
    │       │   ├── test_models_train.feature
    │       │   ├── test_models_updated_dpk.feature
    │       │   └── test_models_validation.feature
    │       ├── notifications/
    │       │   ├── notification_code_base.feature
    │       │   ├── notification_docker.feature
    │       │   ├── notification_import_error.feature
    │       │   ├── notification_init_error.feature
    │       │   ├── notification_oom_error.feature
    │       │   └── notification_requirements.feature
    │       ├── ontologies_repo/
    │       │   ├── test_ontologies_create.feature
    │       │   ├── test_ontologies_delete.feature
    │       │   ├── test_ontologies_edit.feature
    │       │   └── test_ontologies_get.feature
    │       ├── ontology_entity/
    │       │   ├── test_ontology_attributes.feature
    │       │   ├── test_ontology_bamba_icon.feature
    │       │   └── test_ontology_repo_methods.feature
    │       ├── packages_entity/
    │       │   ├── packages_get.feature
    │       │   └── test_package_module.feature
    │       ├── packages_flow/
    │       │   └── packages_flow.feature
    │       ├── packages_repo/
    │       │   ├── package_context.feature
    │       │   ├── package_slot.feature
    │       │   ├── packages_create.feature
    │       │   ├── packages_delete.feature
    │       │   ├── packages_generate.feature
    │       │   ├── packages_get.feature
    │       │   ├── packages_list.feature
    │       │   ├── packages_name_validation.feature
    │       │   ├── packages_push.feature
    │       │   ├── test_package_git_ignore.feature
    │       │   ├── test_package_revision.feature
    │       │   └── test_packages_context.feature
    │       ├── pipeline_entity/
    │       │   ├── pipeline_active_learning.feature
    │       │   ├── pipeline_context.feature
    │       │   ├── pipeline_delete.feature
    │       │   ├── pipeline_execute.feature
    │       │   ├── pipeline_flow.feature
    │       │   ├── pipeline_get.feature
    │       │   ├── pipeline_list.feature
    │       │   ├── pipeline_multiple_add_to_task.feature
    │       │   ├── pipeline_node_input_handling.feature
    │       │   ├── pipeline_output_list.feature
    │       │   ├── pipeline_recomplete_status.feature
    │       │   ├── pipeline_refs.feature
    │       │   ├── pipeline_rerun_cycles_1.feature
    │       │   ├── pipeline_rerun_cycles_2.feature
    │       │   ├── pipeline_rerun_cycles_3.feature
    │       │   ├── pipeline_rerun_cycles_4.feature
    │       │   ├── pipeline_sdk.feature
    │       │   ├── pipeline_service_uninstall.feature
    │       │   ├── pipeline_start_node.feature
    │       │   ├── pipeline_tasks_loop.feature
    │       │   ├── pipeline_update.feature
    │       │   ├── pipline_reset.feature
    │       │   ├── test_pipeline_actions.feature
    │       │   ├── test_pipeline_app_refs.feature
    │       │   ├── test_pipeline_code_node.feature
    │       │   ├── test_pipeline_connectors.feature
    │       │   ├── test_pipeline_dataset_node_trigger_to_pipeline.feature
    │       │   ├── test_pipeline_evalaute_node.feature
    │       │   ├── test_pipeline_filters.feature
    │       │   ├── test_pipeline_install.feature
    │       │   ├── test_pipeline_many_to_one.feature
    │       │   ├── test_pipeline_ml_node.feature
    │       │   ├── test_pipeline_model_compute_configs.feature
    │       │   ├── test_pipeline_multiple_inputs.feature
    │       │   ├── test_pipeline_multiple_inputs_list.feature
    │       │   ├── test_pipeline_predict_node.feature
    │       │   ├── test_pipeline_pulling_task.feature
    │       │   ├── test_pipeline_sdk_sanity.feature
    │       │   ├── test_pipeline_secrets.feature
    │       │   ├── test_pipeline_task_node.feature
    │       │   ├── test_pipeline_train_node.feature
    │       │   ├── test_pipeline_update_view_mode.feature
    │       │   └── test_pipeline_validation.feature
    │       ├── pipeline_resume/
    │       │   └── pipeline_resume.feature
    │       ├── platform_urls/
    │       │   └── test_platform_urls.feature
    │       ├── project_entity/
    │       │   └── test_project_repo_methods.feature
    │       ├── projects_repo/
    │       │   ├── test_project_integrations.feature
    │       │   ├── test_projects_create.feature
    │       │   ├── test_projects_delete.feature
    │       │   ├── test_projects_get.feature
    │       │   └── test_projects_list.feature
    │       ├── recipe_entity/
    │       │   └── test_recipe_repo_methods.feature
    │       ├── recipes_repo/
    │       │   ├── test_recipes_clone.feature
    │       │   ├── test_recipes_create.feature
    │       │   ├── test_recipes_delete.feature
    │       │   └── test_recipes_edit.feature
    │       ├── service_driver_repo/
    │       │   └── test_service_driver.feature
    │       ├── services_entity/
    │       │   ├── test_service_debug_mode.feature
    │       │   ├── test_service_debug_runtime.feature
    │       │   ├── test_service_items_input.feature
    │       │   ├── test_service_machine_types_1.feature
    │       │   ├── test_service_machine_types_2.feature
    │       │   ├── test_service_machine_types_3.feature
    │       │   ├── test_service_machine_types_4.feature
    │       │   ├── test_service_preemptible.feature
    │       │   ├── test_service_status.feature
    │       │   ├── test_services_flow.feature
    │       │   └── test_services_get.feature
    │       ├── services_repo/
    │       │   ├── test_service_output_limit.feature
    │       │   ├── test_services_app_refs.feature
    │       │   ├── test_services_context.feature
    │       │   ├── test_services_crashloop.feature
    │       │   ├── test_services_create.feature
    │       │   ├── test_services_delete.feature
    │       │   ├── test_services_deploy.feature
    │       │   ├── test_services_get.feature
    │       │   ├── test_services_list.feature
    │       │   ├── test_services_logs.feature
    │       │   ├── test_services_long_short_term_1.feature
    │       │   ├── test_services_long_short_term_2.feature
    │       │   ├── test_services_slot.feature
    │       │   ├── test_services_update.feature
    │       │   └── test_services_update_force.feature
    │       ├── settings/
    │       │   └── test_settings.feature
    │       ├── solution/
    │       │   ├── test_dependency_active_learning.feature
    │       │   ├── test_dependency_refs.feature
    │       │   └── test_dependency_sanity.feature
    │       ├── steps/
    │       │   ├── bring_steps.py
    │       │   ├── fixtures.py
    │       │   ├── annotation_collection/
    │       │   │   └── test_annotation_collection.py
    │       │   ├── annotation_entity/
    │       │   │   ├── annotation_entity_interface.py
    │       │   │   ├── test_annotation_add.py
    │       │   │   ├── test_annotation_description.py
    │       │   │   ├── test_annotation_json_to_object.py
    │       │   │   ├── test_annotation_repo_methods.py
    │       │   │   ├── test_segmentation_to_box.py
    │       │   │   ├── test_segmentation_to_polygon.py
    │       │   │   └── test_upload_annotations.py
    │       │   ├── annotations_repo/
    │       │   │   ├── annotations_interface.py
    │       │   │   ├── audio_annotations_interface.py
    │       │   │   ├── image_annotations_interface.py
    │       │   │   ├── show.npy
    │       │   │   ├── test_annotations_adding_multiple_frames.py
    │       │   │   ├── test_annotations_context.py
    │       │   │   ├── test_annotations_delete.py
    │       │   │   ├── test_annotations_download.py
    │       │   │   ├── test_annotations_draw.py
    │       │   │   ├── test_annotations_format_json.py
    │       │   │   ├── test_annotations_get.py
    │       │   │   ├── test_annotations_list.py
    │       │   │   ├── test_annotations_show.py
    │       │   │   ├── test_annotations_update.py
    │       │   │   ├── test_annotations_upload.py
    │       │   │   ├── test_note_annotation_with_messages.py
    │       │   │   ├── test_rotated_box_points.py
    │       │   │   ├── text_annotations_interface.py
    │       │   │   └── video_annotations_interface.py
    │       │   ├── app_entity/
    │       │   │   ├── app_with_fs_panel.py
    │       │   │   ├── test_app_get.py
    │       │   │   ├── test_app_install.py
    │       │   │   ├── test_app_interface.py
    │       │   │   ├── test_app_status.py
    │       │   │   ├── test_app_uninstall.py
    │       │   │   ├── test_app_update.py
    │       │   │   └── test_app_validate.py
    │       │   ├── artifacts_repo/
    │       │   │   ├── test_artifacts_delete.py
    │       │   │   ├── test_artifacts_download.py
    │       │   │   ├── test_artifacts_get.py
    │       │   │   ├── test_artifacts_list.py
    │       │   │   └── test_artifacts_upload.py
    │       │   ├── assignment_entity/
    │       │   │   └── test_assignment_update.py
    │       │   ├── assignments_repo/
    │       │   │   ├── test_assignments_context.py
    │       │   │   ├── test_assignments_create.py
    │       │   │   ├── test_assignments_get.py
    │       │   │   ├── test_assignments_items_operations.py
    │       │   │   ├── test_assignments_list.py
    │       │   │   ├── test_assignments_reassign.py
    │       │   │   └── test_assignments_redistribute.py
    │       │   ├── billing_steps/
    │       │   │   ├── object_creation_steps.py
    │       │   │   ├── test_delete_org.py
    │       │   │   ├── test_get_plan_steps.py
    │       │   │   └── test_plan_resources.py
    │       │   ├── bots_repo/
    │       │   │   ├── test_bots_create.py
    │       │   │   ├── test_bots_delete.py
    │       │   │   ├── test_bots_get.py
    │       │   │   └── test_bots_list.py
    │       │   ├── checkout_testing/
    │       │   │   └── test_checkout.py
    │       │   ├── cli_testing/
    │       │   │   ├── cli_datasets.py
    │       │   │   ├── cli_others.py
    │       │   │   └── cli_projects.py
    │       │   ├── codebase_entity/
    │       │   │   └── test_codebase_repo_methods.py
    │       │   ├── codebases_repo/
    │       │   │   ├── test_codebases_get.py
    │       │   │   ├── test_codebases_init.py
    │       │   │   ├── test_codebases_list.py
    │       │   │   ├── test_codebases_list_versions.py
    │       │   │   ├── test_codebases_pack.py
    │       │   │   └── test_codebases_unpack.py
    │       │   ├── command_entity/
    │       │   │   └── test_command.py
    │       │   ├── compute/
    │       │   │   ├── compute.py
    │       │   │   ├── ecr_integrations.py
    │       │   │   ├── test_compute_create.py
    │       │   │   ├── test_compute_delete.py
    │       │   │   ├── test_compute_get.py
    │       │   │   └── test_compute_inteface.py
    │       │   ├── converter/
    │       │   │   ├── converter.py
    │       │   │   └── conveters_interface.py
    │       │   ├── dataset_entity/
    │       │   │   ├── dataset_entity_interface.py
    │       │   │   ├── test_add_labels_methods.py
    │       │   │   ├── test_dataset_repo_methods.py
    │       │   │   └── test_directory_tree.py
    │       │   ├── datasets_repo/
    │       │   │   ├── datasets_interface.py
    │       │   │   ├── test_dataset_context.py
    │       │   │   ├── test_dataset_upload_annotations.py
    │       │   │   ├── test_dataset_upload_csv.py
    │       │   │   ├── test_datasets_create.py
    │       │   │   ├── test_datasets_delete.py
    │       │   │   ├── test_datasets_download.py
    │       │   │   ├── test_datasets_download_annotations.py
    │       │   │   ├── test_datasets_get.py
    │       │   │   ├── test_datasets_list.py
    │       │   │   ├── test_datasets_readonly.py
    │       │   │   ├── test_datasets_update.py
    │       │   │   └── tets_dataset_upload_labels.py
    │       │   ├── documentation_tests/
    │       │   │   ├── test_contributor_docs.py
    │       │   │   ├── test_dataset_docs.py
    │       │   │   ├── test_projects_docs.py
    │       │   │   └── test_recipe_docs.py
    │       │   ├── dpk_tests/
    │       │   │   ├── dpk_json_to_object.py
    │       │   │   ├── test_dpk_delete.py
    │       │   │   ├── test_dpk_get.py
    │       │   │   ├── test_dpk_interface.py
    │       │   │   ├── test_dpk_list.py
    │       │   │   ├── test_dpk_publish.py
    │       │   │   ├── test_dpk_pull.py
    │       │   │   ├── test_dpk_update.py
    │       │   │   └── test_dpk_validation.py
    │       │   ├── drivers_repo/
    │       │   │   ├── test_drivers_create.py
    │       │   │   └── test_drivers_delete.py
    │       │   ├── execution_monitoring/
    │       │   │   └── test_execution_monitoring.py
    │       │   ├── executions_repo/
    │       │   │   ├── test_execution_rerun.py
    │       │   │   ├── test_execution_validation.py
    │       │   │   ├── test_executions_context.py
    │       │   │   ├── test_executions_create.py
    │       │   │   ├── test_executions_get.py
    │       │   │   ├── test_executions_list.py
    │       │   │   └── test_executions_multiple.py
    │       │   ├── features_vectors/
    │       │   │   ├── test_features_create.py
    │       │   │   └── test_features_delete.py
    │       │   ├── filters_entity/
    │       │   │   ├── filters_interface.py
    │       │   │   └── test_filters.py
    │       │   ├── item_collections/
    │       │   │   └── test_item_collections.py
    │       │   ├── item_entity/
    │       │   │   ├── item_entity_interface.py
    │       │   │   ├── test_item_attributes.py
    │       │   │   ├── test_item_description.py
    │       │   │   ├── test_item_move.py
    │       │   │   ├── test_item_repo_methods.py
    │       │   │   └── test_item_update_status.py
    │       │   ├── items_repo/
    │       │   │   ├── items_interface.py
    │       │   │   ├── test_download_and_upload_ndarray_item.py
    │       │   │   ├── test_items_clone.py
    │       │   │   ├── test_items_context.py
    │       │   │   ├── test_items_delete.py
    │       │   │   ├── test_items_download.py
    │       │   │   ├── test_items_download_batch.py
    │       │   │   ├── test_items_get.py
    │       │   │   ├── test_items_get_all_items.py
    │       │   │   ├── test_items_list.py
    │       │   │   ├── test_items_set_items_entity.py
    │       │   │   ├── test_items_update.py
    │       │   │   ├── test_items_upload.py
    │       │   │   ├── test_items_upload_batch.py
    │       │   │   ├── test_items_upload_dataframe.py
    │       │   │   └── test_upload_and_download_images.py
    │       │   ├── ml_subsets/
    │       │   │   └── test_ml_subsets.py
    │       │   ├── model_entity/
    │       │   │   └── test_model_name.py
    │       │   ├── models_repo/
    │       │   │   ├── test_model_flow.py
    │       │   │   ├── test_models_create.py
    │       │   │   ├── test_models_delete.py
    │       │   │   ├── test_models_get.py
    │       │   │   └── test_models_list.py
    │       │   ├── notifications/
    │       │   │   └── notifications.py
    │       │   ├── ontologies_repo/
    │       │   │   ├── test_ontologies_create.py
    │       │   │   ├── test_ontologies_delete.py
    │       │   │   ├── test_ontologies_get.py
    │       │   │   └── test_ontologies_update.py
    │       │   ├── ontology_entity/
    │       │   │   ├── test_ontology_attributes.py
    │       │   │   ├── test_ontology_bamba_icon.py
    │       │   │   └── test_ontology_repo_methods.py
    │       │   ├── packages_flow/
    │       │   │   └── packages_flow.py
    │       │   ├── packages_repo/
    │       │   │   ├── package_slot.py
    │       │   │   ├── packages_delete.py
    │       │   │   ├── packages_generate.py
    │       │   │   ├── packages_get.py
    │       │   │   ├── packages_list.py
    │       │   │   ├── packages_name_validation.py
    │       │   │   ├── packages_push.py
    │       │   │   ├── test_package_module.py
    │       │   │   └── test_packages_context.py
    │       │   ├── pipeline_entity/
    │       │   │   ├── pipeline_delete.py
    │       │   │   ├── pipeline_execute.py
    │       │   │   ├── pipeline_flow.py
    │       │   │   ├── pipeline_get.py
    │       │   │   ├── pipeline_ml.py
    │       │   │   ├── pipeline_node_input_handling.py
    │       │   │   ├── pipeline_output_list.py
    │       │   │   ├── pipeline_rerun_cycles.py
    │       │   │   ├── pipeline_reset.py
    │       │   │   ├── pipeline_update.py
    │       │   │   ├── pipeline_validation.py
    │       │   │   ├── test_pipeline_actions.py
    │       │   │   ├── test_pipeline_connectors.py
    │       │   │   ├── test_pipeline_dataset_node_trigger_to_pipeline.py
    │       │   │   ├── test_pipeline_execution.py
    │       │   │   ├── test_pipeline_install.py
    │       │   │   ├── test_pipeline_interface.py
    │       │   │   ├── test_pipeline_model_compute_configs.py
    │       │   │   ├── test_pipeline_pulling_task.py
    │       │   │   ├── test_pipeline_refs.py
    │       │   │   └── test_pipeline_task_node.py
    │       │   ├── pipeline_resume/
    │       │   │   └── pipeline_resume.py
    │       │   ├── platform_urls/
    │       │   │   └── test_platform_urls.py
    │       │   ├── project_entity/
    │       │   │   └── test_project_repo_methods.py
    │       │   ├── projects_repo/
    │       │   │   ├── projects_interface.py
    │       │   │   ├── test_project_integrations.py
    │       │   │   ├── test_projects_create.py
    │       │   │   ├── test_projects_delete.py
    │       │   │   ├── test_projects_get.py
    │       │   │   └── test_projects_list.py
    │       │   ├── recipe_entity/
    │       │   │   └── test_recipe_repo_methods.py
    │       │   ├── recipes_repo/
    │       │   │   ├── test_recipes_clone.py
    │       │   │   ├── test_recipes_create.py
    │       │   │   ├── test_recipes_delete.py
    │       │   │   └── test_recipes_update.py
    │       │   ├── service_driver_repo/
    │       │   │   └── test_service_driver_get.py
    │       │   ├── service_entity/
    │       │   │   └── test_service_debug_mode.py
    │       │   ├── services_repo/
    │       │   │   ├── test_service_validation.py
    │       │   │   ├── test_services_context.py
    │       │   │   ├── test_services_crashloop.py
    │       │   │   ├── test_services_create.py
    │       │   │   ├── test_services_delete.py
    │       │   │   ├── test_services_deploy.py
    │       │   │   ├── test_services_get.py
    │       │   │   ├── test_services_list.py
    │       │   │   ├── test_services_slot.py
    │       │   │   └── test_services_update.py
    │       │   ├── settings_context/
    │       │   │   └── test_settings_context.py
    │       │   ├── tasks_repo/
    │       │   │   ├── test_task_context.py
    │       │   │   ├── test_task_priority.py
    │       │   │   ├── test_task_update.py
    │       │   │   ├── test_tasks_add_and_get_items.py
    │       │   │   ├── test_tasks_create.py
    │       │   │   ├── test_tasks_delete.py
    │       │   │   ├── test_tasks_get.py
    │       │   │   ├── test_tasks_list.py
    │       │   │   └── test_tasks_qa_task.py
    │       │   ├── test_cache/
    │       │   │   └── test_cache.py
    │       │   ├── triggers_repo/
    │       │   │   ├── test_pipeline_triggers_create.py
    │       │   │   ├── test_triggers_annotation_update.py
    │       │   │   ├── test_triggers_context.py
    │       │   │   ├── test_triggers_create.py
    │       │   │   ├── test_triggers_delete.py
    │       │   │   ├── test_triggers_get.py
    │       │   │   ├── test_triggers_item_update.py
    │       │   │   ├── test_triggers_list.py
    │       │   │   └── test_triggers_update.py
    │       │   ├── utilities/
    │       │   │   └── platform_interface_steps.py
    │       │   ├── webm_converter/
    │       │   │   └── test_failed_video_message.py
    │       │   └── xray/
    │       │       └── test_xray.py
    │       ├── tasks_repo/
    │       │   ├── task_consensus_create.feature
    │       │   ├── task_qualification_create.feature
    │       │   ├── test_task_context.feature
    │       │   ├── test_task_priority.feature
    │       │   ├── test_task_pulling.feature
    │       │   ├── test_tasks_add_and_get_items.feature
    │       │   ├── test_tasks_assignments.feature
    │       │   ├── test_tasks_create.feature
    │       │   ├── test_tasks_delete.feature
    │       │   ├── test_tasks_get.feature
    │       │   ├── test_tasks_list.feature
    │       │   └── test_tasks_qa_task.feature
    │       ├── triggers_repo/
    │       │   ├── test_cron_trigger_create.feature
    │       │   ├── test_cron_triggers_delete.feature
    │       │   ├── test_cron_triggers_get.feature
    │       │   ├── test_cron_triggers_list.feature
    │       │   ├── test_pipeline_trigger_update.feature
    │       │   ├── test_pipeline_triggers_create.feature
    │       │   ├── test_trigger_filters.feature
    │       │   ├── test_triggers_annotation_deleted.feature
    │       │   ├── test_triggers_annotation_update_always.feature
    │       │   ├── test_triggers_annotation_update_once.feature
    │       │   ├── test_triggers_assignment.feature
    │       │   ├── test_triggers_context.feature
    │       │   ├── test_triggers_create_1.feature
    │       │   ├── test_triggers_create_2.feature
    │       │   ├── test_triggers_dataset_create.feature
    │       │   ├── test_triggers_dataset_delete.feature
    │       │   ├── test_triggers_dataset_update.feature
    │       │   ├── test_triggers_delete.feature
    │       │   ├── test_triggers_get.feature
    │       │   ├── test_triggers_item_deleted.feature
    │       │   ├── test_triggers_item_status.feature
    │       │   ├── test_triggers_item_update.feature
    │       │   ├── test_triggers_list.feature
    │       │   ├── test_triggers_task_create.feature
    │       │   └── test_triggers_update.feature
    │       ├── webm_converter/
    │       │   └── test_failed_video_message.feature
    │       └── xray/
    │           └── test_xray.feature
    └── .github/
        └── ISSUE_TEMPLATE/
            └── bug_report.md

================================================
File: README.md
================================================
![logo.svg](docs%2F_static%2Flogo.svg) 
[![Documentation Status](https://readthedocs.org/projects/dtlpy/badge/?version=latest)](https://sdk-docs.dataloop.ai/en/latest/?badge=latest)
[![pypi](https://img.shields.io/pypi/v/dtlpy.svg)](https://pypi.org/project/dtlpy/)
[![versions](https://img.shields.io/pypi/pyversions/dtlpy.svg)](https://github.com/dataloop-ai/dtlpy)
[![license](https://img.shields.io/github/license/dataloop-ai/dtlpy.svg)](https://github.com/dataloop-ai/dtlpy/blob/master/LICENSE)
[![Downloads](https://static.pepy.tech/personalized-badge/dtlpy?period=total&units=international_system&left_color=grey&right_color=green&left_text=Downloads)](https://pepy.tech/project/dtlpy)

This is the SDK and CLI open source repository for [Dataloop.ai](https://dataloop.ai/) platform

For full platform documentation click [here](https://dataloop.ai/docs)

For full SDK documentation click [here](https://console.dataloop.ai/sdk-docs/latest)

### Python Support

#### Dtlpy supports these Python versions.

| Python            | 3.11 | 3.10 | 3.9 | 3.8 | 3.7 | 3.6 | 3.5 |
|-------------------|------|------|-----|-----|-----|-----|-----|
| dtlpy >= 1.99     | Yes  | Yes  | Yes | Yes | Yes |     |     |
| dtlpy 1.76 - 1.98 | Yes  | Yes  | Yes | Yes | Yes | Yes |     |
| dtlpy >= 1.61     |      | Yes  | Yes | Yes | Yes | Yes |     |
| dtlpy 1.60 - 1.50 |      |      | Yes | Yes | Yes | Yes |     |
| dtlpy <= 1.49     |      |      | Yes | Yes | Yes | Yes | Yes |           


================================================
File: Dockerfile
================================================
FROM python:3.10

ARG BITBUCKET_TAG
ADD ./dist/dtlpy-$BITBUCKET_TAG-py3-none-any.whl /dtlpy-$BITBUCKET_TAG-py3-none-any.whl
RUN mkdir -p /src
ENV PYTHONPATH="$PYTHONPATH:/src"
RUN pip install --no-cache-dir --target=/src /dtlpy-$BITBUCKET_TAG-py3-none-any.whl --upgrade


================================================
File: Dockerfile.docs
================================================
FROM python:3.7

RUN apt-get -y update && apt-get -y upgrade

ADD docs /users/dataloop/docs
ADD dtlpy /users/dataloop/dtlpy
WORKDIR /users/dataloop/docs

RUN pip3 install -r requirements.txt
RUN sphinx-apidoc -f -o . ../dtlpy/
RUN sphinx-build -T -d _build/doctrees-readthedocs -D language=en . _build/html

CMD ["python3", "serve.py"]



================================================
File: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

================================================
File: MANIFEST.in
================================================
include README.rst
include docs/*.txt
include dtlpy/assets/*/*
include dtlpy/assets/*
include dtlpy/repositories/plugins_assets/*


================================================
File: bitbucket-pipelines.yml
================================================
options:
  docker: true
pipelines:
  tags:
    "*":
      - step:
          image: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
          name: Build SDK Container
          script:
            # Get an oauth access token using the client credentials, parsing out the token with jq.
            - apt-get update && apt-get install -y curl jq

            # Make changes and commit back.
            - case "$BITBUCKET_TAG" in "") export BITBUCKET_TAG="build-$BITBUCKET_BUILD_NUMBER"; esac
            - apt-get -y update && apt-get install -y python3-pip
            - python3 setup.py bdist_wheel
            - docker login -u _json_key -p "$GCP_KEY" https://gcr.io
            - cp dist/dtlpy-$BITBUCKET_TAG-py3-none-any.whl dist/dtlpy-latest-py3-none-any.whl
            - echo "$GCP_KEY" >> k.json
            - gcloud auth activate-service-account --key-file=k.json
            - gsutil -m cp -R dist/**  gs://dtlpy/dev/
            - export DOCKER_BUILDKIT=0
            - chmod +x ./build-docker.sh
            - echo $BITBUCKET_TAG
            - ./build-docker.sh $BITBUCKET_TAG
            - git checkout -b "$BITBUCKET_TAG"
            - >
              export access_token=$(curl -s -X POST -u "${CLIENT_ID}:${CLIENT_SECRET}" \
                https://bitbucket.org/site/oauth2/access_token \
                -d grant_type=client_credentials -d scopes="repository"| jq --raw-output '.access_token')
            - git clone https://x-token-auth:${access_token}@bitbucket.org/dataloop-ai/piper-agent-runner.git
            - cd piper-agent-runner
            - git config user.email sdk@dataloop.ai
            - git checkout master
            - python3 bumpversion.py --hard $BITBUCKET_TAG.0
            - git push --follow-tags
            - cd ..
      - step:
          name: Update dataloop-infra
          image: node:20.6.0
          script:
            - apt-get update && apt-get install -y curl jq
            - echo $BITBUCKET_TAG
            - export INFRA_BRANCH="rc"
            - echo $INFRA_BRANCH
            - case "$BITBUCKET_TAG" in "") export BITBUCKET_TAG="build-$BITBUCKET_BUILD_NUMBER"; esac
            - git config --global user.email "devops@dataloop.ai"
            - git config --global user.name "sdk-docs-pipeline"
            - git clone --branch $INFRA_BRANCH git@bitbucket.org:dataloop-ai/dataloop-infra.git && sed -i "s/^DTLPY.*$/DTLPY=$BITBUCKET_TAG/" dataloop-infra/versions/.rc  && cat dataloop-infra/versions/.rc
            - cd dataloop-infra && git commit -am "DTLPY=$BITBUCKET_TAG"
            - >
              export access_token=$(curl -s -X POST -u "${AUTH0_CONSUMERS_DTLPY_PIPELINE_CLIENT_ID}:${AUTH0_CONSUMERS_DTLPY_PIPELINE_CLIENT_SECRET}" \
                https://bitbucket.org/site/oauth2/access_token \
                -d grant_type=client_credentials -d scopes="repository"| jq --raw-output '.access_token')

            - git tag -a $BITBUCKET_TAG -m "DTLPY=$BITBUCKET_TAG" && git push https://x-token-auth:${access_token}@bitbucket.org/dataloop-ai/dataloop-infra.git --follow-tags

    "run-test-python-3.6.11":
      - step:
          image: python:3.6.11
          name: Tests-3.6.11
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - pip install .
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "run-test-python-3.7.8":
      - step:
          image: python:3.7.8
          name: Tests3.7.8
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - pip install .
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "run-test-python-3.8.4":
      - step:
          image: python:3.8.4
          name: Tests-3.8.4
          script:
            - python --version
            - PYTHONPATH=$PWD
            - apt update
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - pip install .
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "run-test-python-3.9.0":
      - step:
          image: python:3.9.0
          name: Tests-3.9.0
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "run-test-python-3.10":
      - step:
          image: python:3.10
          name: Tests-3.10
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "run-test-python-3.11":
      - step:
          image: python:3.11
          name: Tests-3.11
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "run-test-python-3.12":
      - step:
          image: python:3.12
          name: Tests-3.12
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
    "examples":
      - step:
          image: python:3.7.8
          name: Examples-3.7.8
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_examples.py
  custom:
    "xray-hook":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
          - name: TAGS
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh $TAGS
          artifacts:
            - behave_test_report.json

    "xray-pipeline":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
          - name: TAGS
            default: select-component
            allowed-values:
              - select-component
              - Ramsay
              - Piper
              - Piper-Pipelines
              - Piper-Faas
              - Piper-Triggers-Executions
              - Hodor
              - Apps
              - Rubiks
              - Rubiks-Dataset-Recipe
              - Rubiks-Items-Annotations
              - SDK
              - Woz
              - Roberto
              - Hedwig
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh $TAGS
          artifacts:
            - behave_test_report.json

    "xray-nightly-parallel-archived":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          caches:
            - pip
          script:
            - python --version
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - export ARTIFACTS_PATH=$(python -c "import dtlpy; print(dtlpy.client_api.cookie_io.COOKIE)")
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
          artifacts:
            - $ARTIFACTS_PATH
      - parallel:
          steps:
            - step:
                name: Hodor
                script:
                  - echo "Accessing cookie from the default step"
                  - ls -l $ARTIFACTS_PATH
                  - AVOID_TESTRAIL=true
                  - PYTHONPATH=$PWD
                  - ./test.sh Hodor
            - step:
                name: Apps
                script:
                  - echo "Accessing cookie from the default step"
                  - ls -l $ARTIFACTS_PATH
                  - AVOID_TESTRAIL=true
                  - PYTHONPATH=$PWD
                  - ./test.sh Apps

    "xray-Hodor":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Hodor
          artifacts:
            - behave_test_report.json

    "xray-Apps":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Apps
          artifacts:
            - behave_test_report.json

    "xray-Ramsay":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Ramsay
          artifacts:
            - behave_test_report.json

    "xray-Rubiks":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Rubiks
          artifacts:
            - behave_test_report.json

    "xray-Rubiks-Dataset-Recipe":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Rubiks-Dataset-Recipe
          artifacts:
            - behave_test_report.json

    "xray-Rubiks-Items-Annotations":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Rubiks-Items-Annotations
          artifacts:
            - behave_test_report.json

    "xray-SDK":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh SDK
          artifacts:
            - behave_test_report.json

    "xray-Woz":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Woz
          artifacts:
            - behave_test_report.json

    "xray-Roberto":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Roberto
          artifacts:
            - behave_test_report.json

    "xray-Hedwig":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Hedwig
          artifacts:
            - behave_test_report.json

    "xray-Piper":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Piper
          artifacts:
            - behave_test_report.json

    "xray-Piper-Pipelines":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Piper-Pipelines
          artifacts:
            - behave_test_report.json

    "xray-Piper-Faas":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Piper-Faas
          artifacts:
            - behave_test_report.json


    "xray-Piper-Triggers-Executions":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Piper-Triggers-Executions
          artifacts:
            - behave_test_report.json


    "xray-Billing_daily":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Billing_daily
          artifacts:
            - behave_test_report.json



    "xray-Billing-weekly":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - step:
          image: python:3.11
          name: Test & Report to XRay
          script:
            - AVOID_TESTRAIL=true
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r requirements.txt
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - chmod +x ./test.sh
            - ./test.sh Billing_weekly
          artifacts:
            - behave_test_report.json



    "xray-nightly-parallel-1":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - parallel:
          - step:
              image: python:3.11
              name: Hodor
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Hodor
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Woz
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Woz
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Ramsay
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Ramsay
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: SDK
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh SDK
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Piper-Faas
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Piper-Faas
              artifacts:
                - behave_test_report.json


    "xray-nightly-parallel-2":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - parallel:
          - step:
              image: python:3.11
              name: Piper-Triggers-Executions
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Piper-Triggers-Executions
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Apps
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Apps
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Rubiks-Items-Annotations
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Rubiks-Items-Annotations
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Roberto
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Roberto
              artifacts:
                - behave_test_report.json


    "xray-nightly-parallel-3":
      - variables:
          - name: DLP_ENV_NAME
            default: rc
      - parallel:
          - step:
              image: python:3.11
              name: Rubiks-Dataset-Recipe
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Rubiks-Dataset-Recipe
              artifacts:
                - behave_test_report.json
          - step:
              image: python:3.11
              name: Piper-Pipelines
              script:
                - AVOID_TESTRAIL=true
                - python --version
                - PYTHONPATH=$PWD
                - pip install --upgrade pip
                - apt-get -y update && apt-get install -y libgl1-mesa-glx
                - pip install -r tests/requirements.txt
                - python setup.py build
                - python setup.py install
                - python tests/test_login.py
                - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
                - chmod +x ./test.sh
                - ./test.sh Piper-Pipelines
              artifacts:
                - behave_test_report.json


    "schedule-tests":
      - step:
          image: python:3.10
          name: Tests-3.10
          script:
            - python --version
            - PYTHONPATH=$PWD
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
          artifacts:
            - tests/reports/**

    custom-variables:
      - variables:
          - name: DLP_ENV_NAME
          - name: AVOID_TESTRAIL
            default: true
          - name: INDEX_DRIVER_VAR
          - name: API_KEY
      - step:
          image: python:3.10
          name: Tests-3.10
          script:
            - python --version
            - PYTHONPATH=$PWD
            - apt update
            - pip install --upgrade pip
            - apt-get -y update && apt-get install -y libgl1-mesa-glx
            - pip install -r tests/requirements.txt
            - python setup.py build
            - python setup.py install
            - python tests/test_login.py
            - git clone git@bitbucket.org:dataloop-ai/dtlpy-assets.git tests/assets
            - python tests/test_runner.py
          artifacts:
            - tests/reports/**



================================================
File: build-docker.sh
================================================
#!/bin/sh

docker build -t gcr.io/viewo-g/sdk:$1 -f ./Dockerfile --build-arg BITBUCKET_TAG=$1 .
docker tag gcr.io/viewo-g/sdk:$1 gcr.io/viewo-g/sdk:latest

docker push gcr.io/viewo-g/sdk:$1
docker push gcr.io/viewo-g/sdk:latest

================================================
File: files_to_copy.txt
================================================
.pylintrc
.bumpversion.cfg
.readthedocs.yml
.gitignore
bitbucket-pipelines.yml
build-docker.sh
Dockerfile
Dockerfile.docs
docs
dtlpy
LICENSE
MANIFEST.in
README.md
release_notes.txt
requirements.txt
requirements-full.txt
setup.py
tests


================================================
File: release_notes.txt
================================================
Release 1.16.10:

    Features:
        - add ItemStatus resource option to trigger
        - add project contributor
        - make sure workload loads sums to 100
        - added service pause and resume
        - added -v and --version to CLI in packages
        - rql querying to FaaS entities
        - avoid_unnecessary_annotation_download argument in items.download
        - upload yolo/coco/voc dataset from local to paltform
        - add the options to use filters in task.get_items
        - added git remote url to codebase
        - reporter generate json file
        - added wait function to wait until execution completed
        - added support for more than one polygon in binary mask
        - remove open-cv requirement
        - single request for annotations upload. len of page entity returns item count
        - add a flag in packages deploy --package-name
        - remove open cv requirement


    Bugs:
        - raise exception when unknown annotation option in downloader
        - Collection classifier: Upload will not support overwrite option
        - remote_name handling to similarity and multiview
        - more informative log when upload and download error
        - package.services methods that requires project did not work
        - SDK-317 delete logs if directory is over 200MB
        - using global event loop for uploading and fixed defaul num workers bug
        - fix async annotations upload
        - fix refresh token when login using m2m
        - Annotation filter will return all annotations when result is false

    Deprecation:
        - task workload cannot be empty
        - create assignment is now using task.add_items()
        - change_status() was changes to update_status()
        - annotation/s set_status was changes to update status
        - removed show_dirs and show_hidden from tests
        - deprecate num_workers from download

Release 1.14.2
    Features:
        - Plug-119/Apply list by functionName to sdk
        - DAT-3024/add new tasks/assignments functions to SDK
        - added logs on general exception in request
        - added trigger resource information endpoint
        - del also in api client
        - added analytics repository
        - SDK-247/use item information from json when loading annotations
        - model entity
        - models repository
        - checkpoint
        - ability to provide custom filter in filters initiation
        - PLUG-211/ package mockify
        - PLUG-207-Create package type catalog
        - added mockifier to package generation
        - execution url for curl request
        - update system metadata with filters
        - set link mimetime
        - Create function from a method

    Bugs:
        - SDK-155/cant get directory by name
        - changed root functions to user api client direcly
        - SDK-162/bugfix-made custom format example work
        - revert token and env in init
        - SDK-196/annotation list takes too long - removed min() method from from_json()
        - removed getattribute from BaseEntity
        - SDK-198/service.triggers.create failed on project id - using project_id
        - bugfix - tasks.delete()
        - bugfix:SDK-236 betted error message on a large codebase zip
        - PLUG-215- token expired message is more informative

    Tests:
        - Artifacts
        - Taks
        - Assignments

Release 1.13.8
    Features:
        - tasks and assignments new endpoints
        - assignments new methods: reassign, redistribute
        - name param in task/assignments is noe task_name/assignment_name
        - Workload entity
        - annotation hash
        - filters.prepare can generate query only by using param query_only=True
        - Dtlpy instance generator

    Bugs:
        - assignments list from task entity wasn't using task id
        - tasks create uses dataset's project_id as default
        - filters update task and assignments ref on items

Release 1.8.0
    Features:
        - similarity
        - links
        - checkout state shows project name and dataset name
        - test local plugin in multithreading
        - test local plugin in multithreading in cli
        - PluginInput class
        - moved plugins assets to assets folder
        - do_reset to base plugin runner
    Bugs:
        - filters or
        - global threadpool
        - withou-binaries autocompletion
    Docs:
        - Triggers
        - Converters
        - Filters

Release 1.7.53
    Features:
        - directory tree entity
        - dataset entity has now deirectory tree attribute
        - cli now suggest remote path
        - cli fix up
    Bugs:
        -

Release 1.7.50
    Features:
        - using global ThreadPool instead of using one in each method. able to control the number
              of workers in the package level
        - added special exception for annotations show errors
        - using verbose in entire package and logging (also progress bars)

Release 1.7.45
    Bugs:
        - fixed bug in cli auto-complete fro dir path and datasets names
        - fixed: empty filters dont add join
        - fixed: not empty filters add blob and type dir
        - fixed: deploy from local dont add filter
        - fixed: deployment.to_json() dont ignore project
        - fixed: refresh token for m2m learning
        - fixed: token expired error ad import
        - try/catch to self.project in deployment
        - packages list uses filters.show_dirs = True
    Features:
        - added option to deploy plugin from a specified deployment.json file
        - triggers list with params: active, resource, page_offset
        - method pop() to filters
        - method pop_join() to filters
    Test:
        - Deployments repo testing
Release 1.7.44
    Bugs:
        - dtlpy converter bug fix
        - test local plugin - can test local plugin from any directory
        - annotations type can be of any kind
        - some bugs in deploments repository
        - fixed plugin example in docs
        - fixed bug of exception raised while typing space in shell after items ls
        - callback_get_step import issue
    Features:
        - annotation object dont have to include item
        - assignments repository
        - annotation tasks repository
        - assignment entity
        - annotation task entity
        - filters ignores 'hidden=True' rather than .dataloop
        - modified plugin generate files
        - deployments logs added
        - plugins push - if param 'name' is given push will ignore name in plugin.json
        - dataset.download_annotations() can now accept filters and annotation options
        - items list accepts item type as well
        - deployments logs to cli
        - removed error of deployment object with no plugin
        - default page size was raised to 1000
    Tests:
        - Plugin flow
        - Triggers repo
Release 1.7.36
    Bugs:
        - CLI logging now working
        - Added Retries on >500 status codes
    Features:
        - Plugins and Deployments
        - Added support for sub labels
        - Converters - working VOC
    Examples:
        - Converted
        - Filtering with metadata

Release 1.7.11
    Bugs:
        - some dlp shell bug fix
    Features:
        - dlp shell except any cmd commands

Release 1.7.9
    Bugs:
        - item.download() - not to relative path
        - filters -  add /* to end of remote_path
        
Release 1.7.8
    Features:
        - ability to upload from url
        - ability to create directory in cli
    Bugs:
        - fix plugins create
        - plugins get() if checked out

Release 1.7.3
    Features:
        - Moved cli download and upload from "datasets" to "items"
        - Upload option with *
        - Removed ProgressBar and using tqdm instead
        - cd autocomplete iun cli
        - utc timestamp in SDK logger
    Bugs:
        - Fixed video player
        - Fixed video annotations with time classification
        - upload note annotation now dont return an error


Release 1.6.4
    Features:
        - checkout for project and dataset in sdk. no need to enter the project or dataset every time (once checked out)
    Bugs:
        - remove yaml from packages (caused installations error)


Release 1.6.2
    Features:
        - started working with platform RQL queries
        - removed api counter from global cookie file
    Bugs:
        - fixed slow upload and download by threading some pre process


Release 1.6.8
    Bugs:
        - download/upload bug fix
        - itmes.print() wont print thumnail in order for line to not be too long
        - download_annotation bug fix
        - removed 'shell' from cli autocomplete

================================================
File: requirements-full.txt
================================================
tqdm>=4.63
certifi>=2020.12.5
webvtt-py==0.4.3
aiohttp>=3.8
requests-toolbelt>=1.0.0
requests>=2.25.0
numpy>=1.16.2
pandas>=0.24.2
tabulate>=0.8.9
Pillow>=7.2, <9.1
PyJWT>=2.4
jinja2>=2.11.3
attrs<=22.2.0
prompt_toolkit>=2.0.9
fuzzyfinder<=2.1.0
dictdiffer>=0.8.1
validators<=0.18.2
pathspec>=0.8.1
filelock>=3.0.12
diskcache>=5.4
redis>=3.5
urllib3>=1.26

# extra requirements
google-cloud-storage
google-cloud-core
protobuf>=3.17.3
opencv-python
torch>=1.8
torchvision>0.9
tensorflow-gpu
imgaug

================================================
File: requirements.txt
================================================
urllib3>=1.26
tqdm>=4.63
certifi>=2020.12.5
webvtt-py==0.4.3
aiohttp>=3.8
requests-toolbelt>=1.0.0
requests>=2.25.0
numpy>=1.16.2
pandas>=0.24.2
tabulate>=0.8.9
Pillow>=7.2
PyJWT>=2.4
jinja2>=2.11.3
attrs<=22.2.0
prompt_toolkit>=2.0.9
fuzzyfinder<=2.1.0
dictdiffer>=0.8.1
validators<=0.18.2
pathspec>=0.8.1
filelock>=3.0.12
diskcache>=5.4
redis>=3.5
inquirer
dtlpymetrics
dataclasses

================================================
File: setup.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.

from setuptools import setup, find_packages

with open('README.md') as f:
    readme = f.read()

with open('LICENSE') as f:
    license_ = f.read()

with open('requirements.txt') as f:
    requirements = f.read()

setup(name='dtlpy',
      classifiers=[
          'Programming Language :: Python',
          'Programming Language :: Python :: 3',
          'Programming Language :: Python :: 3 :: Only',
          'Programming Language :: Python :: 3.7',
          'Programming Language :: Python :: 3.8',
          'Programming Language :: Python :: 3.9',
          'Programming Language :: Python :: 3.10',
          'Programming Language :: Python :: 3.11',
      ],
      version='1.105.6',
      description='SDK and CLI for Dataloop platform',
      author='Dataloop Team',
      author_email='info@dataloop.ai',
      url='https://github.com/dataloop-ai/dtlpy',
      license='Apache License 2.0',
      long_description=readme,
      long_description_content_type='text/markdown',
      packages=find_packages(exclude=('tests', 'docs', 'samples')),
      setup_requires=['wheel'],
      install_requires=requirements,
      test_suite='tests',
      python_requires='>=3.7',
      scripts=['dtlpy/dlp/dlp.py', 'dtlpy/dlp/dlp.bat', 'dtlpy/dlp/dlp'],
      include_package_data=True,
      entry_points={
          'console_scripts': [
              'dlp=dtlpy.dlp.dlp:main',
          ],
      },
      )


================================================
File: .bumpversion.cfg
================================================
[bumpversion]
current_version = 1.105.6
commit = True
tag = True
tag_name = {new_version}

[bumpversion:file:setup.py]

[bumpversion:file:./dtlpy/__version__.py]

[bumpversion:file:./docs/conf.py]



================================================
File: .pylintrc
================================================
[MASTER]

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code.
extension-pkg-whitelist=cv2

# Add files or directories to the blacklist. They should be base names, not
# paths.
ignore=CVS

# Add files or directories matching the regex patterns to the blacklist. The
# regex matches against base names, not paths.
ignore-patterns=

# Python code to execute, usually for sys.path manipulation such as
# pygtk.require().
#init-hook=

# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
# number of processors available to use.
jobs=1

# Control the amount of potential inferred values when inferring a single
# object. This can help the performance when dealing with large functions or
# complex, nested conditions.
limit-inference-results=100

# List of plugins (as comma separated values of python modules names) to load,
# usually to register additional checkers.
load-plugins=

# Pickle collected data for later comparisons.
persistent=yes

# Specify a configuration file.
#rcfile=

# When enabled, pylint would attempt to guess common misconfiguration and emit
# user-friendly hints instead of false-positive error messages.
suggestion-mode=yes

# Allow loading of arbitrary C extensions. Extensions are imported into the
# active Python interpreter and may run arbitrary code.
unsafe-load-any-extension=no


[MESSAGES CONTROL]

# Only show warnings with the listed confidence levels. Leave empty to show
# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED.
confidence=

# Disable the message, report, category or checker with the given id(s). You
# can either give multiple identifiers separated by comma (,) or put this
# option multiple times (only on the command line, not in the configuration
# file where it should appear only once). You can also use "--disable=all" to
# disable everything first and then reenable specific checks. For example, if
# you want to run only the similarities checker, you can use "--disable=all
# --enable=similarities". If you want to run only the classes checker, but have
# no Warning level messages displayed, use "--disable=all --enable=classes
# --disable=W".
disable=print-statement,
        parameter-unpacking,
        unpacking-in-except,
        old-raise-syntax,
        backtick,
        long-suffix,
        old-ne-operator,
        old-octal-literal,
        import-star-module-level,
        non-ascii-bytes-literal,
        raw-checker-failed,
        bad-inline-option,
        locally-disabled,
        file-ignored,
        suppressed-message,
        useless-suppression,
        deprecated-pragma,
        use-symbolic-message-instead,
        apply-builtin,
        basestring-builtin,
        buffer-builtin,
        cmp-builtin,
        coerce-builtin,
        execfile-builtin,
        file-builtin,
        long-builtin,
        raw_input-builtin,
        reduce-builtin,
        standarderror-builtin,
        unicode-builtin,
        xrange-builtin,
        coerce-method,
        delslice-method,
        getslice-method,
        setslice-method,
        no-absolute-import,
        old-division,
        dict-iter-method,
        dict-view-method,
        next-method-called,
        metaclass-assignment,
        indexing-exception,
        raising-string,
        reload-builtin,
        oct-method,
        hex-method,
        nonzero-method,
        cmp-method,
        input-builtin,
        round-builtin,
        intern-builtin,
        unichr-builtin,
        map-builtin-not-iterating,
        zip-builtin-not-iterating,
        range-builtin-not-iterating,
        filter-builtin-not-iterating,
        using-cmp-argument,
        eq-without-hash,
        div-method,
        idiv-method,
        rdiv-method,
        exception-message-attribute,
        invalid-str-codec,
        sys-max-int,
        bad-python3-import,
        deprecated-string-function,
        deprecated-str-translate-call,
        deprecated-itertools-function,
        deprecated-types-field,
        next-method-defined,
        dict-items-not-iterating,
        dict-keys-not-iterating,
        dict-values-not-iterating,
        deprecated-operator-function,
        deprecated-urllib-function,
        xreadlines-attribute,
        deprecated-sys-function,
        exception-escape,
        comprehension-escape

# Enable the message, report, category or checker with the given id(s). You can
# either give multiple identifier separated by comma (,) or put this option
# multiple time (only on the command line, not in the configuration file where
# it should appear only once). See also the "--disable" option for examples.
enable=c-extension-no-member


[REPORTS]

# Python expression which should return a note less than 10 (10 is the highest
# note). You have access to the variables errors warning, statement which
# respectively contain the number of errors / warnings messages and the total
# number of statements analyzed. This is used by the global evaluation report
# (RP0004).
evaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)

# Template used to display messages. This is a python new-style format string
# used to format the message information. See doc for all details.
#msg-template=

# Set the output format. Available formats are text, parseable, colorized, json
# and msvs (visual studio). You can also give a reporter class, e.g.
# mypackage.mymodule.MyReporterClass.
output-format=text

# Tells whether to display a full report or only the messages.
reports=no

# Activate the evaluation score.
score=yes


[REFACTORING]

# Maximum number of nested blocks for function / method body
max-nested-blocks=5

# Complete name of functions that never returns. When checking for
# inconsistent-return-statements if a never returning function is called then
# it will be considered as an explicit return statement and no message will be
# printed.
never-returning-functions=sys.exit


[BASIC]

# Naming style matching correct argument names.
argument-naming-style=snake_case

# Regular expression matching correct argument names. Overrides argument-
# naming-style.
#argument-rgx=

# Naming style matching correct attribute names.
attr-naming-style=snake_case

# Regular expression matching correct attribute names. Overrides attr-naming-
# style.
#attr-rgx=

# Bad variable names which should always be refused, separated by a comma.
bad-names=foo,
          bar,
          baz,
          toto,
          tutu,
          tata

# Naming style matching correct class attribute names.
class-attribute-naming-style=any

# Regular expression matching correct class attribute names. Overrides class-
# attribute-naming-style.
#class-attribute-rgx=

# Naming style matching correct class names.
class-naming-style=PascalCase

# Regular expression matching correct class names. Overrides class-naming-
# style.
#class-rgx=

# Naming style matching correct constant names.
const-naming-style=UPPER_CASE

# Regular expression matching correct constant names. Overrides const-naming-
# style.
#const-rgx=

# Minimum line length for functions/classes that require docstrings, shorter
# ones are exempt.
docstring-min-length=-1

# Naming style matching correct function names.
function-naming-style=snake_case

# Regular expression matching correct function names. Overrides function-
# naming-style.
#function-rgx=

# Good variable names which should always be accepted, separated by a comma.
good-names=i,
           j,
           k,
           ex,
           Run,
           _

# Include a hint for the correct naming format with invalid-name.
include-naming-hint=no

# Naming style matching correct inline iteration names.
inlinevar-naming-style=any

# Regular expression matching correct inline iteration names. Overrides
# inlinevar-naming-style.
#inlinevar-rgx=

# Naming style matching correct method names.
method-naming-style=snake_case

# Regular expression matching correct method names. Overrides method-naming-
# style.
#method-rgx=

# Naming style matching correct module names.
module-naming-style=snake_case

# Regular expression matching correct module names. Overrides module-naming-
# style.
#module-rgx=

# Colon-delimited sets of names that determine each other's naming style when
# the name regexes allow several styles.
name-group=

# Regular expression which should only match function or class names that do
# not require a docstring.
no-docstring-rgx=^_

# List of decorators that produce properties, such as abc.abstractproperty. Add
# to this list to register other decorators that produce valid properties.
# These decorators are taken in consideration only for invalid-name.
property-classes=abc.abstractproperty

# Naming style matching correct variable names.
variable-naming-style=snake_case

# Regular expression matching correct variable names. Overrides variable-
# naming-style.
#variable-rgx=


[FORMAT]

# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
expected-line-ending-format=

# Regexp for a line that is allowed to be longer than the limit.
ignore-long-lines=^\s*(# )?<?https?://\S+>?$

# Number of spaces of indent required inside a hanging or continued line.
indent-after-paren=4

# String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
# tab).
indent-string='    '

# Maximum number of characters on a single line.
max-line-length=120

# Maximum number of lines in a module.
max-module-lines=1000

# List of optional constructs for which whitespace checking is disabled. `dict-
# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\n222: 2}.
# `trailing-comma` allows a space between comma and closing bracket: (a, ).
# `empty-line` allows space-only lines.
no-space-check=trailing-comma,
               dict-separator

# Allow the body of a class to be on the same line as the declaration if body
# contains single statement.
single-line-class-stmt=no

# Allow the body of an if to be on the same line as the test if there is no
# else.
single-line-if-stmt=no


[LOGGING]

# Format style used to check logging format string. `old` means using %
# formatting, while `new` is for `{}` formatting.
logging-format-style=old

# Logging modules to check that the string format arguments are in logging
# function parameter format.
logging-modules=logging


[MISCELLANEOUS]

# List of note tags to take in consideration, separated by a comma.
notes=FIXME,
      XXX,
      TODO


[SIMILARITIES]

# Ignore comments when computing similarities.
ignore-comments=yes

# Ignore docstrings when computing similarities.
ignore-docstrings=yes

# Ignore imports when computing similarities.
ignore-imports=no

# Minimum lines number of a similarity.
min-similarity-lines=4


[SPELLING]

# Limits count of emitted suggestions for spelling mistakes.
max-spelling-suggestions=4

# Spelling dictionary name. Available dictionaries: none. To make it working
# install python-enchant package..
spelling-dict=

# List of comma separated words that should not be checked.
spelling-ignore-words=

# A path to a file that contains private dictionary; one word per line.
spelling-private-dict-file=

# Tells whether to store unknown words to indicated private dictionary in
# --spelling-private-dict-file option instead of raising a message.
spelling-store-unknown-words=no


[TYPECHECK]

# List of decorators that produce context managers, such as
# contextlib.contextmanager. Add to this list to register other decorators that
# produce valid context managers.
contextmanager-decorators=contextlib.contextmanager

# List of members which are set dynamically and missed by pylint inference
# system, and so shouldn't trigger E1101 when accessed. Python regular
# expressions are accepted.
generated-members=

# Tells whether missing members accessed in mixin class should be ignored. A
# mixin class is detected if its name ends with "mixin" (case insensitive).
ignore-mixin-members=yes

# Tells whether to warn about missing members when the owner of the attribute
# is inferred to be None.
ignore-none=yes

# This flag controls whether pylint should warn about no-member and similar
# checks whenever an opaque object is returned when inferring. The inference
# can return multiple potential results while evaluating a Python object, but
# some branches might not be evaluated, which results in partial inference. In
# that case, it might be useful to still emit no-member and other checks for
# the rest of the inferred objects.
ignore-on-opaque-inference=yes

# List of class names for which member attributes should not be checked (useful
# for classes with dynamically set attributes). This supports the use of
# qualified names.
ignored-classes=optparse.Values,thread._local,_thread._local

# List of module names for which member attributes should not be checked
# (useful for modules/projects where namespaces are manipulated during runtime
# and thus existing member attributes cannot be deduced by static analysis. It
# supports qualified module names, as well as Unix pattern matching.
ignored-modules=

# Show a hint with possible names when a member name was not found. The aspect
# of finding the hint is based on edit distance.
missing-member-hint=yes

# The minimum edit distance a name should have in order to be considered a
# similar match for a missing member name.
missing-member-hint-distance=1

# The total number of similar names that should be taken in consideration when
# showing a hint for a missing member.
missing-member-max-choices=1


[VARIABLES]

# List of additional names supposed to be defined in builtins. Remember that
# you should avoid defining new builtins when possible.
additional-builtins=

# Tells whether unused global variables should be treated as a violation.
allow-global-unused-variables=yes

# List of strings which can identify a callback function by name. A callback
# name must start or end with one of those strings.
callbacks=cb_,
          _cb

# A regular expression matching the name of dummy variables (i.e. expected to
# not be used).
dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

# Argument names that match this expression will be ignored. Default to name
# with leading underscore.
ignored-argument-names=_.*|^ignored_|^unused_

# Tells whether we should check for unused import in __init__ files.
init-import=no

# List of qualified module names which can have objects that can redefine
# builtins.
redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io


[CLASSES]

# List of method names used to declare (i.e. assign) instance attributes.
defining-attr-methods=__init__,
                      __new__,
                      setUp

# List of member names, which should be excluded from the protected access
# warning.
exclude-protected=_asdict,
                  _fields,
                  _replace,
                  _source,
                  _make

# List of valid names for the first argument in a class method.
valid-classmethod-first-arg=cls

# List of valid names for the first argument in a metaclass class method.
valid-metaclass-classmethod-first-arg=cls


[DESIGN]

# Maximum number of arguments for function / method.
max-args=15

# Maximum number of attributes for a class (see R0902).
max-attributes=20

# Maximum number of boolean expressions in an if statement.
max-bool-expr=5

# Maximum number of branch for function / method body.
max-branches=12

# Maximum number of locals for function / method body.
max-locals=25

# Maximum number of parents for a class (see R0901).
max-parents=7

# Maximum number of public methods for a class (see R0904).
max-public-methods=20

# Maximum number of return / yield for function / method body.
max-returns=6

# Maximum number of statements in function / method body.
max-statements=50

# Minimum number of public methods for a class (see R0903).
min-public-methods=2


[IMPORTS]

# Allow wildcard imports from modules that define __all__.
allow-wildcard-with-all=no

# Analyse import fallback blocks. This can be used to support both Python 2 and
# 3 compatible code, which means that the block might have code that exists
# only in one or another interpreter, leading to false positives when analysed.
analyse-fallback-blocks=no

# Deprecated modules which should not be used, separated by a comma.
deprecated-modules=optparse,tkinter.tix

# Create a graph of external dependencies in the given file (report RP0402 must
# not be disabled).
ext-import-graph=

# Create a graph of every (i.e. internal and external) dependencies in the
# given file (report RP0402 must not be disabled).
import-graph=

# Create a graph of internal dependencies in the given file (report RP0402 must
# not be disabled).
int-import-graph=

# Force import order to recognize a module as part of the standard
# compatibility libraries.
known-standard-library=

# Force import order to recognize a module as part of a third party library.
known-third-party=enchant


[EXCEPTIONS]

# Exceptions that will emit a warning when being caught. Defaults to
# "Exception".
overgeneral-exceptions=Exception


================================================
File: .readthedocs.yml
================================================
# .readthedocs.yml
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required
version: 2

build:
  os: "ubuntu-22.04"
  tools:
    python: "3.7"

# Build documentation in the docs/ directory with Sphinx
sphinx:
  builder: html
  configuration: docs/conf.py

# Build documentation with MkDocs
#mkdocs:
#  configuration: mkdocs.yml

# Optionally build your docs in additional formats such as PDF and ePub
#formats: all

# Optionally set the version of Python and requirements required to build your docs
python:
  install:
    - requirements: requirements.txt
    - requirements: docs/requirements.txt

================================================
File: docs/404.rst
================================================
.. meta::
   :robots: noindex, nofollow

.. title:: Page Not Found

Page Not Found
==============

Sorry, the page you are looking for does not exist.

You can visit our `Platform Docs <https://docs.dataloop.ai>`_ or `Developers Docs <https://developers.dataloop.ai/>`_

If you want to stay here, go back to the `homepage <https://sdk-docs.dataloop.ai/en/latest/index.html/>`_


================================================
File: docs/Makefile
================================================
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line.
SPHINXOPTS    =
SPHINXBUILD   = sphinx-build
SOURCEDIR     = source
BUILDDIR      = build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

================================================
File: docs/cli.rst
================================================
Command Line Interface
======================

Options:

.. argparse::
   :filename: ../dtlpy/dlp/dlp.py
   :func: get_parser
   :prog: dlp


================================================
File: docs/conf.py
================================================
# -*- coding: utf-8 -*-
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys
import datetime

sys.path.insert(0, os.path.abspath('../'))

# -- Project information -----------------------------------------------------

project = 'dtlpy'
copyright = '{}, Dataloop Team'.format(datetime.datetime.now().year)
author = 'Dataloop Team'

# The short X.Y version
version = ''
# The full version, including alpha/beta/rc tags
release = '1.105.6'

add_module_names = False

# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    # 'sphinxprettysearchresults',
    'sphinx.ext.autodoc',
    'sphinx.ext.doctest',
    'sphinx.ext.intersphinx',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.napoleon',
    'sphinx.ext.mathjax',
    'sphinx.ext.ifconfig',
    'sphinx.ext.viewcode',
    'sphinx.ext.githubpages',
    'sphinxarg.ext',
    'nbsphinx',
    'sphinx_rtd_theme',
    # 'sphinxcontrib.lunrsearch',
    'sphinx_copybutton',
    'recommonmark',
    'notfound.extension'
]
notfound_urls_prefix = '/en/latest/'


# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
source_suffix = ['.rst', '.md']
# source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = 'en'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', '**.ipynb_checkpoints']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = None

# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'sphinx_rtd_theme'
html_logo = '_static/logo-white.png'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
html_theme_options = {
    'canonical_url': '',
    'analytics_id': 'UA-XXXXXXX-1',  # Provided by Google in your dashboard
    'logo_only': True,
    'display_version': True,
    'prev_next_buttons_location': 'bottom',
    'style_external_links': False,
    'vcs_pageview_mode': '',
    # Toc options
    'collapse_navigation': False,
    'sticky_navigation': True,
    'navigation_depth': 4,
    'includehidden': True,
    'titles_only': False

}

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']
html_css_files = [
    'custom.css'
]

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
html_sidebars = {}

# -- Options for HTMLHelp output ---------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = 'dtlpydoc'

# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',

    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'dtlpy.tex', 'dtlpy Documentation', 'Dataloop Team', 'manual'),
]

# -- Options for manual page output ------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    (master_doc, 'dtlpy', 'dtlpy Documentation',
     [author], 1)
]

# -- Options for Texinfo output ----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'dtlpy', 'dtlpy Documentation',
     author, 'dtlpy', 'One line description of project.',
     'Miscellaneous'),
]

# -- Options for Epub output -------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = project

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#
# epub_identifier = ''

# A unique identification for the text.
#
# epub_uid = ''

# A list of files that should not be packed into the epub file.
epub_exclude_files = ['search.html']

# -- Extension configuration -------------------------------------------------

# -- Options for intersphinx extension ---------------------------------------

# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'https://docs.python.org/': None}

# -- Options for todo extension ----------------------------------------------

# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = True

autodoc_mock_imports = ["tensorflow", "torch", "imgaug", "torchvision"]
#
# html_additional_pages = {
#     '404': '404.html',
# }


================================================
File: docs/entities.rst
================================================
Entities
========


.. toctree::
   :caption: Table of Contents
   :maxdepth: 6

Organization
------------

.. automodule:: dtlpy.entities.organization
  :members:
  :show-inheritance:

Integration
~~~~~~~~~~~

.. automodule:: dtlpy.entities.integration
  :members:
  :show-inheritance:


Project
--------

.. automodule:: dtlpy.entities.project
  :members:
  :show-inheritance:

User
~~~~

.. automodule:: dtlpy.entities.user
  :members:
  :show-inheritance:


Dataset
-------

.. automodule:: dtlpy.entities.dataset
  :members:
  :show-inheritance:

Driver
~~~~~~

.. automodule:: dtlpy.entities.driver
  :members:
  :show-inheritance:

Item
----

.. automodule:: dtlpy.entities.item
  :members:
  :show-inheritance:

Item Link
~~~~~~~~~

.. automodule:: dtlpy.entities.links
  :members:
  :show-inheritance:

Annotation
----------

.. automodule:: dtlpy.entities.annotation
  :members:
  :show-inheritance:

Collection of Annotation entities
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.entities.annotation_collection
  :members:
  :show-inheritance:

Annotation Definition
~~~~~~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.entities.annotation_definitions.base_annotation_definition
  :members:
  :show-inheritance:

Box Annotation Definition
*************************

.. automodule:: dtlpy.entities.annotation_definitions.box
  :members:
  :show-inheritance:

Classification Annotation Definition
*************************

.. automodule:: dtlpy.entities.annotation_definitions.classification
  :members:
  :show-inheritance:

Cuboid Annotation Definition
****************************

.. automodule:: dtlpy.entities.annotation_definitions.cube
  :members:
  :show-inheritance:

Item Description Definition
****************************

.. automodule:: dtlpy.entities.annotation_definitions.description
  :members:
  :show-inheritance:

Ellipse Annotation Definition
*****************************

.. automodule:: dtlpy.entities.annotation_definitions.ellipse
  :members:
  :show-inheritance:

Note Annotation Definition
**************************

.. automodule:: dtlpy.entities.annotation_definitions.note
  :members:
  :show-inheritance:

Point Annotation Definition
***************************

.. automodule:: dtlpy.entities.annotation_definitions.point
  :members:
  :show-inheritance:

Polygon Annotation Definition
*****************************

.. automodule:: dtlpy.entities.annotation_definitions.polygon
  :members:
  :show-inheritance:

Polyline Annotation Definition
******************************

.. automodule:: dtlpy.entities.annotation_definitions.polyline
  :members:
  :show-inheritance:

Pose Annotation Definition
**************************

.. automodule:: dtlpy.entities.annotation_definitions.pose
  :members:
  :show-inheritance:

Segmentation Annotation Definition
**********************************

.. automodule:: dtlpy.entities.annotation_definitions.segmentation
  :members:
  :show-inheritance:

Audio Annotation Definition
**********************************

.. automodule:: dtlpy.entities.annotation_definitions.subtitle
  :members:
  :show-inheritance:

Undefined Annotation Definition
**********************************

.. automodule:: dtlpy.entities.annotation_definitions.undefined_annotation
  :members:
  :show-inheritance:

Similarity
~~~~~~~~~~

.. automodule:: dtlpy.entities.similarity
  :members:
  :show-inheritance:

Filter
------

.. automodule:: dtlpy.entities.filters
  :members:
  :show-inheritance:

Recipe
------

.. automodule:: dtlpy.entities.recipe
  :members:
  :show-inheritance:

Ontology
~~~~~~~~

.. automodule:: dtlpy.entities.ontology
  :members:
  :show-inheritance:

Label
*****

.. automodule:: dtlpy.entities.label
  :members:
  :show-inheritance:

Task
----

.. automodule:: dtlpy.entities.task
  :members:
  :show-inheritance:

Assignment
~~~~~~~~~~

.. automodule:: dtlpy.entities.assignment
  :members:
  :show-inheritance:

Package
-------

.. automodule:: dtlpy.entities.package
  :members:
  :show-inheritance:

Package Function
~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.entities.package_function
  :members:
  :show-inheritance:

Package Module
~~~~~~~~~~~~~~

.. automodule:: dtlpy.entities.package_module
  :members:
  :show-inheritance:

Slot
~~~~

.. automodule:: dtlpy.entities.package_slot
  :members:
  :show-inheritance:

Codebase
~~~~~~~~

.. automodule:: dtlpy.entities.codebase
  :members:
  :show-inheritance:

Service
-------

.. automodule:: dtlpy.entities.service
  :members:
  :show-inheritance:

Bot
~~~

.. automodule:: dtlpy.entities.bot
  :members:
  :show-inheritance:

Trigger
-------

.. automodule:: dtlpy.entities.trigger
  :members:
  :show-inheritance:

Execution
---------

.. automodule:: dtlpy.entities.execution
  :members:
  :show-inheritance:

Model
--------

.. automodule:: dtlpy.entities.model
  :members:
  :show-inheritance:

Pipeline
--------

.. automodule:: dtlpy.entities.pipeline
  :members:
  :show-inheritance:

Pipeline Execution
~~~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.entities.pipeline_execution
  :members:
  :show-inheritance:

Other
-----

Pages
~~~~~

.. automodule:: dtlpy.entities.paged_entities
  :members:
  :show-inheritance:


Base Entity
~~~~~~~~~~~

.. automodule:: dtlpy.entities.base_entity
  :members:
  :show-inheritance:

Command
~~~~~~~

.. automodule:: dtlpy.entities.command
  :members:
  :show-inheritance:

Directory Tree
~~~~~~~~~~~~~~

.. automodule:: dtlpy.entities.directory_tree
  :members:
  :show-inheritance:

================================================
File: docs/index.rst
================================================
Dataloop's SDK and CLI documentation
====================================


Drive your AI to production with end-to-end data management, automation pipelines and a quality-first data labeling platform

**For more documentation click** `here <https://dataloop.ai/docs/>`_

**For developers documentation, code snippets, tutorials and more, go to our** `Developer's Docs <https://developers.dataloop.ai>`_



.. toctree::
   :caption: Table of Contents
   :maxdepth: 2

   cli.rst
   repositories.rst
   entities.rst
   utilities.rst



Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`


================================================
File: docs/make.bat
================================================
@ECHO OFF

pushd %~dp0

REM Command file for Sphinx documentation

if "%SPHINXBUILD%" == "" (
	set SPHINXBUILD=sphinx-build
)
set SOURCEDIR=source
set BUILDDIR=build

if "%1" == "" goto help

%SPHINXBUILD% >NUL 2>NUL
if errorlevel 9009 (
	echo.
	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
	echo.installed, then set the SPHINXBUILD environment variable to point
	echo.to the full path of the 'sphinx-build' executable. Alternatively you
	echo.may add the Sphinx directory to PATH.
	echo.
	echo.If you don't have Sphinx installed, grab it from
	echo.http://sphinx-doc.org/
	exit /b 1
)

%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
goto end

:help
%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%

:end
popd


================================================
File: docs/repositories.rst
================================================
Repositories
============


.. toctree::
   :caption: Table of Contents
   :maxdepth: 6

Organizations
-------------

.. automodule:: dtlpy.repositories.organizations
  :members:
  :show-inheritance:
  :exclude-members: create

Integrations
~~~~~~~~~~~~

.. automodule:: dtlpy.repositories.integrations
  :members:
  :show-inheritance:


Projects
--------

.. automodule:: dtlpy.repositories.projects
  :members:
  :show-inheritance:


Datasets
--------

.. automodule:: dtlpy.repositories.datasets
  :members:
  :show-inheritance:

Drivers
~~~~~~~

.. automodule:: dtlpy.repositories.drivers
  :members:
  :show-inheritance:

Items
-----

.. automodule:: dtlpy.repositories.items
  :members:
  :show-inheritance:

Annotations
-----------

.. automodule:: dtlpy.repositories.annotations
  :members:
  :show-inheritance:

Recipes
-------

.. automodule:: dtlpy.repositories.recipes
  :members:
  :show-inheritance:

Ontologies
~~~~~~~~~~~

.. automodule:: dtlpy.repositories.ontologies
  :members:
  :show-inheritance:

Tasks
-----

.. automodule:: dtlpy.repositories.tasks
  :members:
  :show-inheritance:

Assignments
~~~~~~~~~~~

.. automodule:: dtlpy.repositories.assignments
  :members:
  :show-inheritance:

Packages
--------

.. automodule:: dtlpy.repositories.packages
  :members:
  :show-inheritance:

Codebases
~~~~~~~~~

.. automodule:: dtlpy.repositories.codebases
  :members:
  :show-inheritance:

Services
--------

.. automodule:: dtlpy.repositories.services
  :members:
  :show-inheritance:

Bots
~~~~

.. automodule:: dtlpy.repositories.bots
  :members:
  :show-inheritance:

Triggers
--------

.. automodule:: dtlpy.repositories.triggers
  :members:
  :show-inheritance:

Executions
----------

.. automodule:: dtlpy.repositories.executions
  :members:
  :show-inheritance:

Models
---------

.. automodule:: dtlpy.repositories.models
  :members:
  :show-inheritance:

Pipelines
---------

.. automodule:: dtlpy.repositories.pipelines
  :members:
  :show-inheritance:

Pipeline Executions
~~~~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.repositories.pipeline_executions
  :members:
  :show-inheritance:


General Commands
----------------

.. automodule:: dtlpy.repositories.commands
  :members:
  :show-inheritance:

Download Commands
~~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.repositories.downloader
  :members:
  :show-inheritance:

Upload Commands
~~~~~~~~~~~~~~~~~

.. automodule:: dtlpy.repositories.uploader
  :members:
  :show-inheritance:

================================================
File: docs/requirements.txt
================================================
opencv_python==4.2.0.32
lxml_html_clean
sphinx-argparse==0.2.5
sphinxcontrib-websupport==1.1.2
sphinx-rtd-theme==0.5.1
sphinx-copybutton==0.5.0
nbsphinx==0.4.2
m2r==0.2.1
recommonmark
mistune==0.8.4
sphinx-notfound-page

================================================
File: docs/utilities.rst
================================================
Utilities
=========


.. toctree::
   :caption: Table of Contents
   :maxdepth: 6

converter
---------

.. automodule:: dtlpy.utilities.converter
  :members:
  :show-inheritance:



================================================
File: docs/_static/custom.css
================================================
/* This file intentionally left blank. */
@import url('https://fonts.googleapis.com/css?family=Montserrat&display=swap');
@import 'https://fonts.googleapis.com/css?family=Montserrat';
@import 'https://fonts.googleapis.com/css?family=Lato';
@import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap');

@import url('https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap');


/*Dataloop blue  #3452ff;*/
html,body {
 font-family: 'Montserrat', sans-serif !important;
   font-family: "Montserrat", sans-serif !important;
}
body{
background-color: white!important;
}

/*home icon and "dtlpy" */
.wy-side-nav-search > a, .wy-nav-top > i , .wy-nav-top > a{
color : white!important;
}

.wy-side-nav-search > a:hover{
background-color : #3452ff!important;
}
/* bg color of top nav*/
.wy-side-nav-search,.wy-nav-top{
background-color : #3452ff!important;
}

/*search outline*/
#rtd-search-form > input {
color : black;
}


/*bottom left nav heading text*/
.caption-text{
color : White;
}

/*nav top level*/
.wy-side-scroll toctree-l1:active{
background-color : #eff3f8!important;
}

/*nav second level*/
.wy-side-scroll .toctree-l2{
background-color : #eff3f8;
}


/*nav third level*/
.wy-side-scroll .toctree-l3>a{
background-color : #e5eaf1!important;
}

.wy-side-scroll .toctree-l2.current>a{
background-color : #e5eaf1!important;
}

/*nav forth level*/
.wy-side-scroll .toctree-l4>a{
background-color : #eff3f8!important;
}


/*bottom left nav*/
.wy-side-scroll, .wy-nav-side{
background-color : Black;
}
/*nav hover and active*/

.wy-side-scroll .reference.internal:hover{
background-color : white!important;
color : #3452ff!important;
}

.wy-side-scroll .reference.internal:active{
background-color : white!important;
color : #3452ff!important;
}

================================================
File: dtlpy/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
import warnings
import logging
import sys
import os

##########
# Logger #
##########
from .services import DataloopLogger, DtlpyFilter
from .__version__ import version as __version__

logger = logging.getLogger(name='dtlpy')
logging.getLogger(name='filelock').setLevel(level=logging.CRITICAL)
if len(logger.handlers) == 0:
    logger.setLevel(logging.DEBUG)
    log_filepath = DataloopLogger.get_log_filepath()
    # set file handler to save all logs to file
    stream_formatter = logging.Formatter(
        fmt="[%(asctime)-s][%(levelname).3s][%(name)s:v" + __version__ + "][%(relativepath)-s:%(lineno)-d] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    file_formatter = logging.Formatter(
        fmt="[%(asctime)s.%(msecs)03d][%(threadName)s][%(levelname).3s][%(name)s:v" +
            __version__ + "][%(relativepath)-s:%(lineno)-d](%(funcName)-s): %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    package_path = os.path.dirname(__file__)
    # relative function path filtering
    filtering = DtlpyFilter(package_path)
    fh = DataloopLogger(log_filepath, maxBytes=(1048 * 1000 * 5))
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(file_formatter)
    fh.addFilter(filtering)
    sh = logging.StreamHandler()
    sh.setLevel(logging.WARNING)
    sh.setFormatter(stream_formatter)
    sh.addFilter(filtering)
    # set handlers to main logger
    logger.addHandler(sh)
    logger.addHandler(fh)

from .services.api_client import client as client_api
from .services.api_client import VerboseLoggingLevel, ApiClient
from .services import DataloopLogger, DtlpyFilter, check_sdk, Reporter, service_defaults
from .services.api_reference import api_reference as _api_reference
from .caches.cache import CacheConfig, CacheType
from .exceptions import PlatformException
from . import repositories, exceptions, entities, examples
from .entities import (
    # main entities
    Project, Dataset, ExpirationOptions, ExportVersion, Trigger, Item, Execution, AnnotationCollection, Annotation,
    Recipe, IndexDriver, AttributesTypes, AttributesRange, Dpk, App, AppModule, AppScope,
    Ontology, Label, Task, TaskPriority, ConsensusTaskType, Assignment, Service, Package, Codebase, Model,
    PackageModule, PackageFunction,
    # annotations
    Box, Cube, Cube3d, Point, Note, Message, Segmentation, Ellipse, Classification, Subtitle, Polyline, Pose, Gis, GisType,
    Description,
    Polygon, Text, FreeText, RefImage,
    # filters
    Filters, FiltersKnownFields, FiltersResource, FiltersOperations, FiltersMethod, FiltersOrderByDirection,
    FiltersKnownFields as KnownFields,
    # triggers
    TriggerResource, TriggerAction, TriggerExecutionMode, TriggerType,
    # faas
    FunctionIO, KubernetesAutoscalerType, KubernetesAutuscalerType, KubernetesRabbitmqAutoscaler, KubernetesAutoscaler, KubernetesRuntime,
    InstanceCatalog, PackageInputType, ServiceType, ServiceModeType, KubernetesRPSAutoscaler,
    PackageSlot, SlotPostAction, SlotPostActionType, SlotDisplayScope, SlotDisplayScopeResource, UiBindingPanel,
    # roberto
    DatasetSubsetType, ModelStatus, PlotSample, ArtifactType, Artifact, ItemArtifact, LinkArtifact, LocalArtifact,
    EntityScopeLevel,
    # features
    FeatureEntityType, Feature, FeatureSet,
    #
    RequirementOperator, PackageRequirement,
    Command, CommandsStatus,
    LocalCodebase, GitCodebase, ItemCodebase, FilesystemCodebase, PackageCodebaseType,
    MemberRole, MemberOrgRole,
    Webhook, HttpMethod,
    ViewAnnotationOptions, AnnotationStatus, AnnotationType,
    ItemStatus, ExecutionStatus, ExportMetadata,
    PromptItem, Prompt, PromptType, ItemGis, Layer,
    ItemLink, UrlLink, LinkTypeEnum,
    Modality, ModalityTypeEnum, ModalityRefTypeEnum,
    Workload, WorkloadUnit, ItemAction,
    PipelineExecution, PipelineExecutionStatus, CycleRerunMethod, PipelineExecutionNode, Pipeline, PipelineConnection,
    PipelineNode, TaskNode, CodeNode, PipelineStats, PipelineSettings,
    PipelineNodeType, PipelineNameSpace, PipelineResumeOption, Variable, CompositionStatus,
    FunctionNode, DatasetNode, PipelineConnectionPort, PipelineNodeIO, Organization, OrganizationsPlans, Integration,
    Driver, S3Driver, GcsDriver, AzureBlobDriver, CacheAction, PodType,
    ExternalStorage, IntegrationType, Role, PlatformEntityType, SettingsValueTypes, SettingsTypes, SettingsSectionNames,
    SettingScope, BaseSetting, UserSetting, Setting, ServiceSample, ExecutionSample, PipelineExecutionSample,
    ResourceExecution, Message, NotificationEventContext,
    # compute
    ClusterProvider, ComputeType, ComputeStatus, Toleration, DeploymentResource, DeploymentResources,
    NodePool, AuthenticationIntegration, Authentication, ComputeCluster, ComputeContext, Compute, KubernetesCompute,
    ServiceDriver
)
from .ml import BaseModelAdapter
from .utilities import Converter, BaseServiceRunner, Progress, Context, AnnotationFormat
from .repositories import FUNCTION_END_LINE, PackageCatalog

warnings.simplefilter('once', DeprecationWarning)

# check python version
if sys.version_info.major != 3:
    if sys.version_info.minor not in [5, 6]:
        sys.stderr.write(
            'Error: Your Python version "{}.{}" is NOT supported by Dataloop SDK dtlpy. '
            "Supported version are 3.5, 3.6)\n".format(
                sys.version_info.major, sys.version_info.minor
            )
        )
        sys.exit(-1)

if os.name == "nt":
    # set encoding for windows printing
    os.environ["PYTHONIOENCODING"] = ":replace"

"""
Main Platform Interface module for Dataloop
"""

################
# Repositories #
################
# Create repositories instances
projects = repositories.Projects(client_api=client_api)
datasets = repositories.Datasets(client_api=client_api, project=None)
items = repositories.Items(client_api=client_api, datasets=datasets)
packages = repositories.Packages(client_api=client_api)
executions = repositories.Executions(client_api=client_api)
commands = repositories.Commands(client_api=client_api)
services = repositories.Services(client_api=client_api)
webhooks = repositories.Webhooks(client_api=client_api)
triggers = repositories.Triggers(client_api=client_api)
assignments = repositories.Assignments(client_api=client_api)
tasks = repositories.Tasks(client_api=client_api)
annotations = repositories.Annotations(client_api=client_api)
models = repositories.Models(client_api=client_api)
ontologies = repositories.Ontologies(client_api=client_api)
recipes = repositories.Recipes(client_api=client_api)
pipelines = repositories.Pipelines(client_api=client_api)
pipeline_executions = repositories.PipelineExecutions(client_api=client_api)
feature_sets = repositories.FeatureSets(client_api=client_api)
features = repositories.Features(client_api=client_api)
organizations = repositories.Organizations(client_api=client_api)
analytics = repositories.Analytics(client_api=client_api)
integrations = repositories.Integrations(client_api=client_api)
drivers = repositories.Drivers(client_api=client_api)
settings = repositories.Settings(client_api=client_api)
apps = repositories.Apps(client_api=client_api)
dpks = repositories.Dpks(client_api=client_api)
messages = repositories.Messages(client_api=client_api)
compositions = repositories.Compositions(client_api=client_api)
computes = repositories.Computes(client_api=client_api)
service_drivers = repositories.ServiceDrivers(client_api=client_api)
collections = repositories.Collections(client_api=client_api)

try:
    check_sdk.check(version=__version__, client_api=client_api)
except Exception:
    logger.debug("Failed to check SDK! Continue without")

try:
    check_sdk.resolve_platform_settings(client_api=client_api, settings=settings)
except Exception:
    pass

verbose = client_api.verbose
login = client_api.login
logout = client_api.logout
login_token = client_api.login_token
login_secret = client_api.login_secret
login_api_key = client_api.login_api_key
login_m2m = client_api.login_m2m
add_environment = client_api.add_environment
setenv = client_api.setenv
token_expired = client_api.token_expired
info = client_api.info
cache_state = client_api.cache_state
attributes_mode = client_api.attributes_mode
sdk_cache = client_api.sdk_cache
platform_settings = client_api.platform_settings


def get_secret(secret):
    return os.environ.get(secret, None)


def token():
    """
    token
    :return: token in use
    """
    return client_api.token


def environment():
    """
    environment
    :return: current environment
    """
    return client_api.environment


def init():
    """
    init current directory as a Dataloop working directory
    :return:
    """
    from .services import CookieIO

    client_api.state_io = CookieIO.init_local_cookie(create=True)
    assert isinstance(client_api.state_io, CookieIO)
    logger.info(".Dataloop directory initiated successfully in {}".format(os.getcwd()))


def checkout_state():
    """
    Return the current checked out state
    :return:
    """
    state = client_api.state_io.read_json()
    return state


def use_attributes_2(state: bool = True):
    warnings.warn("Function 'use_attributes_2()' is deprecated as of version 1.99.12 and has been non-functional since version 1.90.39.  To work with attributes 2.0, simply use 'update_attributes()'.", DeprecationWarning)
    client_api.attributes_mode.use_attributes_2 = state


class LoggingLevel:
    DEBUG = "debug"
    WARNING = "warning"
    CRITICAL = "critical"
    INFO = "info"
    ERROR = "error"


#################
#     ENUMS     #
#################
CallbackEvent = client_api.callbacks.CallbackEvent
LOGGING_LEVEL_DEBUG = LoggingLevel.DEBUG
LOGGING_LEVEL_WARNING = LoggingLevel.WARNING
LOGGING_LEVEL_CRITICAL = LoggingLevel.CRITICAL
LOGGING_LEVEL_INFO = LoggingLevel.INFO

VERBOSE_LOGGING_LEVEL_DEBUG = VerboseLoggingLevel.DEBUG
VERBOSE_LOGGING_LEVEL_INFO = VerboseLoggingLevel.INFO
VERBOSE_LOGGING_LEVEL_WARNING = VerboseLoggingLevel.WARNING
VERBOSE_LOGGING_LEVEL_ERROR = VerboseLoggingLevel.ERROR
VERBOSE_LOGGING_LEVEL_CRITICAL = VerboseLoggingLevel.CRITICAL

HTTP_METHOD_POST = HttpMethod.POST
HTTP_METHOD_GET = HttpMethod.GET
HTTP_METHOD_DELETE = HttpMethod.DELETE
HTTP_METHOD_PATCH = HttpMethod.PATCH

VIEW_ANNOTATION_OPTIONS_JSON = ViewAnnotationOptions.JSON
VIEW_ANNOTATION_OPTIONS_MASK = ViewAnnotationOptions.MASK
VIEW_ANNOTATION_OPTIONS_ANNOTATION_ON_IMAGE = ViewAnnotationOptions.ANNOTATION_ON_IMAGE
VIEW_ANNOTATION_OPTIONS_INSTANCE = ViewAnnotationOptions.INSTANCE
VIEW_ANNOTATION_OPTIONS_VTT = ViewAnnotationOptions.VTT
VIEW_ANNOTATION_OPTIONS_OBJECT_ID = ViewAnnotationOptions.OBJECT_ID

ANNOTATION_STATUS_ISSUE = AnnotationStatus.ISSUE
ANNOTATION_STATUS_REVIEW = AnnotationStatus.REVIEW
ANNOTATION_STATUS_APPROVED = AnnotationStatus.APPROVED
ANNOTATION_STATUS_CLEAR = AnnotationStatus.CLEAR

ORGANIZATION_PLAN_FREEMIUM = OrganizationsPlans.FREEMIUM
ORGANIZATION_PLAN_PREMIUM = OrganizationsPlans.PREMIUM

EXPORT_METADATA_FROM_JSON = ExportMetadata.FROM_JSON

EXPORT_VERSION_V1 = ExportVersion.V1
EXPORT_VERSION_V2 = ExportVersion.V2

# class
ANNOTATION_TYPE_BOX = AnnotationType.BOX
ANNOTATION_TYPE_CLASSIFICATION = AnnotationType.CLASSIFICATION
ANNOTATION_TYPE_COMPARISON = AnnotationType.COMPARISON
ANNOTATION_TYPE_ELLIPSE = AnnotationType.ELLIPSE
ANNOTATION_TYPE_NOTE = AnnotationType.NOTE
ANNOTATION_TYPE_POINT = AnnotationType.POINT
ANNOTATION_TYPE_POLYGON = AnnotationType.POLYGON
ANNOTATION_TYPE_POLYLINE = AnnotationType.POLYLINE
ANNOTATION_TYPE_POSE = AnnotationType.POSE
ANNOTATION_TYPE_SEGMENTATION = AnnotationType.SEGMENTATION
ANNOTATION_TYPE_SUBTITLE = AnnotationType.SUBTITLE
ANNOTATION_TYPE_TEXT = AnnotationType.TEXT

ITEM_STATUS_COMPLETED = ItemStatus.COMPLETED
ITEM_STATUS_APPROVED = ItemStatus.APPROVED
ITEM_STATUS_DISCARDED = ItemStatus.DISCARDED

EXECUTION_STATUS_CREATED = ExecutionStatus.CREATED
EXECUTION_STATUS_IN_PROGRESS = ExecutionStatus.IN_PROGRESS
EXECUTION_STATUS_SUCCESS = ExecutionStatus.SUCCESS
EXECUTION_STATUS_FAILED = ExecutionStatus.FAILED

LINK_TYPE_ID = LinkTypeEnum.ID
LINK_TYPE_URL = LinkTypeEnum.URL

KUBERNETES_AUTUSCALER_TYPE_CPU = KubernetesAutoscalerType.CPU
KUBERNETES_AUTUSCALER_TYPE_RABBITMQ = KubernetesAutoscalerType.RABBITMQ

INSTANCE_CATALOG_REGULAR_XS = InstanceCatalog.REGULAR_XS
INSTANCE_CATALOG_REGULAR_S = InstanceCatalog.REGULAR_S
INSTANCE_CATALOG_REGULAR_M = InstanceCatalog.REGULAR_M
INSTANCE_CATALOG_REGULAR_L = InstanceCatalog.REGULAR_L
INSTANCE_CATALOG_HIGHMEM_XS = InstanceCatalog.HIGHMEM_XS
INSTANCE_CATALOG_HIGHMEM_S = InstanceCatalog.HIGHMEM_S
INSTANCE_CATALOG_HIGHMEM_M = InstanceCatalog.HIGHMEM_M
INSTANCE_CATALOG_HIGHMEM_L = InstanceCatalog.HIGHMEM_L
INSTANCE_CATALOG_GPU_K80_S = InstanceCatalog.GPU_K80_S
INSTANCE_CATALOG_GPU_K80_M = InstanceCatalog.GPU_K80_M
INSTANCE_CATALOG_GPU_T4_S = InstanceCatalog.GPU_T4_S
INSTANCE_CATALOG_GPU_T4_M = InstanceCatalog.GPU_T4_M

MODALITY_TYPE_OVERLAY = ModalityTypeEnum.OVERLAY
MODALITY_TYPE_PREVIEW = ModalityTypeEnum.PREVIEW
MODALITY_TYPE_REPLACE = ModalityTypeEnum.REPLACE

MODALITY_REF_TYPE_ID = ModalityRefTypeEnum.ID
MODALITY_REF_TYPE_URL = ModalityRefTypeEnum.URL

FILTERS_KNOWN_FIELDS_DIR = FiltersKnownFields.DIR
FILTERS_KNOWN_FIELDS_ANNOTATED = FiltersKnownFields.ANNOTATED
FILTERS_KNOWN_FIELDS_FILENAME = FiltersKnownFields.FILENAME
FILTERS_KNOWN_FIELDS_CREATED_AT = FiltersKnownFields.CREATED_AT
FILTERS_KNOWN_FIELDS_UPDATED_AT = FiltersKnownFields.UPDATED_AT
FILTERS_KNOWN_FIELDS_LABEL = FiltersKnownFields.LABEL
FILTERS_KNOWN_FIELDS_NAME = FiltersKnownFields.NAME
FILTERS_KNOWN_FIELDS_HIDDEN = FiltersKnownFields.HIDDEN
FILTERS_KNOWN_FIELDS_TYPE = FiltersKnownFields.TYPE

FILTERS_RESOURCE_ITEM = FiltersResource.ITEM
FILTERS_RESOURCE_ANNOTATION = FiltersResource.ANNOTATION
FILTERS_RESOURCE_EXECUTION = FiltersResource.EXECUTION
FILTERS_RESOURCE_PACKAGE = FiltersResource.PACKAGE
FILTERS_RESOURCE_SERVICE = FiltersResource.SERVICE
FILTERS_RESOURCE_TRIGGER = FiltersResource.TRIGGER
FILTERS_RESOURCE_MODEL = FiltersResource.MODEL
FILTERS_RESOURCE_WEBHOOK = FiltersResource.WEBHOOK
FILTERS_RESOURCE_RECIPE = FiltersResource.RECIPE
FILTERS_RESOURCE_DATASET = FiltersResource.DATASET
FILTERS_RESOURCE_ONTOLOGY = FiltersResource.ONTOLOGY

FILTERS_OPERATIONS_OR = FiltersOperations.OR
FILTERS_OPERATIONS_AND = FiltersOperations.AND
FILTERS_OPERATIONS_IN = FiltersOperations.IN
FILTERS_OPERATIONS_NOT_EQUAL = FiltersOperations.NOT_EQUAL
FILTERS_OPERATIONS_EQUAL = FiltersOperations.EQUAL
FILTERS_OPERATIONS_GREATER_THAN = FiltersOperations.GREATER_THAN
FILTERS_OPERATIONS_LESS_THAN = FiltersOperations.LESS_THAN
FILTERS_OPERATIONS_EXISTS = FiltersOperations.EXISTS

FILTERS_METHOD_OR = FiltersMethod.OR
FILTERS_METHOD_AND = FiltersMethod.AND

FILTERS_ORDERBY_DIRECTION_DESCENDING = FiltersOrderByDirection.DESCENDING
FILTERS_ORDERBY_DIRECTION_ASCENDING = FiltersOrderByDirection.ASCENDING

TRIGGER_RESOURCE_ITEM = TriggerResource.ITEM
TRIGGER_RESOURCE_DATASET = TriggerResource.DATASET
TRIGGER_RESOURCE_ANNOTATION = TriggerResource.ANNOTATION
TRIGGER_RESOURCE_ITEM_STATUS = TriggerResource.ITEM_STATUS

TRIGGER_ACTION_CREATED = TriggerAction.CREATED
TRIGGER_ACTION_UPDATED = TriggerAction.UPDATED
TRIGGER_ACTION_DELETED = TriggerAction.DELETED
TRIGGER_ACTION_STATUS_CHANGED = TriggerAction.STATUS_CHANGED
TRIGGER_ACTION_CLONE = TriggerAction.CLONE

TRIGGER_EXECUTION_MODE_ONCE = TriggerExecutionMode.ONCE
TRIGGER_EXECUTION_MODE_ALWAYS = TriggerExecutionMode.ALWAYS

TRIGGER_TYPE_EVENT = TriggerType.EVENT
TRIGGER_TYPE_CRON = TriggerType.CRON

PACKAGE_INPUT_TYPE_ITEM = PackageInputType.ITEM
PACKAGE_INPUT_TYPE_DATASET = PackageInputType.DATASET
PACKAGE_INPUT_TYPE_ANNOTATION = PackageInputType.ANNOTATION
PACKAGE_INPUT_TYPE_JSON = PackageInputType.JSON
PACKAGE_INPUT_TYPE_MODEL = PackageInputType.MODEL
PACKAGE_INPUT_TYPE_PACKAGE = PackageInputType.PACKAGE
PACKAGE_INPUT_TYPE_SERVICE = PackageInputType.SERVICE
PACKAGE_INPUT_TYPE_PROJECT = PackageInputType.PROJECT
PACKAGE_INPUT_TYPE_EXECUTION = PackageInputType.EXECUTION
PACKAGE_INPUT_TYPE_TASK = PackageInputType.TASK
PACKAGE_INPUT_TYPE_ASSIGNMENT = PackageInputType.ASSIGNMENT
PACKAGE_INPUT_TYPE_RECIPE = PackageInputType.RECIPE
PACKAGE_INPUT_TYPE_STRING = PackageInputType.STRING
PACKAGE_INPUT_TYPE_NUMBER = PackageInputType.NUMBER
PACKAGE_INPUT_TYPE_INT = PackageInputType.INT
PACKAGE_INPUT_TYPE_FLOAT = PackageInputType.FLOAT
PACKAGE_INPUT_TYPE_BOOLEAN = PackageInputType.BOOLEAN
PACKAGE_INPUT_TYPE_DATASETS = PackageInputType.DATASETS
PACKAGE_INPUT_TYPE_ITEMS = PackageInputType.ITEMS
PACKAGE_INPUT_TYPE_ANNOTATIONS = PackageInputType.ANNOTATIONS
PACKAGE_INPUT_TYPE_EXECUTIONS = PackageInputType.EXECUTIONS
PACKAGE_INPUT_TYPE_TASKS = PackageInputType.TASKS
PACKAGE_INPUT_TYPE_ASSIGNMENTS = PackageInputType.ASSIGNMENTS
PACKAGE_INPUT_TYPE_SERVICES = PackageInputType.SERVICES
PACKAGE_INPUT_TYPE_PACKAGES = PackageInputType.PACKAGES
PACKAGE_INPUT_TYPE_PROJECTS = PackageInputType.PROJECTS
PACKAGE_INPUT_TYPE_JSONS = PackageInputType.JSONS
PACKAGE_INPUT_TYPE_STRINGS = PackageInputType.STRINGS
PACKAGE_INPUT_TYPE_NUMBERS = PackageInputType.NUMBERS
PACKAGE_INPUT_TYPE_INTS = PackageInputType.INTS
PACKAGE_INPUT_TYPE_FLOATS = PackageInputType.FLOATS
PACKAGE_INPUT_TYPE_BOOLEANS = PackageInputType.BOOLEANS
PACKAGE_INPUT_TYPE_MODELS = PackageInputType.MODELS
PACKAGE_INPUT_TYPE_RECIPES = PackageInputType.RECIPES

FUNCTION_POST_ACTION_TYPE_DOWNLOAD = SlotPostActionType.DOWNLOAD
FUNCTION_POST_ACTION_TYPE_DRAW_ANNOTATION = SlotPostActionType.DRAW_ANNOTATION
FUNCTION_POST_ACTION_TYPE_NO_ACTION = SlotPostActionType.NO_ACTION

FUNCTION_DISPLAY_SCOPE_RESOURCE_ANNOTATION = SlotDisplayScopeResource.ANNOTATION
FUNCTION_DISPLAY_SCOPE_RESOURCE_ITEM = SlotDisplayScopeResource.ITEM
FUNCTION_DISPLAY_SCOPE_RESOURCE_DATASET = SlotDisplayScopeResource.DATASET
FUNCTION_DISPLAY_SCOPE_RESOURCE_DATASET_QUERY = SlotDisplayScopeResource.DATASET_QUERY
FUNCTION_DISPLAY_SCOPE_RESOURCE_TASK = SlotDisplayScopeResource.TASK

UI_BINDING_PANEL_ALL = UiBindingPanel.ALL
UI_BINDING_PANEL_TABLE = UiBindingPanel.TABLE
UI_BINDING_PANEL_STUDIO = UiBindingPanel.STUDIO
UI_BINDING_PANEL_BROWSER = UiBindingPanel.BROWSER

COMMANDS_STATUS_CREATED = CommandsStatus.CREATED
COMMANDS_STATUS_MAKING_CHILDREN = CommandsStatus.MAKING_CHILDREN
COMMANDS_STATUS_WAITING_CHILDREN = CommandsStatus.WAITING_CHILDREN
COMMANDS_STATUS_IN_PROGRESS = CommandsStatus.IN_PROGRESS
COMMANDS_STATUS_ABORTED = CommandsStatus.ABORTED
COMMANDS_STATUS_CANCELED = CommandsStatus.CANCELED
COMMANDS_STATUS_FINALIZING = CommandsStatus.FINALIZING
COMMANDS_STATUS_SUCCESS = CommandsStatus.SUCCESS
COMMANDS_STATUS_FAILED = CommandsStatus.FAILED
COMMANDS_STATUS_TIMEOUT = CommandsStatus.TIMEOUT

MEMBER_ROLE_OWNER = MemberRole.OWNER
MEMBER_ROLE_DEVELOPER = MemberRole.DEVELOPER
MEMBER_ROLE_ANNOTATOR = MemberRole.ANNOTATOR
MEMBER_ROLE_ANNOTATION_MANAGER = MemberRole.ANNOTATION_MANAGER

MEMBER_ORG_ROLE_OWNER = MemberOrgRole.OWNER
MEMBER_ORG_ROLE_ADMIN = MemberOrgRole.ADMIN
MEMBER_ORG_ROLE_MEMBER = MemberOrgRole.MEMBER
MEMBER_ORG_ROLE_WORKER = MemberOrgRole.WORKER

PACKAGE_REQUIREMENT_OP_EQUAL = RequirementOperator.EQUAL
PACKAGE_REQUIREMENT_OP_GREATER_THAN = RequirementOperator.GREATER_THAN
PACKAGE_REQUIREMENT_OP_LESS_THAN = RequirementOperator.LESS_THAN
PACKAGE_REQUIREMENT_OP_EQUAL_OR_LESS_THAN = RequirementOperator.EQUAL_OR_LESS_THAN
PACKAGE_REQUIREMENT_OP_EQUAL_OR_GREATER_THAN = RequirementOperator.EQUAL_OR_GREATER_THAN

INDEX_DRIVER_V1 = IndexDriver.V1
INDEX_DRIVER_V2 = IndexDriver.V2

CACHE_ACTION_APPLY = CacheAction.APPLY
CACHE_ACTION_DESTROY = CacheAction.DESTROY

POD_TYPE_SMALL = PodType.SMALL
POD_TYPE_MEDIUM = PodType.MEDIUM
POD_TYPE_HIGH = PodType.HIGH

ATTRIBUTES_TYPES_CHECKBOX = AttributesTypes.CHECKBOX
ATTRIBUTES_TYPES_RADIO_BUTTON = AttributesTypes.RADIO_BUTTON
ATTRIBUTES_TYPES_YES_NO = AttributesTypes.YES_NO
ATTRIBUTES_TYPES_SLIDER = AttributesTypes.SLIDER
ATTRIBUTES_TYPES_FREE_TEXT = AttributesTypes.FREE_TEXT

TASK_PRIORITY_LOW = TaskPriority.LOW
TASK_PRIORITY_MEDIUM = TaskPriority.MEDIUM
TASK_PRIORITY_HIGH = TaskPriority.HIGH

CONSENSUS_TASK_TYPE_CONSENSUS = ConsensusTaskType.CONSENSUS
CONSENSUS_TASK_TYPE_QUALIFICATION = ConsensusTaskType.QUALIFICATION
CONSENSUS_TASK_TYPE_HONEYPOT = ConsensusTaskType.HONEYPOT

SERVICE_MODE_TYPE_REGULAR = ServiceModeType.REGULAR
SERVICE_MODE_TYPE_DEBUG = ServiceModeType.DEBUG


================================================
File: dtlpy/__version__.py
================================================
version = '1.105.6'


================================================
File: dtlpy/exceptions.py
================================================
import requests

from . import services


class PlatformException(Exception):
    """
    Dataloop Exceptions
    """
    def __init__(self, error=None, message=None):
        """
        :param error: 'str' status code  out of list
        :param message: `str` describing the error
        """
        self.status_code = None
        self.message = message

        exceptions = {
            '400': BadRequest,
            '401': Unauthorized,
            '403': Forbidden,
            '404': NotFound,
            '408': RequestTimeout,
            '409': Conflict,
            '424': FailedDependency,
            '500': InternalServerError,
            '600': TokenExpired,
            '1001': ShowAnnotationError,
            '1002': ExportAnnotationError,
            '2001': MissingEntity,
            '3001': SDKError,
            '3002': EntityPrintError
        }

        if isinstance(error, requests.models.Response) or isinstance(error, services.AsyncResponse):
            if hasattr(error, 'status_code'):
                self.status_code = str(error.status_code)
            if hasattr(error, 'text'):
                try:
                    self.message = error.json().get('message', error.text)
                except Exception:
                    self.message = error.text

        else:
            self.status_code = error
            self.message = message

        if self.status_code in exceptions:
            raise exceptions[self.status_code](status_code=self.status_code, message=self.message)
        else:
            raise UnknownException(status_code=self.status_code, message=self.message)


class ExceptionMain(Exception):
    def __init__(self, status_code='Unknown Status Code', message='Unknown Error Message'):
        self.status_code = status_code
        self.message = message
        super().__init__(status_code, message)


class NotFound(ExceptionMain):
    pass


class InternalServerError(ExceptionMain):
    pass


class Forbidden(ExceptionMain):
    pass


class Conflict(ExceptionMain):
    pass


class FailedDependency(ExceptionMain):
    pass


class BadRequest(ExceptionMain):
    pass


class Unauthorized(ExceptionMain):
    pass


class RequestTimeout(ExceptionMain):
    pass


class TokenExpired(ExceptionMain):
    pass


class UnknownException(ExceptionMain):
    pass


class MissingEntity(ExceptionMain):
    pass


##########################
# annotations exceptions #
##########################
class ShowAnnotationError(ExceptionMain):
    """ raised when error in annotations drawing"""
    pass


class ExportAnnotationError(ExceptionMain):
    """ raised when error in annotations drawing"""
    pass


class SDKError(ExceptionMain):
    """ raised when error in annotations drawing"""
    pass


class EntityPrintError(ExceptionMain):
    """ raised when error in annotations drawing"""
    pass


================================================
File: dtlpy/new_instance.py
================================================
class Dtlpy:
    from .services.api_client import client as client_api
    from .services.api_client import VerboseLoggingLevel, ApiClient
    from .services import DataloopLogger, DtlpyFilter, check_sdk, Reporter, service_defaults
    from .services.api_reference import api_reference as _api_reference
    from .caches.cache import CacheConfig, CacheType
    from .exceptions import PlatformException
    from . import repositories, exceptions, entities, examples
    from .entities import (
        # main entities
        Project, Dataset, ExpirationOptions, ExportVersion, Trigger, Item, Execution, AnnotationCollection, Annotation,
        Recipe, IndexDriver, AttributesTypes, AttributesRange, Dpk, App, AppModule, AppScope,
        Ontology, Label, Task, TaskPriority, ConsensusTaskType, Assignment, Service, Package, Codebase, Model,
        PackageModule, PackageFunction,
        # annotations
        Box, Cube, Cube3d, Point, Note, Message, Segmentation, Ellipse, Classification, Subtitle, Polyline, Pose, Gis, GisType,
        Description,
        Polygon, Text, FreeText, RefImage,
        # filters
        Filters, FiltersKnownFields, FiltersResource, FiltersOperations, FiltersMethod, FiltersOrderByDirection,
        FiltersKnownFields as KnownFields,
        # triggers
        TriggerResource, TriggerAction, TriggerExecutionMode, TriggerType,
        # faas
        FunctionIO, KubernetesAutoscalerType, KubernetesAutuscalerType, KubernetesRabbitmqAutoscaler, KubernetesAutoscaler, KubernetesRuntime,
        InstanceCatalog, PackageInputType, ServiceType, ServiceModeType,
        PackageSlot, SlotPostAction, SlotPostActionType, SlotDisplayScope, SlotDisplayScopeResource, UiBindingPanel,
        # roberto
        DatasetSubsetType, ModelStatus, PlotSample, ArtifactType, Artifact, ItemArtifact, LinkArtifact, LocalArtifact,
        EntityScopeLevel,
        # features
        FeatureEntityType, Feature, FeatureSet,
        #
        RequirementOperator, PackageRequirement,
        Command, CommandsStatus,
        LocalCodebase, GitCodebase, ItemCodebase, FilesystemCodebase, PackageCodebaseType,
        MemberRole, MemberOrgRole,
        Webhook, HttpMethod,
        ViewAnnotationOptions, AnnotationStatus, AnnotationType,
        ItemStatus, ExecutionStatus, ExportMetadata,
        PromptItem, Prompt, PromptType, ItemGis, Layer,
        ItemLink, UrlLink, LinkTypeEnum,
        Modality, ModalityTypeEnum, ModalityRefTypeEnum,
        Workload, WorkloadUnit, ItemAction,
        PipelineExecution, CycleRerunMethod, PipelineExecutionNode, Pipeline, PipelineConnection,
        PipelineNode, TaskNode, CodeNode, PipelineStats, PipelineSettings,
        PipelineNodeType, PipelineNameSpace, PipelineResumeOption, Variable, CompositionStatus,
        FunctionNode, DatasetNode, PipelineConnectionPort, PipelineNodeIO, Organization, OrganizationsPlans,
        Integration,
        Driver, S3Driver, GcsDriver, AzureBlobDriver, CacheAction, PodType,
        ExternalStorage, IntegrationType, Role, PlatformEntityType, SettingsValueTypes, SettingsTypes,
        SettingsSectionNames,
        SettingScope, BaseSetting, UserSetting, Setting, ServiceSample, ExecutionSample, PipelineExecutionSample,
        ResourceExecution, Message, NotificationEventContext
    )
    from .ml import BaseModelAdapter
    from .utilities import Converter, BaseServiceRunner, Progress, Context, AnnotationFormat
    from .repositories import FUNCTION_END_LINE, PackageCatalog

    def __init__(self, cookie_filepath=None):
        self.client_api = self.ApiClient(cookie_filepath=cookie_filepath)
        self.projects = self.repositories.Projects(client_api=self.client_api)
        self.datasets = self.repositories.Datasets(client_api=self.client_api,
                                                   project=None)
        self.items = self.repositories.Items(client_api=self.client_api,
                                             datasets=self.datasets)
        self.packages = self.repositories.Packages(client_api=self.client_api)
        self.executions = self.repositories.Executions(client_api=self.client_api)
        self.services = self.repositories.Services(client_api=self.client_api)
        self.webhooks = self.repositories.Webhooks(client_api=self.client_api)
        self.triggers = self.repositories.Triggers(client_api=self.client_api)
        self.assignments = self.repositories.Assignments(client_api=self.client_api)
        self.tasks = self.repositories.Tasks(client_api=self.client_api)
        self.dpks = self.repositories.Dpks(client_api=self.client_api)
        self.annotations = self.repositories.Annotations(client_api=self.client_api)
        self.models = self.repositories.Models(client_api=self.client_api)
        self.ontologies = self.repositories.Ontologies(client_api=self.client_api)
        self.recipes = self.repositories.Recipes(client_api=self.client_api)
        self.pipelines = self.repositories.Pipelines(client_api=self.client_api)
        self.pipeline_executions = self.repositories.PipelineExecutions(client_api=self.client_api)
        self.feature_sets = self.repositories.FeatureSets(client_api=self.client_api)
        self.features = self.repositories.Features(client_api=self.client_api)
        self.organizations = self.repositories.Organizations(client_api=self.client_api)
        self.analytics = self.repositories.Analytics(client_api=self.client_api)
        self.integrations = self.repositories.Integrations(client_api=self.client_api)
        self.drivers = self.repositories.Drivers(client_api=self.client_api)
        self.settings = self.repositories.Settings(client_api=self.client_api)
        self.apps = self.repositories.Apps(client_api=self.client_api)
        self.dpks = self.repositories.Dpks(client_api=self.client_api)
        self.messages = self.repositories.Messages(client_api=self.client_api)
        self.compositions = self.repositories.Compositions(client_api=self.client_api)

        self.verbose = self.client_api.verbose
        self.login = self.client_api.login
        self.logout = self.client_api.logout
        self.login_token = self.client_api.login_token
        self.login_secret = self.client_api.login_secret
        self.login_api_key = self.client_api.login_api_key
        self.login_m2m = self.client_api.login_m2m
        self.add_environment = self.client_api.add_environment
        self.setenv = self.client_api.setenv
        self.token_expired = self.client_api.token_expired
        self.info = self.client_api.info
        self.cache_state = self.client_api.cache_state
        self.attributes_mode = self.client_api.attributes_mode
        self.sdk_cache = self.client_api.sdk_cache
        self.platform_settings = self.client_api.platform_settings

    def __del__(self):
        for name, pool in self.client_api._thread_pools.items():
            pool.shutdown()

    def token(self):
        """
        token

        :return: token in use
        """
        return self.client_api.token

    def environment(self):
        """
        environment

        :return: current environment
        """
        return self.client_api.environment

    def init(self):
        """
        init current directory as a Dataloop working directory

        :return:
        """
        from .services import CookieIO
        self.client_api.state_io = CookieIO.init_local_cookie(create=True)
        assert isinstance(self.client_api.state_io, CookieIO)

    def checkout_state(self):
        """
        Return the current checked out state

        :return:
        """
        state = self.client_api.state_io.read_json()
        return state

    class ModalityTypeEnum:
        """
        State enum
        """
        OVERLAY = 'overlay'

    class LinkTypeEnum:
        """
        State enum
        """
        ID = 'id'
        URL = 'url'

    class TriggerResource:
        ITEM = 'Item'
        DATASET = 'Dataset'
        ANNOTATION = 'Annotation'

    class TriggerAction:
        CREATED = 'Created'
        UPDATED = 'Updated'
        DELETED = 'Deleted'

    class TriggerExecutionMode:
        ONCE = 'Once'
        ALWAYS = 'Always'

    class PackageInputType:
        ITEM = 'Item'
        DATASET = 'Dataset'
        ANNOTATION = 'Annotation'
        JSON = 'Json'

    class FiltersResource:
        ITEM = 'items'
        ANNOTATION = 'annotations'

    class FiltersOperations:
        OR = 'or'
        AND = 'and'
        IN = 'in'
        NOT_EQUAL = 'ne'
        EQUAL = 'eq'
        GREATER_THAN = 'gt'
        LESS_THAN = 'lt'

    class FiltersMethod:
        OR = 'or'
        AND = 'and'

    class FiltersOrderByDirection:
        DESCENDING = 'descending'
        ASCENDING = 'ascending'

    class KnownFields:
        DIR = 'dir'
        ANNOTATED = 'annotated'
        FILENAME = 'filename'
        CREATED_AT = 'createdAt'
        UPDATED_AT = 'updatedAt'
        LABEL = 'label'
        NAME = 'name'
        HIDDEN = 'hidden'

    class ExecutionStatus:
        SUCCESS = 'success'
        FAILED = 'failed'
        IN_PROGRESS = 'inProgress'
        CREATED = 'created'

    class HttpMethod:
        GET = 'GET'
        POST = 'POST'
        DELETE = 'DELETE'
        PATCH = 'PATCH'

    class ViewAnnotationOptions:
        JSON = 'json'
        MASK = 'mask'
        INSTANCE = 'instance'

    class AnnotationFormat:
        COCO = 'coco'
        VOC = 'voc'
        YOLO = 'yolo'
        DATALOOP = 'dataloop'

    class InstanceCatalog:
        REGULAR_MICRO = 'regular-micro'
        REGULAR_XS = 'regular-xs'
        REGULAR_S = 'regular-s'
        REGULAR_M = 'regular-m'
        REGULAR_L = 'regular-l'
        HIGHMEM_MICRO = 'highmem-micro'
        HIGHMEM_XS = 'highmem-xs'
        HIGHMEM_S = 'highmem-s'
        HIGHMEM_M = 'highmem-m'
        HIGHMEM_L = 'highmem-l'
        GPU_K80_S = "gpu-k80-s"
        GPU_K80_M = "gpu-k80-m"
        GPU_T4_S = "gpu-t4-s"
        GPU_T4_M = "gpu-t4-m"

    class LoggingLevel:
        DEBUG = 'debug'
        WARNING = 'warning'
        CRITICAL = 'critical'
        INFO = 'info'


================================================
File: dtlpy/assets/__init__.py
================================================
import os
from .service_runners import service_runner_paths


class PATHS:
    PACKAGE_FILENAME = 'package.json'
    GITIGNORE_FILENAME = 'package_gitignore'
    MAIN_FILENAME = 'main.py'
    MOCK_FILENAME = 'mock.json'
    MODEL_ADAPTER = 'model_adapter.py'
    PARTIAL_MAIN = 'main_partial.py'
    MODULE_A_FILENAME = 'first_module_class.py'
    MODULE_B_FILENAME = 'second_module_class.py'
    REQUIREMENTS_FILENAME = 'requirements.txt'
    APP_JSON_FILENAME = 'dataloop.json'

    ASSETS_PATH = os.path.dirname(__file__)
    ASSETS_MAIN_FILEPATH = os.path.join(ASSETS_PATH, MAIN_FILENAME)
    ASSETS_MOCK_FILEPATH = os.path.join(ASSETS_PATH, MOCK_FILENAME)
    ASSETS_PACKAGE_FILEPATH = os.path.join(ASSETS_PATH, PACKAGE_FILENAME)
    ASSETS_GITIGNORE_FILEPATH = os.path.join(ASSETS_PATH, GITIGNORE_FILENAME)
    ASSETS_MODEL_ADAPTER_FILEPATH = os.path.join(ASSETS_PATH, MODEL_ADAPTER)
    PARTIAL_MAIN_FILEPATH = os.path.join(ASSETS_PATH, PARTIAL_MAIN)


paths = PATHS()


================================================
File: dtlpy/assets/main.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, item, progress=None):
        """
        Write your main package service here

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Item received! filename: {}'.format(item.filename))
        builder = item.annotations.builder()
        builder.add(dl.Classification(label='from_function'))
        item.annotations.upload(builder)
        print('Annotation uploaded!')

    def hello(self, progress=None):
        """
        Write your main package service here

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Hello World from Dataloop :)')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/main_partial.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """


================================================
File: dtlpy/assets/mock.json
================================================
{
  "init_params": {},
  "inputs": [
    {
      "name": "item",
      "value": {
        "dataset_id": "",
        "item_id": ""
      }
    }
  ]
}

================================================
File: dtlpy/assets/model_adapter.py
================================================
import dtlpy as dl


class ModelAdapter(dl.BaseModelAdapter):
    """
    Specific Model adapter.
    The class bind a dl.Model entity with the package code
    """
    # TODO:
    #   1) docstring for your ModelAdapter
    #   2) implement the virtual methods for full adapter support
    #   3) add your _defaults

    _defaults = {}

    def __init__(self, model_entity):
        super(ModelAdapter, self).__init__(model_entity)

    # ===============================
    # NEED TO IMPLEMENT THESE METHODS
    # ===============================

    def load(self, local_path, **kwargs):
        """ Loads model and populates self.model with a `runnable` model

            Virtual method - need to implement

            This function is called by load_from_model (download to local and then loads)

        :param local_path: `str` directory path in local FileSystem
        """
        raise NotImplementedError("Please implement 'load' method in {}".format(self.__class__.__name__))

    def save(self, local_path, **kwargs):
        """ saves configuration and weights locally

            Virtual method - need to implement

            the function is called in save_to_model which first save locally and then uploads to model entity

        :param local_path: `str` directory path in local FileSystem
        """
        raise NotImplementedError("Please implement 'save' method in {}".format(self.__class__.__name__))

    def train(self, data_path, dump_path, **kwargs):
        """
        Virtual method - need to implement
        Train the model according to data in local_path and save everything to dump_path

        :param data_path: `str` local File System path to where the data was downloaded and converted at
        :param dump_path: `str` local File System path where to dump training mid-results (checkpoints, logs...)
        """
        raise NotImplementedError("Please implement 'train' method in {}".format(self.__class__.__name__))

    def predict(self, batch, **kwargs):
        """ Model inference (predictions) on batch of images

            Virtual method - need to implement

        :param batch: `np.ndarray`
        :return: `list[dl.AnnotationCollection]` each collection is per each image / item in the batch
        """
        raise NotImplementedError("Please implement 'predict' method in {}".format(self.__class__.__name__))

    def convert(self, data_path, **kwargs):
        """ Convert Dataloop structure data to model structured

            Virtual method - need to implement

            e.g. take dlp dir structure and construct annotation file

        :param data_path: `str` local File System directory path where
                           we already downloaded the data from dataloop platform
        :return:
        """
        raise NotImplementedError("Please implement 'convert' method in {}".format(self.__class__.__name__))

    # NOT IN USE
    def convert_dlp(self, items: dl.entities.PagedEntities):
        """ This should implement similar to convert only to work on dlp items.  ->
                   -> meaning create the converted version from items entities"""
        # TODO
        pass


================================================
File: dtlpy/assets/package.json
================================================
{
  "name": "default-package",
  "modules": [
    {
      "name": "default_module",
      "entryPoint": "main.py",
      "initInputs": [],
      "functions": [
        {
          "name": "run",
          "description": "this description for your service",
          "input": [
            {
              "name": "item",
              "type": "Item"
            }
          ],
          "output": []
        },
        {
          "name": "hello",
          "description": "hello function",
          "input": [],
          "output": []
        }
      ]
    }
  ],
  "services": [
    {
      "name": "default-service",
      "packageRevision": "latest",
      "runtime": {
        "gpu": false,
        "replicas": 1,
        "concurrency": 10
      },
      "triggers": [
        {
          "name": "name",
          "active": true,
          "type": "Event",
          "spec": {
            "filter": {},
            "resource": "Item",
            "actions": [
              "Created"
            ],
            "operation": {
              "type": "function",
              "functionName": "run"
            },
            "executionMode": "Once"
          }
        }
      ],
      "initParams": {},
      "moduleName": "default_module"
    }
  ]
}


================================================
File: dtlpy/assets/package_catalog.json
================================================
{
  "name": "default_package",
  "modules": [
    {
      "name": "default_module",
      "entryPoint": "main.py",
      "initInputs": [],
      "functions": [
        {
          "name": "run",
          "description": "this description for your service",
          "input": [
            {
              "name": "item",
              "type": "Item"
            }
          ],
          "output": []
        },
        {
          "name": "hello",
          "description": "hello function",
          "input": [],
          "output": []
        }
      ]
    }
  ]
}


================================================
File: dtlpy/assets/package_gitignore
================================================

# Default .gitignore
# Files within the following rules will be excluded from codebase

### Code ###
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json

### Images ###
# JPEG
*.jpg
*.jpeg
*.jpe
*.jif
*.jfif
*.jfi

# JPEG 2000
*.jp2
*.j2k
*.jpf
*.jpx
*.jpm
*.mj2

# JPEG XR
*.jxr
*.hdp
*.wdp

# Graphics Interchange Format
*.gif

# RAW
*.raw

# Web P
*.webp

# Portable Network Graphics
*.png

# Animated Portable Network Graphics
*.apng

# Multiple-image Network Graphics
*.mng

# Tagged Image File Format
*.tiff
*.tif

# Scalable Vector Graphics
*.svg
*.svgz

# Portable Document Format
*.pdf

# X BitMap
*.xbm

# BMP
*.bmp
*.dib

# ICO
*.ico

# 3D Images
*.3dm
*.max

### PyCharm ###
# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and WebStorm
# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839

# User-specific stuff
.idea/**/workspace.xml
.idea/**/tasks.xml
.idea/**/usage.statistics.xml
.idea/**/dictionaries
.idea/**/shelf

# Generated files
.idea/**/contentModel.xml

# Sensitive or high-churn files
.idea/**/dataSources/
.idea/**/dataSources.ids
.idea/**/dataSources.local.xml
.idea/**/sqlDataSources.xml
.idea/**/dynamic.xml
.idea/**/uiDesigner.xml
.idea/**/dbnavigator.xml

# Gradle
.idea/**/gradle.xml
.idea/**/libraries

# Gradle and Maven with auto-import
# When using Gradle or Maven with auto-import, you should exclude module files,
# since they will be recreated, and may cause churn.  Uncomment if using
# auto-import.
# .idea/modules.xml
# .idea/*.iml
# .idea/modules
# *.iml
# *.ipr

# CMake
cmake-build-*/

# Mongo Explorer plugin
.idea/**/mongoSettings.xml

# File-based project format
*.iws

# IntelliJ
out/

# mpeltonen/sbt-idea plugin
.idea_modules/

# JIRA plugin
atlassian-ide-plugin.xml

# Cursive Clojure plugin
.idea/replstate.xml

# Crashlytics plugin (for Android Studio and IntelliJ)
com_crashlytics_export_strings.xml
crashlytics.properties
crashlytics-build.properties
fabric.properties

# Editor-based Rest Client
.idea/httpRequests

# Android studio 3.1+ serialized cache file
.idea/caches/build_file_checksums.ser

### PyCharm Patch ###
# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721

# *.iml
# modules.xml
# .idea/misc.xml
# *.ipr

# Sonarlint plugin
.idea/**/sonarlint/

# SonarQube Plugin
.idea/**/sonarIssues.xml

# Markdown Navigator plugin
.idea/**/markdown-navigator.xml
.idea/**/markdown-navigator/

### Python ###
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# Mr Developer
.mr.developer.cfg
.project
.pydevproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

### Video ###
*.3g2
*.3gp
*.asf
*.asx
*.avi
*.flv
*.mov
*.mp4
*.mpg
*.rm
*.swf
*.vob
*.wmv

### VirtualEnv ###
# Virtualenv
# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/
[Bb]in
[Ii]nclude
[Ll]ib
[Ll]ib64
[Ll]ocal
[Ss]cripts
pyvenv.cfg
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
pip-selfcheck.json

### h5 ###
*.h5

### dataloop ###
.dataloop/
**/.dataloop/**

================================================
File: dtlpy/assets/voc_annotation_template.xml
================================================
<annotation>
    <folder>{{ folder }}</folder>
    <filename>{{ filename }}</filename>
    <path>{{ path }}</path>
    <source>
        <database>{{ database }}</database>
    </source>
    <size>
        <width>{{ width }}</width>
        <height>{{ height }}</height>
        <depth>{{ depth }}</depth>
    </size>
    <segmented>{{ segmented }}</segmented>
{% for object in objects %}    <object>
        <name>{{ object.name }}</name>
        <attributes>{{ object.attributes }}</attributes>
        <bndbox>
            <xmin>{{ object.xmin }}</xmin>
            <ymin>{{ object.ymin }}</ymin>
            <xmax>{{ object.xmax }}</xmax>
            <ymax>{{ object.ymax }}</ymax>
        </bndbox>
    </object>{% endfor %}
</annotation>

================================================
File: dtlpy/assets/code_server/config.yaml
================================================
bind-addr: 127.0.0.1:3100
auth: none
cert: false

================================================
File: dtlpy/assets/code_server/installation.sh
================================================
#!/bin/bash
cd /tmp/app

PORT=${1?}

# check if curl command exists
if ! command -v curl &> /dev/null
then
    echo "curl could not be found. installing..."
    apt update -y
    apt install -y curl
fi

# check if code-server command exists
if ! command -v code-server &> /dev/null
then
    echo "code-server could not be found. installing..."
    bash <(curl -s https://raw.githubusercontent.com/coder/code-server/main/install.sh)
    code-server --install-extension ms-python.python
    code-server --install-extension okteto.remote-kubernetes
    code-server --install-extension ms-vscode-remote.vscode-remote-extensionpack
    code-server --install-extension ms-vscode-remote.remote-containers
    code-server --install-extension ms-vscode-remote.remote-wsl
fi
code-server --bind-addr 0.0.0.0:${PORT} --auth none --cert false /tmp/app

================================================
File: dtlpy/assets/code_server/launch.json
================================================
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true,
            "cwd": "${fileDirname}"
        }
    ]
}

================================================
File: dtlpy/assets/code_server/settings.json
================================================
{
    "python.defaultInterpreterPath": "/usr/local/bin/python"
}

================================================
File: dtlpy/assets/service_runners/__init__.py
================================================
import os


class PATHS:
    MULTI_METHOD = 'multi_method.py'
    MULTI_METHOD_JSON = 'multi_method_json.py'
    MULTI_METHOD_ITEM = 'multi_method_item.py'
    SINGLE_METHOD_DATASET = 'single_method_dataset.py'
    SINGLE_METHOD = 'single_method.py'
    SINGLE_METHOD_MULTI_INPUT = 'single_method_multi_input.py'
    MULTI_METHOD_DATASET = 'multi_method_dataset.py'
    SINGLE_METHOD_JSON = 'single_method_json.py'
    SINGLE_METHOD_ITEM = 'single_method_item.py'
    SINGLE_METHOD_ANNOTATION = 'single_method_annotation.py'
    MULTI_METHOD_ANNOTATION = 'multi_method_annotation.py'
    CONVERTER = 'converter.py'

    ASSETS_PATH = os.path.dirname(__file__)
    MULTI_METHOD_SR_PATH = os.path.join(ASSETS_PATH, MULTI_METHOD)
    MULTI_METHOD_JSON_SR_PATH = os.path.join(ASSETS_PATH, MULTI_METHOD_JSON)
    MULTI_METHOD_ITEM_SR_PATH = os.path.join(ASSETS_PATH, MULTI_METHOD_ITEM)
    SINGLE_METHOD_DATASET_SR_PATH = os.path.join(ASSETS_PATH, SINGLE_METHOD_DATASET)
    SINGLE_METHOD_SR_PATH = os.path.join(ASSETS_PATH, SINGLE_METHOD)
    MULTI_METHOD_DATASET_SR_PATH = os.path.join(ASSETS_PATH, MULTI_METHOD_DATASET)
    SINGLE_METHOD_JSON_SR_PATH = os.path.join(ASSETS_PATH, SINGLE_METHOD_JSON)
    SINGLE_METHOD_ITEM_SR_PATH = os.path.join(ASSETS_PATH, SINGLE_METHOD_ITEM)
    SINGLE_METHOD_ANNOTATION_SR_PATH = os.path.join(ASSETS_PATH, SINGLE_METHOD_ANNOTATION)
    MULTI_METHOD_ANNOTATION_SR_PATH = os.path.join(ASSETS_PATH, MULTI_METHOD_ANNOTATION)
    SINGLE_METHOD_MULTI_INPUT_SR_PATH = os.path.join(ASSETS_PATH, SINGLE_METHOD_MULTI_INPUT)
    CONVERTER_SR_PATH = os.path.join(ASSETS_PATH, CONVERTER)


service_runner_paths = PATHS()


================================================
File: dtlpy/assets/service_runners/converter.py
================================================
import dtlpy as dl
import logging
import shutil
import tempfile
import os
import zipfile
import datetime

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, dataset, query=None, progress=None):
        """
        In this example we create a task for annotated items in dataset

        :param query: Dictionary
        :param dataset: dl.Dataset
        :param progress: Use this to update the progress of your package
        :return:
        """
        local_path = tempfile.mkdtemp()

        try:

            converted_folder = os.path.join(local_path, dataset.name)
            filters = None
            if query is not None:
                filters = dl.Filters(resource=dl.FiltersResource.ITEM, custom_filter=query)

            pages = dataset.items.list(filters=filters)

            for i_page, page in enumerate(pages):
                for item in page:
                    self._convert_single_item(item=item, local_path=converted_folder)
                progress.update(status='inProgress', progress=(i_page + 1) / pages.total_pages_count)

            zip_filename = os.path.join(local_path,
                                        '{}_{}.zip'.format(dataset.name, int(datetime.datetime.now().timestamp())))
            self._zip_directory(zip_filename=zip_filename, directory=converted_folder)

            zip_item = dataset.items.upload(local_path=zip_filename,
                                            remote_path='/my_converted_annotations',
                                            overwrite=True)

            return zip_item.id

        except Exception:
            # implement exception handling
            pass
        finally:
            shutil.rmtree(local_path)

    def _convert_single_item(self, item, local_path):
        """
        Implement single item conversion here and save converted files to local path
        :param item:
        :return:
        """
        pass

    @staticmethod
    def _zip_directory(zip_filename, directory):
        """
        Method to zip a directory
        :param zip_filename:
        :param directory:
        :return:
        """
        zip_file = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    filepath = os.path.join(root, file)
                    zip_file.write(filepath, arcname=os.path.relpath(filepath, directory))
        finally:
            zip_file.close()


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/multi_method.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def first_method(self, progress=None):
        """
        In this example we print print 'Hello World'

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Hello World from Dataloop :)')

    def second_method(self, progress=None):
        """
        In this example we print print 'Hello World'

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Hello World from Dataloop :)')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/multi_method_annotation.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def first_method(self, annotation, progress=None):
        """
        In this example we update item's metadata if annotation id of specific type

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(annotation, dl.Annotation)
        if annotation.label == 'box' and annotation.label == 'dog':
            logger.info('Dog was detected in item')
            annotation.attributes.append('Dog detection')
            annotation.item.metadata['dogInPicture'] = True
            annotation.update()
            annotation.item.update()
        logger.info('Function finished successfully!')

    def second_method(self, annotation, progress=None):
        """
        In this example we update item's metadata if annotation id of specific type

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(annotation, dl.Annotation)


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/multi_method_dataset.py
================================================
import datetime

import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def first_method(self, dataset, progress=None):
        """
        Write your main package service here

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(dataset, dl.entities.Dataset)
        filters = dl.Filters(field='annotated', values=True)
        filters.add_join(field='label', values='person')
        task = dataset.tasks.create(task_name='AutomatedTask',
                                    due_date=datetime.datetime.now().timestamp() + 60 * 60 * 24 * 7,
                                    assignee_ids=['annotator1@dataloop.ai', 'annotator2@dataloop.ai'])
        logger.info('Task created successfully. Task name: {}'.format(task.name))

    def second_method(self, dataset, progress=None):
        """
        Write your main package service here

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(dataset, dl.entities.Dataset)


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/multi_method_item.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def first_method(self, item, progress=None):
        """
        n this example we upload an annotation to item

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Item received! filename: {}'.format(item.filename))
        builder = item.annotations.builder()
        builder.add(dl.Classification(label='from_function'))
        item.annotations.upload(builder)
        print('Annotation uploaded!')

    def second_method(self, item, progress=None):
        """
        Write your main package service here

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(item, dl.Item)


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/multi_method_json.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def first_method(self, config, progress=None):
        """
        In this example we copy dataset items by query to a new dataset

        :param config: This is a json input
        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        source_dataset = dl.datasets.get(dataset_id=config['source_dataset_id'])
        filters = dl.Filters(custom_filter=config['query'])
        new_dataset = source_dataset.project.datasets.create(dataset_name='{}-copy'.format(source_dataset.name),
                                                             labels=source_dataset.labels)
        new_dataset.items.upload(local_path=source_dataset.items.download(filters=filters))
        logger.info('Dataset copied successfully')

    def second_method(self, config, progress=None):
        """
        Write your main package service here

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/single_method.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, progress=None):
        """
        In this example we print print 'Hello World'

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Hello World from Dataloop :)')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/single_method_annotation.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, annotation, progress=None):
        """
        In this example we update item's metadata if annotation id of specific type

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(annotation, dl.Annotation)
        if annotation.label == 'box' and annotation.label == 'dog':
            logger.info('Dog was detected in item')
            annotation.attributes.append('Dog detection')
            annotation.item.metadata['dogInPicture'] = True
            annotation.update()
            annotation.item.update()
        logger.info('Function finished successfully!')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/single_method_dataset.py
================================================
import dtlpy as dl
import logging
import datetime

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, dataset, progress=None):
        """
        In this example we create a task for annotated items in dataset

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(dataset, dl.entities.Dataset)
        filters = dl.Filters(field='annotated', values=True)
        filters.add_join(field='label', values='person')
        task = dataset.tasks.create(task_name='AutomatedTask',
                                    due_date=datetime.datetime.now().timestamp() + 60*60*24*7,
                                    assignee_ids=['annotator1@dataloop.ai', 'annotator2@dataloop.ai'])
        logger.info('Task created successfully. Task name: {}'.format(task.name))


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/single_method_item.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, item, progress=None):
        """
        In this example we upload an annotation to item

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        progress.update(status='inProgress', progress=0)
        print('Item received! filename: {}'.format(item.filename))
        builder = item.annotations.builder()
        builder.add(dl.Classification(label='from_function'))
        item.annotations.upload(builder)
        print('Annotation uploaded!')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/single_method_json.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, config, progress=None):
        """
        In this example we copy dataset items by query to a new dataset

        :param config: This is a json input
        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        source_dataset = dl.datasets.get(dataset_id=config['source_dataset_id'])
        filters = dl.Filters(custom_filter=config['query'])
        new_dataset = source_dataset.project.datasets.create(dataset_name='{}-copy'.format(source_dataset.name),
                                                             labels=source_dataset.labels)
        new_dataset.items.upload(local_path=source_dataset.items.download(filters=filters))
        logger.info('Dataset copied successfully')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/assets/service_runners/single_method_multi_input.py
================================================
import dtlpy as dl
import logging

logger = logging.getLogger(name='dtlpy')


class ServiceRunner(dl.BaseServiceRunner):
    """
    Package runner class

    """

    def __init__(self):
        """
        Init package attributes here

        :return:
        """

    def run(self, item, dataset, config, annotation, progress=None):
        """
        In this example we upload annotation to item, update its metadata and add it to a task in the dataset

        :param progress: Use this to update the progress of your package
        :return:
        """
        # these lines can be removed
        assert isinstance(progress, dl.Progress)
        assert isinstance(item, dl.Item)
        assert isinstance(dataset, dl.entities.Dataset)
        assert isinstance(annotation, dl.Annotation)
        item.annotations.upload(annotations=annotation)
        item.metadata['user'] = config['metadata']
        item.update()
        if annotation.label == 'street':
            task = dataset.tasks.get(task_name='streetAnnotation')
            task.add_items(items=item)
        logger.info('Function completed successfully')


if __name__ == "__main__":
    """
    Run this main to locally debug your package
    """
    dl.packages.test_local_package()


================================================
File: dtlpy/caches/base_cache.py
================================================

class BaseCache(object):
    def set(self, key, value):
        """
        set or add a key and value to the cache
        :param key: str or int type of key
        :param value: pickled value
        :return:
        """
        pass

    def get(self, key):
        """
        get the value of the key from the cache
        :param key: str or int type of key
        :return: the value of the key
        """
        pass

    def delete(self, key):
        """
        delete the element from the cache
        :param key: str or int type of key
        :return:
        """
        pass

    def keys(self):
        """
        return all the cache keys
        :return: list of the keys
        """
        pass

================================================
File: dtlpy/caches/cache.py
================================================
import json
import os
import shutil
import time
from enum import Enum
from pathlib import Path
import mmap
from filelock import FileLock
import logging
import base64

from .dl_cache import DiskCache
from .redis_cache import RedisCache
from .filesystem_cache import FileSystemCache

logger = logging.getLogger(name='dtlpy')


class ObjectType(str, Enum):
    BINARY = "binary"
    OBJECT = "object"


class CacheType(Enum):
    DISKCACHE = 'diskcache'
    REDIS = 'redis'
    FILESYSTEM = 'filesystem'


class CacheConfig:
    def __init__(self, cache_type=CacheType.DISKCACHE, ttl=1000, level=1, options=None):
        """
        Cache config settings

        :param CacheType cache_type: CacheType diskcache, filesystem, redis
        :param int ttl: time to hold the item in the cache in seconds (SEC)
        :param int level: cache level
        :param dict options: the configs for the caches types
        """
        if isinstance(cache_type, CacheType):
            cache_type = cache_type.value
        if isinstance(cache_type, str) and cache_type not in CacheType._value2member_map_:
            raise ValueError('cache type must be redis or diskcache')

        self.type = cache_type
        self.ttl = ttl
        self.level = level
        self.options = options

    def to_string(self):
        """
        convert object to base 64 string
        """
        base64_bytes = base64.b64encode(json.dumps(self.to_json()).encode("ascii"))
        base64_string = base64_bytes.decode("ascii")
        return base64_string

    @staticmethod
    def from_string(cls, base64_string):
        """
        convert from base 64 string to the class object

        :param str base64_string: string in base64 the have a json configs
        """
        base64_bytes = base64_string.encode("ascii")
        sample_string_bytes = base64.b64decode(base64_bytes)
        _json = json.loads(sample_string_bytes.decode("ascii"))
        return cls(cache_type=_json.get('type', CacheType.DISKCACHE),
                   ttl=_json.get('ttl', 1000),
                   level=_json.get('level', 1),
                   options=_json.get('options', None))

    def to_json(self):
        """
        convert the class to json
        """
        return {
            'type': self.type,
            'ttl': self.ttl,
            'level': self.level,
            'options': self.options,
        }

    @staticmethod
    def from_json(cls, _json):
        """
        make a class attribute from json

        :param _json: _json have the class attributes
        """
        if isinstance(_json, str):
            _json = json.loads(_json)
        return cls(cache_type=_json.get('type', CacheType.DISKCACHE),
                   ttl=_json.get('ttl', 1000),
                   level=_json.get('level', 1),
                   options=_json.get('options', None))


class CacheKey:
    def __init__(self,
                 master_type='**',
                 master_id='**',
                 entity_type='**',
                 entity_id='*',
                 object_type=ObjectType.OBJECT):
        """
        :param str master_type: master type
        :param str master_id: master id
        :param str entity_type: entity type
        :param str entity_id: entity id
        :param str object_type: object type object/binary
        """
        self.master_type = master_type
        self.master_id = master_id
        self.entity_type = entity_type
        self.entity_id = entity_id
        self.object_type = object_type

    def get(self):
        """
        return the build key
        """
        return os.path.join(self.master_type, self.master_id, self.entity_type, self.entity_id, self.object_type)

    def get_key(self):
        """
        return the build key
        """
        return os.path.join(self.entity_type, self.entity_id, self.object_type)


class CacheManger:
    def __init__(self, cache_configs: list, bin_cache_size=1000):
        """
        Cache manger for config and mange the cache

        :param cache_configs: CacheConfig object
        :param bin_cache_size: size on MB for binary cache
        """
        self.cache_levels = dict()
        self._max_level = 1
        self.bin_cache_size = bin_cache_size
        self.bin_cache_path = os.environ['DEFAULT_CACHE_PATH']
        self._current_bin_cache_size = 0
        for config in cache_configs:
            try:
                self.cache_levels[config.level] = self._load_cache_handler(config)
                if config.level < self._max_level:
                    self._max_level = config.level
            except:
                raise "Failed to build Cache"

        self.parent_dict = {
            "annotations": 'items',
            "items": 'datasets',
            "datasets": 'projects',
            "projects": 'org',
            "org": '',
            "annotationtasks": 'datasets',
            "assignments": 'annotationtasks',
            "models": 'packages',
            "packages": 'projects',
            "services": 'packages',
        }

    def _load_cache_handler(self, config: CacheConfig):
        """
        the function the build the cache form the configs that get
        """
        from ..services import DataloopLogger
        cache = None
        if config.type == CacheType.REDIS.value:
            try:
                cache = RedisCache(options=config.options, ttl=config.ttl)
            except:
                logger.warning("Failed to build Redis")
                raise Exception("Failed to build Redis")

        elif config.type == CacheType.DISKCACHE.value:
            cache = DiskCache(name='object_cache', options=config.options, ttl=config.ttl)
        elif config.type == CacheType.FILESYSTEM.value:
            cache = FileSystemCache(options=config.options, ttl=config.ttl)
            DataloopLogger.clean_dataloop_cache(cache_path=cache.root_dir,
                                                max_param={'max_time': cache.ttl})
        DataloopLogger.clean_dataloop_cache(cache_path=self.bin_cache_path,
                                            max_param={'max_time': config.ttl})
        return cache

    def get(self, key: CacheKey):
        """
        Cache get

        :param CacheKey key: CacheKey object
        :return: success, list of the get result
        """
        res = []
        success = False
        for i in range(1, self._max_level + 1):
            res = self.cache_levels[i].get(key=key.get_key())
            if res:
                success = True
                break
        return success, res

    def ping(self):
        """
        Cache ping check if connection is working
        """
        try:
            for i in range(1, self._max_level + 1):
                self.cache_levels[i].ping()
        except Exception as e:
            raise Exception('cache connection failed ')


    def set(self, key: str, value):
        """
        Cache set, add or update the key value

        :param CacheKey key: CacheKey object
        :param value: value to set
        """
        if isinstance(value, dict):
            value = json.dumps(value)
        self.cache_levels[1].set(key, value)

    def _delete_parent(self, key: CacheKey, level):
        parent_key = CacheKey(master_type=self.parent_dict[key.entity_type],
                              entity_type=key.entity_type,
                              entity_id=key.entity_id,
                              object_type='*')
        list_keys = self.cache_levels[level].list(pattern=parent_key.get())
        for k in list_keys:
            if 'binary' in k:
                val = self.cache_levels[level].get(key=k)
                if os.path.isfile(val):
                    os.remove(val)
            self.cache_levels[level].delete(k)

    def delete(self, key: CacheKey):
        """
        Cache delete

        :param CacheKey key: CacheKey object
        """
        for i in range(1, self._max_level + 1):
            self.cache_levels[i].delete(key.get_key())
            self._delete_parent(key=key, level=i)
            key.object_type = '*'
            list_keys = self.cache_levels[i].list(pattern=key.get_key())
            for k in list_keys:
                val = self.cache_levels[i].get(key=k)
                self.cache_levels[i].delete(k)
                if 'binary' in k:
                    if os.path.isfile(val):
                        os.remove(val)
                    continue
                e_type, e_id, e_obj = val.split('\\')
                self.delete(key=CacheKey(entity_type=e_type, entity_id=e_id, object_type=e_obj))

    def build_cache_key(self, entity_json: dict):
        """
        Build a format of the cache key from the entity json we get

        :param dict entity_json: json of an entity
        :return: CacheKey object
        """
        child_entity = False
        if 'url' in entity_json:
            split_url = entity_json['url'].split('/')
            entity_type = split_url[-2]
            child_entity = True
        elif 'org' in entity_json:
            entity_type = 'projects'
        else:
            entity_type = 'org'
        entity_id = entity_json['id']
        master_type = self.parent_dict[entity_type]
        master_id = '**'
        if child_entity:
            master_id_key = master_type[:-1] + 'Id'
            if master_id_key in entity_json:
                master_id = entity_json[master_id_key]
            elif master_type in entity_json:
                master_id = entity_json[master_type][0]
        elif entity_type == 'projects':
            master_id = entity_json[master_type]['id']

        return CacheKey(master_type=master_type, master_id=master_id, entity_type=entity_type, entity_id=entity_id)

    def _update_config_file(self, filepath: str, update: bool, size: float = 0):
        """
        Update the config file the have all the details about binary cache

        :param str filepath: path of the file the work on
        :param bool update: if True update the use of the file
        :param int size: file size
        """
        config_file_path = os.path.join(self.bin_cache_path, 'cacheConfig.json')
        if os.path.isfile(config_file_path):
            with FileLock(config_file_path + ".lock"):
                with open(config_file_path, mode="r", encoding="utf-8") as con:
                    with mmap.mmap(con.fileno(), length=0, access=mmap.ACCESS_READ) as mmap_obj:
                        text = mmap_obj.read().decode('utf8').replace("'", '"')
                        config_file = json.loads(text)
        else:
            config_file = {'size': 0, 'keys': []}

        if update and filepath in config_file['keys']:
            config_file['keys'].remove(filepath)

        if filepath not in config_file['keys']:
            config_file['keys'].append(filepath)
        config_file['size'] += size
        self._current_bin_cache_size = config_file['size']
        json_object = json.dumps(config_file, indent=4)
        with FileLock(config_file_path + ".lock"):
            with open(config_file_path, mode="w", encoding="utf-8") as outfile:
                outfile.write(json_object)

    def _lru_cache(self):
        """
        Make lru on the binary cache remove 30% of the files
        """
        config_file_path = os.path.join(self.bin_cache_path, 'cacheConfig.json')
        with FileLock(config_file_path + ".lock"):
            with open(config_file_path, mode="r", encoding="utf-8") as con:
                with mmap.mmap(con.fileno(), length=0, access=mmap.ACCESS_READ) as mmap_obj:
                    text = mmap_obj.read().decode('utf8').replace("'", '"')
                    config_file = json.loads(text)

        size = config_file['size']
        end = 70 / 100 * self.bin_cache_size

        while size > end and len(config_file['keys']) > 1:
            to_delete = config_file['keys'][0]

            size -= (Path(to_delete).stat().st_size / 1000000)
            os.remove(to_delete)
            config_file['keys'].remove(to_delete)

        config_file['size'] = size
        json_object = json.dumps(config_file, indent=4)

        with FileLock(config_file_path + ".lock"):
            with open(config_file_path, "w") as outfile:
                outfile.write(json_object)

    def read_stream(self, request_path, dataset_id=None):
        """
        Cache binary get

        :param str request_path: the request
        :param str dataset_id: dataset id of the binary object
        :return: success, list of the get result
        """
        entity_id = request_path.split('/')[-2]
        key = CacheKey(master_type='datasets',
                       master_id=dataset_id,
                       entity_id=entity_id,
                       entity_type='items',
                       object_type=ObjectType.BINARY.value)
        hit, response = self.get(key=key)
        if hit:
            source_path = os.path.normpath(response[0])
            self._update_config_file(filepath=source_path, update=True)
            return hit, [source_path]
        else:
            return False, None

    def write_stream(self,
                     request_path,
                     response=None,
                     buffer=None,
                     file_name=None,
                     entity_id=None,
                     dataset_id=None
                     ):
        """
        Cache binary set

        :param  request_path: the request
        :param  response: the response of stream
        :param  buffer: the steam buffer
        :param  file_name: the file name
        :param  entity_id: entity id
        :param  dataset_id: dataset id of the binary object
        :return: the file path of the binary
        """
        if entity_id is None:
            entity_id = request_path.split('/')[-2]
        key = CacheKey(master_type='datasets',
                       master_id=dataset_id,
                       entity_id=entity_id,
                       entity_type='items',
                       object_type=ObjectType.BINARY)
        filepath = self.bin_cache_path
        if file_name is None:
            file_name = (dict(response.headers)['Content-Disposition'].split('=')[1][2:-1])
        filepath = os.path.join(
            filepath,
            'items',
            file_name
        )
        self.set(key=key.get(), value=filepath)
        if not os.path.isfile(filepath):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            if buffer is None:
                try:
                    temp_file_path = filepath + '.download'
                    with open(temp_file_path, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:  # filter out keep-alive new chunks
                                f.write(chunk)
                    shutil.move(temp_file_path, filepath)
                except:
                    if os.path.isfile(temp_file_path):
                        os.remove(temp_file_path)
                    return ''
            else:
                if os.path.isfile(buffer.name):
                    shutil.copyfile(buffer.name, filepath)
                else:
                    with open(filepath, "wb") as f:
                        f.write(buffer.getbuffer())
        self._update_config_file(filepath=filepath, update=False, size=(Path(filepath).stat().st_size / 1000000))
        if (Path(filepath).stat().st_size / 1000000) + self._current_bin_cache_size > self.bin_cache_size:
            self._lru_cache()
        return filepath

    def read(self, request_path: str):
        """
        Cache entity get

        :param str request_path: the request
        :return: success, list of the get result
        """
        entity_id = request_path.split('/')[-1]
        entity_type = request_path.split('/')[-2]
        key = CacheKey(entity_id=entity_id, entity_type=entity_type)
        hit, response = self.get(key=key)
        if hit:
            return hit, response
        return False, None

    def write(self, list_entities_json):
        """
        Add or update the entity cache

        :param list list_entities_json: list of jsons of entities to set
        """
        for entity_json in list_entities_json:
            key = self.build_cache_key(entity_json)
            redis_key = key.get_key()
            self.set(key=redis_key, value=entity_json)
            self.set(key=key.get(), value=redis_key)

    def invalidate(self, path):
        """
        Delete from the caches

        :param str path: the request path
        """
        entity_id = path.split('/')[-1]
        entity_type = path.split('/')[-2]
        key = CacheKey(entity_id=entity_id, entity_type=entity_type)
        self.delete(key)

    def clear(self):
        self.cache_levels[1].clear()

    def keys(self):
        return [k for k in self.cache_levels[1].keys()]


================================================
File: dtlpy/caches/dl_cache.py
================================================
import json
import sqlite3
import re

from diskcache import Cache
import os
from .base_cache import BaseCache


class DiskCache(BaseCache):
    def __init__(self, name, level=1, options=None, enable_stats=False, ttl=1000):
        if options is None:
            options = dict()
        self.name = name
        self.level = level
        self.ttl = ttl
        self.cache_dir = options.get(
            "cachePath", os.path.join(self.dataloop_path, "cache", name)
        )
        self.cache = Cache(directory=self.cache_dir)
        self.cache.stats(enable=enable_stats)

        self.conn = sqlite3.connect(os.path.join(self.cache_dir, 'cache.db'), check_same_thread=False)
        self.cursor = self.conn.cursor()

    @property
    def dataloop_path(self):
        return os.environ['DATALOOP_PATH'] if 'DATALOOP_PATH' in os.environ \
            else os.path.join(os.path.expanduser('~'), '.dataloop')

    def set(self, key, value):
        """
        set or add a key and value to the cache
        :param key: str or int type of key
        :param value: pickled value
        :return:
        """
        if not isinstance(key, str) and not isinstance(key, int):
            raise ValueError("key must be string or int")
        with Cache(self.cache.directory) as reference:
            reference.set(key=key, value=value, expire=self.ttl)

    def _key_fix(self, key):
        if '**' in key:
            key = key.replace('**', '%')
        if '*' in key:
            key = key.replace('*', '%')
        return key

    def list(self, pattern):
        """
        list the keys py pattern from the cache

        :param pattern: str or int type of key
        :return: the value of the key
        """
        self.cache.close()

        key = self._key_fix(pattern)

        try:
            self.cursor.execute("SELECT key FROM Cache WHERE key LIKE '{}'".format(key))

            rows = self.cursor.fetchall()
            for row in rows:
                for key in row:
                    yield key
        except:
            return None

    def ping(self):
        """
        Cache ping check if connection is working
        """
        if os.path.exists(os.path.join(self.cache_dir, 'cache.db')):
            return True
        else:
            raise Exception('cache db not fond')

    def get(self, key):
        """
        get the value of the key from the cache
        :param key: str or int type of key
        :return: the value of the key
        """
        self.cache.close()
        res = self.cache.get(key=key)
        if res is not None:
            try:
                return json.loads(res)
            except:
                return res

    def delete(self, key):
        """
        delete the element from the cache
        :param key: str or int type of key
        :return:
        """
        self.cache.close()
        self.cache.delete(key=key)

    def add(self, key, value):
        """
        add the element to the cache if the key is not already present.
        :param key: str or int type of key
        :param value: pickled value
        :return: bool: True if add False if not
        """
        if not isinstance(key, str) and not isinstance(key, int):
            raise ValueError("key must be string or int")
        self.cache.close()
        return self.cache.add(key=key, value=value, expire=self.ttl)

    def push(self, value):
        """
        push the element to the cache
        :param value: pickled value
        :return: bool: True if add False if not
        """
        self.cache.close()
        return self.cache.push(value=value, expire=self.ttl)

    def incr(self, key, value=1):
        """
        incremented the value of this key
        :param key: str or int type of key
        :param value: the amount of incremented
        :return:
        """
        self.cache.incr(key=key, delta=value)

    def decr(self, key, value=1):
        """
        decremented  the value of this key
        :param key: str or int type of key
        :param value: the amount of decremented
        :return:
        """
        self.cache.decr(key=key, delta=value)

    def pop(self, key):
        """
        delete the element from the cache and return it
        :param key: str or int type of key
        :return: the deleted element
        """
        self.cache.close()
        return self.cache.pop(key)

    def volume(self):
        """
        returns the estimated total size in bytes of the cache directory on disk.
        :return: size in bytes
        """
        return self.cache.volume()

    def clear(self):
        """
        simply removes all items from the cache.
        :return: number of the items
        """
        return self.cache.clear()

    def stats(self):
        """
        returns cache hits and misses.
        :return: tuple (hits, misses)
        """
        hits, misses = self.cache.stats(enable=False, reset=False)
        self.cache.stats(enable=True)
        return hits, misses

    def reset_stats(self):
        """
        reset cache hits and misses.
        :return:
        """
        self.cache.stats(enable=False, reset=True)
        self.cache.stats(enable=True)

    def keys(self):
        """
        return all the cache keys
        :return: list of the keys
        """
        for output in list(self.cache.iterkeys()):
            if output is not None:
                yield output

    def delete_cache(self):
        """
        Delete the cache folder
        """
        self.cache.close()
        import shutil

        try:
            shutil.rmtree(self.cache.directory)
        except OSError:  # Windows wonkiness
            pass


================================================
File: dtlpy/caches/filesystem_cache.py
================================================
import os
import json
import glob
import shutil

from .base_cache import BaseCache


class FileSystemCache(BaseCache):
    def __init__(self, options=None, size=1000, ttl=1000):
        """
        docs
        """
        if options is None:
            options = {}
        root_dir = options.get('rootDir', os.environ['DEFAULT_CACHE_PATH'])
        if not os.path.isdir(root_dir):
            os.makedirs(root_dir, exist_ok=True)
        self.root_dir = root_dir
        self.ttl = ttl
        self.size = size

    def set(self, key, value):
        """
        set or add a key and value to the cache
        :param key: str or int type of key
        :param value: pickled value
        :return:
        """
        if not isinstance(key, str):
            raise ValueError("key must be string")
        filepath = os.path.join(self.root_dir, key + '.json')
        if not os.path.isdir(os.path.dirname(filepath)):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, 'w') as f:
            json.dump(value, f, indent=4)

    def list(self, key):
        """
        get the value of the key from the cache
        :param key: str or int type of key
        :return: the value of the key
        """
        if self.root_dir not in key:
            filepath = os.path.join(self.root_dir, key)
        else:
            filepath = key
        files_list = set(glob.glob(filepath + '.*', recursive=True))
        output_values = []
        for file in files_list:
            with open(file) as file_out:
                output_values.append(json.load(file_out))
        return output_values

    def get(self, key):
        """
        get the value of the key from the cache
        :param key: str or int type of key
        :return: the value of the key
        """
        if self.root_dir not in key:
            filepath = os.path.join(self.root_dir, key)
        else:
            filepath = key

        res = None
        if os.path.isfile(filepath):
            with open(filepath) as file_out:
                res = (json.load(file_out))
        return res

    def delete(self, key):
        """
        delete the element from the cache
        :param key: str or int type of key
        :return:
        """
        if self.root_dir not in key:
            filepath = os.path.join(self.root_dir, os.path.dirname(key))
        else:
            filepath = os.path.dirname(key)
        files_list = set(glob.glob(filepath, recursive=True))
        for file in files_list:
            if os.path.isdir(file):
                try:
                    shutil.rmtree(file)
                except OSError as e:
                    print("Error: %s - %s." % (e.filename, e.strerror))


================================================
File: dtlpy/caches/redis_cache.py
================================================
import redis
import json
import datetime
from .base_cache import BaseCache


class RedisCache(BaseCache):
    def __init__(self, options=None, ttl=1000):
        if options is None:
            options = {}
        self.cache = redis.Redis(
            host=options.get('host', '127.0.0.1'),
            port=options.get('port', 6379),
            socket_keepalive=options.get('keepalive', True)
        )
        self.ttl = ttl

    def set(self, key, value):
        """
        set or add a key and value to the cache
        :param key: str or int type of key
        :param value: pickled value
        :return:
        """
        if not isinstance(key, str):
            raise ValueError("key must be string")
        self.cache.setex(name=key, value=value, time=self.ttl)

    def ping(self):
        """
        Cache ping check if connection is working
        """
        self.cache.ping()

    def list(self, pattern):
        """
        get the value of the key from the cache
        :param pattern: str or int type of key
        :return: the value of the key
        """
        if '\\' in pattern:
            pattern = pattern.replace('\\', '\\\\')
        keys_list = self.cache.keys(pattern=r'{}'.format(pattern))
        for key in keys_list:
            yield key.decode("UTF-8")

    def get(self, key):
        """
        get the value of the key from the cache
        :param key: str or int type of key
        :return: the value of the key
        """
        res = self.cache.get(r'{}'.format(key))
        if res is not None:
            try:
                return json.loads(res.decode("UTF-8"))
            except:
                return res.decode("UTF-8")

    def delete(self, key):
        """
        delete the element from the cache
        :param key: str or int type of key
        :return:
        """
        self.cache.delete(key)

    def keys(self):
        """
        return all the cache keys
        :return: list of the keys
        """
        for output in list(self.cache.keys()):
            if output is not None:
                yield output.decode('utf-8')

    def clear(self):
        """
        return all the cache keys
        :return: list of the keys
        """
        all_keys = self.cache.keys('*')
        for k in all_keys:
            self.cache.delete(k)


================================================
File: dtlpy/dlp/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .cli_utilities import DlpCompleter
from .cli_utilities import FileHistory
from .cli_utilities import get_parser_tree
from .parser import get_parser
from .command_executor import CommandExecutor


================================================
File: dtlpy/dlp/cli_utilities.py
================================================
from prompt_toolkit.completion import Completer, Completion
from prompt_toolkit.history import History
from fuzzyfinder.main import fuzzyfinder
import csv
import threading
import os
import datetime
import logging


class StateEnum:
    """
    State enum
    """
    START = 0
    RUNNING = 1
    DONE = 2
    CONTINUE = 3


def get_parser_tree(parser):
    """
    Creates parser tree for autocomplete
    :param parser: parser
    :return: parser tree
    """
    if parser._subparsers is None:
        p_keywords = list()
        for param in parser._option_string_actions:
            if param.startswith('--'):
                p_keywords.append(param)
    else:
        p_keywords = dict()
        subparsers = parser._subparsers._group_actions[0].choices
        for sub_parser in subparsers:
            p_keywords[sub_parser] = get_parser_tree(subparsers[sub_parser])
    if 'shell' in p_keywords:
        # noinspection PyTypeChecker
        p_keywords.pop('shell')
    return p_keywords


class FileHistory(History):
    """
    :class:`.History` class that stores all strings in a file.
    """

    def __init__(self, filename):
        self.filename = filename
        super(FileHistory, self).__init__()
        self.to_hide = ['password', 'secret']

    def load_history_strings(self):
        strings = []
        lines = []

        def add():
            if lines:
                # Join and drop trailing newline.
                string = ''.join(lines)[:-1]
                hide = any(field in string for field in self.to_hide)
                if not hide:
                    strings.append(string)

        if os.path.exists(self.filename):
            with open(self.filename, 'rb') as f:
                for line in f:
                    line = line.decode('utf-8')

                    if line.startswith('+'):
                        lines.append(line[1:])
                    else:
                        add()
                        lines = []

                add()

        # Reverse the order, because newest items have to go first.
        return reversed(strings)

    def store_string(self, string):
        # Save to file.
        with open(self.filename, 'ab') as f:
            def write(t):
                f.write(t.encode('utf-8'))

            write('\n# %s\n' % datetime.datetime.utcnow())
            for line in string.split('\n'):
                hide = any(field in line for field in self.to_hide)
                if not hide:
                    write('+%s\n' % line)


class DlpCompleter(Completer):
    """
    Autocomplete for dlp shell
    """

    def __init__(self, keywords, dlp):
        super(DlpCompleter, self).__init__()
        # globals
        self.keywords = keywords
        self.param_suggestions = list()
        self.thread_state = StateEnum.START
        self.dlp = dlp

    def get_param_suggestions(self, param, word_before_cursor, cmd):
        """
        Return parap suggestions
        :param param:
        :param word_before_cursor:
        :param cmd:
        :return:
        """
        prev_state = logging.root.manager.disable
        try:
            logging.root.manager.disable = logging.ERROR
            if self.thread_state in [StateEnum.RUNNING, StateEnum.DONE]:
                return
            else:
                if param == '--project-name':
                    self.thread_state = StateEnum.RUNNING
                    project_list = self.dlp.projects.list()
                    self.param_suggestions = ['"{}'.format(project.name) for project in project_list]
                    self.thread_state = StateEnum.DONE

                elif param == '--package-name':
                    self.thread_state = StateEnum.RUNNING
                    if '--project-name' in cmd:
                        project = self.dlp.projects.get(
                            project_name=cmd[cmd.index('--project-name') + 1].replace('"', ''))
                        packages = project.packages
                    else:
                        packages = self.dlp.packages
                    package_list = packages.list()
                    self.param_suggestions = ['"{}'.format(package.name) for package in package_list.items]
                    self.thread_state = StateEnum.DONE

                elif param == '--service-name':
                    self.thread_state = StateEnum.RUNNING
                    if '--package-name' in cmd:
                        package = self.dlp.packages.get(
                            package_name=cmd[cmd.index('--package-name') + 1].replace('"', ''))
                        services = package.services
                    elif '--project-name' in cmd:
                        project = self.dlp.projects.get(
                            project_name=cmd[cmd.index('--project-name') + 1].replace('"', ''))
                        services = project.services
                    else:
                        services = self.dlp.services

                    service_list = services.list()
                    self.param_suggestions = ['"{}'.format(service.name) for service in service_list.items]
                    self.thread_state = StateEnum.DONE

                elif param == '--trigger-name':
                    self.thread_state = StateEnum.RUNNING
                    if '--service-name' in cmd:
                        service = self.dlp.services.get(
                            service_name=cmd[cmd.index('--service-name') + 1].replace('"', ''))
                        triggers = service.triggers
                    elif '--package-name' in cmd:
                        package = self.dlp.packages.get(
                            package_name=cmd[cmd.index('--package-name') + 1].replace('"', ''))
                        triggers = package.services
                    elif '--project-name' in cmd:
                        project = self.dlp.projects.get(
                            project_name=cmd[cmd.index('--project-name') + 1].replace('"', ''))
                        triggers = project.services
                    else:
                        triggers = self.dlp.services

                    trigger_list = triggers.list()
                    self.param_suggestions = ['"{}'.format(trigger.name) for trigger in trigger_list.items]
                    self.thread_state = StateEnum.DONE

                elif param == '--dataset-name':
                    self.thread_state = StateEnum.RUNNING
                    if '--project-name' in cmd:
                        project = self.dlp.projects.get(
                            project_name=cmd[cmd.index('--project-name') + 1].replace('"', ''))
                        dataset_list = project.datasets.list()
                    else:
                        project = self.dlp.projects.get()
                        dataset_list = project.datasets.list()
                    self.param_suggestions = ['"{}'.format(dataset.name) for dataset in dataset_list]
                    self.thread_state = StateEnum.DONE

                elif param == '--remote-path':
                    self.thread_state = StateEnum.RUNNING
                    if '--project-name' in cmd:
                        project = self.dlp.projects.get(
                            project_name=cmd[cmd.index('--project-name') + 1].replace('"', ''))
                    else:
                        project = self.dlp.projects.get()
                    if '--dataset-name' in cmd:
                        dataset = project.datasets.get(
                            dataset_name=cmd[cmd.index('--dataset-name') + 1].replace('"', ''))
                    else:
                        dataset = self.dlp.datasets.get()
                    self.param_suggestions = dataset.directory_tree.dir_names
                    with_quotation = ['"{}'.format(directory) for directory in dataset.directory_tree.dir_names]
                    self.param_suggestions += with_quotation
                    self.thread_state = StateEnum.DONE

                elif param in ['--local-path', '--local-annotations-path', '--service-file']:
                    self.thread_state = StateEnum.CONTINUE
                    param = word_before_cursor.replace('"', '')
                    if param == '':
                        param, path = os.path.splitdrive(os.getcwd())
                        param += os.path.sep
                        self.param_suggestions = ['"{}'.format(os.path.join(param, directory))
                                                  for directory in os.listdir(param)
                                                  if not directory.startswith('.')]
                    elif os.path.isdir(os.path.dirname(param)):
                        base_dir = os.path.dirname(param)
                        self.param_suggestions = ['"{}'.format(os.path.join(base_dir, directory))
                                                  for directory in os.listdir(base_dir)
                                                  if (not directory.startswith('.') and
                                                      param in '"{}'.format(os.path.join(base_dir, directory)))]

                elif param in ['--annotation-options']:
                    self.thread_state = StateEnum.CONTINUE
                    self.param_suggestions = ['mask', 'json', 'instance', '"mask, json"',
                                              '"mask, instance"', '"json, instance"', '"mask, json, instance"']

                elif param in ['--annotation-filter-type']:
                    self.thread_state = StateEnum.CONTINUE
                    self.param_suggestions = ['box', 'segment', 'binary', '"box, segment"']

                elif param in ['--annotation-filter-label']:
                    self.thread_state = StateEnum.CONTINUE

                elif param in ['--actions']:
                    self.thread_state = StateEnum.CONTINUE
                    self.param_suggestions = ['Created', 'Updated', 'Deleted', '"Created, Updated"',
                                              '"Created, Deleted"', '"Updated, Deleted"', '"Created, Updated, Deleted"']

                elif param in ['--resource']:
                    self.thread_state = StateEnum.CONTINUE
                    self.param_suggestions = [value for key, value in self.dlp.TriggerResource.__dict__.items() if
                                              not key.startswith('_')]

                elif param in ['--package-type']:
                    self.thread_state = StateEnum.CONTINUE
                    self.param_suggestions = [value for key, value in self.dlp.PackageCatalog.__dict__.items() if
                                              not key.startswith('_')]

                # elif param in ['cd']:
                #     self.thread_state = StateEnum.CONTINUE
                #     if word_before_cursor != '' and os.path.isdir(os.path.join(os.getcwd(), word_before_cursor)):
                #         self.param_suggestions = [os.path.join(word_before_cursor, directory)
                #                                   for directory in
                #                                   os.listdir(os.path.join(os.getcwd(), word_before_cursor))
                #                                   if os.path.isdir(
                #                 os.path.join(os.getcwd(), word_before_cursor, directory))]
                #     else:
                #         self.param_suggestions = [directory for directory in os.listdir(os.getcwd()) if
                #                                   os.path.isdir(directory)]

                else:
                    self.thread_state = StateEnum.START
                    self.param_suggestions = list()
        except Exception:
            self.param_suggestions = list()
            self.thread_state = StateEnum.START
        finally:
            logging.root.manager.disable = prev_state

    def need_param(self, cmd, word_before_cursor):
        """
        :param cmd:
        :param word_before_cursor:
        """
        need_param = False

        try:
            bool_flags_list = ['--overwrite', '--with-text', '--deploy', '--not-items-folder', '--encode',
                               '--without-binaries']

            if len(cmd) > 2 and cmd[-2].startswith('--') and word_before_cursor != '':
                need_param = self.get_param(cmd=cmd, word_before_cursor=word_before_cursor) not in bool_flags_list
            elif cmd[-1].startswith('-') and word_before_cursor == '':
                need_param = self.get_param(cmd=cmd, word_before_cursor=word_before_cursor) not in bool_flags_list
            elif cmd[0] in ['cd']:
                need_param = True
        except Exception:
            need_param = False

        return need_param

    @staticmethod
    def get_param(cmd, word_before_cursor):
        """
        :param cmd:
        :param word_before_cursor:
        """
        if word_before_cursor == '' or len(cmd) < 2:
            param = cmd[-1]
        else:
            param = cmd[-2]

        return param

    def get_completions(self, document, complete_event):
        """
        Get command completion
        :param document:
        :param complete_event:
        :return:
        """
        # fix input
        cmd = next(csv.reader([" ".join(document.text.split())], delimiter=' '))

        # get current word
        word_before_cursor = document.get_word_before_cursor(WORD=True)

        # suggest keywords
        suggestions = list()
        if self.need_param(cmd=cmd, word_before_cursor=word_before_cursor):
            param = self.get_param(cmd=cmd, word_before_cursor=word_before_cursor)
            if self.thread_state in [StateEnum.START, StateEnum.CONTINUE]:
                if param in ['--project-name',
                             '--dataset-name',
                             '--remote-path',
                             '--package-name',
                             '--service-name',
                             '--trigger-name']:
                    thread = threading.Thread(target=self.get_param_suggestions,
                                              kwargs={"param": param,
                                                      'word_before_cursor': word_before_cursor,
                                                      'cmd': cmd})
                    thread.daemon = True
                    thread.start()

                else:
                    self.get_param_suggestions(param=param, word_before_cursor=word_before_cursor, cmd=cmd)
            if self.thread_state in [StateEnum.DONE, StateEnum.CONTINUE]:
                suggestions = self.param_suggestions
        elif len(cmd) == 0:
            suggestions = list(self.keywords.keys())
        elif len(cmd) == 1:
            self.thread_state = StateEnum.START
            if cmd[0] not in self.keywords.keys() and cmd[0] != '':
                if not word_before_cursor == '':
                    suggestions = list(self.keywords.keys())
            elif cmd[0] == '':
                suggestions = list(self.keywords.keys())
            elif isinstance(self.keywords[cmd[0]], list):
                suggestions = self.keywords[cmd[0]]
            elif isinstance(self.keywords[cmd[0]], dict):
                suggestions = list(self.keywords[cmd[0]].keys())
        elif len(cmd) >= 2:
            self.thread_state = StateEnum.START
            if cmd[0] not in self.keywords.keys():
                suggestions = list()
            elif isinstance(self.keywords[cmd[0]], list):
                suggestions = self.keywords[cmd[0]]
            elif isinstance(self.keywords[cmd[0]], dict):
                if cmd[1] in self.keywords[cmd[0]].keys():
                    suggestions = self.keywords[cmd[0]][cmd[1]]
                else:
                    suggestions = list(self.keywords[cmd[0]].keys())

        matches = fuzzyfinder(word_before_cursor, suggestions)
        for match in matches:
            yield Completion(match, start_position=-len(word_before_cursor))


================================================
File: dtlpy/dlp/command_executor.py
================================================
import subprocess
import inquirer
import logging
import json
import os
import sys
import jwt

from .. import exceptions, entities, repositories, utilities, assets

logger = logging.getLogger(name='dtlpy')


class CommandExecutor:

    def __init__(self, dl, parser):
        self.dl = dl
        self.parser = parser
        self.utils = Utils(dl)

    def run(self, args):
        ########################
        # Run Command if Exist #
        ########################
        if args.operation is None:
            logger.info('See "dlp --help" for options')
            return

        operation = args.operation.lower().replace('-', '_')
        if hasattr(self, operation):
            getattr(self, operation)(args)
        ###############
        # Catch typos #
        ###############
        elif args.operation in ["project", 'dataset', 'item', 'service', 'package', 'video', 'deploy', 'generate']:
            self.typos(args=args)
        #######################
        # Catch other options #
        #######################
        elif args.operation:
            print('dlp: "%s" is not an dlp command' % args.operation)
            print('See "dlp --help" for options')
        else:
            print('See "dlp --help" for options')

    def help(self, args):
        self.parser.print_help()

    def logout(self, args):
        self.dl.logout()
        logger.info('logout successful')

    # noinspection PyUnusedLocal
    def login(self, args):
        self.dl.login()
        self.dl.info(with_token=False)

    def login_token(self, args):
        self.dl.login_token(args.token)
        self.dl.info(with_token=False)

    def login_api_key(self, args):
        self.dl.login_api_key(api_key=args.api_key)

    def login_secret(self, args):
        self.login_m2m(args=args)

    def login_m2m(self, args):
        self.dl.login_m2m(email=args.email,
                          password=args.password,
                          client_id=args.client_id,
                          client_secret=args.client_secret)
        self.dl.info(with_token=False)

    def upgrade(self, args):
        url = 'dtlpy'
        if args.url is None:
            try:
                payload = jwt.decode(self.dl.client_api.token, algorithms=['HS256'],
                                     verify=False, options={'verify_signature': False})
                if 'admin' in payload['https://dataloop.ai/authorization']['roles']:
                    url = "https://storage.googleapis.com/dtlpy/dev/dtlpy-latest-py3-none-any.whl"
            except Exception:
                pass
        else:
            url = args.url

        logger.info("Update DTLPy from {}".format(url))
        logger.info("Installing using pip...")
        cmd = "pip install {} --upgrade".format(url)
        subprocess.Popen(cmd, shell=True)
        sys.exit(0)

    # noinspection PyUnusedLocal
    def init(self, args):
        self.dl.init()

    # noinspection PyUnusedLocal
    def checkout_state(self, args):
        state = self.dl.checkout_state()
        logger.info('Checked-out:')
        for key, val in state.items():
            try:
                msg = '{entity} name: {name}\t{entity} id: {id}'.format(entity=key, name=val['name'], id=val['id'])
            except KeyError:
                msg = '{entity} Not Found'.format(entity=key)
            logger.info(msg)

    # noinspection PyUnusedLocal
    def version(self, args):
        logger.info("Dataloop SDK Version: {}".format(self.dl.__version__))

    def development(self, args):
        if args.development == "local":
            # start the local development
            if args.local == 'start':
                development = self.utils.dl.client_api.state_io.get('development')
                # create default values
                if development is None:
                    development = dict()
                if 'port' not in development:
                    development['port'] = 5802
                if 'docker_image' not in development:
                    development['docker_image'] = 'dataloopai/dtlpy-agent:1.57.3.gpu.cuda11.5.py3.8.opencv'

                # get values from input
                if args.docker_image is not None:
                    development['docker_image'] = args.docker_image
                if args.port is not None:
                    development['port'] = int(args.port)

                # set values to local state
                self.utils.dl.client_api.state_io.put('development', development)
                utilities.local_development.start_session()
            elif args.local == 'pause':
                utilities.local_development.pause_session()
            elif args.local == 'stop':
                utilities.local_development.stop_session()
            else:
                print('Must select one of "start", "pause", "stop". Type "dlp development start --help" for options')
        elif args.development == "remote":
            logger.warning('FUTURE! This is not supported yet..')
        else:
            print('Type "dlp development --help" for options')

    # noinspection PyUnusedLocal

    def api(self, args):
        if args.api == "info":
            information = self.dl.info()
            logger.info('-- Dataloop info --')
            _ = [logger.info('{}: {}'.format(key, val)) for key, val in information.items()]

        if args.api == "setenv":
            self.dl.setenv(args.env)
            logger.info("Platform environment: {}".format(self.dl.environment()))

    def projects(self, args):
        if args.projects == "ls":
            self.dl.projects.list().print()
        elif args.projects == "web":
            if args.project_name is None:
                args.project_name = self.dl.projects.get().name
            self.dl.projects.open_in_web(project_name=args.project_name)
        elif args.projects == "create":
            project = self.dl.projects.create(args.project_name)
            project.print()
            project.checkout()
        elif args.projects == "checkout":
            self.dl.projects.checkout(project_name=args.project_name)
        else:
            print('Type "dlp projects --help" for options')

    def datasets(self, args):
        if args.datasets == "ls":
            self.utils.get_datasets_repo(args=args).list().print()

        elif args.datasets == "checkout":
            self.dl.datasets.checkout(dataset_name=args.dataset_name)

        elif args.datasets == "web":
            if args.dataset_name is None:
                args.dataset_name = self.dl.datasets.get().name
            self.utils.get_datasets_repo(args=args).open_in_web(dataset_name=args.dataset_name)

        elif args.datasets == "create":
            dataset = self.utils.get_datasets_repo(args=args).create(dataset_name=args.dataset_name)
            dataset.print()
            if args.checkout:
                dataset.checkout()
        else:
            print('Type "dlp datasets --help" for options')

    def items(self, args):
        if self.dl.token_expired():
            logger.error("token expired, please login.")
            return

        if args.items == "ls":
            project = self.dl.projects.get(project_name=args.project_name)
            dataset = project.datasets.get(dataset_name=args.dataset_name)
            if isinstance(args.page, str):
                try:
                    args.page = int(args.page)
                except ValueError:
                    raise ValueError("Input --page must be integer")
            filters = self.dl.Filters()

            # add filters
            if args.remote_path is not None:
                if isinstance(args.remote_path, list):
                    filters.add(field="filename", values=args.remote_path, operator=entities.FiltersOperations.IN)
                else:
                    filters.add(field="filename", values=args.remote_path)
            if args.type is not None:
                if isinstance(args.type, list):
                    filters.add(field='metadata.system.mimetype', values=args.type,
                                operator=entities.FiltersOperations.IN)
                else:
                    filters.add(field="metadata.system.mimetype", values=args.type)

            pages = dataset.items.list(filters=filters, page_offset=args.page)
            pages.print()
            print("Displaying page %d/%d" % (args.page, pages.total_pages_count - 1))

        elif args.items == "web":
            project = self.dl.projects.get(project_name=args.project_name)
            dataset = project.datasets.get(dataset_name=args.dataset_name)
            dataset.items.open_in_web(filepath=args.remote_path)

        elif args.items == "upload":
            logger.info("Uploading directory...")
            if isinstance(args.file_types, str):
                args.file_types = [t.strip() for t in args.file_types.split(",")]
            project = self.dl.projects.get(project_name=args.project_name)
            dataset = project.datasets.get(dataset_name=args.dataset_name)

            dataset.items.upload(local_path=args.local_path,
                                 remote_path=args.remote_path,
                                 file_types=args.file_types,
                                 overwrite=args.overwrite,
                                 local_annotations_path=args.local_annotations_path)

        elif args.items == "download":
            logger.info("Downloading dataset...")
            project = self.dl.projects.get(project_name=args.project_name)
            dataset = project.datasets.get(dataset_name=args.dataset_name)

            annotation_options = None
            if args.annotation_options is not None:
                annotation_options = [t.strip() for t in args.annotation_options.split(",")]

            annotation_filters = None
            if args.annotation_filter_type is not None or args.annotation_filter_label is not None:
                annotation_filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
                if args.annotation_filter_type is not None:
                    annotation_filter_type = [t.strip() for t in args.annotation_filter_type.split(",")]
                    annotation_filters.add(field='type',
                                           values=annotation_filter_type,
                                           operator=entities.FiltersOperations.IN)
                if args.annotation_filter_label is not None:
                    annotation_filter_label = [t.strip() for t in args.annotation_filter_label.split(",")]
                    annotation_filters.add(field='label',
                                           values=annotation_filter_label,
                                           operator=entities.FiltersOperations.IN)
            # create remote path filters
            filters = self.dl.Filters()
            if args.remote_path is not None:
                remote_path = [t.strip() for t in args.remote_path.split(",")]
                if len(remote_path) == 1:
                    remote_path = remote_path[0]
                    filters.add(field="filename", values=remote_path)
                elif len(remote_path) > 1:
                    for item in remote_path:
                        if '*' in item:
                            filters.add(field="dir", values=item, method='or')
                            remote_path.pop(remote_path.index(item))
                    filters.add(field="dir", values=remote_path, operator=entities.FiltersOperations.IN, method='or')

            if not args.without_binaries:
                dataset.items.download(filters=filters,
                                       local_path=args.local_path,
                                       annotation_options=annotation_options,
                                       annotation_filters=annotation_filters,
                                       overwrite=args.overwrite,
                                       with_text=args.with_text,
                                       thickness=int(args.thickness),
                                       to_items_folder=not args.not_items_folder)
            else:
                dataset.download_annotations(filters=filters,
                                             local_path=args.local_path,
                                             annotation_options=annotation_options,
                                             annotation_filters=annotation_filters,
                                             overwrite=args.overwrite,
                                             with_text=args.with_text,
                                             thickness=int(args.thickness))

        else:
            print('Type "dlp items --help" for options')

    def videos(self, args):
        if self.dl.token_expired():
            print("[ERROR] token expired, please login.")
            return

        if args.videos == "play":
            from dtlpy.utilities.videos.video_player import VideoPlayer

            VideoPlayer(
                item_filepath=args.item_path,
                dataset_name=args.dataset_name,
                project_name=args.project_name,
            )

        elif args.videos == "upload":
            if (
                    (args.split_chunks is not None)
                    or (args.split_seconds is not None)
                    or (args.split_times is not None)
            ):
                # upload with split
                if isinstance(args.split_chunks, str):
                    args.split_chunks = int(args.split_chunks)
                if isinstance(args.split_seconds, str):
                    args.split_seconds = int(args.split_seconds)
                if isinstance(args.split_times, str):
                    args.split_times = [int(sec) for sec in args.split_times.split(",")]
                self.dl.utilities.videos.Videos.split_and_upload(
                    project_name=args.project_name,
                    dataset_name=args.dataset_name,
                    filepath=args.filename,
                    remote_path=args.remote_path,
                    split_chunks=args.split_chunks,
                    split_seconds=args.split_seconds,
                    split_pairs=args.split_times,
                )
        else:
            print('Type "dlp videos --help" for options')

    def services(self, args):
        if self.dl.token_expired():
            print("[ERROR] token expired, please login.")
            return

        elif args.services == "delete":
            service = self.utils.get_services_repo(args=args).get(service_name=args.service_name)
            service.delete()
            logger.info('Service: "{}" deleted successfully'.format(service.name))

        elif args.services == "ls":
            self.utils.get_services_repo(args=args).list().print()
        elif args.services == "log":
            project = self.dl.projects.get(project_name=args.project_name)
            service = project.services.get(service_name=args.service_name)

            logs = service.log(start=args.start)
            try:
                for log in logs:
                    if isinstance(log, list):
                        for log_record in log:
                            print(log_record)
                    else:
                        print(log)
            except KeyboardInterrupt:
                pass

        elif args.services == "execute":
            try:
                if args.service_name:
                    service = self.utils.get_services_repo(args=args).get(service_name=args.service_name)
                else:
                    service = self.dl.services.get()
            except Exception:
                logger.info('Please checkout a service')
                return

            if args.annotation_id is not None:
                resource = 'Annotation'
            elif args.item_id is not None:
                resource = 'Item'
            elif args.dataset_id is not None:
                resource = 'Dataset'
            else:
                resource = None

            try:
                execution_input = json.loads(args.inputs)
            except Exception:
                logger.info('Input should be json serializable')
                return
            if len(execution_input) == 0:
                execution_input = None

            logger.info('executing')
            service.execute(sync=not args.asynchronous,
                            execution_input=execution_input,
                            function_name=args.function_name,
                            resource=resource,
                            item_id=args.item_id,
                            dataset_id=args.dataset_id,
                            annotation_id=args.annotation_id)
            logger.info("Successfully executed function: {}".format(args.function_name))

        else:
            logger.info('Type "dlp packages --help" for options')

    def deploy(self, args):
        project = self.dl.projects.get(project_name=args.project_name)
        json_filepath = args.json_file
        deployed_services, package = self.dl.packages.deploy_from_file(project=project, json_filepath=json_filepath)
        logger.info("Successfully deployed {} from file: {}\nServices: {}".format(len(deployed_services),
                                                                                  json_filepath,
                                                                                  [s.name for s in deployed_services]))

    def generate(self, args):
        package_type = args.package_type if args.package_type else self.dl.PackageCatalog.DEFAULT_PACKAGE_TYPE
        self.dl.packages.generate(name=args.package_name, src_path=os.getcwd(), package_type=package_type)
        self.utils.dl.client_api.state_io.put('package', {'name': args.package_name})
        logger.info('Successfully generated package files')

    def triggers(self, args):

        if args.triggers == "create":
            args.actions = [t.strip() for t in args.actions.split(",")]
            try:
                service = self.utils.get_services_repo(args).get(service_name=args.service_name)
            except exceptions.NotFound:
                logger.critical('Service not found. Please check-out a service or provide valid service name')
                return

            trigger = service.triggers.create(name=args.name,
                                              filters=json.loads('{}'.format(args.filters.replace("'", '"'))),
                                              function_name=args.function_name,
                                              resource=args.resource,
                                              actions=args.actions)
            logger.info('Trigger created successfully: {}'.format(trigger.name))

        elif args.triggers == "delete":
            triggers = self.utils.get_triggers_repo(args=args)
            triggers.get(trigger_name=args.trigger_name).delete()
            logger.info('Trigger deleted successfully: {}'.format(args.trigger_name))
        elif args.triggers == "ls":
            triggers = self.utils.get_triggers_repo(args=args)
            triggers.list().print()
        else:
            logger.info('Type "dlp packages --help" for options')

    def packages(self, args):
        if self.dl.token_expired():
            logger.error("token expired, please login.")
            return

        elif args.packages == "delete":
            package = self.utils.get_packages_repo(args=args).get(package_name=args.package_name)
            package.delete()
            logger.info('Successfully deleted package {}'.format(package.name))

        elif args.packages == "ls":
            self.utils.get_packages_repo(args=args).list().print()

        elif args.packages == "checkout":
            self.dl.packages.checkout(package_name=args.package_name)

        elif args.packages == "push":
            packages = self.utils.get_packages_repo(args=args)

            package = packages.push(src_path=args.src_path,
                                    package_name=args.package_name,
                                    checkout=args.checkout)

            logger.info("Successfully pushed package to platform\n"
                        "Package id:{}\nPackage version:{}".format(package.id,
                                                                   package.version))
        elif args.packages == "deploy":
            packages = self.utils.get_packages_repo(args=args)

            package = packages.deploy(package_name=args.package_name,
                                      checkout=args.checkout,
                                      module_name=args.module_name)

            logger.info("Successfully pushed package to platform\n"
                        "Package id:{}\nPackage version:{}".format(package.id,
                                                                   package.version))

        elif args.packages == "test":
            go_back = False
            if 'src' in os.listdir(os.getcwd()):
                go_back = True
                os.chdir('./src')
            try:
                self.dl.packages.test_local_package(concurrency=int(args.concurrency),
                                                    function_name=args.function_name)
            except Exception:
                logger.exception('failed during test')
            finally:
                if go_back:
                    os.chdir('..')

        else:
            logger.info('Type "dlp packages --help" for options')

    def app(self, args):
        if args.app == 'pack':
            path = self.dl.dpks.pack()
            logger.info(f'Packed to {path}')
        elif args.app == 'init':
            app_filename = assets.paths.APP_JSON_FILENAME
            if os.path.isfile(app_filename):
                questions = [
                    inquirer.Confirm(name='overwrite',
                                     message=f"Dataloop app already initialized. Re-initialize?",
                                     default=False)]
                answers = inquirer.prompt(questions)
                if answers.get('overwrite') is False:
                    return
            name = args.name
            description = args.description
            attributes = args.attributes
            icon = args.icon
            scope = args.scope
            as_array = [name, description, attributes, icon, scope]
            if as_array.count(None) == len(as_array):  # No one value is initialized
                dir_name = os.path.basename(os.getcwd())
                questions = [
                    inquirer.Text(name='name',
                                  message=f"Enter the name of the app (or press enter for '{dir_name}'):",
                                  default=dir_name),
                    inquirer.Text(name='description',
                                  message="Enter the description (or enter for empty): "),
                    inquirer.Text(name='attributes',
                                  message="Enter the attributes (an object, or enter for empty): ",
                                  default=None),
                    inquirer.Text(name='icon',
                                  message="Enter the path to the icon (or enter for empty): "),
                    inquirer.Text(name='scope',
                                  message="Enter the scope (or enter for 'organization'): ",
                                  default='project'),
                ]
                answers = inquirer.prompt(questions)
                name = answers.get('name')
                description = answers.get('description')
                attributes = answers.get('attributes')
                icon = answers.get('icon')
                scope = answers.get('scope')

            if attributes is not None:
                attributes = json.loads(attributes)

            self.dl.dpks.init(name=name,
                              description=description,
                              attributes=attributes,
                              icon=icon,
                              scope=scope)
        elif args.app == 'add':
            if args.panel is True:
                default_panel_name = "myPanel"
                choices = list(entities.dpk.SlotType)

                questions = [
                    inquirer.Text(name='name',
                                  message=f"Enter PANEL NAME (or press enter for '{default_panel_name}'): ",
                                  default='default_panel_name'),
                    inquirer.List(name='support_slot_type',
                                  message="Enter SUPPORTED SLOT TYPE:",
                                  choices=choices),
                ]

                answers = inquirer.prompt(questions)
                #####
                # create a dir for that panel
                os.makedirs(answers.get('name'), exist_ok=True)
                # dump to dataloop.json
                app_filename = assets.paths.APP_JSON_FILENAME
                if not os.path.isfile(app_filename):
                    logger.error(f"Can't find app config file ({app_filename}), please run `dlp app init` first")
                else:
                    with open(app_filename, 'r') as f:
                        dpk = entities.Dpk.from_json(json.load(f))
                    dpk.components.panels.append(entities.Panel(name=answers.get('name'),
                                                                supported_slots=[answers.get('support_slot_type')]))
                    with open(app_filename, 'w') as f:
                        json.dump(dpk.to_json(), f, indent=4)

        elif args.app == 'publish':
            dpk = self.utils.get_dpks_repo(args).publish()
            if dpk:
                logger.info(f'Published the application: id={dpk.id}')
            else:
                logger.info("Couldn't publish the application")
        elif args.app == 'update':
            # TODO: I think it changed, not implemented
            logger.info('App updated successfully')
        elif args.app == 'resume':
            succeed = self.utils.get_apps_repo(args).resume(app_id=args.app_id)
            if succeed is True:
                logger.info('Resumed application successfully')
            else:
                logger.info('Application resume failed')
        elif args.app == 'pause':
            succeed = self.utils.get_apps_repo(args).pause(app_id=args.app_id)
            if succeed is True:
                logger.info('Paused application successfully')
            else:
                logger.info('Application pause failed')
        elif args.app == 'install':
            app = self.utils.get_apps_repo(args).install(
                dpk=self.utils.get_dpks_repo(args).get(dpk_id=args.dpk_id),
                organization_id=args.org_id
            )
            if app is not None:
                logger.info('App installed successfully')
            else:
                logger.info("Couldn't install the app")
        elif args.app == 'pull':
            succeed = self.dl.dpks.pull(dpk_name=args.dpk_name)
            if succeed is True:
                logger.info("Pulled successfully")
            else:
                logger.info("Couldn't pull")
        elif args.app == 'list':
            self.dl.apps.list().print()

    # noinspection PyUnusedLocal
    @staticmethod
    def pwd(args):
        print(os.getcwd())

    @staticmethod
    def cd(args):
        directory = args.dir
        if directory == '..':
            directory = os.path.split(os.getcwd())[0]
        os.chdir(directory)
        print(os.getcwd())

    @staticmethod
    def mkdir(args):
        os.mkdir(args.name)

    # noinspection PyUnusedLocal
    @staticmethod
    def ls(args):
        print(os.getcwd())
        dirs = os.listdir(os.getcwd())
        import pprint

        pp = pprint.PrettyPrinter(indent=3)
        pp.pprint(dirs)

    # noinspection PyUnusedLocal
    @staticmethod
    def clear(args):
        if os.name == "nt":
            os.system('cls')
        else:
            os.system('clear')

    @staticmethod
    def typos(args):
        print('dlp: "{op}" is not an dlp command. Did you mean "{op}s"?'.format(op=args.operation))


class Utils:
    def __init__(self, dl):
        self.dl = dl

    def get_packages_repo(self, args):
        if args.project_name is not None:
            project = self.dl.projects.get(project_name=args.project_name)
            packages = project.packages
        else:
            try:
                packages = self.dl.projects.get().packages
            except Exception:
                packages = self.dl.packages

        assert isinstance(packages, repositories.Packages)
        return packages

    def get_datasets_repo(self, args):
        if args.project_name is not None:
            project = self.dl.projects.get(project_name=args.project_name)
            datasets = project.datasets
        else:
            try:
                datasets = self.dl.projects.get().datasets
            except Exception:
                datasets = self.dl.datasets

        assert isinstance(datasets, repositories.Datasets)
        return datasets

    def get_services_repo(self, args):
        if args.project_name is not None:
            project = self.dl.projects.get(project_name=args.project_name)
            packages = project.packages
            services = project.services
        else:
            try:
                packages = self.dl.projects.get().packages
                services = self.dl.projects.get().services
            except Exception:
                packages = self.dl.packages
                services = self.dl.services

        if args.package_name is not None:
            package = packages.get(package_name=args.package_name)
            services = package.services

        assert isinstance(services, repositories.Services)
        return services

    def get_apps_repo(self, args) -> repositories.Apps:
        if args.project_name is not None:
            project = self.dl.projects.get(project_name=args.project_name)
            apps = project.apps
        else:
            try:
                apps = self.dl.projects.get().apps
            except Exception:
                apps = self.dl.apps
        assert isinstance(apps, repositories.Apps)
        return apps

    def get_dpks_repo(self, args) -> repositories.Dpks:
        if args.project_name is None and args.project_id is None:
            try:
                dpks = self.dl.projects.get().dpks
            except Exception:
                dpks = self.dl.dpks
        else:
            project = self.dl.projects.get(project_name=args.project_name, project_id=args.project_id)
            dpks = project.dpks
        if dpks.project is None:
            raise ValueError("Must input one of `project-name` or `project-id`") from None
        assert isinstance(dpks, repositories.Dpks)
        return dpks

    def get_triggers_repo(self, args):
        if args.project_name is not None:
            project = self.dl.projects.get(project_name=args.project_name)
            packages = project.packages
            services = project.services
            triggers = project.triggers
        else:
            try:
                packages = self.dl.projects.get().packages
                services = self.dl.projects.get().services
                triggers = self.dl.projects.get().triggers
            except Exception:
                packages = self.dl.packages
                services = self.dl.services
                triggers = self.dl.triggers

        if args.package_name is not None:
            package = packages.package_name(name=args.package_name)
            services = package.services

        if args.service_name is not None:
            service = services.get(service_name=args.service_name)
            triggers = service.triggers

        assert isinstance(triggers, repositories.Triggers)
        return triggers


================================================
File: dtlpy/dlp/dlp
================================================
dlp.py $*


================================================
File: dtlpy/dlp/dlp.bat
================================================
@echo off
call python %~dp0\dlp.py %*

================================================
File: dtlpy/dlp/dlp.py
================================================
#! /usr/bin/python3
import subprocess
import traceback
import datetime
import logging
import shlex
import sys
import os
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit import prompt
import dtlpy as dlp
from dtlpy import exceptions
from dtlpy.dlp.cli_utilities import FileHistory, get_parser_tree, DlpCompleter
from dtlpy.dlp.parser import get_parser
from dtlpy.dlp.command_executor import CommandExecutor

dlp.client_api.is_cli = True

##########
# Logger #
##########
# set levels for CLI
logger = logging.getLogger(name='dtlpy')
handler_id = next((i for i, hand in enumerate(logger.handlers) if isinstance(hand, logging.StreamHandler)), 0)
logger.handlers[handler_id].setLevel(level=logging.INFO)
logger.propagate = False


def dlp_exit():
    print(datetime.datetime.utcnow())
    print("Goodbye ;)")
    sys.exit(0)


def main():
    try:
        parser = get_parser()
        keywords = get_parser_tree(parser=parser)
        args = parser.parse_args()
        history_file = os.path.join(os.getcwd(), ".history.txt")
        command_executor = CommandExecutor(dl=dlp, parser=parser)

        if args.version:
            logger.info("Dataloop SDK Version: {}".format(dlp.__version__))

        elif args.operation == "shell":
            #######################
            # Open Dataloop shell #
            #######################
            while True:
                text = prompt(u"dl>",
                              history=FileHistory(history_file),
                              auto_suggest=AutoSuggestFromHistory(),
                              completer=DlpCompleter(keywords=keywords, dlp=dlp))
                if text == '':
                    # in new line
                    continue

                try:
                    if text in ["-h", "--help"]:
                        text = "help"
                    text = shlex.split(text)
                    if text[0] not in keywords:
                        p = subprocess.Popen(text)
                        p.communicate()
                        continue
                    parser = get_parser()
                    args = parser.parse_args(text)
                    if args.operation == "exit":
                        dlp_exit()
                    else:
                        command_executor.run(args=args)
                except exceptions.TokenExpired:
                    print(datetime.datetime.utcnow())
                    print("[ERROR] token expired, please login.")
                    continue
                except SystemExit as e:
                    # exit
                    if e.code == 0:
                        if "-h" in text or "--help" in text:
                            continue
                        else:
                            sys.exit(0)
                    # error
                    else:
                        print(datetime.datetime.utcnow())
                        print('"{command}" is not a valid command'.format(command=text))
                        continue
                except Exception as e:
                    print(datetime.datetime.utcnow())
                    if hasattr(e, 'message'):
                        print(e.message)
                    else:
                        print(e.__str__())
                    continue

        else:
            ######################
            # Run single command #
            ######################
            try:
                command_executor.run(args=args)
                sys.exit(0)
            except exceptions.TokenExpired:
                print(datetime.datetime.utcnow())
                print("[ERROR] token expired, please login.")
                sys.exit(1)
            except Exception as e:
                print(datetime.datetime.utcnow())
                print(traceback.format_exc())
                print(e)
                sys.exit(1)
    except KeyboardInterrupt:
        dlp_exit()
    except EOFError:
        dlp_exit()
    except Exception:
        print(traceback.format_exc())
        dlp_exit()


if __name__ == "__main__":
    try:
        main()
    except Exception as err:
        print(datetime.datetime.utcnow())
        print("[ERROR]\t%s" % err)
    print("Dataloop.ai CLI. Type dlp --help for options")


================================================
File: dtlpy/dlp/parser.py
================================================
import argparse


def get_parser():
    """
    Build the parser for CLI
    :return: parser object
    """
    parser = argparse.ArgumentParser(
        description="CLI for Dataloop",
        formatter_class=argparse.RawTextHelpFormatter
    )

    ###############
    # sub parsers #
    ###############
    subparsers = parser.add_subparsers(dest="operation", help="supported operations")

    ########
    # shell #
    ########
    subparsers.add_parser("shell", help="Open interactive Dataloop shell")

    ########
    # shell #
    ########
    a = subparsers.add_parser("upgrade", help="Update dtlpy package")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-u", "--url", metavar='\b', help="Package url. default 'dtlpy'", default=None)

    ##################
    # Login / Logout #
    ##################
    subparsers.add_parser("logout", help="Logout")

    subparsers.add_parser("login", help="Login using web Auth0 interface")

    a = subparsers.add_parser("login-token", help="Login by passing a valid token")
    required = a.add_argument_group("required named arguments")
    required.add_argument(
        "-t", "--token", metavar='\b', help="valid token", required=True
    )

    a = subparsers.add_parser("login-secret", help="Login client id and secret")
    required = a.add_argument_group("required named arguments")
    required.add_argument(
        "-e", "--email", metavar='\b', help="user email", required=False, default=None
    )
    required.add_argument(
        "-p", "--password", metavar='\b', help="user password", required=False, default=None
    )
    required.add_argument(
        "-i", "--client-id", metavar='\b', help="client id", required=False, default=None
    )
    required.add_argument(
        "-s", "--client-secret", metavar='\b', help="client secret", required=False, default=None
    )

    a = subparsers.add_parser("login-m2m", help="Login client id and secret")
    required = a.add_argument_group("required named arguments")
    required.add_argument(
        "-e", "--email", metavar='\b', help="user email", required=False, default=None
    )
    required.add_argument(
        "-p", "--password", metavar='\b', help="user password", required=False, default=None
    )
    required.add_argument(
        "-i", "--client-id", metavar='\b', help="client id", required=False, default=None
    )
    required.add_argument(
        "-s", "--client-secret", metavar='\b', help="client secret", required=False, default=None
    )
    ########
    # Init #
    ########
    subparsers.add_parser("init", help="Initialize a .dataloop context")

    ##################
    # Checkout state #
    ##################
    subparsers.add_parser("checkout-state", help="Print checkout state")

    ########
    # Help #
    ########
    subparsers.add_parser("help", help="Get help")

    ###########
    # version #
    ###########
    parser.add_argument("-v", "--version", action="store_true", help="dtlpy version")
    subparsers.add_parser("version", help="DTLPY SDK version")

    #######
    # API #
    #######
    subparser = subparsers.add_parser("api", help="Connection and environment")
    subparser_parser = subparser.add_subparsers(dest="api", help="gate operations")

    # ACTIONS #

    # info
    subparser_parser.add_parser("info", help="Print api information")

    # setenv
    a = subparser_parser.add_parser("setenv", help="Set platform environment")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-e", "--env", metavar='\b', help="working environment", required=True)

    ###############
    # Development #
    ###############
    subparser = subparsers.add_parser(name="development", help="Start a development session")
    subparser_parser = subparser.add_subparsers(dest="development", help="local development session")
    local_subparser_parser = subparser_parser.add_parser(name="local",
                                                         help="local development using docker and code-server session")
    remote_subparser_parser = subparser_parser.add_parser(name="remote",
                                                          help="remote development (dl.Service) using code-server session")

    #############################
    # Local Development Session #
    #############################
    # start
    local_subparser_subparser = local_subparser_parser.add_subparsers(dest="local", help="Local development session")
    a = local_subparser_subparser.add_parser(name="start", help="Start a local development session")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--port",
                          default=None,
                          help="Local port for the docker connection, default: 5802")
    optional.add_argument("-d", "--docker-image",
                          default=None,
                          help="Docker image to create. default: 'dataloopai/dtlpy-agent:1.57.3.gpu.cuda11.5.py3.8.opencv'")
    # pause
    _ = local_subparser_subparser.add_parser(name="pause", help="Pause the local development session (container pause)")
    # stop
    _ = local_subparser_subparser.add_parser(name="stop", help="Stop the local development session (container kill)")
    ##############################
    # Remote Development Session #
    ##############################

    ############
    # Projects #
    ############
    subparser = subparsers.add_parser("projects", help="Operations with projects")
    subparser_parser = subparser.add_subparsers(dest="projects", help="projects operations")

    # ACTIONS #

    # list
    subparser_parser.add_parser("ls", help="List all projects")

    # create
    a = subparser_parser.add_parser("create", help="Create a new project")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-p", "--project-name", metavar='\b', help="project name")

    # checkout
    a = subparser_parser.add_parser("checkout", help="checkout a project")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-p", "--project-name", metavar='\b', help="project name")

    # open web
    a = subparser_parser.add_parser("web", help="Open in web browser")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', help="project name")

    ############
    # Datasets #
    ############
    subparser = subparsers.add_parser("datasets", help="Operations with datasets")
    subparser_parser = subparser.add_subparsers(dest="datasets", help="datasets operations")

    # ACTIONS #
    # open web
    a = subparser_parser.add_parser("web", help="Open in web browser")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', help="project name")
    optional.add_argument("-d", "--dataset-name", metavar='\b', help="dataset name")

    # list
    a = subparser_parser.add_parser("ls", help="List of datasets in project")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', default=None,
                          help="project name. Default taken from checked out (if checked out)")

    # create
    a = subparser_parser.add_parser("create", help="Create a new dataset")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-d", "--dataset-name", metavar='\b', help="dataset name", required=True)
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', default=None,
                          help="project name. Default taken from checked out (if checked out)")
    optional.add_argument("-c", "--checkout", action='store_true', default=False, help="checkout the new dataset")

    # checkout
    a = subparser_parser.add_parser("checkout", help="checkout a dataset")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-d", "--dataset-name", metavar='\b', help="dataset name")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', default=None,
                          help="project name. Default taken from checked out (if checked out)")

    #########
    # items #
    #########
    subparser = subparsers.add_parser("items", help="Operations with items")
    subparser_parser = subparser.add_subparsers(dest="items", help="items operations")

    # ACTIONS #

    a = subparser_parser.add_parser("web", help="Open in web browser")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-r", "--remote-path", metavar='\b', help="remote path")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', help="project name")
    optional.add_argument("-d", "--dataset-name", metavar='\b', help="dataset name")

    # list
    a = subparser_parser.add_parser("ls", help="List of items in dataset")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', default=None,
                          help="project name. Default taken from checked out (if checked out)")
    optional.add_argument("-d", "--dataset-name", metavar='\b', default=None,
                          help="dataset name. Default taken from checked out (if checked out)")
    optional.add_argument("-o", "--page", metavar='\b', help="page number (integer)", default=0)
    optional.add_argument("-r", "--remote-path", metavar='\b', help="remote path", default=None)
    optional.add_argument("-t", "--type", metavar='\b', help="Item type", default=None)

    # upload
    a = subparser_parser.add_parser("upload", help="Upload directory to dataset")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-l", "--local-path", required=True, metavar='\b',
                          help="local path")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', default=None,
                          help="project name. Default taken from checked out (if checked out)")
    optional.add_argument("-d", "--dataset-name", metavar='\b', default=None,
                          help="dataset name. Default taken from checked out (if checked out)")
    optional.add_argument("-r", "--remote-path", metavar='\b', default=None,
                          help="remote path to upload to. default: /")
    optional.add_argument("-f", "--file-types", metavar='\b', default=None,
                          help='Comma separated list of file types to upload, e.g ".jpg,.png". default: all')
    optional.add_argument("-lap", "--local-annotations-path", metavar='\b', default=None,
                          help="Path for local annotations to upload with items")
    optional.add_argument("-ow", "--overwrite", dest="overwrite", action='store_true', default=False,
                          help="Overwrite existing item")

    # download
    a = subparser_parser.add_parser("download", help="Download dataset to a local directory")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", metavar='\b', default=None,
                          help="project name. Default taken from checked out (if checked out)")
    optional.add_argument("-d", "--dataset-name", metavar='\b', default=None,
                          help="dataset name. Default taken from checked out (if checked out)")
    optional.add_argument("-ao", "--annotation-options", metavar='\b',
                          help="which annotation to download. options: json,instance,mask", default=None)
    optional.add_argument("-aft", "--annotation-filter-type", metavar='\b',
                          help="annotation type filter when downloading annotations. "
                               "options: box,segment,binary etc", default=None)
    optional.add_argument("-afl", "--annotation-filter-label", metavar='\b',
                          help="labels filter when downloading annotations.", default=None)
    optional.add_argument("-r", "--remote-path", metavar='\b', default=None,
                          help="remote path to upload to. default: /")
    optional.add_argument("-ow", "--overwrite", action='store_true', default=False,
                          help="Overwrite existing item")
    optional.add_argument("-t", "--not-items-folder", action='store_true', default=False,
                          help="Download WITHOUT 'items' folder")
    optional.add_argument("-wt", "--with-text", action='store_true', default=False,
                          help="Annotations will have text in mask")
    optional.add_argument("-th", "--thickness", metavar='\b', default="1",
                          help="Annotation line thickness")
    optional.add_argument("-l", "--local-path", metavar='\b', default=None,
                          help="local path")
    optional.add_argument("-wb", "--without-binaries", action='store_true', default=False,
                          help="Don't download item binaries")

    ##########
    # videos #
    ##########
    subparser = subparsers.add_parser("videos", help="Operations with videos")
    subparser_parser = subparser.add_subparsers(dest="videos", help="videos operations")

    # ACTIONS #

    # play
    a = subparser_parser.add_parser("play", help="Play video")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument(
        "-l",
        "--item-path",
        metavar='\b',
        default=None,
        help="Video remote path in platform. e.g /dogs/dog.mp4",
    )
    optional.add_argument(
        "-p",
        "--project-name",
        metavar='\b',
        default=None,
        help="project name. Default taken from checked out (if checked out)",
    )
    optional.add_argument(
        "-d",
        "--dataset-name",
        metavar='\b',
        default=None,
        help="dataset name. Default taken from checked out (if checked out)",
    )

    # upload
    a = subparser_parser.add_parser("upload", help="Upload a single video")
    required = a.add_argument_group("required named arguments")
    required.add_argument(
        "-f", "--filename", metavar='\b', help="local filename to upload", required=True
    )
    required.add_argument(
        "-p", "--project-name", metavar='\b', help="project name", required=True
    )
    required.add_argument(
        "-d", "--dataset-name", metavar='\b', help="dataset name", required=True
    )
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument(
        "-r", "--remote-path", metavar='\b', help="remote path", default="/"
    )

    # split video to chunks
    optional.add_argument(
        "-sc",
        "--split-chunks",
        metavar='\b',
        default=None,
        help="Video splitting parameter: Number of chunks to split",
    )
    optional.add_argument(
        "-ss",
        "--split-seconds",
        metavar='\b',
        default=None,
        help="Video splitting parameter: Seconds of each chuck",
    )
    optional.add_argument(
        "-st",
        "--split-times",
        metavar='\b',
        default=None,
        help="Video splitting parameter: List of seconds to split at. e.g 600,1800,2000",
    )
    # encode
    optional.add_argument(
        "-e",
        "--encode",
        action="store_true",
        default=False,
        help="encode video to mp4, remove bframes and upload",
    )

    ###############
    # Application #
    ###############
    subparser = subparsers.add_parser("app", help="Operations with application")
    subparser_parser = subparser.add_subparsers(dest="app", help="application operations")

    # ACTIONS #

    # init
    a = subparser_parser.add_parser('init', help="Initialize the structure in order to deploy a dpk")
    optional = a.add_argument_group("Optional named arguments")
    optional.add_argument('--name', required=False, dest='name', help="the name of the app")
    optional.add_argument('--description', required=False, dest='description', help="the description of the app")
    optional.add_argument('--attributes', required=False, dest='attributes',
                          help="the attributes of the app (comma separated)")
    optional.add_argument('--icon', required=False, dest='icon', help="the icon of the app")
    optional.add_argument('--scope', required=False, dest='scope',
                          help="the scope of the app (default is organization)")
    # add components
    a = subparser_parser.add_parser('add', help="Add component to the dataloop json file")
    optional = a.add_argument_group("Optional named arguments")
    optional.add_argument('--panel', action='store_true', help="builder for panel component")
    optional.add_argument('--module', action='store_true', help="builder for module component")
    optional.add_argument('--toolbar', action='store_true', help="builder for toolbar component")

    # pack
    a = subparser_parser.add_parser("pack", help="Pack the project as dpk file")
    # publish
    a = subparser_parser.add_parser("publish", help="Publish the app")
    optional = a.add_argument_group("Optional named arguments")
    optional.add_argument("--project-name", dest="project_name", help="The name of the project")
    optional.add_argument("--project-id", dest="project_id", help="The ID of the project")

    # update
    a = subparser_parser.add_parser("update", help="Update the app")
    required = a.add_argument_group("Required named arguments")
    required.add_argument("--app-name", dest="app_name", required=True, help="Locates the app by the name")
    required.add_argument("--new-version", dest="new_version", required=True,
                          help="Sets the new version of the specified app")
    optional = a.add_argument_group("Required named arguments")
    optional.add_argument("--project-id", dest="project_id", default=None, help="The id of the project")
    optional.add_argument("--project-name", dest="project_name", default=None, help="The name of the project")

    # install
    a = subparser_parser.add_parser("install", help="Install the app to the platform")
    required = a.add_argument_group("Required named arguments")
    required.add_argument("--dpk-id", dest="dpk_id", required=True, help="The id of the dpk")
    optional = a.add_argument_group("Optional named arguments")
    optional.add_argument("--project-id", dest="project_id", default=None, help="The id of the project")
    optional.add_argument("--project-name", dest="project_name", default=None, help="The name of the project")
    optional.add_argument("--org-id", dest="org_id", default=None, help="The name of the org")

    # pull
    a = subparser_parser.add_parser('pull', help="Pull the app from the marketplace")
    required = a.add_argument_group("Required named arguments")
    required.add_argument('--dpk-name', dest='app_name', required=True, help='The name of the dpk')

    # list
    a = subparser_parser.add_parser('list', help="List all installed apps")
    required = a.add_argument_group("Required named arguments")
    required.add_argument('--project-name', dest='project_name', required=False, help='The name of the project')

    ############
    # Services #
    ############
    subparser = subparsers.add_parser("services", help="Operations with services")
    subparser_parser = subparser.add_subparsers(dest="services", help="services operations")

    # ACTIONS #

    # execute
    a = subparser_parser.add_parser("execute", help="Create an execution")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-f", "--function-name", dest="function_name", default=None,
                          help="which function to run")
    optional.add_argument("-s", "--service-name", dest="service_name", default=None,
                          help="which service to run")
    optional.add_argument("-pr", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-as", "--async", dest="asynchronous", default=True, action='store_false',
                          help="Async execution ")
    optional.add_argument("-i", "--item-id", dest="item_id", default=None,
                          help="Item input")
    optional.add_argument("-d", "--dataset-id", dest="dataset_id", default=None,
                          help="Dataset input")
    optional.add_argument("-a", "--annotation-id", dest="annotation_id", default=None,
                          help="Annotation input")
    optional.add_argument("-in", "--inputs", dest="inputs", default='{}',
                          help="Dictionary string input")

    # tear-down
    a = subparser_parser.add_parser(
        "tear-down", help="tear-down service of service.json file"
    )
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-l", "--local-path", dest="local_path", default=None,
                          help="path to service.json file")
    optional.add_argument("-pr", "--project-name", dest="project_name", default=None,
                          help="Project name")

    # ls
    a = subparser_parser.add_parser("ls", help="List project's services")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-pr", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-pkg", "--package-name", dest="package_name", default=None,
                          help="Package name")

    # log
    a = subparser_parser.add_parser("log", help="Get services log")
    optional = a.add_argument_group("required named arguments")
    optional.add_argument("-pr", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-f", "--service-name", dest="service_name", default=None,
                          help="Project name")
    optional.add_argument("-t", "--start", dest="start", default=None,
                          help="Log start time")

    # delete
    a = subparser_parser.add_parser("delete", help="Delete Service")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-f", "--service-name", dest="service_name", default=None,
                          help="Service name")
    optional.add_argument("-p", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-pkg", "--package-name", dest="package_name", default=None,
                          help="Package name")

    ############
    # Triggers #
    ############
    subparser = subparsers.add_parser("triggers", help="Operations with triggers")
    subparser_parser = subparser.add_subparsers(dest="triggers", help="triggers operations")

    # ACTIONS #
    # create
    a = subparser_parser.add_parser("create", help="Create a Service Trigger")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-r", "--resource", dest="resource",
                          help="Resource name", required=True)
    required.add_argument("-a", "--actions", dest="actions", help="Actions", required=True)

    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-pkg", "--package-name", dest="package_name", default=None,
                          help="Package name")
    optional.add_argument("-f", "--service-name", dest="service_name",
                          help="Service name", default=None)
    optional.add_argument("-n", "--name", dest="name",
                          help="Trigger name", default=None)
    optional.add_argument("-fl", "--filters", dest="filters", default='{}',
                          help="Json filter")
    optional.add_argument("-fn", "--function-name", dest="function_name", default='run',
                          help="Function name")

    # delete
    a = subparser_parser.add_parser("delete", help="Delete Trigger")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-t", "--trigger-name", dest="trigger_name", default=None,
                          help="Trigger name", required=True)

    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-f", "--service-name", dest="service_name", default=None,
                          help="Service name")
    optional.add_argument("-p", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-pkg", "--package-name", dest="package_name", default=None,
                          help="Package name")

    a = subparser_parser.add_parser("ls", help="List triggers")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-pr", "--project-name", dest="project_name", default=None,
                          help="Project name")
    optional.add_argument("-pkg", "--package-name", dest="package_name", default=None,
                          help="Package name")
    optional.add_argument("-s", "--service-name", dest="service_name", default=None,
                          help="Service name")

    ############
    # Deploy   #
    ############
    # subparsers.add_parser("deploy", help="Login using web Auth0 interface")

    a = subparsers.add_parser("deploy", help="deploy with json file")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-f", dest="json_file", default=None,
                          help="Path to json file")
    required.add_argument("-p", dest="project_name", default=None,
                          help="Project name")

    ############
    # Generate   #
    ############
    # subparsers.add_parser("deploy", help="Login using web Auth0 interface")

    a = subparsers.add_parser("generate", help="generate a json file")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("--option", dest="package_type", default=None,
                          help="cataluge of examples")
    optional.add_argument("-p", "--package-name", dest="package_name", default=None,
                          help="Package name")

    ############
    # packages #
    ############
    subparser = subparsers.add_parser("packages", help="Operations with packages")
    subparser_parser = subparser.add_subparsers(
        dest="packages", help="package operations"
    )

    # ACTIONS #
    # ls
    a = subparser_parser.add_parser("ls", help="List packages")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--project-name", dest="project_name", default=None,
                          help="Project name")

    # push
    a = subparser_parser.add_parser("push", help="Create package in platform")

    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-src", "--src-path", metavar='\b', default=None,
                          help="Revision to deploy if selected True")
    optional.add_argument("-cid", "--codebase-id", metavar='\b', default=None,
                          help="Revision to deploy if selected True")
    optional.add_argument("-pr", "--project-name", metavar='\b', default=None,
                          help="Project name")
    optional.add_argument("-p", "--package-name", metavar='\b', default=None,
                          help="Package name")
    optional.add_argument("-c", "--checkout", action='store_true', default=False, help="checkout the new package")

    # deploy
    a = subparser_parser.add_parser("deploy", help="Deploy package to platform")
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-p", "--package-name", metavar='\b', default=None,
                          help="Package name")
    optional.add_argument("-pr", "--project-name", metavar='\b', default=None,
                          help="Project name")
    optional.add_argument("--module-name", metavar='\b', default='default_module',
                          help="Package module name")
    optional.add_argument("-c", "--checkout", action='store_true', default=False, help="checkout the new package")

    # test
    a = subparser_parser.add_parser(
        "test", help="Tests that Package locally using mock.json"
    )
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-c", "--concurrency", metavar='\b', default=10,
                          help="Revision to deploy if selected True")
    optional.add_argument("-f", "--function-name", metavar='\b', default='run',
                          help="Function to test")
    # checkout
    a = subparser_parser.add_parser("checkout", help="checkout a package")
    required = a.add_argument_group("required named arguments")
    required.add_argument("-p", "--package-name", metavar='\b', help="package name")

    # delete
    a = subparser_parser.add_parser(
        "delete", help="Delete Package"
    )
    optional = a.add_argument_group("optional named arguments")
    optional.add_argument("-pkg", "--package-name", dest="package_name", default=None,
                          help="Package name")

    optional.add_argument("-p", "--project-name", dest="project_name", default=None,
                          help="Project name")

    #########
    # Shell #
    #########
    # ls
    subparsers.add_parser("ls", help="List directories")
    #
    # pwd
    subparsers.add_parser("pwd", help="Get current working directory")

    # cd
    subparser = subparsers.add_parser("cd", help="Change current working directory")
    subparser.add_argument(dest='dir')

    # mkdir
    subparser = subparsers.add_parser("mkdir", help="Make directory")
    subparser.add_argument(dest='name')

    # clear
    subparsers.add_parser("clear", help="Clear shell")

    ########
    # Exit #
    ########
    subparsers.add_parser("exit", help="Exit interactive shell")

    return parser


================================================
File: dtlpy/entities/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from . import package_defaults
from .annotation_definitions import BaseAnnotationDefinition
from .base_entity import BaseEntity, DlEntity, DlProperty, DlList, EntityScopeLevel
from .item import Item, ItemStatus, Modality, ModalityTypeEnum, ModalityRefTypeEnum, ExportMetadata
from .links import Link, ItemLink, UrlLink, LinkTypeEnum
from .trigger import Trigger, TriggerResource, TriggerAction, TriggerExecutionMode, BaseTrigger, CronTrigger, \
    TriggerType
from .project import Project, MemberRole
from .artifact import ItemArtifact, LocalArtifact, LinkArtifact, ArtifactType, Artifact
from .dataset import Dataset, ExpirationOptions, IndexDriver, ExportType
from .codebase import Codebase
from .annotation import Annotation, FrameAnnotation, ViewAnnotationOptions, AnnotationStatus, AnnotationType, \
    ExportVersion
from .annotation_collection import AnnotationCollection
from .paged_entities import PagedEntities
from .filters import Filters, FiltersKnownFields, FiltersResource, FiltersOperations, FiltersMethod, \
    FiltersOrderByDirection
from .recipe import Recipe
from .ontology import Ontology, AttributesTypes, AttributesRange
from .annotation_definitions import Box, Cube, Cube3d, Point, Segmentation, Polygon, Ellipse, Classification, \
    Subtitle, Text, FreeText, RefImage, \
    Polyline, Comparison, UndefinedAnnotationType, Note, Message, Description, Pose, Gis, GisType
from .label import Label
from .codebase import Codebase, PackageCodebaseType, ItemCodebase, GitCodebase, FilesystemCodebase, LocalCodebase
from .package import Package, RequirementOperator, PackageRequirement
from .package_module import PackageModule
from .package_slot import PackageSlot, SlotPostAction, SlotPostActionType, SlotDisplayScope, SlotDisplayScopeResource, \
    UiBindingPanel
from .package_function import PackageFunction, FunctionIO, PackageInputType
from .time_series import TimeSeries
from .service import Service, KubernetesAutoscalerType, KubernetesAutuscalerType, KubernetesRabbitmqAutoscaler, KubernetesAutoscaler, KubernetesRPSAutoscaler, \
    InstanceCatalog, KubernetesRuntime, ServiceType, ServiceModeType
from .execution import Execution, ExecutionStatus
from .command import Command, CommandsStatus
from .assignment import Assignment, Workload, WorkloadUnit
from .task import Task, ItemAction, TaskPriority, ConsensusTaskType
from .directory_tree import DirectoryTree
from .user import User
from .bot import Bot
from .webhook import Webhook, HttpMethod
from .model import Model, DatasetSubsetType, PlotSample, ModelStatus
from .driver import Driver, S3Driver, GcsDriver, AzureBlobDriver
from .pipeline import Pipeline, PipelineStats, PipelineResumeOption, PipelineSettings, Variable, CompositionStatus
from .node import PipelineConnection, PipelineNode, PipelineConnectionPort, PipelineNodeIO, TaskNode, \
    CodeNode, FunctionNode, PipelineNodeType, PipelineNameSpace, DatasetNode
from .pipeline_execution import PipelineExecution, PipelineExecutionNode, PipelineExecutionStatus, CycleRerunMethod
from .feature import Feature
from .feature_set import FeatureSet, FeatureEntityType
from .organization import Organization, OrganizationsPlans, MemberOrgRole, CacheAction, PodType
from .analytic import ServiceSample, ExecutionSample, PipelineExecutionSample
from .integration import Integration, IntegrationType
from .driver import Driver, ExternalStorage
from .setting import Role, PlatformEntityType, SettingsValueTypes, SettingsTypes, SettingsSectionNames, SettingScope, \
    BaseSetting, UserSetting, Setting
from .reflect_dict import ReflectDict
from .dpk import Dpk, Panel, Toolbar, Components, DpkComputeConfig, DpkComponentChannel, PipelineNode, \
    CustomNodeScope, ToolbarInvoke
from .app import App, AppScope
from .app_module import AppModule
from .resource_execution import ResourceExecution
from .message import Message, NotificationEventContext
from .prompt_item import Prompt, PromptItem, PromptType
from .compute import ClusterProvider, ComputeType, ComputeStatus, Toleration, DeploymentResource, DeploymentResources, \
    NodePool, AuthenticationIntegration, Authentication, ComputeCluster, ComputeContext, Compute, KubernetesCompute, \
    ServiceDriver
from .gis_item import  ItemGis, Layer
from .collection import Collection


================================================
File: dtlpy/entities/analytic.py
================================================
import logging
from .. import entities

logger = logging.getLogger(name='dtlpy')


class BaseSample:
    def __init__(self,
                 start_time,
                 end_time,
                 project_id,
                 org_id,
                 pipeline_id,
                 event_type,
                 action,
                 status,
                 other_keys: dict = None
                 ):
        self.start_time = start_time
        self.end_time = end_time
        self.project_id = project_id
        self.org_id = org_id
        self.pipeline_id = pipeline_id
        self.event_type = event_type
        self.action = action
        self.status = status
        self.other_keys = other_keys

    def to_json(self):
        _json = {
            'startTime': self.start_time,
            'endTime': self.end_time,
            'context': {
                'projectId': self.project_id,
                'projectOrgId': self.org_id,
                'pipelineId': self.pipeline_id
            },
            'eventType': self.event_type,
            'action': self.action,
            'data': {
                'status': self.status
            }
        }
        if self.other_keys is not None:
            if 'context' in self.other_keys:
                _json['context'].update(self.other_keys['context'])
            if 'data' in self.other_keys:
                _json['data'].update(self.other_keys['data'])
            _json.update({k: v for k, v in self.other_keys.items() if k not in ['context', 'data']})

        return _json


class ServiceSample(BaseSample):
    def __init__(self,
                 start_time=None,
                 end_time=None,
                 user_id=None,
                 project_id=None,
                 org_id=None,
                 pipeline_id=None,
                 pipeline_node_id=None,
                 service_id=None,
                 pod_id=None,
                 pod_type=None,
                 event_type=None,
                 entity_type=None,
                 action=None,
                 status=None,
                 num_restarts=None,
                 cpu=None,
                 ram=None,
                 queue_size=None,
                 num_executions=None,
                 service_type: entities.ServiceType = None,
                 interval: int = None,
                 driver_id: str = None,
                 other_keys: dict = None
                 ):
        super().__init__(
            start_time=start_time,
            end_time=end_time,
            project_id=project_id,
            org_id=org_id,
            pipeline_id=pipeline_id,
            event_type=event_type,
            action=action,
            status=status,
            other_keys=other_keys
        )
        self.user_id = user_id
        self.pipeline_node_id = pipeline_node_id
        self.service_id = service_id
        self.pod_id = pod_id
        self.pod_type = pod_type
        self.entity_type = entity_type
        self.num_restarts = num_restarts
        self.cpu = cpu
        self.ram = ram
        self.queue_size = queue_size
        self.num_executions = num_executions
        self.service_type = service_type if service_type is not None else entities.ServiceType.REGULAR
        self.interval = interval
        self.driver_id = driver_id

    def to_json(self):
        _json = super().to_json()
        _json['context'].update({
            'userId': self.user_id,
            'pipelineNodeId': self.pipeline_node_id,
            'serviceId': self.service_id,
            'podId': self.pod_id,
            'podType': self.pod_type,
            'serviceType': self.service_type
        })
        _json['data'].update({
            'numRestarts': self.num_restarts,
            'cpu': self.cpu,
            'ram': self.ram,
            'queueSize': self.queue_size,
            'numExecutions': self.num_executions,
            'interval': self.interval,
            'driverId': self.driver_id
        })
        _json.update({
            'entityType': self.entity_type
        })
        _json['context'] = {k: v for k, v in _json['context'].items() if v is not None}
        _json['data'] = {k: v for k, v in _json['data'].items() if v is not None}
        return {k: v for k, v in _json.items() if v is not None}

    @classmethod
    def from_json(cls, _json):
        inst = cls(
            start_time=_json.get('startTime', None),
            end_time=_json.get('endTime', None),
            user_id=_json.get('context', {}).get('userId', None),
            project_id=_json.get('context', {}).get('projectId', None),
            org_id=_json.get('context', {}).get('projectOrgId', None),
            pipeline_id=_json.get('context', {}).get('pipelineId', None),
            pipeline_node_id=_json.get('context', {}).get('pipelineNodeId', None),
            service_id=_json.get('context', {}).get('serviceId', None),
            pod_id=_json.get('context', {}).get('podId', None),
            pod_type=_json.get('context', {}).get('podType', None),
            event_type=_json.get('eventType', None),
            entity_type=_json.get('EntityType', None),
            action=_json.get('action', None),
            status=_json.get('data', {}).get('status', None),
            num_restarts=_json.get('data', {}).get('numRestarts', None),
            cpu=_json.get('data', {}).get('cpu', None),
            ram=_json.get('data', {}).get('ram', None),
            queue_size=_json.get('data', {}).get('queueSize', None),
            num_executions=_json.get('data', {}).get('numExecutions', None),
            service_type=_json.get('type', entities.ServiceType.REGULAR),
            interval=_json.get('data', {}).get('interval', None),
            driver_id=_json.get('data', {}).get('driverId', None)
        )
        return inst


class ExecutionSample(BaseSample):
    def __init__(self,
                 start_time=None,
                 end_time=None,
                 project_id=None,
                 org_id=None,
                 pipeline_id=None,
                 event_type=None,
                 action=None,
                 status=None,
                 user_id=None,
                 account_id=None,
                 pipeline_node_id=None,
                 pipeline_execution_id=None,
                 service_id=None,
                 execution_id=None,
                 trigger_id=None,
                 function_name=None,
                 duration=None,
                 other_keys: dict = None
                 ):
        super().__init__(
            start_time=start_time,
            end_time=end_time,
            project_id=project_id,
            org_id=org_id,
            pipeline_id=pipeline_id,
            event_type=event_type,
            action=action,
            status=status,
            other_keys=other_keys
        )
        self.user_id = user_id
        self.account_id = account_id
        self.pipeline_node_id = pipeline_node_id
        self.pipeline_execution_id = pipeline_execution_id
        self.service_id = service_id
        self.execution_id = execution_id
        self.trigger_id = trigger_id
        self.function_name = function_name
        self.duration = duration

    def to_json(self):
        _json = super().to_json()
        _json['context'].update({
            'userId': self.user_id,
            'accountId': self.account_id,
            'pipelineNodeId': self.pipeline_node_id,
            'pipelineExecutionId': self.pipeline_execution_id,
            'serviceId': self.service_id,
            'triggerId': self.trigger_id
        })
        _json['data'].update({
            'functionName': self.function_name,
            'duration': self.duration
        })
        _json['context'] = {k: v for k, v in _json['context'].items() if v is not None}
        _json['data'] = {k: v for k, v in _json['data'].items() if v is not None}
        return {k: v for k, v in _json.items() if v is not None}

    @classmethod
    def from_json(cls, _json):
        inst = cls(
            start_time=_json.get('startTime', None),
            end_time=_json.get('endTime', None),
            user_id=_json.get('context', {}).get('userId', None),
            project_id=_json.get('context', {}).get('projectId', None),
            org_id=_json.get('context', {}).get('projectOrgId', None),
            account_id=_json.get('context', {}).get('accountId', None),
            pipeline_id=_json.get('context', {}).get('pipelineId', None),
            pipeline_node_id=_json.get('context', {}).get('pipelineNodeId', None),
            pipeline_execution_id=_json.get('context', {}).get('pipelineExecutionId', None),
            service_id=_json.get('context', {}).get('serviceId', None),
            execution_id=_json.get('context', {}).get('pipelineExecutionId', None),
            trigger_id=_json.get('context', {}).get('triggerId', None),
            event_type=_json.get('eventType', None),
            action=_json.get('action', None),
            status=_json.get('data', {}).get('status', None),
            function_name=_json.get('data', {}).get('functionName', None),
            duration=_json.get('data', {}).get('duration', None)
        )
        return inst


class PipelineExecutionSample(BaseSample):
    def __init__(self,
                 start_time=None,
                 end_time=None,
                 project_id=None,
                 org_id=None,
                 pipeline_id=None,
                 event_type=None,
                 action=None,
                 status=None,
                 account_id=None,
                 pipeline_node_id=None,
                 pipeline_execution_id=None,
                 trigger_id=None,
                 node_status=None,
                 other_keys: dict = None
                 ):
        super().__init__(
            start_time=start_time,
            end_time=end_time,
            project_id=project_id,
            org_id=org_id,
            pipeline_id=pipeline_id,
            event_type=event_type,
            action=action,
            status=status,
            other_keys=other_keys
        )
        self.account_id = account_id
        self.pipeline_node_id = pipeline_node_id
        self.pipeline_execution_id = pipeline_execution_id
        self.trigger_id = trigger_id
        self.node_status = node_status

    def to_json(self):
        _json = super().to_json()
        _json['context'].update({
            'accountId': self.account_id,
            'pipelineNodeId': self.pipeline_node_id,
            'pipelineExecutionId': self.pipeline_execution_id,
            'triggerId': self.trigger_id
        })
        _json['data'].update({
            'nodeStatus': self.node_status
        })
        _json['context'] = {k: v for k, v in _json['context'].items() if v is not None}
        _json['data'] = {k: v for k, v in _json['data'].items() if v is not None}
        return {k: v for k, v in _json.items() if v is not None}

    @classmethod
    def from_json(cls, _json):
        inst = cls(
            start_time=_json.get('startTime', None),
            end_time=_json.get('endTime', None),
            project_id=_json.get('context', {}).get('projectId', None),
            org_id=_json.get('context', {}).get('projectOrgId', None),
            account_id=_json.get('context', {}).get('accountId', None),
            pipeline_id=_json.get('context', {}).get('pipelineId', None),
            pipeline_execution_id=_json.get('context', {}).get('pipelineExecutionId', None),
            trigger_id=_json.get('context', {}).get('triggerId', None),
            event_type=_json.get('eventType', None),
            action=_json.get('action', None),
            status=_json.get('data', {}).get('status', None),
            pipeline_node_id=_json.get('data', {}).get('pipelineNodeId', None),
            node_status=_json.get('data', {}).get('nodeStatus', None)
        )
        return inst


================================================
File: dtlpy/entities/annotation_collection.py
================================================
import mimetypes
import traceback
import datetime
import webvtt
import logging
import attr
import json
import os

import numpy as np
from PIL import Image

from .. import entities, PlatformException, miscellaneous

logger = logging.getLogger(name='dtlpy')


@attr.s
class AnnotationCollection(entities.BaseEntity):
    """
        Collection of Annotation entity
    """
    item = attr.ib(default=None)
    annotations = attr.ib()  # type: miscellaneous.List[entities.Annotation]
    _dataset = attr.ib(repr=False, default=None)
    _colors = attr.ib(repr=False, default=None)

    @property
    def dataset(self):
        if self._dataset is None:
            self._dataset = self.item.dataset
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @annotations.default
    def set_annotations(self):
        return list()

    def __iter__(self):
        for annotation in self.annotations:
            yield annotation

    def __getitem__(self, index: int):
        return self.annotations[index]

    def __len__(self):
        return len(self.annotations)

    def add(self,
            annotation_definition,
            object_id=None,
            frame_num=None,
            end_frame_num=None,
            start_time=None,
            end_time=None,
            automated=True,
            fixed=True,
            object_visible=True,
            metadata=None,
            parent_id=None,
            prompt_id=None,
            model_info=None):
        """
        Add annotations to collection

        :param annotation_definition: dl.Polygon, dl.Segmentation, dl.Point, dl.Box etc.
        :param object_id: Object id (any id given by user). If video - must input to match annotations between frames
        :param frame_num: video only, number of frame
        :param end_frame_num: video only, the end frame of the annotation
        :param start_time: video only, start time of the annotation
        :param end_time: video only, end time of the annotation
        :param automated:
        :param fixed: video only, mark frame as fixed
        :param object_visible: video only, does the annotated object is visible
        :param metadata: optional, metadata dictionary for annotation
        :param parent_id: set a parent for this annotation (parent annotation ID)
        :param prompt_id: Connect the annotation with a specific prompt in a dl.PromptItem
        :param model_info: optional - set model on annotation {'confidence': 0, # [Mandatory], (Float between 0-1)
                                                               'name': '', # [Optional], ('name' refers to 'model_name')
                                                               'model_id': ''} # [Optional]
        :return:
        """
        if model_info is not None:
            if not isinstance(model_info, dict) or 'confidence' not in model_info:
                raise ValueError('"model_info" must be a dict with key: "confidence"')
            if metadata is None:
                metadata = dict()
            if 'user' not in metadata:
                metadata['user'] = dict()
            confidence = float(model_info['confidence'])
            if confidence < 0 or confidence > 1:
                raise ValueError('"confidence" must be a float between 0 and 1')
            metadata['user']['model'] = {'confidence': confidence,
                                         'name': model_info.get('name'),
                                         'model_id': model_info.get('model_id'),
                                         }
        if prompt_id is not None:
            if metadata is None:
                metadata = dict()
            if 'system' not in metadata:
                metadata['system'] = dict()
            metadata['system']['promptId'] = prompt_id

        # to support list of definitions with same parameters
        if not isinstance(annotation_definition, list):
            annotation_definition = [annotation_definition]

        for single_definition in annotation_definition:
            annotation = entities.Annotation.new(item=self.item,
                                                 annotation_definition=single_definition,
                                                 frame_num=frame_num,
                                                 automated=automated,
                                                 metadata=metadata,
                                                 object_id=object_id,
                                                 parent_id=parent_id,
                                                 start_time=start_time,
                                                 end_time=end_time)
            #  add frame if exists
            if (frame_num is not None or start_time is not None) and (
                    self.item is None or 'audio' not in self.item.metadata.get('system').get('mimetype', '')):
                if object_id is None:
                    raise ValueError('Video Annotation must have object_id.')
                else:
                    if isinstance(object_id, int):
                        object_id = '{}'.format(object_id)
                    elif not isinstance(object_id, str) or not object_id.isnumeric():
                        raise ValueError('Object id must be an int or a string containing only numbers.')
                # find matching element_id
                matched_ind = [i_annotation
                               for i_annotation, annotation in enumerate(self.annotations)
                               if annotation.object_id == object_id]
                if len(matched_ind) == 0:
                    # no matching object id found - create new one
                    self.annotations.append(annotation)
                    matched_ind = len(self.annotations) - 1
                elif len(matched_ind) == 1:
                    matched_ind = matched_ind[0]
                else:
                    raise PlatformException(error='400',
                                            message='more than one annotation with same object id: {}'.format(
                                                object_id))

                self.annotations[matched_ind].add_frames(annotation_definition=single_definition,
                                                         frame_num=frame_num,
                                                         end_frame_num=end_frame_num,
                                                         start_time=start_time,
                                                         end_time=end_time,
                                                         fixed=fixed,
                                                         object_visible=object_visible)
            else:
                # add new annotation to list
                self.annotations.append(annotation)

    ############
    # Plotting #
    ############
    def show(self,
             image=None,
             thickness=None,
             with_text=False,
             height=None,
             width=None,
             annotation_format: entities.ViewAnnotationOptions = entities.ViewAnnotationOptions.MASK,
             label_instance_dict=None,
             color=None,
             alpha=1.,
             frame_num=None):
        """
        Show annotations according to annotation_format

        **Prerequisites**: Any user can upload annotations.

        :param ndarray image: empty or image to draw on
        :param int height: height
        :param int width: width
        :param int thickness: line thickness
        :param bool with_text: add label to annotation
        :param dl.ViewAnnotationOptions annotation_format: how to show thw annotations. options: list(dl.ViewAnnotationOptions)
        :param dict label_instance_dict: instance label map {'Label': 1, 'More': 2}
        :param tuple color: optional - color tuple
        :param float alpha: opacity value [0 1], default 1
        :param int frame_num: for video annotation, show specific frame
        :return: ndarray of the annotations

        **Example**:

        .. code-block:: python

            image = builder.show(image='ndarray',
                        thickness=1,
                        annotation_format=dl.VIEW_ANNOTATION_OPTIONS_MASK)
        """
        # if 'video' in self.item.mimetype and (annotation_format != 'json' or annotation_format != ['json']):
        #     raise PlatformException('400', 'Cannot show mask or instance of video item')
        # height/weight
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        # split the annotations to binary and not binary to put the binaries first
        segment_annotations = list()
        rest_annotations = list()
        for annotation in self.annotations:
            if annotation.type == 'binary':
                segment_annotations.append(annotation)
            else:
                rest_annotations.append(annotation)
        all_annotations = segment_annotations + rest_annotations
        # gor over all annotations and put the id where the annotations are
        for annotation in all_annotations:
            # get the mask of the annotation
            image = annotation.show(thickness=thickness,
                                    with_text=with_text,
                                    height=height,
                                    width=width,
                                    label_instance_dict=label_instance_dict,
                                    annotation_format=annotation_format,
                                    image=image,
                                    alpha=alpha,
                                    color=color,
                                    frame_num=frame_num)
        return image

    def _video_maker(self,
                     input_filepath,
                     output_filepath,
                     thickness=1,
                     annotation_format=entities.ViewAnnotationOptions.ANNOTATION_ON_IMAGE,
                     with_text=False,
                     alpha=1.):
        """
        create a video from frames

        :param input_filepath: str - input file path
        :param output_filepath: str - out put file path
        :param thickness: int - thickness of the annotations
        :param annotation_format: str - ViewAnnotationOptions - annotations format
        :param with_text: bool - if True show the label in the output
        :param float alpha: opacity value [0 1], default 1

        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        is_color = True
        if annotation_format in [entities.ViewAnnotationOptions.INSTANCE,
                                 entities.ViewAnnotationOptions.OBJECT_ID]:
            is_color = False
        # read input video
        fps = self.item.system.get('fps', 0)
        height = self.item.system.get('height', 0)
        width = self.item.system.get('width', 0)
        nb_frames = int(self.item.system.get('ffmpeg', {}).get('nb_read_frames'))
        writer = cv2.VideoWriter(filename=output_filepath,
                                 fourcc=cv2.VideoWriter_fourcc(*"mp4v"),
                                 fps=fps,
                                 frameSize=(width, height),
                                 isColor=is_color)
        if input_filepath is not None and is_color:
            reader = cv2.VideoCapture(input_filepath)
        else:
            reader = None

        for frame_num in range(nb_frames):
            if reader is not None:
                ret, frame = reader.read()
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)
            else:
                frame = None
            frame = self.show(image=frame,
                              annotation_format=annotation_format,
                              thickness=thickness,
                              alpha=alpha,
                              height=height,
                              width=width,
                              with_text=with_text,
                              frame_num=frame_num)
            if is_color:
                frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)
            writer.write(frame)
        writer.release()
        if reader is not None:
            reader.release()

    @staticmethod
    def _set_flip_args(orientation):
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. that make the exif orientation not supported')
            raise
        if orientation == 3:
            return cv2.ROTATE_180, cv2.ROTATE_180, 0
        elif orientation == 4:
            return cv2.ROTATE_180, cv2.ROTATE_180, 1
        elif orientation == 5:
            return cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, 1
        elif orientation == 6:
            return cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, 0
        elif orientation == 7:
            return cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_90_CLOCKWISE, 1
        else:
            return cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_90_CLOCKWISE, 0

    def download(self,
                 filepath,
                 img_filepath=None,
                 annotation_format: entities.ViewAnnotationOptions = entities.ViewAnnotationOptions.JSON,
                 height=None,
                 width=None,
                 thickness=1,
                 with_text=False,
                 orientation=0,
                 alpha=1):
        """
        Save annotations to file

        **Prerequisites**: Any user can upload annotations.

        :param str filepath: path to save annotation
        :param str img_filepath: img file path - needed for img_mask
        :param dl.ViewAnnotationOptions annotation_format: how to show thw annotations. options: list(dl.ViewAnnotationOptions)
        :param int height: height
        :param int width: width
        :param int thickness: thickness
        :param bool with_text: add a text to an image
        :param int orientation: the image orientation
        :param float alpha: opacity value [0 1], default 1
        :return: file path of the download annotation
        :rtype: str

        **Example**:

        .. code-block:: python

            filepath = builder.download(filepath='filepath', annotation_format=dl.ViewAnnotationOptions.MASK)
        """
        dir_name, ex = os.path.splitext(filepath)
        if annotation_format == entities.ViewAnnotationOptions.JSON:
            if not ex:
                filepath = os.path.join(dir_name, '{}.json'.format(os.path.splitext(self.item.name)[0]))
            _json = dict()
            if self.item is not None:
                _json = {'_id': self.item.id,
                         'filename': self.item.filename}
            annotations = list()
            for ann in self.annotations:
                annotations.append(ann.to_json())
            _json['annotations'] = annotations
            if self.item is not None:
                _json['metadata'] = self.item.metadata
            with open(filepath, 'w+') as f:
                json.dump(_json, f, indent=2)
        elif annotation_format in [entities.ViewAnnotationOptions.MASK,
                                   entities.ViewAnnotationOptions.INSTANCE,
                                   entities.ViewAnnotationOptions.OBJECT_ID,
                                   entities.ViewAnnotationOptions.ANNOTATION_ON_IMAGE]:
            if not ex:
                if 'video' in self.item.mimetype:
                    filepath = os.path.join(dir_name, '{}.mp4'.format(os.path.splitext(self.item.name)[0]))
                else:
                    filepath = os.path.join(dir_name, '{}.png'.format(os.path.splitext(self.item.name)[0]))
            image = None
            if 'video' in self.item.mimetype:
                if annotation_format == entities.ViewAnnotationOptions.ANNOTATION_ON_IMAGE:
                    if img_filepath is None:
                        img_filepath = self.item.download()
                    annotation_format = entities.ViewAnnotationOptions.MASK
                self._video_maker(input_filepath=img_filepath,
                                  output_filepath=filepath,
                                  thickness=thickness,
                                  annotation_format=annotation_format,
                                  with_text=with_text,
                                  alpha=alpha
                                  )
                return filepath
            if annotation_format == entities.ViewAnnotationOptions.ANNOTATION_ON_IMAGE:
                if img_filepath is None:
                    img_filepath = self.item.download()
                annotation_format = entities.ViewAnnotationOptions.MASK
                image = np.asarray(Image.open(img_filepath))
            if orientation in [3, 4, 5, 6, 7, 8]:
                try:
                    import cv2
                except (ImportError, ModuleNotFoundError):
                    logger.error(
                        'Import Error! Cant import cv2. that make the exif orientation not supported')
                    raise
                first_rotate, second_rotate, flip = self._set_flip_args(orientation=orientation)
                image = cv2.rotate(image, first_rotate)
                if flip:
                    image = np.flip(image, 1)
            mask = self.show(image=image,
                             thickness=thickness,
                             with_text=with_text,
                             height=height,
                             width=width,
                             alpha=alpha,
                             annotation_format=annotation_format)
            if orientation not in [3, 4, 5, 6, 7, 8]:
                img = Image.fromarray(mask.astype(np.uint8))
            else:
                img = Image.fromarray(cv2.rotate(mask.astype(np.uint8), second_rotate))
            img.save(filepath)
        elif annotation_format == entities.ViewAnnotationOptions.VTT:
            if not ex:
                filepath = '{}/{}.vtt'.format(dir_name, os.path.splitext(self.item.name)[0])
            annotations_dict = [{'start_time': annotation.start_time,
                                 'end_time': annotation.end_time,
                                 'text': annotation.coordinates['text']} for annotation in self.annotations if
                                annotation.type in ['subtitle']]
            sorted_by_start_time = sorted(annotations_dict, key=lambda i: i['start_time'])
            vtt = webvtt.WebVTT()
            for ann in sorted_by_start_time:
                s = str(datetime.timedelta(seconds=ann['start_time']))
                if len(s.split('.')) == 1:
                    s += '.000'
                e = str(datetime.timedelta(seconds=ann['end_time']))
                if len(e.split('.')) == 1:
                    e += '.000'
                caption = webvtt.Caption(
                    '{}'.format(s),
                    '{}'.format(e),
                    '{}'.format(ann['text'])
                )
                vtt.captions.append(caption)
            vtt.save(filepath)
        else:
            raise PlatformException(error="400", message="Unknown annotation option: {}".format(annotation_format))
        return filepath

    ############
    # Platform #
    ############
    def update(self, system_metadata=True):
        """
        Update an existing annotation in host.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param system_metadata: True, if you want to change metadata system
        :return: Annotation object
        :rtype: dtlpy.entities.annotation.Annotation

        **Example**:

        .. code-block:: python

            annotation = builder.update()
        """
        if self.item is None:
            raise PlatformException('400', 'missing item to perform platform update')
        return self.item.annotations.update(annotations=self.annotations, system_metadata=system_metadata)

    def delete(self):
        """
        Remove an annotation from item

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            is_deleted = builder.delete()
        """
        if self.item is None:
            raise PlatformException('400', 'missing item to perform platform delete')
        return [self.item.annotations.delete(annotation_id=annotation.id) for annotation in self.annotations]

    def upload(self):
        """
        Create a new annotation in host

        **Prerequisites**: Any user can upload annotations.

        :return: Annotation entity
        :rtype: dtlpy.entities.annotation.Annotation

        **Example**:

        .. code-block:: python

            annotation = builder.upload()
        """
        if self.item is None:
            raise PlatformException('400', 'missing item to perform platform upload')
        return self.item.annotations.upload(self.annotations)

    @staticmethod
    def _json_to_annotation(item: entities.Item, w_json: dict, is_video=None, fps=25, item_metadata=None,
                            client_api=None):
        try:
            annotation = entities.Annotation.from_json(_json=w_json,
                                                       fps=fps,
                                                       item_metadata=item_metadata,
                                                       is_video=is_video,
                                                       item=item,
                                                       client_api=client_api)
            status = True
        except Exception:
            annotation = traceback.format_exc()
            status = False
        return status, annotation

    @classmethod
    def from_json(cls, _json: list, item=None, is_video=None, fps=25, height=None, width=None,
                  client_api=None, is_audio=None) -> 'AnnotationCollection':
        """
        Create an annotation collection object from platform json

        :param dict _json: platform json
        :param dtlpy.entities.item.Item item: item
        :param client_api: ApiClient entity
        :param bool is_video: whether the item is video
        :param fps: frame rate of the video
        :param float height: height
        :param float width: width
        :param bool is_audio: is audio
        :return: annotation object
        :rtype: dtlpy.entities.annotation.Annotation
        """
        if item is None:
            if isinstance(_json, dict):
                metadata = _json.get('metadata', dict())
                system_metadata = metadata.get('system', dict())
                if is_video is None:
                    if 'mimetype' in system_metadata:
                        is_video = 'video' in system_metadata['mimetype']
                        is_audio = 'audio' in system_metadata['mimetype']
                    elif 'filename' in _json:
                        ext = os.path.splitext(_json['filename'])[-1]
                        try:
                            is_video = 'video' in mimetypes.types_map[ext.lower()]
                            is_audio = 'is_audio' in mimetypes.types_map[ext.lower()]
                        except Exception:
                            logger.info("Unknown annotation's item type. Default item type is set to: image")
                    else:
                        logger.info("Unknown annotation's item type. Default item type is set to: image")
                if is_video:
                    fps = system_metadata.get('fps', fps)
                    ffmpeg_info = system_metadata.get('ffmpeg', dict())
                    height = ffmpeg_info.get('height', None)
                    width = ffmpeg_info.get('width', None)
                elif is_audio:
                    fps = system_metadata.get('fps', 1000)
                    height = system_metadata.get('height', None)
                    width = system_metadata.get('width', None)
                else:
                    fps = 0
                    height = system_metadata.get('height', None)
                    width = system_metadata.get('width', None)
        else:
            if client_api is None:
                client_api = item._client_api
            fps = item.fps
            height = item.height
            width = item.width

        item_metadata = {
            'fps': fps,
            'height': height,
            'width': width
        }

        if 'annotations' in _json:
            _json = _json['annotations']

        results = list([None for _ in range(len(_json))])
        for i_json, single_json in enumerate(_json):
            results[i_json] = cls._json_to_annotation(item=item,
                                                      fps=fps,
                                                      item_metadata=item_metadata,
                                                      is_video=is_video,
                                                      w_json=single_json,
                                                      client_api=client_api)
        # log errors
        _ = [logger.warning(j[1]) for j in results if j[0] is False]

        # return good jobs
        annotations = [j[1] for j in results if j[0] is True]

        # sort
        if is_video:
            annotations.sort(key=lambda x: x.start_frame)
        else:
            annotations.sort(key=lambda x: x.label)

        return cls(annotations=miscellaneous.List(annotations), item=item)

    @classmethod
    def from_json_file(cls, filepath, item=None):
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls.from_json(_json=data, item=item)

    def from_vtt_file(self, filepath):
        """
        convert annotation from vtt format

        :param str filepath: path to the file
        """
        for caption in webvtt.read(filepath):
            h, m, s = caption.start.split(':')
            start_time = datetime.timedelta(hours=float(h), minutes=float(m), seconds=float(s)).total_seconds()
            h, m, s = caption.end.split(':')
            end_time = datetime.timedelta(hours=float(h), minutes=float(m), seconds=float(s)).total_seconds()
            annotation_definition = entities.Subtitle(text=caption.text, label='Text')
            annotation = entities.Annotation.new(
                annotation_definition=annotation_definition,
                item=self.item,
                start_time=start_time)

            annotation.add_frames(annotation_definition=annotation_definition,
                                  start_time=start_time,
                                  end_time=end_time)

            self.annotations.append(annotation)

    def from_instance_mask(self, mask, instance_map=None):
        """
        convert annotation from instance mask format

        :param mask: the mask annotation
        :param instance_map: labels
        """
        if instance_map is None:
            instance_map = self.item.dataset.instance_map
        # go over all instance ids
        for label, instance_id in instance_map.items():
            # find a binary mask per instance
            class_mask = instance_id == mask
            if not np.any(class_mask):
                continue
            # add the binary mask to the annotation builder
            self.add(annotation_definition=entities.Segmentation(geo=class_mask, label=label))

    def to_json(self):
        """
        Convert annotation object to a platform json representation

        :return: platform json
        :rtype: dict
        """
        if self.item is None:
            item_id = None
            item_name = None
        else:
            item_id = self.item.id
            item_name = self.item.filename

        _json = {
            "_id": item_id,
            "filename": item_name,
            'annotations': [annotation.to_json() for annotation in self.annotations]
        }

        return _json

    def print(self, to_return=False, columns=None):
        """

        :param to_return:
        :param columns:
        """
        return miscellaneous.List(self.annotations).print(to_return=to_return, columns=columns)

    #########################
    # For video annotations #
    #########################
    def get_frame(self, frame_num):
        """
        Get frame

        :param int frame_num: frame num
        :return: AnnotationCollection
        """
        frame_collection = AnnotationCollection(item=self.item)
        for annotation in self.annotations:
            if frame_num in annotation.frames:
                annotation.set_frame(frame=frame_num)
                frame_collection.annotations.append(annotation)
        return frame_collection

    def video_player(self):
        from ..utilities.videos.video_player import VideoPlayer
        _ = VideoPlayer(dataset_id=self.item.dataset.id,
                        item_id=self.item.id)


================================================
File: dtlpy/entities/app.py
================================================
from collections import namedtuple
import traceback
import logging
from enum import Enum

import attr

from .. import entities, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class AppScope(str, Enum):
    """ The scope of the app.

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - SYSTEM
         - Dataloop internal app
       * - PROJECT
         - Project app
    """
    SYSTEM = 'system'
    PROJECT = 'project'


@attr.s
class App(entities.BaseEntity):
    id = attr.ib(type=str)
    name = attr.ib(type=str)
    url = attr.ib(type=str)
    created_at = attr.ib(type=str)
    updated_at = attr.ib(type=str)
    creator = attr.ib(type=str)
    project_id = attr.ib(type=str)
    org_id = attr.ib(type=str)
    dpk_name = attr.ib(type=str)
    dpk_version = attr.ib(type=str)
    composition_id = attr.ib(type=str)
    scope = attr.ib(type=str)
    routes = attr.ib(type=dict)
    custom_installation = attr.ib(type=dict)
    metadata = attr.ib(type=dict)
    status = attr.ib(type=entities.CompositionStatus)
    settings = attr.ib(type=dict)

    # sdk
    _project = attr.ib(type=entities.Project, repr=False)
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)
    integrations = attr.ib(type=list, default=None)

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories', field_names=['projects', 'apps', 'compositions'])
        return reps(
            projects=repositories.Projects(client_api=self._client_api),
            apps=repositories.Apps(client_api=self._client_api, project=self._project),
            compositions=repositories.Compositions(client_api=self._client_api, project=self._project)
        )

    @property
    def project(self):
        if self._project is None:
            self._project = self.projects.get(project_id=self.project_id)
        return self._project

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def apps(self):
        assert isinstance(self._repositories.apps, repositories.Apps)
        return self._repositories.apps

    @property
    def compositions(self):
        assert isinstance(self._repositories.compositions, repositories.Compositions)
        return self._repositories.compositions

    def uninstall(self):
        """
        Uninstall an app installed app from the project.

        **Example**
        .. code-block:: python
            succeed = app.uninstall()
        """
        return self.apps.uninstall(self.id)

    def update(self):
        """
        Update the current app to the new configuration

        :return bool whether the operation ran successfully or not

        **Example**
        .. code-block:: python
            succeed = app.update()
        """
        return self.apps.update(self)

    def resume(self):
        """
        Resume the current app

        :return bool whether the operation ran successfully or not

        **Example**
        .. code-block:: python
            succeed = app.resume()
        """
        return self.apps.resume(self)

    def pause(self):
        """
        Pause the current app

        :return bool whether the operation ran successfully or not

        **Example**
        .. code-block:: python
            succeed = app.pause()
        """
        return self.apps.pause(self)

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json:  platform json
        :param client_api: ApiClient entity
        :return:
        """
        try:
            package = App.from_json(_json=_json,
                                    client_api=client_api,
                                    project=project,
                                    is_fetched=is_fetched)
            status = True
        except Exception:
            package = traceback.format_exc()
            status = False
        return status, package

    def to_json(self):
        _json = {}
        if self.id is not None:
            _json['id'] = self.id
        if self.name is not None:
            _json['name'] = self.name
        if self.url is not None:
            _json['url'] = self.url
        if self.created_at is not None:
            _json['createdAt'] = self.created_at
        if self.updated_at is not None:
            _json['updatedAt'] = self.updated_at
        if self.creator is not None:
            _json['creator'] = self.creator
        if self.project_id is not None:
            _json['projectId'] = self.project_id
        if self.org_id is not None:
            _json['orgId'] = self.org_id
        if self.dpk_name is not None:
            _json['dpkName'] = self.dpk_name
        if self.dpk_version is not None:
            _json['dpkVersion'] = self.dpk_version
        if self.composition_id is not None:
            _json['compositionId'] = self.composition_id
        if self.scope is not None:
            _json['scope'] = self.scope
        if self.routes != {}:
            _json['routes'] = self.routes
        if self.custom_installation != {}:
            _json['customInstallation'] = self.custom_installation
        if self.metadata is not None:
            _json['metadata'] = self.metadata
        if self.status is not None:
            _json['status'] = self.status
        if self.settings != {}:
            _json['settings'] = self.settings
        if self.integrations is not None:
            _json['integrations'] = self.integrations

        return _json

    @classmethod
    def from_json(cls, _json, client_api: ApiClient, project: entities.Project=None, is_fetched=True):
        app = cls(
            id=_json.get('id', None),
            name=_json.get('name', None),
            url=_json.get('url', None),
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            creator=_json.get('creator', None),
            project_id=_json.get('projectId', None),
            org_id=_json.get('orgId', None),
            dpk_name=_json.get('dpkName', None),
            dpk_version=_json.get('dpkVersion', None),
            composition_id=_json.get('compositionId', None),
            scope=_json.get('scope', None),
            routes=_json.get('routes', {}),
            custom_installation=_json.get('customInstallation', {}),
            client_api=client_api,
            project=project,
            metadata=_json.get('metadata', None),
            status=_json.get('status', None),
            settings=_json.get('settings', {}),
            integrations=_json.get('integrations', None)
        )
        app.is_fetched = is_fetched
        return app


================================================
File: dtlpy/entities/app_module.py
================================================
import logging
import typing
from enum import Enum

from .. import entities, utilities

logger = logging.getLogger(name='dtlpy')


class AppModule(entities.PackageModule, utilities.BaseServiceRunner):
    def __init__(self, name, description):
        super(AppModule, self).__init__()
        self.name = name
        self.description = description

    def init(self):
        """
        Default init function
        :return:
        """
        ...

    ###############
    # Decorators #
    ###############
    def set_init(self, inputs=None):
        def decorator(func: typing.Callable):
            setattr(self, 'init', func)
            setattr(self, func.__name__, func)
            init_inputs, _ = self._function_io_from_def(func=func, inputs=inputs, outputs=None)
            self.init_inputs = AppModule._parse_io(io_list=init_inputs)
            return func

        return decorator

    def add_function(self, display_name=None, inputs=None, outputs=None):
        """
        Add a function to the instance to be able to run in the FaaS.

        :param display_name: Display name of the function
        :param inputs: function inputs variable definitions. dictionary with strings for name and type {name: type}
        :param outputs: function output variable definitions. dictionary with strings for name and type {name: type}
        :return:
        """

        def decorator(func: typing.Callable):
            if display_name is None:
                d_name = func.__name__
            else:
                d_name = display_name
            module_inputs, module_outputs = self._function_io_from_def(func=func, inputs=inputs, outputs=outputs)
            defs = {"name": func.__name__,
                    "displayName": d_name,
                    "input": AppModule._parse_io(io_list=module_inputs),
                    "output": AppModule._parse_io(io_list=module_outputs)}
            func.__dtlpy__ = defs
            setattr(self, func.__name__, func)
            self.functions.append(entities.PackageFunction.from_json(defs))
            return func

        return decorator

    #########
    # Utils #
    #########
    @staticmethod
    def _function_io_from_def(func, inputs, outputs):
        input_types = typing.get_type_hints(func)
        hint_outputs = input_types.pop('return', None)

        if outputs is None:
            if hint_outputs is not None:
                func_outputs = {'output': hint_outputs}
            else:
                func_outputs = dict()
        else:
            func_outputs = outputs
        if inputs is None:
            func_inputs = {name: t for name, t in input_types.items()}
        else:
            func_inputs = inputs
        return func_inputs, func_outputs

    @staticmethod
    def _function_type_mapping(io_type):
        mapping = {'str': entities.PackageInputType.STRING}
        if io_type in mapping:
            return mapping[io_type]
        else:
            return io_type

    @staticmethod
    def _parse_io(io_list: dict):
        output = list()
        if io_list is not None:
            for io_name, io_type in io_list.items():
                if isinstance(io_type, Enum):
                    io_type = io_type.name
                if hasattr(io_type, '__args__'):
                    io_type = io_type.__args__[0]
                if isinstance(io_type, type):
                    io_type = io_type.__name__
                else:
                    io_type = str(io_type)
                output.append({"name": io_name,
                               "type": AppModule._function_type_mapping(str(io_type))})
        return output


================================================
File: dtlpy/entities/artifact.py
================================================
import traceback
import logging
import attr
import os

from .. import entities

logger = logging.getLogger(name='dtlpy')


class ArtifactType:
    ITEM = 'item'
    LOCAL = 'local'
    LINK = 'link'


@attr.s
class Artifact:
    @staticmethod
    def _protected_from_json(
            _json: dict,
            client_api,
            dataset=None,
            project=None,
            is_fetched: bool = True):
        """
        Same as from_json but with try-except to catch if error

        :param dict _json: platform json
        :param client_api: ApiClient entity
        :param dtlpy.entities.dataset.Dataset dataset: dataset entity
        :param dtlpy.entities.project.Project project: project entity
        :param bool is_fetched: is Entity fetched from Platform
        :return: status and the artifact object
        """
        try:
            artifact = Artifact.from_json(_json=_json,
                                          client_api=client_api,
                                          dataset=dataset,
                                          project=project,
                                          is_fetched=is_fetched)
            status = True
        except Exception:
            artifact = traceback.format_exc()
            status = False
        return status, artifact

    @classmethod
    def from_json(cls,
                  _json: dict,
                  client_api,
                  dataset=None,
                  project=None,
                  model=None,
                  is_fetched: bool = True):
        artifact_type = _json.get('type', None)
        if artifact_type is None:
            raise ValueError('Missing `type` in artifact.')
        if artifact_type in ['file', 'dir']:
            # used as item directly
            artifact = ItemArtifact.from_json(_json=_json,
                                              client_api=client_api,
                                              dataset=dataset,
                                              project=project,
                                              model=model,
                                              is_fetched=is_fetched
                                              )
        elif artifact_type in ['item']:
            # used as an item reference
            artifact = ItemArtifact.from_json(_json={'id': _json.get('itemId'),
                                                     'type': 'item'},
                                              client_api=client_api,
                                              dataset=dataset,
                                              project=project,
                                              model=model,
                                              is_fetched=False
                                              )
        elif artifact_type in ['local']:
            artifact = LocalArtifact.from_json(_json=_json)
        elif artifact_type in ['link']:
            artifact = LinkArtifact.from_json(_json=_json)
        else:
            raise ValueError('Unknown dl.Artifact `type`: {}'.format(artifact_type))
        return artifact


@attr.s
class ItemArtifact(entities.Item, Artifact):

    def to_json(self, as_artifact=False):
        """
        Return dict representation of the artifact
        Using a flag to keep old artifact behaviour (for packages, returning the item itself and not the artifact)

        :param as_artifact: it True, return the Artifact json, otherwise, return item for backward compatibility
        :return:
        """
        if as_artifact:
            _json = {'type': self.type,
                     'itemId': self.id}
        else:
            _json = super(ItemArtifact, self).to_json()
        return _json


@attr.s
class LocalArtifact(Artifact):
    type = 'local'
    local_path = attr.ib(repr=True)

    @classmethod
    def from_json(cls, _json: dict):
        """
        Build an Artifact entity object from a json

        :param dict _json: platform json
        :return: Artifact object
        :rtype: dtlpy.entities.artifact.Artifact
        """

        inst = cls(local_path=_json.get('localPath'))
        return inst

    def to_json(self, as_artifact=True):
        """
        Return dict representation of the artifact
        Using a flag to keep old artifact behaviour (for packages).
        This flag is only relevant for ItemArtifact

        :param as_artifact: not is use, only to keep same signature for all artifacts
        :return:
        """
        _json = {'type': self.type,
                 'localPath': self.local_path}
        return _json


@attr.s
class LinkArtifact(Artifact):
    type = 'link'
    url = attr.ib(repr=True)
    filename = attr.ib(repr=True)

    @classmethod
    def from_json(cls,
                  _json: dict,
                  is_fetched: bool = True):
        """
        Build an Artifact entity object from a json

        :param dict _json: platform json
        :rtype: dtlpy.entities.artifact.Artifact
        """
        url = _json.get('url')
        filename = _json.get('filename')
        if filename is None:
            filename = os.path.basename(url)
        inst = cls(url=url,
                   filename=filename)
        inst.is_fetched = is_fetched
        return inst

    def to_json(self, as_artifact=True):
        """
        Return dict representation of the artifact
        Using a flag to keep old artifact behaviour (for packages).
        This flag is only relevant for ItemArtifact

        :param as_artifact: not is use, only to keep same signature for all artifacts
        :return:
        """
        _json = {'type': self.type,
                 'url': self.url}
        return _json


================================================
File: dtlpy/entities/assignment.py
================================================
import attr
import logging
from .. import repositories, exceptions, entities

logger = logging.getLogger(name='dtlpy')


@attr.s
class Assignment(entities.BaseEntity):
    """
    Assignment object
    """

    # platform
    name = attr.ib()
    annotator = attr.ib()
    status = attr.ib()
    project_id = attr.ib()
    metadata = attr.ib(repr=False)
    id = attr.ib()
    url = attr.ib(repr=False)
    updated_at = attr.ib(repr=False)
    updated_by = attr.ib(repr=False)
    task_id = attr.ib(repr=False)
    dataset_id = attr.ib(repr=False)
    annotation_status = attr.ib(repr=False)
    item_status = attr.ib(repr=False)
    total_items = attr.ib()
    for_review = attr.ib()
    issues = attr.ib()
    # sdk

    _client_api = attr.ib(repr=False)
    _task = attr.ib(default=None, repr=False)
    _assignments = attr.ib(default=None, repr=False)
    _project = attr.ib(default=None, repr=False)
    _dataset = attr.ib(default=None, repr=False)
    _datasets = attr.ib(default=None, repr=False)

    @classmethod
    def from_json(cls, _json, client_api, project=None, task=None, dataset=None):
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Assignment has been fetched from a project that is not belong to it')
                project = None

        metadata = _json.get('metadata', dict())
        dataset_id = metadata.get('datasetId', None)
        task_id = metadata.get('taskId', None)
        if dataset_id is None:
            system_metadata = metadata.get('system', dict())
            dataset_id = system_metadata.get('datasetId', None)
        if task_id is None:
            system_metadata = metadata.get('system', dict())
            task_id = system_metadata.get('taskId', None)

        assignment = cls(
            name=_json.get('name', None),
            annotator=_json.get('annotator', None),
            status=_json.get('status', None),
            project_id=_json.get('projectId', None),
            task_id=task_id,
            dataset_id=dataset_id,
            metadata=metadata,
            url=_json.get('url', None),
            id=_json['id'],
            updated_by=_json.get('updatedBy', None),
            updated_at=_json.get('updatedAt', None),
            client_api=client_api,
            project=project,
            dataset=dataset,
            task=task,
            annotation_status=_json.get('annotationStatus', None),
            item_status=_json.get('itemStatus', None),
            total_items=_json.get('totalItems', None),
            for_review=_json.get('forReview', None),
            issues=_json.get('issues', None)
        )

        if dataset is not None:
            if assignment.dataset_id != dataset.id:
                logger.warning('Assignment has been fetched from a dataset that is not belong to it')
                assignment._dataset = None

        if task is not None:
            if assignment.task_id != task.id:
                logger.warning('Assignment has been fetched from a task that is not belong to it')
                assignment._task = None

        return assignment

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/assignments/{}".format(self.project_id, self.id))

    @property
    def assignments(self):
        if self._assignments is None:
            self._assignments = repositories.Assignments(client_api=self._client_api,
                                                         project=self._project,
                                                         task=self._task,
                                                         dataset=self._dataset)
        assert isinstance(self._assignments, repositories.Assignments)
        return self._assignments

    @property
    def project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id,
                                                                                   fetch=None)

        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def datasets(self):
        if self._datasets is None:
            self._datasets = repositories.Datasets(client_api=self._client_api, project=self._project)
        assert isinstance(self._datasets, repositories.Datasets)
        return self._datasets

    @property
    def dataset(self):
        if self._dataset is None:
            self._dataset = self.datasets.get(dataset_id=self.dataset_id)
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @property
    def task(self):
        if self._task is None:
            self._task = self.project.tasks.get(task_id=self.task_id)
        assert isinstance(self._task, entities.Task)
        return self._task

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Assignment)._client_api,
                                                              attr.fields(Assignment)._project,
                                                              attr.fields(Assignment).project_id,
                                                              attr.fields(Assignment)._assignments,
                                                              attr.fields(Assignment)._dataset,
                                                              attr.fields(Assignment)._task,
                                                              attr.fields(Assignment).annotation_status,
                                                              attr.fields(Assignment).updated_at,
                                                              attr.fields(Assignment).updated_by,
                                                              attr.fields(Assignment).item_status,
                                                              attr.fields(Assignment).total_items,
                                                              attr.fields(Assignment).for_review,
                                                              attr.fields(Assignment).issues))

        _json['projectId'] = self.project_id
        return _json

    def update(self, system_metadata=False):
        """
        Update an assignment

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param bool system_metadata: True, if you want to change metadata system
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment assignment

        **Example**:

        .. code-block:: python

            assignment = assignment.update(system_metadata=False)
        """
        return self.assignments.update(assignment=self, system_metadata=system_metadata)

    def open_in_web(self):
        """
        Open the assignment in web platform

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :return:

        **Example**:

        .. code-block:: python

            assignment.open_in_web()
        """
        self._client_api._open_in_web(url=self.platform_url)

    def get_items(self, dataset=None, filters=None):
        """
        Get all the items in the assignment

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param dtlpy.entities.dataset.Dataset dataset: dataset object, the dataset that refer to the assignment
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: pages of the items
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            items = task.assignments.get_items()
        """
        if dataset is None:
            dataset = self.dataset

        return self.assignments.get_items(dataset=dataset, assignment=self, filters=filters)

    def reassign(self, assignee_id, wait=True):
        """
        Reassign an assignment

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param str assignee_id: the email of the user that want to assign the assignment
        :param bool wait: wait until reassign assignment finish
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment

        **Example**:

        .. code-block:: python

            assignment = assignment.reassign(assignee_ids='annotator1@dataloop.ai')
        """
        return self.assignments.reassign(assignment=self,
                                         task=self._task,
                                         task_id=self.metadata['system'].get('taskId'),
                                         assignee_id=assignee_id,
                                         wait=wait)

    def redistribute(self, workload, wait=True):
        """
        Redistribute an assignment

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param dtlpy.entities.assignment.Workload workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param bool wait: wait until redistribute assignment finish
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment assignment

        **Example**:

        .. code-block:: python

            assignment = assignment.redistribute(workload=dl.Workload([dl.WorkloadUnit(assignee_id="annotator1@dataloop.ai", load=50),
                                                         dl.WorkloadUnit(assignee_id="annotator2@dataloop.ai", load=50)]))
        """
        return self.assignments.redistribute(assignment=self,
                                             task=self._task,
                                             task_id=self.metadata['system'].get('taskId'),
                                             workload=workload,
                                             wait=wait)

    def set_status(self, status: str, operation: str, item_id: str):
        """
        Set item status within assignment

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param str status: string the describes the status
        :param str operation: the status action need 'create' or 'delete'
        :param str item_id: item id that want to set his status
        :return: True id success
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = assignment.set_status(status='complete',
                                    operation='created',
                                    item_id='item_id')
        """
        return self.assignments.set_status(status=status, operation=operation, item_id=item_id, assignment_id=self.id)


@attr.s
class WorkloadUnit:
    """
    WorkloadUnit object
    """

    # platform
    assignee_id = attr.ib(type=str)
    load = attr.ib(type=float, default=0)

    # noinspection PyUnusedLocal
    @load.validator
    def check_load(self, attribute, value):
        if value < 0 or value > 100:
            raise exceptions.PlatformException('400', 'Value must be a number between 0 to 100')

    @classmethod
    def from_json(cls, _json):
        return cls(
            assignee_id=_json.get('assigneeId', None),
            load=_json.get('load', None)
        )

    def to_json(self):
        return {
            'assigneeId': self.assignee_id,
            'load': self.load
        }


@attr.s
class Workload:
    """
    Workload object
    """

    # platform
    workload = attr.ib(type=list)

    def __iter__(self):
        for w_l_u in self.workload:
            yield w_l_u

    @workload.default
    def set_workload(self):
        workload = list()
        return workload

    @classmethod
    def from_json(cls, _json):
        return cls(
            workload=[WorkloadUnit.from_json(workload_unit) for workload_unit in _json]
        )

    @staticmethod
    def _loads_are_correct(loads):
        return round(sum(loads)) == 100

    @staticmethod
    def _get_loads(num_assignees):
        loads = [0 for _ in range(num_assignees)]
        index = 0
        for i in range(10000):
            loads[index] += 1
            if index < num_assignees - 1:
                index += 1
            else:
                index = 0
        loads = [l / 100 for l in loads]
        return loads

    def _redistribute(self):
        load = self._get_loads(num_assignees=len(self.workload))
        for i_w_l, w_l in self.workload:
            w_l.load = load

    @classmethod
    def generate(cls, assignee_ids, loads=None):
        """
        generate the loads for the given assignee
        :param assignee_ids:
        :param loads:
        """
        if not isinstance(assignee_ids, list):
            assignee_ids = [assignee_ids]

        if loads is None:
            div = len(assignee_ids)
            loads = [100 // div + (1 if x < 100 % div else 0) for x in range(div)]
        else:
            if not isinstance(loads, list):
                loads = [loads]
            if not Workload._loads_are_correct(loads=loads):
                raise exceptions.PlatformException('400', 'Loads must summarized to 100')

        if len(assignee_ids) != len(loads):
            raise exceptions.PlatformException('400', 'Assignee ids and loads must be of same length')

        return cls(
            workload=[WorkloadUnit(assignee_id=assignee_id, load=loads[i_assignee_id]) for i_assignee_id, assignee_id in
                      enumerate(assignee_ids)])

    def to_json(self):
        return [workload_unit.to_json() for workload_unit in self.workload]

    def add(self, assignee_id):
        """
        add a assignee

        :param assignee_id:
        """
        self.workload.append(WorkloadUnit(assignee_id=assignee_id))
        if not self._loads_are_correct(loads=[w_l.load for w_l in self.workload]):
            self._redistribute()


================================================
File: dtlpy/entities/base_entity.py
================================================
import functools
import logging
import enum

from .. import miscellaneous

logger = logging.getLogger('dtlpy')


class EntityScopeLevel(str, enum.Enum):
    PRIVATE = 'private',
    PROJECT = 'project',
    ORG = 'org',
    PUBLIC = 'public'


class BaseEntity(object):
    is_fetched = True

    def print(self, to_return=False, columns=None):
        """
        :param to_return:
        :param columns:
        """
        return miscellaneous.List([self]).print(to_return=to_return, columns=columns)

    def to_df(self, show_all=False, columns=None):
        """
        :param show_all:
        :param columns:
        """
        return miscellaneous.List([self]).to_df(show_all=show_all, columns=columns)


# for auto-complete
from typing import TYPE_CHECKING

RUNTIME = not TYPE_CHECKING
if RUNTIME:
    def base_model(model):
        return model
else:
    from dataclasses import dataclass as base_model


class DlList(list):
    def __init__(self, _list=None, _dict=None, _type=None):
        if _list is None:
            _list = list()
        if _dict is None:
            _dict = list()
        self._dict = _dict
        self._type = _type
        super(DlList, self).__init__(_list)

    def append(self, item) -> None:
        if self._type is not None:
            assert isinstance(item, self._type), f'Cannot append type: {type(item)}. Must be type {self._type}'
        self._dict.append(item.to_json())
        super(DlList, self).append(item)  # append the item to itself (the list)


@base_model
class DlEntity(object):
    is_fetched = True

    def __init__(self, _dict=None, **kwargs):
        # using dict by reference and not creating a new one each time
        if _dict is None:
            _dict = dict()
        self._dict = _dict
        # this will set all the inputs to right location (using the dl.Property definitions)
        for key, value in kwargs.items():
            if key == "_dict":
                self._dict.update(value.copy())
            else:
                self.__setattr__(key, value)
        self._set_defaults()

    def _set_defaults(self):
        # getting all attribute (that are DlProperty) to set defaults to _dict
        # https://stackoverflow.com/questions/21962769/is-it-possible-to-test-if-object-property-uses-a-descriptor
        for att, a_type in type(self).__mro__[0].__dict__.items():
            if isinstance(a_type, DlProperty):
                _ = self.__getattribute__(att)

    def print(self, to_return=False, columns=None):
        """
        :param to_return:
        :param columns:
        """
        return miscellaneous.List([self]).print(to_return=to_return, columns=columns)

    def to_df(self, show_all=False, columns=None):
        """
        :param show_all:
        :param columns:
        """
        return miscellaneous.List([self]).to_df(show_all=show_all, columns=columns)


class DlProperty:
    def __init__(self, location=None, default='NODEFAULT', _type=None, _kls=None):
        self.location = location
        self._default = default
        self._type = _type
        self._validator = None
        self._kls = _kls

    ##############
    # decorators #
    ##############
    def default(self, method=None):
        @functools.wraps(method)
        def _default(self):
            return method(self)

        self._default = _default
        return self._default

    def validator(self, method):
        @functools.wraps(method)
        def _validator(self, value):
            return method(self, value)

        self._validator = _validator
        return self._validator

    ############
    # privates #
    ############
    @staticmethod
    def _get_class(kls):
        import importlib
        module = importlib.import_module(f'.entities', package='dtlpy')
        kls = getattr(module, kls)
        return kls

    def _get_default(self, instance):
        if callable(self._default):
            default = self._default(self=instance)
        else:
            default = self._default
        return default

    def _to_dict(self, inst):
        if self._kls is not None:
            if isinstance(inst, list) and all(hasattr(v, 'to_json') for v in inst):
                _dict = [v.to_json() for v in inst]
            elif hasattr(inst, 'to_json'):
                _dict = inst.to_json()
            else:
                _dict = inst
        else:
            _dict = inst
        return _dict

    def _to_instance(self, _dict):
        if self._kls is not None:
            kls = self._get_class(kls=self._kls)
            if isinstance(_dict, list) and all(isinstance(v, dict) for v in _dict):
                value = DlList(_list=[kls.from_json(v) for v in _dict],
                               _type=kls,
                               _dict=_dict)
            elif isinstance(_dict, dict):
                value = kls(_dict=_dict, **_dict)
            else:
                value = _dict
        else:
            value = _dict
        return value

    def __get__(self, instance, owner):
        _dict = instance._dict
        for key in self.location[:-1]:
            _dict = _dict.get(key, dict())

        # get default if set - can also be callable
        try:
            value = _dict[self.location[-1]]
        except KeyError:
            # get the default only of value doesnt exists
            value = self._get_default(instance=instance)
            if value == 'NODEFAULT':
                # if NODEFAULT - set value to None and DONT save in the _dict
                value = None
            else:
                # if default value is set - add the value into the _dict
                _dict[self.location[-1]] = value

        # instantiate dictionary to the type
        value = self._to_instance(_dict=value)
        return value

    def __set__(self, instance, value):

        # validate - if validator is set
        if self._validator is not None:
            # instantiate the value before validation
            inst = self._to_instance(_dict=value)
            self._validator(self=instance, value=inst)

        # convert to the value's kls
        value = self._to_dict(inst=value)

        # set value in the dictionary
        _dict = instance._dict
        for key in self.location[:-1]:
            _tmp_dict = _dict.get(key, None)
            # create the dict inside to be able to point
            if _tmp_dict is None:
                _dict[key] = dict()
            _dict = _dict.get(key, None)
        _dict[self.location[-1]] = value


================================================
File: dtlpy/entities/bot.py
================================================
import traceback
import logging
import attr

from .. import entities, exceptions, repositories

logger = logging.getLogger(name='dtlpy')


@attr.s
class Bot(entities.User):
    """
    Bot entity
    """
    _bots = attr.ib(repr=False, default=None)
    password = attr.ib(default=None)

    @staticmethod
    def _protected_from_json(_json, project, client_api, bots=None):
        """
        Same as from_json but with try-except to catch if error
        :param _json: platform json
        :param project: project entity
        :param client_api: ApiClient entity
        :param bots:
        :return:
        """
        try:
            bot = Bot.from_json(_json=_json,
                                project=project,
                                bots=bots,
                                client_api=client_api)
            status = True
        except Exception:
            bot = traceback.format_exc()
            status = False
        return status, bot

    @classmethod
    def from_json(cls, _json, project, client_api, bots=None):
        """
        Build a Bot entity object from a json

        :param _json: _json response from host
        :param project: project entity
        :param client_api: ApiClient entity
        :param bots: Bots repository
        :return: User object
        """
        return cls(
            created_at=_json.get('createdAt', None),
            name=_json.get('firstName', None),
            updated_at=_json.get('updatedAt', None),
            last_name=_json.get('lastName', None),
            username=_json.get('username', None),
            avatar=_json.get('avatar', None),
            email=_json.get('email', None),
            type=_json.get('type', None),
            role=_json.get('role', None),
            org=_json.get('org', None),
            id=_json.get('id', None),
            project=project,
            client_api=client_api,
            bots=bots,
            password=_json.get('password', None),
        )

    @property
    def bots(self):
        if self._bots is None:
            self._bots = repositories.Bots(client_api=self._client_api, project=self._project)
        assert isinstance(self._bots, repositories.Bots)
        return self._bots

    @property
    def project(self):
        if self._project is None:
            raise exceptions.PlatformException(error='2001',
                                               message='Missing entity "project".')
        assert isinstance(self._project, entities.Project)
        return self._project

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(Bot)._project,
                                                        attr.fields(Bot).name,
                                                        attr.fields(Bot)._client_api,
                                                        attr.fields(Bot)._bots,
                                                        attr.fields(Bot)._users,
                                                        attr.fields(Bot).last_name,
                                                        attr.fields(Bot).created_at,
                                                        attr.fields(Bot).updated_at,
                                                        ))
        _json['firstName'] = self.name
        _json['lastName'] = self.last_name
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        return _json

    def delete(self):
        """
        Delete the bot

        :return: True
        :rtype: bool
        """
        return self.bots.delete(bot_id=self.id)


================================================
File: dtlpy/entities/codebase.py
================================================
import logging
import os

from .. import entities, repositories, services

logger = logging.getLogger(name='dtlpy')


class PackageCodebaseType:
    ITEM = 'item'
    GIT = 'git'
    FILESYSTEM = 'filesystem'
    LOCAL = 'local'


def Codebase(**kwargs):
    """
    Factory function to init all codebases types
    """
    client_api = kwargs.pop('client_api', None)
    # take it out because we dont need it from the factory method
    _dict = kwargs.pop('_dict', None)

    if kwargs['type'] == PackageCodebaseType.GIT:
        cls = GitCodebase.from_json(_json=kwargs,
                                    client_api=client_api)
    elif kwargs['type'] == PackageCodebaseType.ITEM:
        cls = ItemCodebase.from_json(_json=kwargs,
                                     client_api=client_api)
    elif kwargs['type'] == PackageCodebaseType.FILESYSTEM:
        cls = FilesystemCodebase.from_json(_json=kwargs,
                                           client_api=client_api)
    elif kwargs['type'] == PackageCodebaseType.LOCAL:
        cls = LocalCodebase.from_json(_json=kwargs,
                                      client_api=client_api)
    else:
        raise ValueError('[Codebase constructor] Unknown codebase type: {}'.format(kwargs['type']))
    return cls


class GitCodebase(entities.DlEntity):
    type = entities.DlProperty(location=['type'], _type=str, default='git')
    git_url = entities.DlProperty(location=['gitUrl'], _type=str)
    git_tag = entities.DlProperty(location=['gitTag'], _type=str)
    credentials = entities.DlProperty(location=['credentials'], _type=dict)
    _codebases = None
    client_api: 'services.ClientApi'

    @property
    def is_remote(self):
        """ Return whether the codebase is managed remotely and supports upload-download"""
        return True

    @property
    def is_local(self):
        """ Return whether the codebase is locally and has no management implementations"""
        return not self.is_remote

    @property
    def codebases(self):
        if self._codebases is None:
            if self._item is not None:
                dataset = self.item.dataset
            else:
                dataset = None
            self._codebases = repositories.Codebases(client_api=self.client_api,
                                                     dataset=dataset)
        assert isinstance(self._codebases, repositories.Codebases)
        return self._codebases

    def to_json(self) -> dict:
        _dict = self._dict.copy()
        return _dict

    @classmethod
    def from_json(cls, _json: dict, client_api):
        """
        :param _json: platform json
        :param client_api: ApiClient entity
        """
        return cls(_dict=_json.copy(),
                   client_api=client_api)

    @property
    def git_user_name(self):
        return self.git_url.split('/')[-2]

    @property
    def git_repo_name(self):
        last = self.git_url.split('/')[-1]
        return os.path.splitext(last)[0]

    @property
    def git_username(self):
        if self.credentials is not None:
            return os.environ.get(
                self.credentials.get('username', {}).get('key', ''),
                None
            )
        return None

    @property
    def git_password(self):
        if self.credentials is not None:
            return os.environ.get(
                self.credentials.get('password', {}).get('key', ''),
                None
            )
        return None

    @staticmethod
    def is_git_repo(path):
        """
        :param path: `str` TODO: Currently only for local folder
        :return: `bool` testing if the path is valid git repo
        """
        return os.path.isdir(os.path.join(path, '.git'))

    def unpack(self, local_path):
        """
        Clones the git codebase
        :param local_path:
        """
        return self.codebases.clone_git(
            codebase=self,
            local_path=local_path
        )


class LocalCodebase(entities.DlEntity):
    type: str
    local_path: str
    _client_api: 'services.ClientApi'

    @property
    def is_remote(self):
        """ Return whether the codebase is managed remotely and supports upload-download"""
        return False

    @property
    def is_local(self):
        """ Return whether the codebase is locally and has no management implementations"""
        return not self.is_remote

    def to_json(self):
        _json = {'type': self.type,
                 'localPath': self._local_path}
        return _json

    @classmethod
    def from_json(cls, _json: dict, client_api):
        """
        :param _json: platform json
        :param client_api: ApiClient entity
        """
        return cls(
            client_api=client_api,
            local_path=_json.get('localPath', None),
            type=_json.get('type', None)
        )


class FilesystemCodebase(entities.DlEntity):
    type: str
    host_path: str
    container_path: str
    _client_api: 'services.ClientApi'

    @property
    def is_remote(self):
        """ Return whether the codebase is managed remotely and supports upload-download"""
        return False

    @property
    def is_local(self):
        """ Return whether the codebase is locally and has no management implementations"""
        return not self.is_remote

    def to_json(self):
        _json = super().to_json()
        if self.host_path is not None:
            _json['hostPath'] = self.host_path
        if self.container_path is not None:
            _json['containerPath'] = self.container_path
        return _json

    @classmethod
    def from_json(cls, _json: dict, client_api):
        """
        :param _json: platform json
        :param client_api: ApiClient entity
        """
        return cls(
            client_api=client_api,
            container_path=_json.get('containerPath', None),
            host_path=_json.get('hostPath', None),
            type=_json.get('type', None)
        )


class ItemCodebase(entities.DlEntity):
    type = entities.DlProperty(location=['type'], _type=str)
    item_id = entities.DlProperty(location=['itemId'], _type=str)
    _item = entities.DlProperty(location=['item'], _type=str, default=None)
    _codebases = None
    client_api: 'services.ClientApi'

    @property
    def item(self):
        if self._item is None:
            self._item = self.codebases.items_repository.get(item_id=self.item_id)
        return self._item

    @property
    def is_remote(self):
        """ Return whether the codebase is managed remotely and supports upload-download"""
        return True

    @property
    def is_local(self):
        """ Return whether the codebase is locally and has no management implementations"""
        return not self.is_remote

    @property
    def codebases(self):
        if self._codebases is None:
            if self._item is not None:
                dataset = self.item.dataset
            else:
                dataset = None
            self._codebases = repositories.Codebases(client_api=self.client_api,
                                                     dataset=dataset)
        assert isinstance(self._codebases, repositories.Codebases)
        return self._codebases

    def to_json(self) -> dict:
        _dict = self._dict.copy()
        _dict.pop('item', None)
        return _dict

    @classmethod
    def from_json(cls, _json: dict, client_api):
        """
        :param _json: platform json
        :param client_api: ApiClient entity
        """
        return cls(_dict=_json.copy(),
                   client_api=client_api)

    def unpack(self, local_path):
        """
        Clones the git codebase
        :param local_path:
        """
        return self.codebases.unpack(
            codebase_id=self.item_id,
            local_path=local_path
        )

    @property
    def version(self):
        return str(self.item.name.split('.')[0])

    @property
    def md5(self):
        md5 = None
        if 'system' in self.item.metadata:
            md5 = self.item.metadata['system'].get('md5', None)
        return md5

    @md5.setter
    def md5(self, md5):
        if 'system' not in self.item.metadata:
            self.item.metadata['system'] = dict()
        self.item.metadata['system']['md5'] = md5

    @property
    def description(self):
        description = None
        if 'system' in self.item.metadata:
            description = self.item.metadata['system'].get('description', None)
        return description

    @description.setter
    def description(self, description):
        if 'system' not in self.item.metadata:
            self.item.metadata['system'] = dict()
        self.item.metadata['system']['description'] = description

    def list_versions(self):
        """
        List Codebase versions
        """
        # get codebase name
        codebase_name = self.item.filename.split('/')[len(self.item.filename.split('/')) - 2]
        return self.codebases.list_versions(codebase_name=codebase_name)


================================================
File: dtlpy/entities/collection.py
================================================
from .. import entities
from ..services.api_client import ApiClient
import attr

@attr.s
class Collection(entities.BaseEntity):
    """
    Represents a collection in the dataset.
    """

    # sdk
    _client_api = attr.ib(type=ApiClient, repr=False)

    key = attr.ib(type=str)
    name = attr.ib(type=str)

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Create a single Collection entity from the dataset JSON.

        :param _json: A dictionary containing collection data in the format:
                    { "metadata.system.collections.c0": {"name": "Justice League"} }
        :param client_api: The client API instance.
        :param is_fetched: Whether the entity was fetched from the platform.
        :return: A single Collection entity.
        """
        full_key, value = next(iter(_json.items()))
        # Strip the prefix
        key = full_key.replace("metadata.system.collections.", "")
        name = value.get("name")

        inst = cls(
            key=key,
            name=name,
            client_api=client_api,
        )
        inst.is_fetched = is_fetched
        return inst

================================================
File: dtlpy/entities/command.py
================================================
import attr
import traceback
from enum import Enum
from collections import namedtuple

from .. import repositories, entities
from ..services.api_client import ApiClient


class CommandsStatus(str, Enum):
    CREATED = 'created'
    MAKING_CHILDREN = 'making-children'
    WAITING_CHILDREN = 'waiting-children'
    IN_PROGRESS = 'in-progress'
    ABORTED = 'aborted'
    CANCELED = 'canceled'
    FINALIZING = 'finalizing'
    SUCCESS = 'success'
    FAILED = 'failed'
    TIMEOUT = 'timeout'
    CLEANING_UP = 'cleaning-up'
    ON_ERROR = 'on-error'


@attr.s
class Command(entities.BaseEntity):
    """
    Com entity
    """
    # platform
    id = attr.ib()
    url = attr.ib(repr=False)
    status = attr.ib()
    created_at = attr.ib()
    updated_at = attr.ib()
    type = attr.ib()
    progress = attr.ib()
    spec = attr.ib()
    error = attr.ib()

    # sdk
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['commands'])

        commands_repo = repositories.Commands(client_api=self._client_api)

        r = reps(commands=commands_repo)
        return r

    @property
    def commands(self):
        assert isinstance(self._repositories.commands, repositories.Commands)
        return self._repositories.commands

    @staticmethod
    def _protected_from_json(_json, client_api, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return:
        """
        try:
            command = Command.from_json(_json=_json,
                                        client_api=client_api,
                                        is_fetched=is_fetched)
            status = True
        except Exception:
            command = traceback.format_exc()
            status = False
        return status, command

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Command entity object from a json

        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Command object
        """
        inst = cls(
            id=_json.get('id', None),
            url=_json.get('url', None),
            status=_json.get('status', None),
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            type=_json.get('type', None),
            progress=_json.get('progress', None),
            spec=_json.get('spec', None),
            error=_json.get('error', None),
            client_api=client_api
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        # get excluded
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Command)._client_api,
                                                              attr.fields(Command).created_at,
                                                              attr.fields(Command).updated_at))
        # rename
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at

        return _json

    def abort(self):
        """
        abort command

        :return:
        """
        return self.commands.abort(command_id=self.id)

    def in_progress(self):
        """
        Check if command is still in one of the in progress statuses

        :return: True if command still in progress
        :rtype: bool
        """
        if self.status not in {status for status in entities.CommandsStatus}:
            raise ValueError('status is not a valid CommandsStatus')
        return self.status not in [entities.CommandsStatus.SUCCESS,
                                   entities.CommandsStatus.FAILED,
                                   entities.CommandsStatus.TIMEOUT,
                                   entities.CommandsStatus.CANCELED,
                                   entities.CommandsStatus.ABORTED
                                   ]

    def wait(self, timeout=0, step=None, backoff_factor=1):
        """
        Wait for Command to finish

        :param int timeout: int, seconds to wait until TimeoutError is raised. if 0 - wait until done
        :param int step: int, seconds between polling
        :param float backoff_factor: A backoff factor to apply between attempts after the second try
        :return: Command  object
        """
        if not self.in_progress():
            return self

        return self.commands.wait(command_id=self.id,
                                  timeout=timeout,
                                  step=step,
                                  url=self.url,
                                  backoff_factor=backoff_factor
                                  )


================================================
File: dtlpy/entities/compute.py
================================================
from enum import Enum
from typing import List, Optional, Dict
from ..services.api_client import ApiClient
from .. import repositories


class ClusterProvider(str, Enum):
    GCP = 'gcp'
    AWS = 'aws'
    AZURE = 'azure'
    HPC = 'hpc'
    LOCAL = 'local'


class ComputeType(str, Enum):
    KUBERNETES = "kubernetes"


class ComputeStatus(str, Enum):
    READY = "ready"
    INITIALIZING = "initializing"
    PAUSE = "pause"


class Toleration:
    def __init__(self, effect: str, key: str, operator: str, value: str):
        self.effect = effect
        self.key = key
        self.operator = operator
        self.value = value

    @classmethod
    def from_json(cls, _json):
        return cls(
            effect=_json.get('effect'),
            key=_json.get('key'),
            operator=_json.get('operator'),
            value=_json.get('value')
        )

    def to_json(self):
        return {
            'effect': self.effect,
            'key': self.key,
            'operator': self.operator,
            'value': self.value
        }


class DeploymentResource:
    def __init__(self, gpu: int, cpu: int, memory: str):
        self.gpu = gpu
        self.cpu = cpu
        self.memory = memory

    @classmethod
    def from_json(cls, _json):
        return cls(
            gpu=_json.get('gpu', None),
            cpu=_json.get('cpu', None),
            memory=_json.get('memory', None)
        )

    def to_json(self):
        _json = {}
        if self.gpu is not None:
            _json['gpu'] = self.gpu
        if self.cpu is not None:
            _json['cpu'] = self.cpu
        if self.memory is not None:
            _json['memory'] = self.memory


class DeploymentResources:
    def __init__(self, request: DeploymentResource, limit: DeploymentResource):
        self.request = request
        self.limit = limit

    @classmethod
    def from_json(cls, _json):
        return cls(
            request=DeploymentResource.from_json(_json.get('request', dict())),
            limit=DeploymentResource.from_json(_json.get('limit', dict()))
        )

    def to_json(self):
        return {
            'request': self.request.to_json(),
            'limit': self.limit.to_json()
        }


class NodePool:
    def __init__(
            self,
            name: str,
            is_dl_type_default: bool,
            dl_types: Optional[List[str]] = None,
            tolerations: Optional[List[Toleration]] = None,
            description: str = "",
            node_selector: str = "",
            preemtible: bool = False,
            deployment_resources: DeploymentResources = None
    ):
        self.name = name
        self.is_dl_type_default = is_dl_type_default
        self.dl_types = dl_types
        self.tolerations = tolerations if tolerations is not None else []
        self.description = description
        self.node_selector = node_selector
        self.preemtible = preemtible
        self.deployment_resources = deployment_resources

    @classmethod
    def from_json(cls, _json):
        node_pool = cls(
            name=_json.get('name'),
            is_dl_type_default=_json.get('isDlTypeDefault'),
            dl_types=_json.get('dlTypes'),
            description=_json.get('description'),
            node_selector=_json.get('nodeSelector'),
            preemtible=_json.get('preemtible'),
            deployment_resources=DeploymentResources.from_json(_json.get('deploymentResources', dict())),
            tolerations=[Toleration.from_json(t) for t in _json.get('tolerations', list())]
        )

        return node_pool

    def to_json(self):
        _json = {
            'name': self.name,
            'isDlTypeDefault': self.is_dl_type_default,
            'description': self.description,
            'nodeSelector': self.node_selector,
            'preemtible': self.preemtible,
            'deploymentResources': self.deployment_resources.to_json(),
            'tolerations': [t.to_json() for t in self.tolerations]
        }

        if self.dl_types is not None:
            _json['dlTypes'] = self.dl_types

        return _json


class AuthenticationIntegration:
    def __init__(self, id: str, type: str):
        self.id = id
        self.type = type

    @classmethod
    def from_json(cls, _json):
        return cls(
            id=_json.get('id'),
            type=_json.get('type')
        )

    def to_json(self):
        return {
            'id': self.id,
            'type': self.type
        }


class Authentication:
    def __init__(self, integration: AuthenticationIntegration):
        self.integration = integration

    @classmethod
    def from_json(cls, _json):
        return cls(
            integration=AuthenticationIntegration.from_json(_json.get('integration', dict()))
        )

    def to_json(self):
        return {
            'integration': self.integration.to_json()
        }


class ComputeCluster:
    def __init__(
            self,
            name: str,
            endpoint: str,
            kubernetes_version: str,
            provider: ClusterProvider,
            node_pools: Optional[List[NodePool]] = None,
            metadata: Optional[Dict] = None,
            authentication: Optional[Authentication] = None,
    ):
        self.name = name
        self.endpoint = endpoint
        self.kubernetes_version = kubernetes_version
        self.provider = provider
        self.node_pools = node_pools if node_pools is not None else []
        self.metadata = metadata if metadata is not None else {}
        self.authentication = authentication if authentication is not None else Authentication(
            AuthenticationIntegration("", ""))

    @classmethod
    def from_json(cls, _json):
        return cls(
            name=_json.get('name'),
            endpoint=_json.get('endpoint'),
            kubernetes_version=_json.get('kubernetesVersion'),
            provider=ClusterProvider(_json.get('provider')),
            node_pools=[NodePool.from_json(np) for np in _json.get('nodePools', list())],
            metadata=_json.get('metadata'),
            authentication=Authentication.from_json(_json.get('authentication', dict()))
        )

    def to_json(self):
        return {
            'name': self.name,
            'endpoint': self.endpoint,
            'kubernetesVersion': self.kubernetes_version,
            'provider': self.provider.value,
            'nodePools': [np.to_json() for np in self.node_pools],
            'metadata': self.metadata,
            'authentication': self.authentication.to_json()
        }

    @classmethod
    def from_setup_json(cls, devops_output, integration):
        node_pools = [NodePool.from_json(n) for n in devops_output['config']['nodePools']]
        return cls(
            devops_output['config']['name'],
            devops_output['config']['endpoint'],
            devops_output['config']['kubernetesVersion'],
            ClusterProvider(devops_output['config']['provider']),
            node_pools,
            {},
            Authentication(AuthenticationIntegration(integration.id, integration.type))
        )


class ComputeContext:
    def __init__(self, labels: List[str], org: str, project: Optional[str] = None):
        self.labels = labels
        self.org = org
        self.project = project

    @classmethod
    def from_json(cls, _json):
        return cls(
            labels=_json.get('labels', list()),
            org=_json.get('org'),
            project=_json.get('project')
        )

    def to_json(self):
        return {
            'labels': self.labels,
            'org': self.org,
            'project': self.project
        }


class Compute:
    def __init__(
            self,
            id: str,
            name: str,
            context: ComputeContext,
            client_api: ApiClient,
            shared_contexts: Optional[List[ComputeContext]] = None,
            global_: Optional[bool] = None,
            status: ComputeStatus = ComputeStatus.INITIALIZING,
            type: ComputeType = ComputeType.KUBERNETES,
            features: Optional[Dict] = None,
            metadata: Optional[Dict] = None,
    ):
        self.id = id
        self.name = name
        self.context = context
        self.shared_contexts = shared_contexts if shared_contexts is not None else []
        self.global_ = global_
        self.status = status
        self.type = type
        self.features = features if features is not None else dict()
        self.metadata = metadata if metadata is not None else dict()
        self._client_api = client_api
        self._computes = None
        self._serviceDrivers = None

    @property
    def computes(self):
        if self._computes is None:
            self._computes = repositories.Computes(client_api=self._client_api)
        return self._computes

    @property
    def service_drivers(self):
        if self._serviceDrivers is None:
            self._serviceDrivers = repositories.ServiceDrivers(client_api=self._client_api)
        return self._serviceDrivers

    def delete(self):
        return self.computes.delete(compute_id=self.id)

    def update(self):
        return self.computes.update(compute=self)

    @classmethod
    def from_json(cls, _json, client_api: ApiClient):
        return cls(
            id=_json.get('id'),
            name=_json.get('name'),
            context=ComputeContext.from_json(_json.get('context', dict())),
            shared_contexts=[ComputeContext.from_json(sc) for sc in _json.get('sharedContexts', list())],
            global_=_json.get('global'),
            status=ComputeStatus(_json.get('status')),
            type=ComputeType(_json.get('type')),
            features=_json.get('features'),
            client_api=client_api,
            metadata=_json.get('metadata')
        )

    def to_json(self):
        return {
            'id': self.id,
            'context': self.context.to_json(),
            'sharedContexts': [sc.to_json() for sc in self.shared_contexts],
            'global': self.global_,
            'status': self.status.value,
            'type': self.type.value,
            'features': self.features,
            'metadata': self.metadata
        }


class KubernetesCompute(Compute):
    def __init__(
            self,
            id: str,
            context: ComputeContext,
            cluster: ComputeCluster,
            shared_contexts: Optional[List[ComputeContext]] = None,
            global_: Optional[bool] = None,
            status: ComputeStatus = ComputeStatus.INITIALIZING,
            type: ComputeType = ComputeType.KUBERNETES,
            features: Optional[Dict] = None,
            metadata: Optional[Dict] = None,
            client_api: ApiClient = None
    ):
        super().__init__(id=id, context=context, shared_contexts=shared_contexts, global_=global_, status=status,
                         type=type, features=features, metadata=metadata, client_api=client_api)
        self.cluster = cluster

    @classmethod
    def from_json(cls, _json, client_api: ApiClient):
        return cls(
            id=_json.get('id'),
            context=ComputeContext.from_json(_json.get('context', dict())),
            cluster=ComputeCluster.from_json(_json.get('cluster', dict())),
            shared_contexts=[ComputeContext.from_json(sc) for sc in _json.get('sharedContexts', list())],
            global_=_json.get('global'),
            status=ComputeStatus(_json.get('status')),
            type=ComputeType(_json.get('type')),
            features=_json.get('features'),
            metadata=_json.get('metadata'),
            client_api=client_api
        )

    def to_json(self):
        return {
            'id': self.id,
            'context': self.context.to_json(),
            'cluster': self.cluster.to_json(),
            'sharedContexts': [sc.to_json() for sc in self.shared_contexts],
            'global': self.global_,
            'status': self.status.value,
            'type': self.type.value,
            'features': self.features
        }


class ServiceDriver:
    def __init__(
            self,
            name: str,
            context: ComputeContext,
            compute_id: str,
            client_api: ApiClient,
            type: ComputeType = None,
            created_at: str = None,
            updated_at: str = None,
            namespace: str = None,
            metadata: Dict = None,
            url: str = None,
            archived: bool = None,
            id: str = None,
            is_cache_available: bool = None
    ):
        self.name = name
        self.context = context
        self.compute_id = compute_id
        self.client_api = client_api
        self.type = type or ComputeType.KUBERNETES
        self.created_at = created_at
        self.updated_at = updated_at
        self.namespace = namespace
        self.metadata = metadata
        self.url = url
        self.archived = archived
        self.id = id
        self.is_cache_available = is_cache_available

    @classmethod
    def from_json(cls, _json, client_api: ApiClient):
        return cls(
            name=_json.get('name'),
            context=ComputeContext.from_json(_json.get('context', dict())),
            compute_id=_json.get('computeId'),
            client_api=client_api,
            type=_json.get('type', None),
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            namespace=_json.get('namespace', None),
            metadata=_json.get('metadata', None),
            url=_json.get('url', None),
            archived=_json.get('archived', None),
            id=_json.get('id', None),
            is_cache_available=_json.get('isCacheAvailable', None)
        )

    def to_json(self):
        _json = {
            'name': self.name,
            'context': self.context.to_json(),
            'computeId': self.compute_id,
            'type': self.type,
        }
        if self.created_at is not None:
            _json['createdAt'] = self.namespace
        if self.updated_at is not None:
            _json['updatedAt'] = self.updated_at
        if self.namespace is not None:
            _json['namespace'] = self.namespace
        if self.metadata is not None:
            _json['metadata'] = self.metadata
        if self.url is not None:
            _json['url'] = self.url
        if self.archived is not None:
            _json['archived'] = self.archived
        if self.id is not None:
            _json['id'] = self.id
        if self.is_cache_available is not None:
            _json['isCacheAvailable'] = self.is_cache_available

        return _json


================================================
File: dtlpy/entities/dataset.py
================================================
from collections import namedtuple
import traceback
import logging
from enum import Enum

import attr
import os

from .. import repositories, entities, services, exceptions
from ..services.api_client import ApiClient
from .annotation import ViewAnnotationOptions, AnnotationType, ExportVersion

logger = logging.getLogger(name='dtlpy')


class IndexDriver(str, Enum):
    V1 = "v1"
    V2 = "v2"


class ExportType(str, Enum):
    JSON = "json"
    ZIP = "zip"


class ExpirationOptions:
    """
    ExpirationOptions object
    """

    def __init__(self, item_max_days: int = None):
        """
        :param item_max_days: int. items in dataset will be auto delete after this number id days
        """
        self.item_max_days = item_max_days

    def to_json(self):
        _json = dict()
        if self.item_max_days is not None:
            _json["itemMaxDays"] = self.item_max_days
        return _json

    @classmethod
    def from_json(cls, _json: dict):
        item_max_days = _json.get('itemMaxDays', None)
        if item_max_days:
            return cls(item_max_days=item_max_days)
        return None


@attr.s
class Dataset(entities.BaseEntity):
    """
    Dataset object
    """
    # dataset information
    id = attr.ib()
    url = attr.ib()
    name = attr.ib()
    annotated = attr.ib(repr=False)
    creator = attr.ib()
    projects = attr.ib(repr=False)
    items_count = attr.ib()
    metadata = attr.ib(repr=False)
    directoryTree = attr.ib(repr=False)
    expiration_options = attr.ib()
    index_driver = attr.ib()
    enable_sync_with_cloned = attr.ib(repr=False)

    # name change when to_json
    created_at = attr.ib()
    updated_at = attr.ib()
    updated_by = attr.ib()
    items_url = attr.ib(repr=False)
    readable_type = attr.ib(repr=False)
    access_level = attr.ib(repr=False)
    driver = attr.ib(repr=False)
    src_dataset = attr.ib(repr=False)
    _readonly = attr.ib(repr=False)
    annotations_count = attr.ib()

    # api
    _client_api = attr.ib(type=ApiClient, repr=False)

    # entities
    _project = attr.ib(default=None, repr=False)

    # repositories
    _datasets = attr.ib(repr=False, default=None)
    _repositories = attr.ib(repr=False)

    # defaults
    _ontology_ids = attr.ib(default=None, repr=False)
    _labels = attr.ib(default=None, repr=False)
    _directory_tree = attr.ib(default=None, repr=False)
    _recipe = attr.ib(default=None, repr=False)
    _ontology = attr.ib(default=None, repr=False)

    @property
    def itemsCount(self):
        return self.items_count

    @staticmethod
    def _protected_from_json(project: entities.Project,
                             _json: dict,
                             client_api: ApiClient,
                             datasets=None,
                             is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param project: dataset's project
        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param datasets: Datasets repository
        :param is_fetched: is Entity fetched from Platform
        :return: Dataset object
        """
        try:
            dataset = Dataset.from_json(project=project,
                                        _json=_json,
                                        client_api=client_api,
                                        datasets=datasets,
                                        is_fetched=is_fetched)
            status = True
        except Exception:
            dataset = traceback.format_exc()
            status = False
        return status, dataset

    @classmethod
    def from_json(cls,
                  project: entities.Project,
                  _json: dict,
                  client_api: ApiClient,
                  datasets=None,
                  is_fetched=True):
        """
        Build a Dataset entity object from a json

        :param project: dataset's project
        :param dict _json: _json response from host
        :param client_api: ApiClient entity
        :param datasets: Datasets repository
        :param bool is_fetched: is Entity fetched from Platform
        :return: Dataset object
        :rtype: dtlpy.entities.dataset.Dataset
        """
        projects = _json.get('projects', None)
        if project is not None and projects is not None:
            if project.id not in projects:
                logger.warning('Dataset has been fetched from a project that is not in it projects list')
                project = None

        expiration_options = _json.get('expirationOptions', None)
        if expiration_options:
            expiration_options = ExpirationOptions.from_json(expiration_options)
        inst = cls(metadata=_json.get('metadata', None),
                   directoryTree=_json.get('directoryTree', None),
                   readable_type=_json.get('readableType', None),
                   access_level=_json.get('accessLevel', None),
                   created_at=_json.get('createdAt', None),
                   updated_at=_json.get('updatedAt', None),
                   updated_by=_json.get('updatedBy', None),
                   annotations_count=_json.get("annotationsCount", None),
                   items_count=_json.get('itemsCount', None),
                   annotated=_json.get('annotated', None),
                   readonly=_json.get('readonly', None),
                   projects=projects,
                   creator=_json.get('creator', None),
                   items_url=_json.get('items', None),
                   driver=_json.get('driver', None),
                   name=_json.get('name', None),
                   url=_json.get('url', None),
                   id=_json.get('id', None),
                   datasets=datasets,
                   client_api=client_api,
                   project=project,
                   expiration_options=expiration_options,
                   index_driver=_json.get('indexDriver', None),
                   enable_sync_with_cloned=_json.get('enableSyncWithCloned', None),
                   src_dataset=_json.get('srcDataset', None))
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Dataset)._client_api,
                                                              attr.fields(Dataset)._project,
                                                              attr.fields(Dataset)._readonly,
                                                              attr.fields(Dataset)._datasets,
                                                              attr.fields(Dataset)._repositories,
                                                              attr.fields(Dataset)._ontology_ids,
                                                              attr.fields(Dataset)._labels,
                                                              attr.fields(Dataset)._recipe,
                                                              attr.fields(Dataset)._ontology,
                                                              attr.fields(Dataset)._directory_tree,
                                                              attr.fields(Dataset).access_level,
                                                              attr.fields(Dataset).readable_type,
                                                              attr.fields(Dataset).created_at,
                                                              attr.fields(Dataset).updated_at,
                                                              attr.fields(Dataset).updated_by,
                                                              attr.fields(Dataset).annotations_count,
                                                              attr.fields(Dataset).items_url,
                                                              attr.fields(Dataset).expiration_options,
                                                              attr.fields(Dataset).items_count,
                                                              attr.fields(Dataset).index_driver,
                                                              attr.fields(Dataset).enable_sync_with_cloned,
                                                              attr.fields(Dataset).src_dataset,
                                                              ))
        _json.update({'items': self.items_url})
        _json['readableType'] = self.readable_type
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        _json['updatedBy'] = self.updated_by
        _json['annotationsCount'] = self.annotations_count
        _json['accessLevel'] = self.access_level
        _json['readonly'] = self._readonly
        _json['itemsCount'] = self.items_count
        _json['indexDriver'] = self.index_driver
        if self.expiration_options and self.expiration_options.to_json():
            _json['expirationOptions'] = self.expiration_options.to_json()
        if self.enable_sync_with_cloned is not None:
            _json['enableSyncWithCloned'] = self.enable_sync_with_cloned
        if self.src_dataset is not None:
            _json['srcDataset'] = self.src_dataset
        return _json

    @property
    def labels(self):
        if self._labels is None:
            self._labels = self._get_ontology().labels
        return self._labels

    @property
    def readonly(self):
        return self._readonly

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/datasets/{}/items".format(self.project.id, self.id))

    @readonly.setter
    def readonly(self, state):
        import warnings
        warnings.warn("`readonly` flag on dataset is deprecated, doing nothing.", DeprecationWarning)

    @property
    def labels_flat_dict(self):
        return self._get_ontology().labels_flat_dict

    @property
    def instance_map(self) -> dict:
        return self._get_ontology().instance_map

    @instance_map.setter
    def instance_map(self, value: dict):
        """
        instance mapping for creating instance mask

        :param value: dictionary {label: map_id}
        """
        if not isinstance(value, dict):
            raise ValueError('input must be a dictionary of {label_name: instance_id}')
        self._get_ontology().instance_map = value

    @property
    def ontology_ids(self):
        if self._ontology_ids is None:
            self._ontology_ids = list()
            if self.metadata is not None and 'system' in self.metadata and 'recipes' in self.metadata['system']:
                recipe_ids = self.get_recipe_ids()
                for rec_id in recipe_ids:
                    recipe = self.recipes.get(recipe_id=rec_id)
                    self._ontology_ids += recipe.ontology_ids
        return self._ontology_ids

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['items', 'recipes', 'datasets', 'assignments', 'tasks', 'annotations',
                                       'ontologies', 'features', 'settings', 'schema', 'collections'])
        if self._project is None:
            datasets = repositories.Datasets(client_api=self._client_api, project=self._project)
        else:
            datasets = self._project.datasets

        return reps(
            items=repositories.Items(client_api=self._client_api, dataset=self, datasets=datasets),
            recipes=repositories.Recipes(client_api=self._client_api, dataset=self),
            assignments=repositories.Assignments(project=self._project, client_api=self._client_api, dataset=self),
            tasks=repositories.Tasks(client_api=self._client_api, project=self._project, dataset=self),
            annotations=repositories.Annotations(client_api=self._client_api, dataset=self),
            datasets=datasets,
            ontologies=repositories.Ontologies(client_api=self._client_api, dataset=self),
            features=repositories.Features(client_api=self._client_api, project=self._project, dataset=self),
            settings=repositories.Settings(client_api=self._client_api, dataset=self),
            schema=repositories.Schema(client_api=self._client_api, dataset=self),
            collections=repositories.Collections(client_api=self._client_api, dataset=self)
        )

    @property
    def settings(self):
        assert isinstance(self._repositories.settings, repositories.Settings)
        return self._repositories.settings

    @property
    def items(self):
        assert isinstance(self._repositories.items, repositories.Items)
        return self._repositories.items

    @property
    def ontologies(self):
        assert isinstance(self._repositories.ontologies, repositories.Ontologies)
        return self._repositories.ontologies

    @property
    def recipes(self):
        assert isinstance(self._repositories.recipes, repositories.Recipes)
        return self._repositories.recipes

    @property
    def datasets(self):
        assert isinstance(self._repositories.datasets, repositories.Datasets)
        return self._repositories.datasets

    @property
    def assignments(self):
        assert isinstance(self._repositories.assignments, repositories.Assignments)
        return self._repositories.assignments

    @property
    def tasks(self):
        assert isinstance(self._repositories.tasks, repositories.Tasks)
        return self._repositories.tasks

    @property
    def annotations(self):
        assert isinstance(self._repositories.annotations, repositories.Annotations)
        return self._repositories.annotations

    @property
    def features(self):
        assert isinstance(self._repositories.features, repositories.Features)
        return self._repositories.features

    @property
    def collections(self):
        assert isinstance(self._repositories.collections, repositories.Collections)
        return self._repositories.collections

    @property
    def schema(self):
        assert isinstance(self._repositories.schema, repositories.Schema)
        return self._repositories.schema

    @property
    def project(self):
        if self._project is None:
            # get from cache
            project = self._client_api.state_io.get('project')
            if project is not None:
                # build entity from json
                p = entities.Project.from_json(_json=project, client_api=self._client_api)
                # check if dataset belongs to project
                if p.id in self.projects:
                    self._project = p
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.projects[0],
                                                                                   fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    @property
    def directory_tree(self):
        if self._directory_tree is None:
            self._directory_tree = self.project.datasets.directory_tree(dataset_id=self.id)
        assert isinstance(self._directory_tree, entities.DirectoryTree)
        return self._directory_tree

    def __copy__(self):
        return Dataset.from_json(_json=self.to_json(),
                                 project=self._project,
                                 client_api=self._client_api,
                                 is_fetched=self.is_fetched,
                                 datasets=self.datasets)

    def __get_local_path__(self):
        if self._project is not None:
            local_path = os.path.join(services.service_defaults.DATALOOP_PATH,
                                      'projects',
                                      self.project.name,
                                      'datasets',
                                      self.name)
        else:
            local_path = os.path.join(services.service_defaults.DATALOOP_PATH,
                                      'datasets',
                                      '%s_%s' % (self.name, self.id))
        return local_path

    def _get_recipe(self):
        recipes = self.recipes.list()
        if len(recipes) > 0:
            return recipes[0]
        else:
            raise exceptions.PlatformException('404', 'Dataset {} has no recipe'.format(self.name))

    def _get_ontology(self):
        if self._ontology is None:
            ontologies = self._get_recipe().ontologies.list()
            if len(ontologies) > 0:
                self._ontology = ontologies[0]
            else:
                raise exceptions.PlatformException('404', 'Dataset {} has no ontology'.format(self.name))
        return self._ontology

    @staticmethod
    def serialize_labels(labels_dict):
        """
        Convert hex color format to rgb

        :param dict labels_dict: dict of labels
        :return: dict of converted labels
        """
        dataset_labels_dict = dict()
        for label, color in labels_dict.items():
            dataset_labels_dict[label] = '#%02x%02x%02x' % color
        return dataset_labels_dict

    def get_recipe_ids(self):
        """
        Get dataset recipe Ids

        :return: list of recipe ids
        :rtype: list
        """
        return self.metadata['system']['recipes']

    def switch_recipe(self, recipe_id=None, recipe=None):
        """
        Switch the recipe that linked to the dataset with the given one

        :param str recipe_id: recipe id
        :param dtlpy.entities.recipe.Recipe recipe: recipe entity

        **Example**:

        .. code-block:: python

            dataset.switch_recipe(recipe_id='recipe_id')
        """
        if recipe is None and recipe_id is None:
            raise exceptions.PlatformException('400', 'Must provide recipe or recipe_id')
        if recipe_id is None:
            if not isinstance(recipe, entities.Recipe):
                raise exceptions.PlatformException('400', 'Recipe must me entities.Recipe type')
            else:
                recipe_id = recipe.id

        # add recipe id to dataset metadata
        if 'system' not in self.metadata:
            self.metadata['system'] = dict()
        if 'recipes' not in self.metadata['system']:
            self.metadata['system']['recipes'] = list()
        self.metadata['system']['recipes'] = [recipe_id]
        self.update(system_metadata=True)

    def delete(self, sure=False, really=False):
        """
        Delete a dataset forever!

        **Prerequisites**: You must be an *owner* or *developer* to use this method.

        :param bool sure: are you sure you want to delete?
        :param bool really: really really?
        :return: True is success
        :rtype: bool

        **Example**:

        .. code-block:: python

            is_deleted = dataset.delete(sure=True, really=True)
        """
        return self.datasets.delete(dataset_id=self.id,
                                    sure=sure,
                                    really=really)

    def update(self, system_metadata=False):
        """
        Update dataset field

        **Prerequisites**: You must be an *owner* or *developer* to use this method.

        :param bool system_metadata: bool - True, if you want to change metadata system
        :return: Dataset object
        :rtype: dtlpy.entities.dataset.Dataset

        **Example**:

        .. code-block:: python

            dataset = dataset.update()
        """
        return self.datasets.update(dataset=self,
                                    system_metadata=system_metadata)

    def set_readonly(self, state: bool):
        """
        Set dataset readonly mode

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param bool state: state

        **Example**:

        .. code-block:: python

            dataset.set_readonly(state=True)
        """
        import warnings
        warnings.warn("`readonly` flag on dataset is deprecated, doing nothing.", DeprecationWarning)

    def clone(self,
              clone_name=None,
              filters=None,
              with_items_annotations=True,
              with_metadata=True,
              with_task_annotations_status=True,
              dst_dataset_id=None,
              target_directory=None,
              ):
        """
        Clone dataset

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str clone_name: new dataset name
        :param dtlpy.entities.filters.Filters filters: Filters entity or a query dict
        :param bool with_items_annotations: clone all item's annotations
        :param bool with_metadata: clone metadata
        :param bool with_task_annotations_status: clone task annotations status
        :param str dst_dataset_id: destination dataset id
        :param str target_directory: target directory
        :return: dataset object
        :rtype: dtlpy.entities.dataset.Dataset

        **Example**:

        .. code-block:: python

            dataset = dataset.clone(dataset_id='dataset_id',
                          clone_name='dataset_clone_name',
                          with_metadata=True,
                          with_items_annotations=False,
                          with_task_annotations_status=False)
        """
        return self.datasets.clone(dataset_id=self.id,
                                   filters=filters,
                                   clone_name=clone_name,
                                   with_metadata=with_metadata,
                                   with_items_annotations=with_items_annotations,
                                   with_task_annotations_status=with_task_annotations_status,
                                   dst_dataset_id=dst_dataset_id,
                                   target_directory=target_directory)

    def sync(self, wait=True):
        """
        Sync dataset with external storage

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param bool wait: wait for the command to finish
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = dataset.sync()
        """
        return self.datasets.sync(dataset_id=self.id, wait=wait)

    def download_annotations(self,
                             local_path=None,
                             filters=None,
                             annotation_options: ViewAnnotationOptions = None,
                             annotation_filters=None,
                             overwrite=False,
                             thickness=1,
                             with_text=False,
                             remote_path=None,
                             include_annotations_in_output=True,
                             export_png_files=False,
                             filter_output_annotations=False,
                             alpha=1,
                             export_version=ExportVersion.V1
                             ):
        """
        Download dataset by filters.
        Filtering the dataset for items and save them local
        Optional - also download annotation, mask, instance and image mask of the item

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str local_path: local folder or filename to save to.
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param list(dtlpy.entities.annotation.ViewAnnotationOptions) annotation_options: download annotations options: list(dl.ViewAnnotationOptions)
        :param dtlpy.entities.filters.Filters annotation_filters: Filters entity to filter annotations for download
        :param bool overwrite: optional - default = False
        :param int thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param bool with_text: optional - add text to annotations, default = False
        :param str remote_path: DEPRECATED and ignored
        :param bool include_annotations_in_output: default - False , if export should contain annotations
        :param bool export_png_files: default - if True, semantic annotations should be exported as png files
        :param bool filter_output_annotations: default - False, given an export by filter - determine if to filter out annotations
        :param float alpha: opacity value [0 1], default 1
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :return: local_path of the directory where all the downloaded item
        :rtype: str

        **Example**:

        .. code-block:: python

            local_path = dataset.download_annotations(dataset='dataset_entity',
                                         local_path='local_path',
                                         annotation_options=[dl.ViewAnnotationOptions.JSON, dl.ViewAnnotationOptions.MASK],
                                         overwrite=False,
                                         thickness=1,
                                         with_text=False,
                                         alpha=1
                                         )
        """

        return self.datasets.download_annotations(
            dataset=self,
            local_path=local_path,
            overwrite=overwrite,
            filters=filters,
            annotation_options=annotation_options,
            annotation_filters=annotation_filters,
            thickness=thickness,
            with_text=with_text,
            remote_path=remote_path,
            include_annotations_in_output=include_annotations_in_output,
            export_png_files=export_png_files,
            filter_output_annotations=filter_output_annotations,
            alpha=alpha,
            export_version=export_version
        )

    def export(self,
               local_path=None,
               filters=None,
               annotation_filters=None,
               feature_vector_filters=None,
               include_feature_vectors: bool = False,
               include_annotations: bool = False,
               export_type: ExportType = ExportType.JSON,
               timeout: int = 0):
        """
        Export dataset items and annotations.

        **Prerequisites**: You must be an *owner* or *developer* to use this method.

        You must provide at least ONE of the following params: dataset, dataset_name, dataset_id.

        :param str local_path: The local path to save the exported dataset
        :param Union[dict, dtlpy.entities.filters.Filters] filters: Filters entity or a query dictionary
        :param dtlpy.entities.filters.Filters annotation_filters: Filters entity
        :param dtlpy.entities.filters.Filters feature_vector_filters: Filters entity
        :param bool include_feature_vectors: Include item feature vectors in the export
        :param bool include_annotations: Include item annotations in the export
        :param entities.ExportType export_type: Type of export ('json' or 'zip')
        :param int timeout: Maximum time in seconds to wait for the export to complete
        :return: Exported item
        :rtype: dtlpy.entities.item.Item

        **Example**:

        .. code-block:: python

            export_item = dataset.export(filters=filters,
                                         include_feature_vectors=True,
                                         include_annotations=True,
                                         export_type=dl.ExportType.JSON)
        """

        return self.datasets.export(dataset=self,
                                    local_path=local_path,
                                    filters=filters,
                                    annotation_filters=annotation_filters,
                                    feature_vector_filters=feature_vector_filters,
                                    include_feature_vectors=include_feature_vectors,
                                    include_annotations=include_annotations,
                                    export_type=export_type,
                                    timeout=timeout)

    def upload_annotations(self,
                           local_path,
                           filters=None,
                           clean=False,
                           remote_root_path='/',
                           export_version=ExportVersion.V1
                           ):
        """
        Upload annotations to dataset.

        **Prerequisites**: You must have a dataset with items that are related to the annotations. The relationship between the dataset and annotations is shown in the name. You must be in the role of an *owner* or *developer*.

        :param str local_path: str - local folder where the annotations files is.
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param bool clean: bool - if True it remove the old annotations
        :param str remote_root_path: str - the remote root path to match remote and local items
        :param str export_version:  `V2` - exported items will have original extension in filename, `V1` - no original extension in filenames

        For example, if the item filepath is a/b/item and remote_root_path is /a the start folder will be b instead of a

        **Example**:

        .. code-block:: python

            dataset.upload_annotations(dataset='dataset_entity',
                                     local_path='local_path',
                                     clean=False,
                                     export_version=dl.ExportVersion.V1
                                     )
        """

        return self.datasets.upload_annotations(
            dataset=self,
            local_path=local_path,
            filters=filters,
            clean=clean,
            remote_root_path=remote_root_path,
            export_version=export_version
        )

    def checkout(self):
        """
        Checkout the dataset

        """
        self.datasets.checkout(dataset=self)

    def open_in_web(self):
        """
        Open the dataset in web platform

        """
        self._client_api._open_in_web(url=self.platform_url)

    def add_label(self, label_name, color=None, children=None, attributes=None, display_label=None, label=None,
                  recipe_id=None, ontology_id=None, icon_path=None):
        """
        Add single label to dataset

        **Prerequisites**: You must have a dataset with items that are related to the annotations. The relationship between the dataset and annotations is shown in the name. You must be in the role of an *owner* or *developer*.

        :param str label_name: str - label name
        :param tuple color: RGB color of the annotation, e.g (255,0,0) or '#ff0000' for red
        :param children: children (sub labels). list of sub labels of this current label, each value is either dict or dl.Label
        :param list attributes: add attributes to the labels
        :param str display_label: name that display label
        :param dtlpy.entities.label.Label label: label object
        :param str recipe_id: optional recipe id
        :param str ontology_id: optional ontology id
        :param str icon_path: path to image to be display on label
        :return: label entity
        :rtype: dtlpy.entities.label.Label

        **Example**:

        .. code-block:: python

            dataset.add_label(label_name='person', color=(34, 6, 231), attributes=['big', 'small'])
        """
        # get recipe
        if recipe_id is None:
            recipe_id = self.get_recipe_ids()[0]
        recipe = self.recipes.get(recipe_id=recipe_id)

        # get ontology
        if ontology_id is None:
            ontology_id = recipe.ontology_ids[0]
        ontology = recipe.ontologies.get(ontology_id=ontology_id)
        # ontology._dataset = self

        # add label
        added_label = ontology.add_label(label_name=label_name,
                                         color=color,
                                         children=children,
                                         attributes=attributes,
                                         display_label=display_label,
                                         label=label,
                                         update_ontology=True,
                                         icon_path=icon_path)

        return added_label

    def add_labels(self, label_list, ontology_id=None, recipe_id=None):
        """
        Add labels to dataset

        **Prerequisites**: You must have a dataset with items that are related to the annotations. The relationship between the dataset and annotations is shown in the name. You must be in the role of an *owner* or *developer*.

        :param list label_list: a list of labels to add to the dataset's ontology. each value should be a dict, dl.Label or a string
        :param str ontology_id: optional ontology id
        :param str recipe_id: optional recipe id
        :return: label entities

        **Example**:

        .. code-block:: python

            dataset.add_labels(label_list=label_list)
        """
        # get recipe
        if recipe_id is None:
            recipe_id = self.get_recipe_ids()[0]
        recipe = self.recipes.get(recipe_id=recipe_id)

        # get ontology
        if ontology_id is None:
            ontology_id = recipe.ontology_ids[0]
        ontology = recipe.ontologies.get(ontology_id=ontology_id)

        # add labels to ontology
        added_labels = ontology.add_labels(label_list=label_list, update_ontology=True)

        return added_labels

    def update_label(self, label_name, color=None, children=None, attributes=None, display_label=None, label=None,
                     recipe_id=None, ontology_id=None, upsert=False, icon_path=None):
        """
        Add single label to dataset

        **Prerequisites**: You must have a dataset with items that are related to the annotations. The relationship between the dataset and annotations is shown in the name. You must be in the role of an *owner* or *developer*.

        :param str label_name: str - label name
        :param tuple color: color
        :param children: children (sub labels)
        :param list attributes: add attributes to the labels
        :param str display_label: name that display label
        :param dtlpy.entities.label.Label label: label
        :param str recipe_id: optional recipe id
        :param str ontology_id: optional ontology id
        :param str icon_path: path to image to be display on label

        :return: label entity
        :rtype: dtlpy.entities.label.Label

        **Example**:

        .. code-block:: python

            dataset.update_label(label_name='person', color=(34, 6, 231), attributes=['big', 'small'])
        """
        # get recipe

        if recipe_id is None:
            recipe_id = self.get_recipe_ids()[0]
        recipe = self.recipes.get(recipe_id=recipe_id)

        # get ontology
        if ontology_id is None:
            ontology_id = recipe.ontology_ids[0]
        ontology = recipe.ontologies.get(ontology_id=ontology_id)

        # add label
        added_label = ontology.update_label(label_name=label_name,
                                            color=color,
                                            children=children,
                                            attributes=attributes,
                                            display_label=display_label,
                                            label=label,
                                            update_ontology=True,
                                            upsert=upsert,
                                            icon_path=icon_path)

        return added_label

    def update_labels(self, label_list, ontology_id=None, recipe_id=None, upsert=False):
        """
        Add labels to dataset

        **Prerequisites**: You must have a dataset with items that are related to the annotations. The relationship between the dataset and annotations is shown in the name. You must be in the role of an *owner* or *developer*.

        :param list label_list: label list
        :param str ontology_id: optional ontology id
        :param str recipe_id: optional recipe id
        :param bool upsert: if True will add in case it does not existing

        :return: label entities
        :rtype: dtlpy.entities.label.Label

        **Example**:

        .. code-block:: python

            dataset.update_labels(label_list=label_list)
        """
        # get recipe
        if recipe_id is None:
            recipe_id = self.get_recipe_ids()[0]
        recipe = self.recipes.get(recipe_id=recipe_id)

        # get ontology
        if ontology_id is None:
            ontology_id = recipe.ontology_ids[0]
        ontology = recipe.ontologies.get(ontology_id=ontology_id)

        # add labels to ontology
        added_labels = ontology.update_labels(label_list=label_list, update_ontology=True, upsert=upsert)

        return added_labels

    def download(
            self,
            filters=None,
            local_path=None,
            file_types=None,
            annotation_options: ViewAnnotationOptions = None,
            annotation_filters=None,
            overwrite=False,
            to_items_folder=True,
            thickness=1,
            with_text=False,
            without_relative_path=None,
            alpha=1,
            export_version=ExportVersion.V1
    ):
        """
        Download dataset by filters.
        Filtering the dataset for items and save them local
        Optional - also download annotation, mask, instance and image mask of the item

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param str local_path: local folder or filename to save to.
        :param list file_types: a list of file type to download. e.g ['video/webm', 'video/mp4', 'image/jpeg', 'image/png']
        :param list annotation_options: type of download annotations: list(dl.ViewAnnotationOptions)
        :param dtlpy.entities.filters.Filters annotation_filters: Filters entity to filter annotations for download
        :param bool overwrite: optional - default = False to overwrite the existing files
        :param bool to_items_folder: Create 'items' folder and download items to it
        :param int thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param bool with_text: optional - add text to annotations, default = False
        :param bool without_relative_path: bool - download items without the relative path from platform
        :param float alpha: opacity value [0 1], default 1
        :param str export_version:  `V2` - exported items will have original extension in filename, `V1` - no original extension in filenames
        :return: `List` of local_path per each downloaded item

        **Example**:

        .. code-block:: python

            dataset.download(local_path='local_path',
                             annotation_options=[dl.ViewAnnotationOptions.JSON, dl.ViewAnnotationOptions.MASK],
                             overwrite=False,
                             thickness=1,
                             with_text=False,
                             alpha=1
                             )
        """
        return self.items.download(filters=filters,
                                   local_path=local_path,
                                   file_types=file_types,
                                   annotation_options=annotation_options,
                                   annotation_filters=annotation_filters,
                                   overwrite=overwrite,
                                   to_items_folder=to_items_folder,
                                   thickness=thickness,
                                   with_text=with_text,
                                   without_relative_path=without_relative_path,
                                   alpha=alpha,
                                   export_version=export_version)

    def download_folder(
            self,
            folder_path,
            filters=None,
            local_path=None,
            file_types=None,
            annotation_options: ViewAnnotationOptions = None,
            annotation_filters=None,
            overwrite=False,
            to_items_folder=True,
            thickness=1,
            with_text=False,
            without_relative_path=None,
            alpha=1,
            export_version=ExportVersion.V1
    ):
        """
        Download dataset folder.
        Optional - also download annotation, mask, instance and image mask of the item

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str folder_path: the path of the folder that want to download
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param str local_path: local folder or filename to save to.
        :param list file_types: a list of file type to download. e.g ['video/webm', 'video/mp4', 'image/jpeg', 'image/png']
        :param list annotation_options: type of download annotations: list(dl.ViewAnnotationOptions)
        :param dtlpy.entities.filters.Filters annotation_filters: Filters entity to filter annotations for download
        :param bool overwrite: optional - default = False to overwrite the existing files
        :param bool to_items_folder: Create 'items' folder and download items to it
        :param int thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param bool with_text: optional - add text to annotations, default = False
        :param bool without_relative_path: bool - download items without the relative path from platform
        :param float alpha: opacity value [0 1], default 1
        :param str export_version:  `V2` - exported items will have original extension in filename, `V1` - no original extension in filenames
        :return: `List` of local_path per each downloaded item

        **Example**:

        .. code-block:: python

            dataset.download_folder(folder_path='folder_path'
                             local_path='local_path',
                             annotation_options=[dl.ViewAnnotationOptions.JSON, dl.ViewAnnotationOptions.MASK],
                             overwrite=False,
                             thickness=1,
                             with_text=False,
                             alpha=1,
                             save_locally=True
                             )
        """
        filters = self.datasets._bulid_folder_filter(folder_path=folder_path, filters=filters)
        return self.items.download(filters=filters,
                                   local_path=local_path,
                                   file_types=file_types,
                                   annotation_options=annotation_options,
                                   annotation_filters=annotation_filters,
                                   overwrite=overwrite,
                                   to_items_folder=to_items_folder,
                                   thickness=thickness,
                                   with_text=with_text,
                                   without_relative_path=without_relative_path,
                                   alpha=alpha,
                                   export_version=export_version)

    def delete_labels(self, label_names):
        """
        Delete labels from dataset's ontologies

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param label_names: label object/ label name / list of label objects / list of label names

        **Example**:

        .. code-block:: python

            dataset.delete_labels(label_names=['myLabel1', 'Mylabel2'])
        """
        for recipe in self.recipes.list():
            for ontology in recipe.ontologies.list():
                ontology.delete_labels(label_names=label_names)
        self._labels = None

    def update_attributes(self,
                          title: str,
                          key: str,
                          attribute_type,
                          recipe_id: str = None,
                          ontology_id: str = None,
                          scope: list = None,
                          optional: bool = None,
                          values: list = None,
                          attribute_range=None):
        """
        ADD a new attribute or update if exist

        :param str ontology_id: ontology_id
        :param str title: attribute title
        :param str key: the key of the attribute must br unique
        :param AttributesTypes attribute_type: dl.AttributesTypes your attribute type
        :param list scope: list of the labels or * for all labels
        :param bool optional: optional attribute
        :param list values: list of the attribute values ( for checkbox and radio button)
        :param dict or AttributesRange attribute_range: dl.AttributesRange object
        :return: true in success
        :rtype: bool

        **Example**:

        .. code-block:: python

            dataset.update_attributes(ontology_id='ontology_id',
                                      key='1',
                                      title='checkbox',
                                      attribute_type=dl.AttributesTypes.CHECKBOX,
                                      values=[1,2,3])
        """
        # get recipe
        if recipe_id is None:
            recipe_id = self.get_recipe_ids()[0]
        recipe = self.recipes.get(recipe_id=recipe_id)

        # get ontology
        if ontology_id is None:
            ontology_id = recipe.ontology_ids[0]
        ontology = recipe.ontologies.get(ontology_id=ontology_id)

        # add attribute to ontology
        attribute = ontology.update_attributes(
            title=title,
            key=key,
            attribute_type=attribute_type,
            scope=scope,
            optional=optional,
            values=values,
            attribute_range=attribute_range)

        return attribute

    def delete_attributes(self, keys: list,
                          recipe_id: str = None,
                          ontology_id: str = None):
        """
        Delete a bulk of attributes

        :param str recipe_id: recipe id
        :param str ontology_id: ontology id
        :param list keys: Keys of attributes to delete
        :return: True if success
        :rtype: bool
        """

        # get recipe
        if recipe_id is None:
            recipe_id = self.get_recipe_ids()[0]
        recipe = self.recipes.get(recipe_id=recipe_id)

        # get ontology
        if ontology_id is None:
            ontology_id = recipe.ontology_ids[0]
        ontology = recipe.ontologies.get(ontology_id=ontology_id)
        return ontology.delete_attributes(ontology_id=ontology.id, keys=keys)

    def split_ml_subsets(self,
                         items_query = None,
                         percentages: dict = None ):
        """
        Split dataset items into ML subsets.

        :param dl.Filters items_query: Filters object to select items.
        :param dict percentages: {'train': x, 'validation': y, 'test': z}.
        :return: True if the split operation was successful.
        :rtype: bool
        """
        return self.datasets.split_ml_subsets(dataset_id=self.id,
                                              items_query=items_query,
                                              ml_split_list=percentages)

    def assign_subset_to_items(self, subset: str, items_query=None) -> bool:
        """
        Assign a specific ML subset (train/validation/test) to items defined by the given filters.
        This will set the chosen subset to True and the others to None.

        :param dl.Filters items_query: Filters to select items
        :param str subset: 'train', 'validation', or 'test'
        :return: True if successful
        :rtype: bool
        """
   
        return self.datasets.bulk_update_ml_subset(dataset_id=self.id,
                                                   items_query=items_query,
                                                   subset=subset)

    def remove_subset_from_items(self, items_query= None,) -> bool:
        """
        Remove any ML subset assignment from items defined by the given filters.
        This sets train, validation, and test tags to None.

        :param dl.Filters items_query: Filters to select items
        :return: True if successful
        :rtype: bool
        """
        return self.datasets.bulk_update_ml_subset(dataset_id=self.id,
                                                   items_query=items_query,
                                                   subset=None,
                                                   deleteTag=True)

    def get_items_missing_ml_subset(self, filters = None) -> list:
        """
        Get the list of item IDs that are missing ML subset assignment.
        An item is considered missing ML subset if train, validation, and test tags are not True (all None).

        :param dl.Filters filters: optional filters to narrow down items. If None, will use a default filter for files.
        :return: list of item IDs
        :rtype: list
        """
        if filters is None:
            filters = entities.Filters()
        filters.add(field='metadata.system.tags.train', values=None)
        filters.add(field='metadata.system.tags.validation', values=None)
        filters.add(field='metadata.system.tags.test', values=None)
        missing_ids = []
        pages = self.items.list(filters=filters)
        for page in pages:
            for item in page:
                # item that pass filters means no subsets assigned
                missing_ids.append(item.id)
        return missing_ids


================================================
File: dtlpy/entities/directory_tree.py
================================================
import logging

logger = logging.getLogger(name='dtlpy')


class DirectoryTree:
    """
    Dataset DirectoryTree

    """

    def __init__(self, _json):
        self.dirs = list()
        self.root = SingleDirectory(directory_tree=self, children=_json.get('children', None),
                                    value=_json.get('value', None))
        self.tree = _json

    @property
    def dir_names(self):
        return [directory.path for directory in self.dirs]


class SingleDirectory:
    """
    DirectoryTree single directory

    """

    def __init__(self, value, directory_tree, children=None):
        self.id = value.get('id', None)
        self.name = value.get('name', None)
        self.parent = value.get('dir', None)
        self.path = value.get('filename', None)
        self.metadata = value.get('metadata', dict())

        self.children = list()

        if children is not None:
            for child in children:
                self.children.append(
                    SingleDirectory(directory_tree=directory_tree, children=child.get('children', None),
                                    value=child['value']))

        directory_tree.dirs.append(self)


================================================
File: dtlpy/entities/dpk.py
================================================
from collections import namedtuple
from typing import List, Union
import traceback
import enum

from .. import entities, repositories, exceptions
from ..services.api_client import ApiClient


class SlotType(str, enum.Enum):
    ITEM_VIEWER = 'itemViewer'
    FLOATING_WINDOW = 'floatingWindow'


DEFAULT_STOPS = {SlotType.ITEM_VIEWER: {"type": "itemViewer",
                                        "configuration": {"layout": {"leftBar": True,
                                                                     "rightBar": True,
                                                                     "bottomBar": True,
                                                                     }
                                                          }
                                        },
                 SlotType.FLOATING_WINDOW: {"type": "floatingWindow",
                                            "configuration": {"layout": {"width": 455,
                                                                         "height": 340,
                                                                         "resizable": True,
                                                                         "backgroundColor": "dl-color-studio-panel"
                                                                         }
                                                              }
                                            }
                 }


class Slot(entities.DlEntity):
    type: str = entities.DlProperty(location=['type'], _type=str)
    configuration: dict = entities.DlProperty(location=['configuration'], _type=dict)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class Toolbar(entities.DlEntity):
    display_name: str = entities.DlProperty(location=['displayName'], _type=str)
    conditions: dict = entities.DlProperty(location=['conditions'], _type=dict)
    invoke: dict = entities.DlProperty(location=['invoke'], _type=dict)
    location: str = entities.DlProperty(location=['location'], _type=str)
    icon: str = entities.DlProperty(location=['icon'], _type=str)
    action: str = entities.DlProperty(location=['action'], _type=str, default=None)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class Panel(entities.DlEntity):
    name: str = entities.DlProperty(location=['name'], _type=str)
    path: str = entities.DlProperty(location=['path'], _type=str, default=None)
    min_role: list = entities.DlProperty(location=['minRole'], _type=list)
    supported_slots: list = entities.DlProperty(location=['supportedSlots'], _type=list)

    metadata = entities.DlProperty(location=['metadata'], _type=list)
    default_settings = entities.DlProperty(location=['defaultSettings'], _type=list)
    conditions = entities.DlProperty(location=['conditions'], _type=list)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class ToolbarInvoke(entities.DlEntity):
    type: str = entities.DlProperty(location=['type'], _type=str)
    panel: str = entities.DlProperty(location=['panel'], _type=str)
    namespace: str = entities.DlProperty(location=['namespace'], _type=str)
    source: str = entities.DlProperty(location=['source'], _type=str)
    input_options: dict = entities.DlProperty(location=['inputOptions'], _type=dict)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class CustomNodeScope(str, enum.Enum):
    GLOBAL = "global",
    PROJECT = "project",
    NODE = 'node'


class PipelineNode(entities.DlEntity):
    display_name: str = entities.DlProperty(location=['displayName'], _type=str)
    panel: str = entities.DlProperty(location=['panel'], _type=str)
    invoke: ToolbarInvoke = entities.DlProperty(location=['invoke'], _kls='ToolbarInvoke')
    icon: str = entities.DlProperty(location=['icon'], _type=str)
    categories: List[str] = entities.DlProperty(location=['categories'], _type=list)
    description: str = entities.DlProperty(location=['description'], _type=str)
    configuration: dict = entities.DlProperty(location=['configuration'], _type=dict)
    scope: CustomNodeScope = entities.DlProperty(location=['scope'], _type=CustomNodeScope)
    compute_config: str = entities.DlProperty(location=['computeConfig'], _type=str, default=None)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class DpkComputeConfig(entities.DlEntity):
    run_execution_as_process: bool = entities.DlProperty(location=['runExecutionAsProcess'], _type=bool)
    execution_timeout: int = entities.DlProperty(location=['executionTimeout'], _type=int)
    drain_time: int = entities.DlProperty(location=['drainTime'], _type=int)
    on_reset: str = entities.DlProperty(location=['onReset'], _type=str)
    runtime: dict = entities.DlProperty(location=['runtime'], _type=dict)
    bot_user_name: str = entities.DlProperty(location=['botUserName'], _type=str)
    max_attempts: int = entities.DlProperty(location=['maxAttempts'], _type=int)
    use_user_jwt: bool = entities.DlProperty(location=['useUserJwt'], _type=bool)
    driver_id: str = entities.DlProperty(location=['driverId'], _type=str)
    versions: dict = entities.DlProperty(location=['versions'], _type=dict)
    name: str = entities.DlProperty(location=['name'], _type=str)
    integrations: List[dict] = entities.DlProperty(location=['integrations'], _type=list)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class DpkComponentChannel(entities.DlEntity):
    name: str = entities.DlProperty(location=['name'], _type=str)
    icon: str = entities.DlProperty(location=['icon'], _type=str)
    description: str = entities.DlProperty(location=['description'], _type=str)
    is_global: bool = entities.DlProperty(location=['global'], _type=bool)
    metadata: dict = entities.DlProperty(location=['metadata'], _type=dict)
    context: dict = entities.DlProperty(location=['context'], _type=dict)
    filters: List[dict] = entities.DlProperty(location=['filters'], _type=list)

    def to_json(self) -> dict:
        return self._dict.copy()

    @classmethod
    def from_json(cls, _json):
        return cls(_dict=_json)


class Components(entities.DlEntity):
    panels: List[Panel] = entities.DlProperty(location=['panels'], _kls='Panel')
    modules: List[entities.PackageModule] = entities.DlProperty(location=['modules'], _kls='PackageModule')
    services: List[dict] = entities.DlProperty(location=['services'])
    triggers: List[dict] = entities.DlProperty(location=['triggers'])
    pipeline_nodes: List[PipelineNode] = entities.DlProperty(location=['pipelineNodes'])
    toolbars: List[Toolbar] = entities.DlProperty(location=['toolbars'], _kls='Toolbar')
    models: List[dict] = entities.DlProperty(location=['models'])
    compute_configs: List[DpkComputeConfig] = entities.DlProperty(location=['computeConfigs'], _kls='DpkComputeConfig')
    channels: List[DpkComponentChannel] = entities.DlProperty(location=['channels'], _kls='DpkComponentChannel')
    pipeline_templates: List[dict] = entities.DlProperty(location=['pipelineTemplates'])
    integrations: List[dict] = entities.DlProperty(location=['integrations'])

    @panels.default
    def default_panels(self):
        self._dict['panels'] = list()
        return self._dict['panels']

    @modules.default
    def default_modules(self):
        self._dict['modules'] = list()
        return self._dict['modules']

    @services.default
    def default_services(self):
        self._dict['services'] = list()
        return self._dict['services']

    @triggers.default
    def default_triggers(self):
        self._dict['triggers'] = list()
        return self._dict['triggers']

    @pipeline_nodes.default
    def default_pipelines(self):
        self._dict['pipelines'] = list()
        return self._dict['pipelines']

    @toolbars.default
    def default_toolbars(self):
        self._dict['toolbars'] = list()
        return self._dict['toolbars']

    @models.default
    def default_models(self):
        self._dict['models'] = list()
        return self._dict['models']

    @compute_configs.default
    def default_compute_configs(self):
        self._dict['computeConfigs'] = list()
        return self._dict['computeConfigs']

    @channels.default
    def default_channels(self):
        self._dict['channels'] = list()
        return self._dict['channels']

    @pipeline_templates.default
    def default_pipeline_templates(self):
        self._dict['pipelineTemplates'] = list()
        return self._dict['pipelineTemplates']

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst

    def to_json(self):
        return self._dict.copy()


class Dpk(entities.DlEntity):
    # name change
    id: str = entities.DlProperty(location=['id'], _type=str)
    base_id: str = entities.DlProperty(location=['baseId'], _type=str)
    name: str = entities.DlProperty(location=['name'], _type=str)
    version: str = entities.DlProperty(location=['version'], _type=str)
    attributes: dict = entities.DlProperty(location=['attributes'], _type=dict)
    created_at: str = entities.DlProperty(location=['createdAt'], _type=str)
    updated_at: str = entities.DlProperty(location=['updatedAt'], _type=str)
    creator: str = entities.DlProperty(location=['creator'], _type=str)
    display_name: str = entities.DlProperty(location=['displayName'], _type=str)
    icon: str = entities.DlProperty(location=['icon'], _type=str)
    tags: list = entities.DlProperty(location=['tags'], _type=list)
    codebase: Union[entities.Codebase, None] = entities.DlProperty(location=['codebase'], _kls="Codebase")
    scope: str = entities.DlProperty(location=['scope'], _type=str)
    context: dict = entities.DlProperty(location=['context'], _type=dict)
    metadata: dict = entities.DlProperty(location=['metadata'], _type=dict)
    dependencies: dict = entities.DlProperty(location=['dependencies'], _type=List[dict])

    # defaults
    components: Components = entities.DlProperty(location=['components'], _kls='Components')
    description: str = entities.DlProperty(location=['description'], _type=str)
    url: str = entities.DlProperty(location=['url'], _type=str)

    # sdk
    client_api: ApiClient
    project: entities.Project
    _revisions = None
    __repositories = None

    @components.default
    def default_components(self):
        self._dict['components'] = dict()
        return self._dict['components']

    ################
    # repositories #
    ################
    @property
    def _repositories(self):
        if self.__repositories is None:
            reps = namedtuple('repositories',
                              field_names=['dpks', 'codebases', 'organizations', 'services'])

            self.__repositories = reps(
                dpks=repositories.Dpks(client_api=self.client_api, project=self.project),
                codebases=repositories.Codebases(client_api=self.client_api),
                organizations=repositories.Organizations(client_api=self.client_api),
                services=repositories.Services(client_api=self.client_api, project=self.project, package=self),
            )

        return self.__repositories

    @property
    def codebases(self):
        assert isinstance(self._repositories.codebases, repositories.Codebases)
        return self._repositories.codebases

    @property
    def organizations(self):
        assert isinstance(self._repositories.organizations, repositories.Organizations)
        return self._repositories.organizationsFapp

    @property
    def dpks(self) -> 'repositories.Dpks':
        assert isinstance(self._repositories.dpks, repositories.Dpks)
        return self._repositories.dpks

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    ###########
    # methods #
    ###########
    def publish(self):
        """
        Publish the dpk to Dataloop platform.

        **Example**
        .. code-block:: python
            published_dpk = dpk.publish()
        """
        return self.dpks.publish(dpk=self)

    def update(self):
        """
        Update the dpk attributes to Dataloop platform.

        **Example**
        .. code-block:: python
            updated_dpk = dpk.update()
        """
        return self.dpks.update(dpk=self)

    def pull(self, local_path):
        """
        Pulls the app from the platform as dpk file.

        Note: you must pass either dpk_name or dpk_id to the function.
        :param local_path: the path where you want to install the dpk file.
        :return local path where the package pull

        **Example**
        ..code-block:: python
            path = dl.dpks.pull(dpk_name='my-app')
        """
        return self.dpks.pull(dpk=self, local_path=local_path)

    def delete(self):
        """
        Delete the dpk from the app store.

        Note: after removing the dpk, you cant get it again, it's advised to pull it first.

        :return whether the operation ran successfully
        :rtype bool
        """
        return self.dpks.delete(self.id)

    def _get_revision_pages(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        returns the available versions of the dpk.

        :param entities.Filters filters: the filters to apply to the search.
        :return the available versions of the dpk.

        ** Example **
        ..code-block:: python
            versions = dl.dpks.revisions(dpk_name='name')
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.DPK)
        elif not isinstance(filters, entities.Filters):
            raise ValueError('Unknown filters type: {!r}'.format(type(filters)))
        elif filters.resource != entities.FiltersResource.DPK:
            raise TypeError('Filters resource must to be FiltersResource.DPK. Got: {!r}'.format(filters.resource))

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self.client_api,
                                       list_function=self._list_revisions)
        paged.get_page()
        return paged

    def _build_entities_from_response(self, response_items):
        return self.dpks._build_entities_from_response(response_items=response_items)

    def _list_revisions(self, filters: entities.Filters):
        url = '/app-registry/{}/revisions'.format(self.name)
        # request
        success, response = self.client_api.gen_request(req_type='post',
                                                        path=url,
                                                        json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @property
    def revisions(self):
        """
        Returns the available versions of the dpk.

        :return List[Dpk]

        ** Example **
        ..code-block:: python
        versions = dpk.revisions
        """
        if self._revisions is None:
            self._revisions = self._get_revision_pages()
        return self._revisions

    def get_revisions(self, version: str):
        """
        Get the dpk with the specified version.

        :param str version: the version of the dpk to get.
        :return: Dpk

        ** Example **
        ..code-block:: python
        dpk = dpk.get_revisions(version='1.0.0')
        """
        return self.dpks.get_revisions(dpk_id=self.base_id, version=version)

    @staticmethod
    def _protected_from_json(_json, client_api, project, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json:  platform json
        :param client_api: ApiClient entity
        :return:
        """
        try:
            package = Dpk.from_json(_json=_json,
                                    client_api=client_api,
                                    project=project,
                                    is_fetched=is_fetched)
            status = True
        except Exception:
            package = traceback.format_exc()
            status = False
        return status, package

    def to_json(self):
        """
        convert the class to json
        """
        _json = self._dict.copy()
        return _json

    @classmethod
    def from_json(cls,
                  _json,
                  client_api: ApiClient = None,
                  project: entities.Project = None,
                  is_fetched=True) -> 'Dpk':
        """
        Turn platform representation of app into an app entity

        :param dict _json: platform representation of package
        :param dl.ApiClient client_api: ApiClient entity
        :param dl.entities.Project project: The project where the dpk resides
        :param bool is_fetched: is Entity fetched from Platform
        :return: App entity
        :rtype: dtlpy.entities.App
        """
        res = cls(
            _dict=_json,
            client_api=client_api,
            project=project,
            is_fetched=is_fetched
        )

        return res


================================================
File: dtlpy/entities/driver.py
================================================
import logging
import attr
from enum import Enum
from collections import namedtuple
from .. import entities, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class ExternalStorage(str, Enum):
    """ The type of the Integration.

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - S3
         - AWS S3 drivers
       * - GCS
         - Google GCS drivers
       * - AZUREBLOB
         - Microsoft AZURE BLOB drivers
       * - AZURE_DATALAKE_GEN2
         - Microsoft AZURE GEN2 drivers
    """
    S3 = "s3"
    GCS = "gcs"
    AZUREBLOB = "azureblob"
    AZURE_DATALAKE_GEN2 = 'azureDatalakeGen2'
    KEY_VALUE = "key_value"
    AWS_STS = 'aws-sts'


@attr.s()
class Driver(entities.BaseEntity):
    """
    Driver entity
    """
    creator = attr.ib()
    allow_external_delete = attr.ib()
    allow_external_modification = attr.ib()
    created_at = attr.ib()
    type = attr.ib()
    integration_id = attr.ib()
    integration_type = attr.ib()
    metadata = attr.ib(repr=False)
    name = attr.ib()
    id = attr.ib()
    path = attr.ib()
    # api
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['drivers'])
        return reps(
            drivers=repositories.Drivers(client_api=self._client_api),
        )

    @property
    def drivers(self):
        assert isinstance(self._repositories.drivers, repositories.Drivers)
        return self._repositories.drivers

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Driver entity object from a json

        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Driver object
        """
        inst = cls(creator=_json.get('creator', None),
                   allow_external_delete=_json.get('allowExternalDelete', None),
                   allow_external_modification=_json.get('allowExternalModification', None),
                   created_at=_json.get('createdAt', None),
                   type=_json.get('type', None),
                   integration_id=_json.get('integrationId', None),
                   integration_type=_json.get('integrationType', None),
                   metadata=_json.get('metadata', None),
                   name=_json.get('name', None),
                   id=_json.get('id', None),
                   client_api=client_api,
                   path=_json.get('path', None))

        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        output_dict = attr.asdict(self,
                                  filter=attr.filters.exclude(attr.fields(Driver)._client_api,
                                                              attr.fields(Driver).allow_external_delete,
                                                              attr.fields(Driver).allow_external_modification,
                                                              attr.fields(Driver).created_at,
                                                              attr.fields(Driver).integration_id,
                                                              attr.fields(Driver).integration_type,
                                                              attr.fields(Driver).path
                                                              ))
        output_dict['allowExternalDelete'] = self.allow_external_delete
        output_dict['allowExternalModification'] = self.allow_external_modification
        output_dict['createdAt'] = self.created_at
        output_dict['integrationId'] = self.integration_id
        output_dict['integrationType'] = self.integration_type

        if self.path is not None:
            output_dict['path'] = self.path

        return output_dict

    def delete(self, sure=False, really=False):
        """
        Delete a driver forever!

        **Prerequisites**: You must be an *owner* or *developer* to use this method.

        :param bool sure: are you sure you want to delete?
        :param bool really: really really?
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            driver.delete(sure=True, really=True)
        """
        return self.drivers.delete(driver_id=self.id,
                                   sure=sure,
                                   really=really)


@attr.s()
class AzureBlobDriver(Driver):
    container_name = attr.ib(default=None)

    def to_json(self):
        _json = super().to_json()
        _json['containerName'] = self.container_name
        return _json

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Driver entity object from a json

        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Driver object
        """
        inst = super().from_json(_json, client_api, is_fetched=True)
        inst.container_name = _json.get('containerName', None)
        return inst


@attr.s()
class GcsDriver(Driver):
    bucket = attr.ib(default=None)

    def to_json(self):
        _json = super().to_json()
        _json['bucket'] = self.bucket
        return _json

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Driver entity object from a json

        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Driver object
        """
        inst = super().from_json(_json, client_api, is_fetched=True)
        inst.bucket = _json.get('bucket', None)
        return inst


@attr.s()
class S3Driver(Driver):
    bucket_name = attr.ib(default=None)
    region = attr.ib(default=None)
    storage_class = attr.ib(default=None)

    def to_json(self):
        _json = super().to_json()
        _json['bucketName'] = self.bucket_name
        if self.region is not None:
            _json['region'] = self.region
        if self.storage_class is not None:
            _json['storageClass'] = self.storage_class
        return _json

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Driver entity object from a json

        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Driver object
        """
        inst = super().from_json(_json, client_api, is_fetched=True)
        inst.bucket_name = _json.get('bucketName', None)
        inst.region = _json.get('region', None)
        inst.storage_class = _json.get('storageClass', None)
        return inst


================================================
File: dtlpy/entities/execution.py
================================================
import attr
import logging
import traceback
from enum import Enum
from collections import namedtuple

from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class ExecutionStatus(str, Enum):
    SUCCESS = "success"
    FAILED = "failed"
    IN_PROGRESS = "in-progress"
    CREATED = "created"
    TERMINATED = 'terminated',
    ABORTED = 'aborted'
    CANCELED = 'canceled'
    SYSTEM_FAILURE = 'system-failure'


@attr.s
class Execution(entities.BaseEntity):
    """
    Service execution entity
    """
    # platform
    id = attr.ib()
    url = attr.ib(repr=False)
    creator = attr.ib()
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    input = attr.ib()
    output = attr.ib(repr=False)
    feedback_queue = attr.ib(repr=False)
    _status = attr.ib(repr=False)
    status_log = attr.ib(repr=False)
    sync_reply_to = attr.ib(repr=False)
    latest_status = attr.ib()
    function_name = attr.ib()
    duration = attr.ib()
    attempts = attr.ib()
    max_attempts = attr.ib()
    to_terminate = attr.ib(type=bool)

    # name changed
    trigger_id = attr.ib()
    service_id = attr.ib()
    project_id = attr.ib()
    service_version = attr.ib()
    package_id = attr.ib()
    package_name = attr.ib()
    package_revision = attr.ib()

    # sdk
    _client_api = attr.ib(type=ApiClient, repr=False)
    _service = attr.ib(repr=False)
    _project = attr.ib(repr=False, default=None)
    _repositories = attr.ib(repr=False)

    # optional
    pipeline = attr.ib(type=dict, default=None, repr=False)
    model = attr.ib(type=dict, default=None, repr=False)
    app = attr.ib(default=None)
    driver_id = attr.ib(default=None)

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['executions', 'services'])

        if self._project is not None:
            services_repo = self._project.services
            executions_repo = self._project.executions
        elif self._service is not None:
            services_repo = self._service.services
            executions_repo = self._service.executions
        else:
            services_repo = repositories.Services(client_api=self._client_api,
                                                  project=self._project,
                                                  package=None)
            executions_repo = repositories.Executions(client_api=self._client_api,
                                                      project=self._project,
                                                      service=self._service)

        r = reps(executions=executions_repo,
                 services=services_repo)
        return r

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @property
    def syncReplyTo(self):
        return self.sync_reply_to

    @property
    def feedbackQueue(self):
        return self.feedback_queue

    @property
    def status(self):
        return self._status

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    @property
    def executions(self):
        assert isinstance(self._repositories.executions, repositories.Executions)
        return self._repositories.executions

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, service=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error
        :param _json: platform json
        :param client_api: ApiClient entity
        :return:
        """
        try:
            execution = Execution.from_json(_json=_json,
                                            client_api=client_api,
                                            project=None,
                                            service=service,
                                            is_fetched=is_fetched)
            status = True
        except Exception:
            execution = traceback.format_exc()
            status = False
        return status, execution

    @classmethod
    def from_json(cls, _json, client_api, project=None, service=None, is_fetched=True):
        """
        :param dict _json: platform json
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: project entity
        :param dtlpy.entities.service.Service service:
        :param is_fetched: is Entity fetched from Platform
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Execution has been fetched from a project that is not belong to it')
                project = None

        if service is not None:
            if service.id != _json.get('serviceId', None):
                logger.warning('Execution has been fetched from a service that is not belong to it')
                service = None

        inst = cls(
            feedback_queue=_json.get('feedbackQueue', None),
            service_id=_json.get('serviceId', None),
            project_id=_json.get('projectId', None),
            latest_status=_json.get('latestStatus', None),
            sync_reply_to=_json.get('syncReplyTo', None),
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            creator=_json.get('creator', None),
            trigger_id=_json.get('triggerId', None),
            attempts=_json.get('attempts', None),
            max_attempts=_json.get('maxAttempts', None),
            output=_json.get('output', None),
            status=_json.get('status', None),
            status_log=_json.get('statusLog', None),
            duration=_json.get('duration', None),
            function_name=_json.get('functionName', entities.package_defaults.DEFAULT_PACKAGE_FUNCTION_NAME),
            input=_json.get('input', None),
            url=_json.get('url', None),
            id=_json.get('id', None),
            to_terminate=_json.get('toTerminate', False),
            client_api=client_api,
            project=project,
            service=service,
            service_version=_json.get('serviceVersion', False),
            package_id=_json.get('packageId', None),
            package_name=_json.get('packageName', None),
            pipeline=_json.get('pipeline', None),
            model=_json.get('model', None),
            package_revision=_json.get('packageRevision', None),
            app=_json.get('app', None),
            driver_id=_json.get('driverId', None)
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        # get excluded
        _json = attr.asdict(
            self, filter=attr.filters.exclude(
                attr.fields(Execution)._client_api,
                attr.fields(Execution)._service,
                attr.fields(Execution)._project,
                attr.fields(Execution).to_terminate,
                attr.fields(Execution)._repositories,
                attr.fields(Execution).project_id,
                attr.fields(Execution).service_id,
                attr.fields(Execution).trigger_id,
                attr.fields(Execution).function_name,
                attr.fields(Execution).max_attempts,
                attr.fields(Execution).latest_status,
                attr.fields(Execution).service_version,
                attr.fields(Execution).package_id,
                attr.fields(Execution).package_name,
                attr.fields(Execution).status_log,
                attr.fields(Execution)._status,
                attr.fields(Execution).created_at,
                attr.fields(Execution).updated_at,
                attr.fields(Execution).feedback_queue,
                attr.fields(Execution).sync_reply_to,
                attr.fields(Execution).pipeline,
                attr.fields(Execution).model,
                attr.fields(Execution).package_revision,
                attr.fields(Execution).driver_id,
            )
        )

        # rename
        _json['projectId'] = self.project_id
        _json['triggerId'] = self.trigger_id
        _json['serviceId'] = self.service_id
        _json['functionName'] = self.function_name
        _json['latestStatus'] = self.latest_status
        _json['maxAttempts'] = self.max_attempts
        _json['toTerminate'] = self.to_terminate
        _json['serviceVersion'] = self.service_version
        _json['packageId'] = self.package_id
        _json['packageName'] = self.package_name
        _json['statusLog'] = self.status_log
        _json['status'] = self._status
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        _json['feedbackQueue'] = self.feedback_queue
        _json['syncReplyTo '] = self.sync_reply_to
        _json['packageRevision'] = self.package_revision
        _json['driverId'] = self.driver_id

        if self.pipeline:
            _json['pipeline'] = self.pipeline
        if self.model:
            _json['model'] = self.model

        return _json

    @property
    def pipeline_id(self) -> str:
        pipeline_id = None
        if self.pipeline:
            pipeline_id = self.pipeline.get('id', None)
        return pipeline_id

    @property
    def node_id(self) -> str:
        node_id = None
        if self.pipeline:
            node_id = self.pipeline.get('nodeId', None)
        return node_id

    @property
    def pipeline_execution_id(self) -> str:
        pipeline_execution_id = None
        if self.pipeline:
            pipeline_execution_id = self.pipeline.get('executionId', None)
        return pipeline_execution_id

    @property
    def service(self):
        if self._service is None:
            self._service = self.services.get(service_id=self.service_id, fetch=None)
        assert isinstance(self._service, entities.Service)
        return self._service

    @property
    def project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id,
                                                                                   fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    def get_latest_status(self):
        self.latest_status = self.executions.get(execution_id=self.id).latest_status
        return self.latest_status

    @service.setter
    def service(self, service):
        if not isinstance(service, entities.Service):
            raise ValueError('Must input a valid service entity')
        self._service = service

    def progress_update(
            self,
            status: ExecutionStatus = None,
            percent_complete: int = None,
            message: str = None,
            output: str = None,
            service_version: str = None
    ):
        """
        Update Execution Progress

        :param str status: ExecutionStatus
        :param int percent_complete: percent complete
        :param str message: message to update the progress state
        :param str output: output
        :param str service_version: service version
        :return: Service execution object
        """
        return self.executions.progress_update(
            execution_id=self.id,
            status=status,
            percent_complete=percent_complete,
            message=message,
            output=output,
            service_version=service_version
        )

    def update(self):
        """
        Update execution changes to platform

        :return: execution entity
        """
        return self.executions.update(execution=self)

    def logs(self, follow=False, log_level='DEBUG'):
        """
        Print logs for execution

        :param follow: keep stream future logs
        :param str log_level: the log level to display
        """
        self.services.log(execution_id=self.id,
                          view=True,
                          service=self.service,
                          follow=follow,
                          start=self.created_at,
                          log_level=log_level)

    def increment(self):
        """
        Increment attempts

        :return:
        """
        self.attempts = self.executions.increment(execution=self)

    def rerun(self, sync: bool = False):
        """
        Re-run

        :return: Execution object
        """
        return self.executions.rerun(execution=self, sync=sync)

    def terminate(self):
        """
        Terminate execution

        :return: execution object
        """
        return self.executions.terminate(execution=self)

    def wait(self):
        """
        Wait for execution

        :return: Service execution object
        """
        return self.executions.wait(execution_id=self.id)

    def in_progress(self):
        return self.latest_status['status'] not in [ExecutionStatus.FAILED,
                                                    ExecutionStatus.SUCCESS,
                                                    ExecutionStatus.TERMINATED,
                                                    ExecutionStatus.ABORTED,
                                                    ExecutionStatus.CANCELED,
                                                    ExecutionStatus.SYSTEM_FAILURE]


================================================
File: dtlpy/entities/feature.py
================================================
import attr
import traceback
from collections import namedtuple
from enum import Enum

from .. import repositories, entities
from ..services.api_client import ApiClient


@attr.s
class Feature(entities.BaseEntity):
    """
    Com entity
    """
    # platform
    id = attr.ib()
    entity_id = attr.ib()
    url = attr.ib(repr=False)
    created_at = attr.ib()
    feature_set_id = attr.ib()
    version = attr.ib()
    value = attr.ib()
    parent_id = attr.ib()
    project_id = attr.ib()
    org_id = attr.ib()
    creator = attr.ib()

    # sdk
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['features', 'features_sets'])
        features_repo = repositories.Features(client_api=self._client_api)
        r = reps(features=features_repo,
                 features_sets=repositories.FeatureSets(client_api=self._client_api))
        return r

    @property
    def features(self):
        assert isinstance(self._repositories.features, repositories.Features)
        return self._repositories.features

    @staticmethod
    def _protected_from_json(_json, client_api, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error
        :param _json:
        :param client_api:
        :return:
        """
        try:
            feature = Feature.from_json(_json=_json,
                                        client_api=client_api,
                                        is_fetched=is_fetched)
            status = True
        except Exception:
            feature = traceback.format_exc()
            status = False
        return status, feature

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Feature entity object from a json

        :param is_fetched: is Entity fetched from Platform
        :param _json: _json response from host
        :param client_api: client_api
        :return: Feature object
        """
        inst = cls(
            id=_json.get('id', None),
            feature_set_id=_json.get('featureSetId', None),
            entity_id=_json.get('entityId', None),
            url=_json.get('url', None),
            project_id=_json.get('project', None),
            created_at=_json.get('createdAt', None),
            version=_json.get('version', None),
            value=_json.get('value', None),
            parent_id=_json.get('parentId', None),
            client_api=client_api,
            org_id=_json.get('org', None),
            creator=_json.get('creator', None),
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """

        _json = {'createdAt': self.created_at,
                 'entityId': self.entity_id,
                 'id': self.id,
                 'featureSetId': self.feature_set_id,
                 'url': self.url,
                 'project': self.project_id,
                 'creator': self.creator,
                 'version': self.version,
                 'value': self.value,
                 }
        if self.parent_id is not None:
            _json['parentId'] = self.parent_id
        if self.org_id is not None:
            _json['org'] = self.org_id
        return _json

    def delete(self):
        """
        Delete Feature Vector object

        :return: True
        """
        return self.features.delete(feature_id=self.id)


================================================
File: dtlpy/entities/feature_set.py
================================================
from enum import Enum

import attr
import traceback
from collections import namedtuple

from .. import repositories, entities
from ..services.api_client import ApiClient


class FeatureEntityType(str, Enum):
    """Available types for Feature Set entities"""
    ITEM = 'item'
    ANNOTATION = 'annotation'
    DATASET = 'dataset'


@attr.s
class FeatureSet(entities.BaseEntity):
    """
    Com entity
    """
    # platform
    id = attr.ib()
    name = attr.ib()
    url = attr.ib(repr=False)
    creator = attr.ib(repr=False)
    created_at = attr.ib()
    updated_by = attr.ib()
    size = attr.ib()
    set_type = attr.ib()
    entity_type = attr.ib()
    project_id = attr.ib()
    model_id = attr.ib()
    org_id = attr.ib()

    # sdk
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['feature_sets', 'features'])
        feature_sets_repo = repositories.FeatureSets(client_api=self._client_api,
                                                     project_id=self.project_id)
        features_repo = repositories.Features(client_api=self._client_api,
                                              project_id=self.project_id,
                                              feature_set=self, )
        r = reps(feature_sets=feature_sets_repo,
                 features=features_repo)
        return r

    @property
    def feature_sets(self):
        assert isinstance(self._repositories.feature_sets, repositories.FeatureSets)
        return self._repositories.feature_sets

    @property
    def features(self):
        assert isinstance(self._repositories.features, repositories.Features)
        return self._repositories.features

    @staticmethod
    def _protected_from_json(_json, client_api, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json: entity's object json
        :param client_api:
        :return:
        """
        try:
            feature_set = FeatureSet.from_json(_json=_json,
                                               client_api=client_api,
                                               is_fetched=is_fetched)
            status = True
        except Exception:
            feature_set = traceback.format_exc()
            status = False
        return status, feature_set

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Feature Set entity object from a json

        :param is_fetched: is Entity fetched from Platform
        :param _json: entity's object json
        :param client_api: client_api
        :return: Feature object
        """
        inst = cls(
            id=_json.get('id', None),
            name=_json.get('name', None),
            set_type=_json.get('type', None),
            entity_type=_json.get('entityType', None),
            size=_json.get('size', None),
            url=_json.get('url', None),
            project_id=_json.get('project', None),
            model_id=_json.get('modelId', None),
            created_at=_json.get('createdAt', None),
            creator=_json.get('creator', None),
            updated_by=_json.get('updatedBy', None),
            client_api=client_api,
            org_id=_json.get('org', None),
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """

        _json = {'id': self.id,
                 'type': self.set_type,
                 'entityType': self.entity_type,
                 'project': self.project_id,
                 'modelId': self.model_id,
                 'creator': self.creator,
                 'createdAt': self.created_at,
                 'updatedBy': self.updated_by,
                 'name': self.name,
                 'size': self.size,
                 'url': self.url}
        if self.org_id is not None:
            _json['org'] = self.org_id

        return _json

    def delete(self):
        """
        Delete the feature set

        :return: success
        :rtype: bool
        """
        return self.feature_sets.delete(feature_set_id=self.id)


================================================
File: dtlpy/entities/filters.py
================================================
import urllib.parse
import logging
import json
import os
import io
from enum import Enum

from .. import exceptions, entities

logger = logging.getLogger(name='dtlpy')


class FiltersKnownFields(str, Enum):
    DIR = "dir"
    ANNOTATED = "annotated"
    FILENAME = "filename"
    CREATED_AT = "createdAt"
    UPDATED_AT = "updatedAt"
    LABEL = "label"
    NAME = "name"
    HIDDEN = "hidden"
    TYPE = 'type'


class FiltersResource(str, Enum):
    ITEM = "items"
    ANNOTATION = "annotations"
    EXECUTION = "executions"
    PACKAGE = "packages"
    DPK = "dpks"
    APP = "apps"
    SERVICE = "services"
    TRIGGER = "triggers"
    MODEL = "models"
    WEBHOOK = "webhooks"
    RECIPE = 'recipe'
    DATASET = 'datasets'
    ONTOLOGY = 'ontology'
    TASK = 'tasks'
    PIPELINE = 'pipeline'
    PIPELINE_EXECUTION = 'pipelineState'
    COMPOSITION = 'composition'
    FEATURE = 'feature_vectors'
    FEATURE_SET = 'feature_sets'
    ORGANIZATIONS = 'organizations'
    DRIVERS = 'drivers'
    SETTINGS = 'setting'
    RESOURCE_EXECUTION = 'resourceExecution'
    METRICS = 'metrics'


class FiltersOperations(str, Enum):
    OR = "or"
    AND = "and"
    IN = "in"
    NOT_EQUAL = "ne"
    EQUAL = "eq"
    GREATER_THAN = "gt"
    LESS_THAN = "lt"
    EXISTS = "exists"
    MATCH = "match"
    NIN = 'nin'


class FiltersMethod(str, Enum):
    OR = "or"
    AND = "and"


class FiltersOrderByDirection(str, Enum):
    DESCENDING = "descending"
    ASCENDING = "ascending"


class Filters:
    """
    Filters entity to filter items from pages in platform
    """

    def __init__(
            self,
            field=None,
            values=None,
            operator: FiltersOperations = None,
            method: FiltersMethod = None,
            custom_filter=None,
            resource: FiltersResource = FiltersResource.ITEM,
            use_defaults=True,
            context=None,
            page_size=None,
    ):
        if page_size is None:
            if resource in [FiltersResource.EXECUTION,
                            FiltersResource.PIPELINE_EXECUTION,
                            FiltersResource.DPK]:
                page_size = 100
            else:
                page_size = 1000

        self.or_filter_list = list()
        self.and_filter_list = list()
        self._unique_fields = list()
        self.custom_filter = custom_filter
        self.known_operators = ['or', 'and', 'in', 'ne', 'eq', 'gt', 'lt', 'exists']
        self._resource = resource
        self.page = 0
        self.page_size = page_size
        self.method = FiltersMethod.AND
        self.sort = dict()
        self.join = None
        self.recursive = True

        # system only - task and assignment attributes
        self._user_query = 'true'
        self._ref_task = False
        self._ref_assignment = False
        self._ref_op = None
        self._ref_assignment_id = None
        self._ref_task_id = None
        self._system_space = None

        self._use_defaults = use_defaults
        self.__add_defaults()
        self.context = context

        if field is not None:
            self.add(field=field, values=values, operator=operator, method=method)

    def __validate_page_size(self):
        max_page_size = self.__max_page_size
        if self.page_size > max_page_size:
            logger.warning('Cannot list {} with page size greater than {}. Changing page_size to {}.'.format(
                self.resource, max_page_size, max_page_size
            ))
            self.page_size = max_page_size

    @property
    def __max_page_size(self):
        page_size = 1000
        if self.resource in [FiltersResource.EXECUTION, FiltersResource.PIPELINE_EXECUTION]:
            page_size = 100
        return page_size

    @property
    def resource(self):
        return f'{self._resource.value}' if isinstance(self._resource, FiltersResource) else f'{self._resource}'

    @resource.setter
    def resource(self, resource):
        self._resource = resource
        self.reset()
        self.__add_defaults()

    @property
    def system_space(self):
        return self._system_space

    @system_space.setter
    def system_space(self, val: bool):
        self._system_space = val

    def reset(self):
        self.or_filter_list = list()
        self.and_filter_list = list()
        self._unique_fields = list()
        self.custom_filter = None
        self.page = 0
        self.page_size = 1000
        self.method = FiltersMethod.AND
        self.sort = dict()
        self.join = None
        self.recursive = True
        self._nullify_refs()

    def _nullify_refs(self):
        self._ref_task = False
        self._ref_assignment = False
        self._ref_op = None
        self._ref_assignment_id = None
        self._ref_task_id = None

    def add(self, field, values, operator: FiltersOperations = None, method: FiltersMethod = None):
        """
        Add filter

        :param str field: Metadata field / attribute
        :param values: field values
        :param dl.FiltersOperations operator: optional - in, gt, lt, eq, ne
        :param dl.FiltersMethod method: Optional - or/and

        **Example**:

        .. code-block:: python

            filter.add(field='metadata.user', values=['1','2'], operator=dl.FiltersOperations.IN)
        """
        if method is None:
            method = self.method
        if 'metadata.system.refs.metadata' in field and self.resource == FiltersResource.ITEM:
            logger.warning('Filtering by metadata.system.refs.metadata may cause incorrect results. please use match operator')

        # create SingleFilter object and add to self.filter_list
        if method == FiltersMethod.OR:
            self.or_filter_list.append(SingleFilter(field=field, values=values, operator=operator))
        elif method == FiltersMethod.AND:
            self.__override(field=field, values=values, operator=operator)
        else:
            raise exceptions.PlatformException(error='400',
                                               message='Unknown method {}, please select from: or/and'.format(method))

    def __override(self, field, values, operator=None):
        if field in self._unique_fields:
            for i_single_filter, single_filter in enumerate(self.and_filter_list):
                if single_filter.field == field:
                    self.and_filter_list.pop(i_single_filter)
        self.and_filter_list.append(
            SingleFilter(field=field, values=values, operator=operator)
        )

    def generate_url_query_params(self, url):
        """
        generate url query params

        :param str url:
        """
        url = '{}?'.format(url)
        for f in self.and_filter_list:
            if isinstance(f.values, list):
                url = '{}{}={}&'.format(url, f.field, ','.join(f.values))
            else:
                url = '{}{}={}&'.format(url, f.field, f.values)
        return '{}&pageOffset={}&pageSize={}'.format(url, self.page, self.page_size)

    def has_field(self, field):
        """
        is filter has field

        :param str field: field to check
        :return: Ture is have it
        :rtype: bool
        """
        for single_filter in self.or_filter_list:
            if single_filter.field == field:
                return True

        for single_filter in self.and_filter_list:
            if single_filter.field == field:
                return True

        return False

    def pop(self, field):
        """
        Pop filed

        :param str field: field to pop
        """
        for single_filter in self.or_filter_list:
            if single_filter.field == field:
                self.or_filter_list.remove(single_filter)

        for single_filter in self.and_filter_list:
            if single_filter.field == field:
                self.and_filter_list.remove(single_filter)

    def pop_join(self, field):
        """
        Pop join

        :param str field: field to pop
        """
        if self.join is not None:
            for single_filter in self.join['filter']['$and']:
                if field in single_filter:
                    self.join['filter']['$and'].remove(single_filter)

    def add_join(self, field,
                 values,
                 operator: FiltersOperations = None,
                 method: FiltersMethod = FiltersMethod.AND
                 ):
        """
        join a query to the filter

        :param str field: Metadata field / attribute
        :param str or list values: field values
        :param dl.FiltersOperations operator: optional - in, gt, lt, eq, ne
        :param method: optional - str - FiltersMethod.AND, FiltersMethod.OR

        **Example**:

        .. code-block:: python

            filter.add_join(field='metadata.user', values=['1','2'], operator=dl.FiltersOperations.IN)
        """
        if self.resource not in [FiltersResource.ITEM, FiltersResource.ANNOTATION]:
            raise exceptions.PlatformException(error='400',
                                               message='Cannot join to {} filters'.format(self.resource))

        if self.join is None:
            self.join = dict()
        if 'on' not in self.join:
            if self.resource == FiltersResource.ITEM:
                self.join['on'] = {'resource': FiltersResource.ANNOTATION.value, 'local': 'itemId', 'forigen': 'id'}
            else:
                self.join['on'] = {'resource': FiltersResource.ITEM.value, 'local': 'id', 'forigen': 'itemId'}
        if 'filter' not in self.join:
            self.join['filter'] = dict()
        join_method = '$' + method
        if join_method not in self.join['filter']:
            self.join['filter'][join_method] = list()
        self.join['filter'][join_method].append(SingleFilter(field=field, values=values, operator=operator).prepare())

    def __add_defaults(self):
        if self._use_defaults:
            # add items defaults
            if self.resource == FiltersResource.ITEM:
                self._unique_fields = ['type', 'hidden']
                self.add(field='hidden', values=False, method=FiltersMethod.AND)
                self.add(field='type', values='file', method=FiltersMethod.AND)
            # add service defaults
            elif self.resource == FiltersResource.SERVICE:
                self._unique_fields = ['global']
                self.add(field='global', values=True, operator=FiltersOperations.NOT_EQUAL, method=FiltersMethod.AND)
            elif self.resource == FiltersResource.PACKAGE:
                self._unique_fields = ['global']
                self.add(field='global', values=True, operator=FiltersOperations.NOT_EQUAL, method=FiltersMethod.AND)
            # add annotations defaults
            elif self.resource == FiltersResource.ANNOTATION:
                self._unique_fields = ['type']
                self.add(field='type',
                         values=['box', 'class', 'comparison', 'ellipse', 'point', 'segment', 'polyline', 'binary',
                                 'subtitle', 'cube', 'cube_3d', 'pose', 'text_mark', 'text', 'ref_image', 'gis'],
                         operator=FiltersOperations.IN,
                         method=FiltersMethod.AND)

    def __generate_query(self):
        filters_dict = dict()

        if len(self.or_filter_list) > 0:
            or_filters = list()
            for single_filter in self.or_filter_list:
                or_filters.append(
                    single_filter.prepare(recursive=self.recursive and self.resource == FiltersResource.ITEM))
            filters_dict['$or'] = or_filters

        if len(self.and_filter_list) > 0:
            and_filters = list()
            for single_filter in self.and_filter_list:
                and_filters.append(
                    single_filter.prepare(recursive=self.recursive and self.resource == FiltersResource.ITEM))
            filters_dict['$and'] = and_filters

        return filters_dict

    def __generate_custom_query(self):
        filters_dict = dict()
        if 'filter' in self.custom_filter or 'join' in self.custom_filter:
            if 'filter' in self.custom_filter:
                filters_dict = self.custom_filter['filter']
            self.join = self.custom_filter.get('join', self.join)
        else:
            filters_dict = self.custom_filter
        return filters_dict

    def __generate_ref_query(self):
        refs = list()
        if self._ref_task:
            task_refs = list()
            if not isinstance(self._ref_task_id, list):
                self._ref_task_id = [self._ref_task_id]

            for ref_id in self._ref_task_id:
                task_refs.append({'type': 'task', 'id': ref_id})

            refs += task_refs

        if self._ref_assignment:
            assignment_refs = list()
            if not isinstance(self._ref_assignment_id, list):
                self._ref_assignment_id = [self._ref_assignment_id]

            for ref_id in self._ref_assignment_id:
                assignment_refs.append({'type': 'assignment', 'id': ref_id})

            refs += assignment_refs

        return refs

    def prepare(self, operation=None, update=None, query_only=False, system_update=None, system_metadata=False):
        """
        To dictionary for platform call

        :param str operation: operation
        :param update: update
        :param bool query_only: query only
        :param system_update: system update
        :param system_metadata: True, if you want to change metadata system
        :return: dict of the filter
        :rtype: dict
        """
        ########
        # json #
        ########
        _json = dict()

        if self.custom_filter is None:
            _json['filter'] = self.__generate_query()
        else:
            _json['filter'] = self.__generate_custom_query()

        ##################
        # filter options #
        ##################
        if not query_only:
            if len(self.sort) > 0:
                _json['sort'] = self.sort

            self.__validate_page_size()

            _json['page'] = self.page
            _json['pageSize'] = self.page_size
            _json['resource'] = self.resource

        ########
        # join #
        ########
        if self.join is not None:
            _json['join'] = self.join

        #####################
        # operation or refs #
        #####################
        if self._ref_assignment or self._ref_task:
            _json['references'] = {
                'operation': self._ref_op,
                'refs': self.__generate_ref_query()
            }
        elif operation is not None:
            if operation == 'update':
                if update:
                    _json[operation] = {'metadata': {'user': update}}
                else:
                    _json[operation] = dict()
                if system_metadata and system_update:
                    _json['systemSpace'] = True
                    _json[operation]['metadata'] = _json[operation].get('metadata', dict())
                    _json[operation]['metadata']['system'] = system_update
            elif operation == 'delete':
                _json[operation] = True
                _json.pop('sort', None)
                if self.resource == FiltersResource.ITEM:
                    _json.pop('page', None)
                    _json.pop('pageSize', None)
            else:
                raise exceptions.PlatformException(error='400',
                                                   message='Unknown operation: {}'.format(operation))

        if self.context is not None:
            _json['context'] = self.context
        if self._system_space is not None:
            _json['systemSpace'] = self._system_space
        return _json

    def print(self, indent=2):
        print(json.dumps(self.prepare(), indent=indent))

    def sort_by(self, field, value: FiltersOrderByDirection = FiltersOrderByDirection.ASCENDING):
        """
        sort the filter

        :param str field: field to sort by it
        :param dl.FiltersOrderByDirection value: FiltersOrderByDirection.ASCENDING, FiltersOrderByDirection.DESCENDING

        **Example**:

        .. code-block:: python

            filter.sort_by(field='metadata.user', values=dl.FiltersOrderByDirection.ASCENDING)
        """
        if value not in [FiltersOrderByDirection.ASCENDING, FiltersOrderByDirection.DESCENDING]:
            raise exceptions.PlatformException(error='400', message='Sort can be by ascending or descending order only')
        self.sort[field] = value.value if isinstance(value, FiltersOrderByDirection) else value

    def platform_url(self, resource) -> str:
        """
        Build a url with filters param to open in web browser

        :param str resource: dl entity to apply filter on. currently only supports dl.Dataset
        :return: url string
        :rtype: str
        """
        _json = self.prepare()
        # add the view option
        _json['view'] = 'icons'
        # convert from enum to string
        _json["resource"] = f'{_json["resource"]}'
        # convert the dictionary to a json string
        _json['dqlFilter'] = json.dumps({'filter': _json.pop('filter'),
                                         'join': _json.pop('join', None),
                                         'sort': _json.get('sort', None)})
        # set the page size as the UI default
        _json['pageSize'] = 100
        _json['page'] = _json['page']
        # build the url for the dataset data browser
        if isinstance(resource, entities.Dataset):
            url = resource.platform_url + f'?{urllib.parse.urlencode(_json)}'
        else:
            raise NotImplementedError('Not implemented for resource type: {}'.format(type(resource)))
        return url

    def open_in_web(self, resource):
        """
        Open the filter in the platform data browser (in a new web browser)

        :param str resource: dl entity to apply filter on. currently only supports dl.Dataset
        """
        if isinstance(resource, entities.Dataset):
            resource._client_api._open_in_web(url=self.platform_url(resource=resource))
        else:
            raise NotImplementedError('Not implemented for resource type: {}'.format(type(resource)))

    def save(self, project: entities.Project, filter_name: str):
        """
        Save the current DQL filter to the project

        :param project: dl.Project
        :param filter_name: the saved filter's name
        :return: True if success
        """
        _json_filter = self.prepare()
        shebang_dict = {"type": "dql",
                        "shebang": "dataloop",
                        "metadata": {
                            "version": "1.0.0",
                            "system": {
                                "mimetype": "dql"
                            },
                            "dltype": "filter",
                            "filterFieldsState": [],
                            "resource": "items",
                            "filter": _json_filter.pop('filter'),
                            "join": _json_filter.pop('join')
                        }
                        }
        b_dataset = project.datasets._get_binaries_dataset()
        byte_io = io.BytesIO()
        byte_io.name = filter_name
        byte_io.write(json.dumps(shebang_dict).encode())
        byte_io.seek(0)
        b_dataset.items.upload(local_path=byte_io,
                               remote_path='/.dataloop/dqlfilters/items',
                               remote_name=filter_name)
        return True

    @classmethod
    def load(cls, project: entities.Project, filter_name: str) -> 'Filters':
        """
        Load a saved filter from the project by name

        :param project: dl.Project entity
        :param filter_name: filter name
        :return: dl.Filters
        """
        b_dataset = project.datasets._get_binaries_dataset()
        f = entities.Filters(custom_filter={
            'filter': {'$and': [{'filename': f'/.dataloop/dqlfilters/items/{filter_name}'}]},
            'page': 0,
            'pageSize': 1000,
            'resource': 'items'
        })
        pages = b_dataset.items.list(filters=f)
        if pages.items_count == 0:
            raise exceptions.NotFound(
                f'Saved filter not found: {filter_name}. Run `Filters.list()` to list existing filters')
        with open(pages.items[0].download()) as f:
            data = json.load(f)
            custom_filter = data['metadata']['filter']
            custom_filter['join'] = data['metadata']['join']
        return cls(custom_filter=custom_filter)

    @staticmethod
    def list(project: entities.Project) -> list:
        """
        List all saved filters for a project
        :param project: dl.Project entity
        :return: a list of all the saved filters' names
        """
        b_dataset = project.datasets._get_binaries_dataset()
        f = entities.Filters(use_defaults=False,
                             field='dir',
                             values='/.dataloop/dqlfilters/items')
        pages = b_dataset.items.list(filters=f)
        all_filter_items = list(pages.all())
        names = [i.name for i in all_filter_items]
        return names


class SingleFilter:
    def __init__(self, field, values, operator: FiltersOperations = None):
        self.field = field
        self.values = values
        self.operator = operator

    @staticmethod
    def __add_recursive(value):
        if not value.endswith('*') and not os.path.splitext(value)[-1].startswith('.'):
            if value.endswith('/'):
                value = value + '**'
            else:
                value = value + '/**'
        return value

    def prepare(self, recursive=False):
        """
        To dictionary for platform call

        :param recursive:recursive
        """
        _json = dict()
        values = self.values

        if recursive and self.field == 'filename':
            if isinstance(values, str):
                values = self.__add_recursive(value=values)
            elif isinstance(values, list):
                for i_value, value in enumerate(values):
                    values[i_value] = self.__add_recursive(value=value)

        if self.operator is None:
            _json[self.field] = values
        else:
            value = dict()
            op = self.operator.value if isinstance(self.operator, FiltersOperations) else self.operator
            value['${}'.format(op)] = values
            _json[self.field] = value

        return _json

    def print(self, indent=2):
        print(json.dumps(self.prepare(), indent=indent))


================================================
File: dtlpy/entities/gis_item.py
================================================
import json
from typing import List
import logging
import os

logger = logging.getLogger(name='dtlpy')


class Layer:
    def __init__(self, name, layer_type, url):
        self.name = name
        self.type = layer_type
        self.url = url


class ItemGis:
    def __init__(self,
                 name: str,
                 data: dict = None,
                 layer: Layer = None,
                 optional_layers: List[Layer] = None,
                 zoom: int = None,
                 min_zoom: int = None,
                 max_zoom: int = None,
                 epsg: str = None,
                 bounds: list = None,
                 aoi: list = None):
        self.name = name

        self.layer = layer or Layer(name=data.get('name', None), layer_type=data.get('type', None),
                                    url=data.get('url', None))
        if self.layer is None:
            raise ValueError('layer is required')
        elif self.layer is not None and isinstance(self.layer, dict):
            self.layer = Layer(name=self.layer.get('name', None), layer_type=self.layer.get('type', None), url=self.layer.get('url', None))


        self.optional_layers = optional_layers or [
            Layer(name=layer.get('name', None), layer_type=layer.get('type', None), url=layer.get('url', None)) for
            layer in data.get('optionalLayers', [])]

        if self.optional_layers is not None and isinstance(optional_layers, list):
            new_optional_layers = []
            for op_layer in self.optional_layers:
                if isinstance(op_layer, dict):
                    new_optional_layers.append(Layer(name=op_layer.get('name', None), layer_type=op_layer.get('type', None), url=op_layer.get('url', None)))
                else:
                    new_optional_layers.append(op_layer)
            self.optional_layers = new_optional_layers

        self.epsg = epsg or data.get('epsg', None)
        if self.epsg is None:
            raise ValueError('epsg is required')

        self.zoom = zoom or data.get('zoom', None)
        self.min_zoom = min_zoom or data.get('minZoom', None)
        self.max_zoom = max_zoom or data.get('maxZoom', None)
        self.bounds = bounds or data.get('bounds', None)
        self.aoi = aoi or data.get('aoi', None)

    def to_json(self):
        _json = {
            "type": "gis",
            "shebang": "dataloop",
            "metadata": {
                "dltype": "gis"
            },
            'layer': {
                'name': self.layer.name,
                'type': self.layer.type,
                'url': self.layer.url
            },
            "epsg": self.epsg
        }
        if self.optional_layers is not None:
            _json['optionalLayers'] = [
                {
                    'name': layer.name,
                    'type': layer.type,
                    'url': layer.url
                } for layer in self.optional_layers
            ]
        if self.zoom is not None:
            _json['zoom'] = self.zoom
        if self.min_zoom is not None:
            _json['minZoom'] = self.min_zoom
        if self.max_zoom is not None:
            _json['maxZoom'] = self.max_zoom
        if self.bounds is not None:
            _json['bounds'] = self.bounds
        if self.aoi is not None:
            _json['aoi'] = self.aoi
        return _json

    @classmethod
    def from_local_file(cls, filepath):
        """
        Create a new prompt item from a file
        :param filepath: path to the file
        :return: PromptItem object
        """
        if os.path.exists(filepath) is False:
            raise FileNotFoundError(f'File does not exists: {filepath}')
        if 'json' not in os.path.splitext(filepath)[-1]:
            raise ValueError(f'Expected path to json item, got {os.path.splitext(filepath)[-1]}')
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls(name=os.path.basename(filepath), data=data)

================================================
File: dtlpy/entities/integration.py
================================================
from enum import Enum
import logging
import attr

from .. import entities, exceptions, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class IntegrationType(str, Enum):
    """ The type of the Integration.

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - S3
         - S3 Integration - for S3 drivers
       * - AWS_CROSS_ACCOUNT
         - AWS CROSS ACCOUNT Integration - for S3 drivers
       * - AWS_STS
         - AWS STS Integration - for S3 drivers
       * - GCS
         - GCS Integration - for GCS drivers
       * - GCP_CROSS_PROJECT
         - GCP CROSS PROJECT Integration - for GCP drivers
       * - AZUREBLOB
         - AZURE BLOB Integration - for S3 AZUREBLOB and AZURE_DATALAKE_GEN2 drivers
       * - KEY_VALUE
         - KEY VALUE Integration - for save secrets in the platform
       * - GCP_WORKLOAD_IDENTITY_FEDERATION
         - GCP Workload Identity Federation Integration - for GCP drivers
       * - PRIVATE_REGISTRY
         - PRIVATE REGISTRY Integration - for private registry drivers
    """
    S3 = "s3"
    AWS_CROSS_ACCOUNT = 'aws-cross'
    AWS_STS = 'aws-sts'
    GCS = "gcs"
    GCS_CROSS = "gcp-cross"
    AZUREBLOB = "azureblob"
    KEY_VALUE = "key_value"
    GCP_WORKLOAD_IDENTITY_FEDERATION = "gcp-workload-identity-federation",
    PRIVATE_REGISTRY = "private-registry"


@attr.s
class Integration(entities.BaseEntity):
    """
    Integration object
    """
    id = attr.ib()
    name = attr.ib()
    type = attr.ib()
    org = attr.ib()
    created_at = attr.ib()
    creator = attr.ib()
    update_at = attr.ib()
    url = attr.ib()
    _client_api = attr.ib(type=ApiClient, repr=False)
    metadata = attr.ib(default=None, repr=False)
    _project = attr.ib(default=None, repr=False)

    @classmethod
    def from_json(cls,
                  _json: dict,
                  client_api: ApiClient,
                  is_fetched=True):
        """
        Build a Integration entity object from a json

        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Integration object
        """
        inst = cls(id=_json.get('id', None),
                   name=_json.get('name', None),
                   creator=_json.get('creator', None),
                   created_at=_json.get('createdAt', None),
                   update_at=_json.get('updatedAt', None),
                   type=_json.get('type', None),
                   org=_json.get('org', None),
                   client_api=client_api,
                   metadata=_json.get('metadata', None),
                   url=_json.get('url', None))
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Integration)._client_api,
                                                              attr.fields(Integration)._project))
        return _json

    @property
    def project(self):
        return self._project

    @project.setter
    def project(self, project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def update(self,
               new_name: str = None,
               new_options: dict = None):
        """
        Update the integration's name.

        **Prerequisites**: You must be an *owner* in the organization.

        :param str new_name: new name
        :param dict new_options: new value
        :return: Integration object
        :rtype: dtlpy.entities.integration.Integration

        **Examples for options include**:
        s3 - {key: "", secret: ""};
        gcs - {key: "", secret: "", content: ""};
        azureblob - {key: "", secret: "", clientId: "", tenantId: ""};
        key_value - {key: "", value: ""}
        aws-sts - {key: "", secret: "", roleArns: ""}
        aws-cross - {roleArns: ""}

        **Example**:

        .. code-block:: python

            project.integrations.update(integrations_id='integrations_id', new_name="new_integration_name")
        """
        if self.project is not None:
            identifier = self.project
        elif self.org is not None:
            identifier = repositories.organizations.Organizations(client_api=self._client_api).get(
                organization_id=self.org)
        else:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        identifier.integrations.update(new_name=new_name,
                                       integrations_id=self.id,
                                       integration=self,
                                       new_options=new_options)

    def delete(self,
               sure: bool = False,
               really: bool = False) -> bool:
        """
        Delete integrations from the Organization

        :param bool sure: are you sure you want to delete?
        :param bool really: really really?
        :return: True
        :rtype: bool
        """
        if self.project is not None:
            identifier = self.project
        elif self.org is not None:
            identifier = repositories.organizations.Organizations(client_api=self._client_api).get(
                organization_id=self.org)
        else:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        return identifier.integrations.delete(integrations_id=self.id, sure=sure, really=really)


================================================
File: dtlpy/entities/item.py
================================================
import warnings
from collections import namedtuple
from enum import Enum
import traceback
import logging
import attr
import copy
import os
import io
from .. import repositories, entities, exceptions
from .annotation import ViewAnnotationOptions, ExportVersion
from ..services.api_client import ApiClient
from ..services.api_client import client as client_api
import json
from typing import List
import requests

logger = logging.getLogger(name='dtlpy')


class ExportMetadata(Enum):
    FROM_JSON = 'from_json'


class ItemStatus(str, Enum):
    COMPLETED = "completed"
    APPROVED = "approved"
    DISCARDED = "discard"


@attr.s
class Item(entities.BaseEntity):
    """
    Item object
    """
    # item information
    annotations_link = attr.ib(repr=False)
    dataset_url = attr.ib()
    thumbnail = attr.ib(repr=False)
    created_at = attr.ib()
    updated_at = attr.ib()
    updated_by = attr.ib()
    dataset_id = attr.ib()
    annotated = attr.ib(repr=False)
    metadata = attr.ib(repr=False)
    filename = attr.ib()
    stream = attr.ib(repr=False)
    name = attr.ib()
    type = attr.ib()
    url = attr.ib(repr=False)
    id = attr.ib()
    hidden = attr.ib(repr=False)
    dir = attr.ib(repr=False)
    spec = attr.ib()
    creator = attr.ib()
    _description = attr.ib()
    _src_item = attr.ib(repr=False)

    # name change
    annotations_count = attr.ib()

    # api
    _client_api = attr.ib(type=ApiClient, repr=False)
    _platform_dict = attr.ib(repr=False)

    # entities
    _dataset = attr.ib(repr=False)
    _model = attr.ib(repr=False)
    _project = attr.ib(repr=False)
    _project_id = attr.ib(repr=False)

    # repositories
    _repositories = attr.ib(repr=False)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def datasetId(self):
        return self.dataset_id

    @staticmethod
    def _protected_from_json(_json, client_api, dataset=None):
        """
        Same as from_json but with try-except to catch if error
        :param _json: platform json
        :param client_api: ApiClient entity
        :param dataset: dataset entity
        :return:
        """
        try:
            item = Item.from_json(_json=_json,
                                  client_api=client_api,
                                  dataset=dataset)
            status = True
        except Exception:
            item = traceback.format_exc()
            status = False
        return status, item

    @classmethod
    def from_json(cls, _json, client_api, dataset=None, project=None, model=None, is_fetched=True):
        """
        Build an item entity object from a json

        :param dtlpy.entities.project.Project project: project entity
        :param dict _json: _json response from host
        :param dtlpy.entities.dataset.Dataset dataset: dataset in which the annotation's item is located
        :param dtlpy.entities.dataset.Model model: the model entity if item is an artifact of a model
        :param dlApiClient client_api: ApiClient entity
        :param bool is_fetched: is Entity fetched from Platform
        :return: Item object
        :rtype: dtlpy.entities.item.Item
        """
        dataset_id = None
        if dataset is not None:
            dataset_id = _json.get('datasetId', None)
            if dataset.id != dataset_id and dataset_id is not None:
                logger.warning('Item has been fetched from a dataset that is not belong to it')
                dataset = None
            else:
                dataset_id = dataset.id

        metadata = _json.get('metadata', dict())
        project_id = _json.get('projectId', None)
        if project_id is None:
            project_id = project.id if project else None
        inst = cls(
            # sdk
            platform_dict=copy.deepcopy(_json),
            client_api=client_api,
            dataset=dataset,
            project=project,
            model=model,
            # params
            annotations_link=_json.get('annotations', None),
            thumbnail=_json.get('thumbnail', None),
            dataset_id=_json.get('datasetId', dataset_id),
            annotated=_json.get('annotated', None),
            dataset_url=_json.get('dataset', None),
            created_at=_json.get('createdAt', None),
            annotations_count=_json.get('annotationsCount', None),
            hidden=_json.get('hidden', False),
            stream=_json.get('stream', None),
            dir=_json.get('dir', None),
            filename=_json.get('filename', None),
            metadata=metadata,
            name=_json.get('name', None),
            type=_json.get('type', None),
            url=_json.get('url', None),
            id=_json.get('id', None),
            spec=_json.get('spec', None),
            creator=_json.get('creator', None),
            project_id=project_id,
            description=_json.get('description', None),
            src_item=_json.get('srcItem', None),
            updated_at=_json.get('updatedAt', None),
            updated_by=_json.get('updatedBy', None)
        )
        inst.is_fetched = is_fetched
        return inst

    def __getstate__(self):
        # dump state to json
        return self.to_json()

    def __setstate__(self, state):
        # create a new item, and update the current one with the same state
        # this way we can have _client_api, and all the repositories and entities which are not picklable
        self.__dict__.update(entities.Item.from_json(_json=state,
                                                     client_api=client_api).__dict__)

    ############
    # entities #
    ############
    @property
    def dataset(self):
        if self._dataset is None:
            self._dataset = self.datasets.get(dataset_id=self.dataset_id, fetch=None)
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @property
    def model(self):
        return self._model

    def __update_item_binary(self, _json):
        binary = io.BytesIO()
        binary.write(json.dumps(_json).encode())
        binary.seek(0)
        binary.name = self.name
        success, resp = client_api.gen_request(req_type='post',
                                      path=f'/items/{self.id}/revisions',
                                      files={'file': (binary.name, binary)})
        if not success:
            raise exceptions.PlatformException(resp)

    @property
    def project(self):
        if self._project is None:
            if self._dataset is None:
                self._dataset = self.datasets.get(dataset_id=self.dataset_id, fetch=None)
            self._project = self._dataset.project
            if self._project is None:
                raise exceptions.PlatformException(error='2001',
                                                   message='Missing entity "project".')
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def project_id(self):
        if self._project_id is None:
            if self._dataset is None:
                self._dataset = self.datasets.get(dataset_id=self.dataset_id, fetch=None)
            self._project_id = self._dataset.project.id
        return self._project_id

    ################
    # repositories #
    ################

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['annotations', 'datasets', 'items', 'codebases', 'artifacts', 'modalities',
                                       'features', 'assignments', 'tasks', 'resource_executions', 'collections'])
        reps.__new__.__defaults__ = (None, None, None, None, None, None, None, None, None)

        if self._dataset is None:
            items = repositories.Items(
                client_api=self._client_api,
                dataset=self._dataset,
                dataset_id=self.dataset_id,
                datasets=repositories.Datasets(client_api=self._client_api, project=None)
            )
            datasets = items.datasets

        else:
            items = self.dataset.items
            datasets = self.dataset.datasets

        r = reps(
            annotations=repositories.Annotations(
                client_api=self._client_api,
                dataset_id=self.dataset_id,
                item=self,
                dataset=self._dataset
            ),
            items=items,
            datasets=datasets,
            codebases=None,
            artifacts=None,
            modalities=Modalities(item=self),
            features=repositories.Features(
                client_api=self._client_api,
                project=self._project,
                item=self
            ),
            tasks=repositories.Tasks(
                client_api=self._client_api,
                project=self._project,
                dataset=self._dataset
            ),
            assignments=repositories.Assignments(
                client_api=self._client_api,
                project=self._project,
                dataset=self._dataset
            ),
            resource_executions=repositories.ResourceExecutions(
                client_api=self._client_api,
                project=self._project,
                resource=self
            ),
            collections=repositories.Collections(client_api=self._client_api, item=self, dataset=self._dataset)
        )
        return r

    @property
    def modalities(self):
        assert isinstance(self._repositories.modalities, Modalities)
        return self._repositories.modalities

    @property
    def annotations(self):
        assert isinstance(self._repositories.annotations, repositories.Annotations)
        return self._repositories.annotations

    @property
    def datasets(self):
        assert isinstance(self._repositories.datasets, repositories.Datasets)
        return self._repositories.datasets

    @property
    def assignments(self):
        assert isinstance(self._repositories.assignments, repositories.Assignments)
        return self._repositories.assignments

    @property
    def tasks(self):
        assert isinstance(self._repositories.tasks, repositories.Tasks)
        return self._repositories.tasks

    @property
    def resource_executions(self):
        assert isinstance(self._repositories.resource_executions, repositories.ResourceExecutions)
        return self._repositories.resource_executions

    @property
    def items(self):
        assert isinstance(self._repositories.items, repositories.Items)
        return self._repositories.items

    @property
    def features(self):
        assert isinstance(self._repositories.features, repositories.Features)
        return self._repositories.features
    
    @property
    def collections(self):
        assert isinstance(self._repositories.collections, repositories.Collections)
        return self._repositories.collections

    ##############
    # Properties #
    ##############
    @property
    def height(self):
        return self.metadata.get('system', dict()).get('height', None)

    @height.setter
    def height(self, val):
        if 'system' not in self.metadata:
            self.metadata['system'] = dict()
        self.metadata['system']['height'] = val

    @property
    def width(self):
        return self.metadata.get('system', dict()).get('width', None)

    @width.setter
    def width(self, val):
        if 'system' not in self.metadata:
            self.metadata['system'] = dict()
        self.metadata['system']['width'] = val

    @property
    def fps(self):
        return self.metadata.get('fps', None)

    @fps.setter
    def fps(self, val):
        self.metadata['fps'] = val

    @property
    def mimetype(self):
        return self.metadata.get('system', dict()).get('mimetype', None)

    @property
    def size(self):
        return self.metadata.get('system', dict()).get('size', None)

    @property
    def system(self):
        return self.metadata.get('system', dict())

    @property
    def description(self):
        description = None
        if self._description is not None:
            description = self._description
        elif 'description' in self.metadata:
            description = self.metadata['description'].get('text', None)
        return description

    @property
    def platform_url(self):
        return self._client_api._get_resource_url(
            "projects/{}/datasets/{}/items/{}".format(self.dataset.projects[-1], self.dataset.id, self.id))

    @description.setter
    def description(self, text: str):
        """
        Update Item description

        :param text: if None or "" description will be deleted
        :return
        """
        self.set_description(text=text)

    ###########
    # Functions #
    ###########
    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(Item)._repositories,
                                                        attr.fields(Item)._dataset,
                                                        attr.fields(Item)._model,
                                                        attr.fields(Item)._project,
                                                        attr.fields(Item)._client_api,
                                                        attr.fields(Item)._platform_dict,
                                                        attr.fields(Item).annotations_count,
                                                        attr.fields(Item).dataset_url,
                                                        attr.fields(Item).annotations_link,
                                                        attr.fields(Item).spec,
                                                        attr.fields(Item).creator,
                                                        attr.fields(Item).created_at,
                                                        attr.fields(Item).dataset_id,
                                                        attr.fields(Item)._project_id,
                                                        attr.fields(Item)._description,
                                                        attr.fields(Item)._src_item,
                                                        attr.fields(Item).updated_at,
                                                        attr.fields(Item).updated_by
                                                        ))

        _json.update({'annotations': self.annotations_link,
                      'annotationsCount': self.annotations_count,
                      'dataset': self.dataset_url,
                      'createdAt': self.created_at,
                      'datasetId': self.dataset_id,
                      })
        if self.spec is not None:
            _json['spec'] = self.spec
        if self.creator is not None:
            _json['creator'] = self.creator
        if self._description is not None:
            _json['description'] = self.description
        if self._src_item is not None:
            _json['srcItem'] = self._src_item

        _json['updatedAt'] = self.updated_at
        _json['updatedBy'] = self.updated_by

        return _json

    def download(
            self,
            # download options
            local_path=None,
            file_types=None,
            save_locally=True,
            to_array=False,
            annotation_options: ViewAnnotationOptions = None,
            overwrite=False,
            to_items_folder=True,
            thickness=1,
            with_text=False,
            annotation_filters=None,
            alpha=1,
            export_version=ExportVersion.V1
    ):
        """
        Download dataset by filters.
        Filtering the dataset for items and save them local
        Optional - also download annotation, mask, instance and image mask of the item

        :param str local_path: local folder or filename to save to.
        :param list file_types: a list of file type to download. e.g ['video/webm', 'video/mp4', 'image/jpeg', 'image/png']
        :param bool save_locally: bool. save to disk or return a buffer
        :param bool to_array: returns Ndarray when True and local_path = False
        :param list annotation_options: download annotations options:  list(dl.ViewAnnotationOptions)
        :param dtlpy.entities.filters.Filters annotation_filters: Filters entity to filter annotations for download
        :param bool overwrite: optional - default = False
        :param bool to_items_folder: Create 'items' folder and download items to it
        :param int thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param bool with_text: optional - add text to annotations, default = False
        :param float alpha: opacity value [0 1], default 1
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :return: generator of local_path per each downloaded item
        :rtype: generator or single item

        **Example**:

        .. code-block:: python

            item.download(local_path='local_path',
                         annotation_options=dl.ViewAnnotationOptions.MASK,
                         overwrite=False,
                         thickness=1,
                         with_text=False,
                         alpha=1,
                         save_locally=True
                         )
        """
        # if dir - concatenate local path and item name
        if local_path is not None:
            if os.path.isdir(local_path):
                local_path = os.path.join(local_path, self.name)
            else:
                _, ext = os.path.splitext(local_path)
                _, item_ext = os.path.splitext(self.name)
                if not ext or ext != item_ext:
                    os.makedirs(local_path, exist_ok=True)
                    local_path = os.path.join(local_path, self.name)

        # download
        filters = None
        items = self
        if self.type == 'dir':
            filters = self.datasets._bulid_folder_filter(folder_path=self.filename)
            items = None

        return self.items.download(items=items,
                                   local_path=local_path,
                                   file_types=file_types,
                                   save_locally=save_locally,
                                   to_array=to_array,
                                   annotation_options=annotation_options,
                                   overwrite=overwrite,
                                   to_items_folder=to_items_folder,
                                   annotation_filters=annotation_filters,
                                   thickness=thickness,
                                   alpha=alpha,
                                   with_text=with_text,
                                   export_version=export_version,
                                   filters=filters)

    def delete(self):
        """
        Delete item from platform

        :return: True
        :rtype: bool
        """
        return self.items.delete(item_id=self.id)

    def update(self, system_metadata=False):
        """
        Update items metadata

        :param bool system_metadata: bool - True, if you want to change metadata system
        :return: Item object
        :rtype: dtlpy.entities.item.Item
        """
        return self.items.update(item=self, system_metadata=system_metadata)

    def move(self, new_path):
        """
        Move item from one folder to another in Platform
        If the directory doesn't exist it will be created

        :param str new_path: new full path to move item to.
        :return: True if update successfully
        :rtype: bool
        """
        assert isinstance(new_path, str)
        if not new_path.startswith('/'):
            new_path = '/' + new_path
        if new_path.endswith('/'):
            self.filename = new_path + self.name
        else:
            try:
                self.items.get(filepath=new_path, is_dir=True)
                self.filename = new_path + '/' + self.name
            except exceptions.NotFound:
                self.filename = new_path

        return self.update(system_metadata=True)

    def clone(self, dst_dataset_id=None, remote_filepath=None, metadata=None, with_annotations=True,
              with_metadata=True, with_task_annotations_status=False, allow_many=False, wait=True):
        """
        Clone item

        :param str dst_dataset_id: destination dataset id
        :param str remote_filepath: complete filepath
        :param dict metadata: new metadata to add
        :param bool with_annotations: clone annotations
        :param bool with_metadata: clone metadata
        :param bool with_task_annotations_status: clone task annotations status
        :param bool allow_many: `bool` if True, using multiple clones in single dataset is allowed, (default=False)
        :param bool wait: wait for the command to finish
        :return: Item object
        :rtype: dtlpy.entities.item.Item

        **Example**:

        .. code-block:: python

            item.clone(item_id='item_id',
                    dst_dataset_id='dist_dataset_id',
                    with_metadata=True,
                    with_task_annotations_status=False,
                    with_annotations=False)
        """
        if remote_filepath is None:
            remote_filepath = self.filename
        if dst_dataset_id is None:
            dst_dataset_id = self.dataset_id
        return self.items.clone(item_id=self.id,
                                dst_dataset_id=dst_dataset_id,
                                remote_filepath=remote_filepath,
                                metadata=metadata,
                                with_annotations=with_annotations,
                                with_metadata=with_metadata,
                                with_task_annotations_status=with_task_annotations_status,
                                allow_many=allow_many,
                                wait=wait)

    def open_in_web(self):
        """
        Open the items in web platform

        :return:
        """
        self._client_api._open_in_web(url=self.platform_url)

    def _set_action(self, status: str, operation: str, assignment_id: str = None, task_id: str = None):
        """
        update item status

        :param status: str - string the describes the status
        :param operation: str -  'create' or 'delete'
        :param assignment_id: str - assignment id
        :param task_id: str - task id

        :return :True/False
        """
        if assignment_id:
            success = self.assignments.set_status(
                status=status,
                operation=operation,
                item_id=self.id,
                assignment_id=assignment_id
            )
        elif task_id:
            success = self.tasks.set_status(
                status=status,
                operation=operation,
                item_ids=[self.id],
                task_id=task_id
            )

        else:
            raise exceptions.PlatformException('400', 'Must provide task_id or assignment_id')

        return success

    def update_status(self, status: str, clear: bool = False, assignment_id: str = None, task_id: str = None):
        """
        update item status

        :param str status: "completed" ,"approved" ,"discard"
        :param bool clear: if true delete status
        :param str assignment_id: assignment id
        :param str task_id: task id

        :return :True/False
        :rtype: bool

        **Example**:

        .. code-block:: python

            item.update_status(status='complete',
                               task_id='task_id')

        """
        if not assignment_id and not task_id:
            system_metadata = self.metadata.get('system', dict())
            if 'refs' in system_metadata:
                refs = system_metadata['refs']
                if len(refs) <= 2:
                    for ref in refs:
                        if ref.get('type', '') == 'assignment':
                            assignment_id = ref['id']
                        if ref.get('type', '') == 'task':
                            task_id = ref['id']

        if assignment_id or task_id:
            if clear:
                self._set_action(status=status, operation='delete', assignment_id=assignment_id, task_id=task_id)
            else:
                self._set_action(status=status, operation='create', assignment_id=assignment_id, task_id=task_id)
        else:
            raise exceptions.PlatformException('400', 'must provide assignment_id or task_id')

    def status(self, assignment_id: str = None, task_id: str = None):
        """
        Get item status

        :param str assignment_id: assignment id
        :param str task_id: task id

        :return: status
        :rtype: str

        **Example**:

        .. code-block:: python

            status = item.status(task_id='task_id')
        """
        if not assignment_id and not task_id:
            raise exceptions.PlatformException('400', 'must provide assignment_id or task_id')
        status = None
        resource_id = assignment_id if assignment_id else task_id
        for ref in self.metadata.get('system', dict()).get('refs', []):
            if ref.get('id') == resource_id:
                status = ref.get('metadata', {}).get('status', None)
                break
        return status

    def set_description(self, text: str):
        """
        Update Item description

        :param str text: if None or "" description will be deleted

        :return
        """
        if text is None:
            text = ""
        if not isinstance(text, str):
            raise ValueError("Description must get string")
        self._description = text
        self._platform_dict = self.update()._platform_dict
        return self

    def assign_subset(self, subset: str):
        """
        Assign a single ML subset (train/validation/test) to this item.
        Sets the chosen subset to True and the others to None.
        Then calls item.update(system_metadata=True).

        :param str subset: 'train', 'validation', or 'test'
        """
        if subset not in ['train', 'validation', 'test']:
            raise ValueError("subset must be one of: 'train', 'validation', 'test'")

        if 'system' not in self.metadata:
            self.metadata['system'] = {}
        if 'tags' not in self.metadata['system']:
            self.metadata['system']['tags'] = {}

        self.metadata['system']['tags']['train'] = True if subset == 'train' else None
        self.metadata['system']['tags']['validation'] = True if subset == 'validation' else None
        self.metadata['system']['tags']['test'] = True if subset == 'test' else None

        return self.update(system_metadata=True)


    def remove_subset(self):
        """
        Remove any ML subset assignment from this item.
        Sets train, validation, and test to None.
        Then calls item.update(system_metadata=True).
        """
        if 'system' not in self.metadata:
            self.metadata['system'] = {}
        if 'tags' not in self.metadata['system']:
            self.metadata['system']['tags'] = {}

        self.metadata['system']['tags']['train'] = None
        self.metadata['system']['tags']['validation'] = None
        self.metadata['system']['tags']['test'] = None

        return self.update(system_metadata=True)


    def get_current_subset(self) -> str:
        """
        Get the current ML subset assignment of this item.
        Returns 'train', 'validation', 'test', or None if not assigned.

        :return: subset name or None
        :rtype: str or None
        """
        tags = self.metadata.get('system', {}).get('tags', {})
        for subset in ['train', 'validation', 'test']:
            if tags.get(subset) is True:
                return subset
        return None
    
    def assign_collection(self, collections: List[str]) -> bool:
        """
        Assign this item to one or more collections.

        :param collections: List of collection names to assign the item to.
        :return: True if the assignment was successful, otherwise False.
        """
        return self.collections.assign(dataset_id=self.dataset_id, collections=collections, item_id=self.id,)

    def unassign_collection(self, collections: List[str]) -> bool:
        """
        Unassign this item from one or more collections.

        :param collections: List of collection names to unassign the item from.
        :return: True if the unassignment was successful, otherwise False.
        """
        return self.collections.unassign(dataset_id=self.dataset_id, item_id=self.id, collections=collections)

    def list_collections(self) -> List[dict]:
        """
        List all collections associated with this item.

        :return: A list of dictionaries containing collection keys and their respective names.
                Each dictionary has the structure: {"key": <collection_key>, "name": <collection_name>}.
        """
        collections = self.metadata.get("system", {}).get("collections", {})
        if not isinstance(collections, dict):
            # Ensure collections is a dictionary
            return []
    
        # Retrieve collection names by their keys
        return [
            {"key": key, "name": self.collections.get_name_by_key(key)}
            for key in collections.keys()
        ]

    def list_missing_collections(self) -> List[str]:
        """
        List all items in the dataset that are not assigned to any collection.

        :return: A list of item IDs that are not part of any collection.
        """
        filters = entities.Filters()
        filters.add(field='metadata.system.collections', values=None)
        filters.add(field='datasetId', values=self._dataset.id)
        return self._dataset.items.list(filters=filters)

class ModalityTypeEnum(str, Enum):
    """
    State enum
    """
    OVERLAY = "overlay"
    REPLACE = "replace"
    PREVIEW = "preview"


class ModalityRefTypeEnum(str, Enum):
    """
    State enum
    """
    ID = "id"
    URL = "url"


class Modality:
    def __init__(self, _json=None, modality_type=None, ref=None, ref_type=ModalityRefTypeEnum.ID,
                 name=None, timestamp=None, mimetype=None):
        """
        :param _json: json represent of all modality params
        :param modality_type: ModalityTypeEnum.OVERLAY,ModalityTypeEnum.REPLACE
        :param ref: id or url of the item reference
        :param ref_type: ModalityRefTypeEnum.ID, ModalityRefTypeEnum.URL
        :param name:
        :param timestamp: ISOString, epoch of UTC
        :param mimetype: str - type of the file
        """
        if _json is None:
            _json = dict()
        self.type = _json.get('type', modality_type)
        self.ref_type = _json.get('refType', ref_type)
        self.ref = _json.get('ref', ref)
        self.name = _json.get('name', name)
        self.timestamp = _json.get('timestamp', timestamp)
        self.mimetype = _json.get('mimetype', mimetype)

    def to_json(self):
        _json = {"type": self.type,
                 "ref": self.ref,
                 "refType": self.ref_type}
        if self.name is not None:
            _json['name'] = self.name
        if self.timestamp is not None:
            _json['timestamp'] = self.timestamp
        if self.mimetype is not None:
            _json['mimetype'] = self.mimetype
        return _json


class Modalities:
    def __init__(self, item):
        assert isinstance(item, Item)
        self.item = item
        if 'system' not in self.item.metadata:
            self.item.metadata['system'] = dict()

    @property
    def modalities(self):
        mod = None
        if 'system' in self.item.metadata:
            mod = self.item.metadata['system'].get('modalities', None)
        return mod

    def create(self, name, ref,
               ref_type: ModalityRefTypeEnum = ModalityRefTypeEnum.ID,
               modality_type: ModalityTypeEnum = ModalityTypeEnum.OVERLAY,
               timestamp=None,
               mimetype=None,
               ):
        """
        create Modalities entity

        :param name: name
        :param ref: id or url of the item reference
        :param ref_type: ModalityRefTypeEnum.ID, ModalityRefTypeEnum.URL
        :param modality_type: ModalityTypeEnum.OVERLAY,ModalityTypeEnum.REPLACE
        :param timestamp: ISOString, epoch of UTC
        :param mimetype: str - type of the file
        """
        if self.modalities is None:
            self.item.metadata['system']['modalities'] = list()

        _json = {"type": modality_type,
                 "ref": ref,
                 "refType": ref_type}
        if name is not None:
            _json['name'] = name
        if timestamp is not None:
            _json['timestamp'] = timestamp
        if mimetype is not None:
            _json['mimetype'] = mimetype

        self.item.metadata['system']['modalities'].append(_json)

        return Modality(_json=_json)

    def delete(self, name):
        """
        :param name:
        """
        if self.modalities is not None:
            for modality in self.item.metadata['system']['modalities']:
                if name == modality['name']:
                    self.item.metadata['system']['modalities'].remove(modality)
                    return Modality(_json=modality)
        return None

    def list(self):
        modalities = list()
        if self.modalities is not None:
            modalities = list()
            for modality in self.item.metadata['system']['modalities']:
                modalities.append(Modality(_json=modality))
        return modalities


================================================
File: dtlpy/entities/label.py
================================================
import attr
import logging
import random

from .. import PlatformException

logger = logging.getLogger(name='dtlpy')


@attr.s
class Label:
    tag = attr.ib()
    display_data = attr.ib()
    color = attr.ib(default=None)
    display_label = attr.ib(default=None)
    attributes = attr.ib()
    children = attr.ib()

    @attributes.default
    def set_attributes(self):
        attributes = list()
        return attributes

    @children.default
    def set_children(self):
        children = list()
        return children


    @display_data.default
    def set_display_data(self):
        display_data = dict()
        return display_data

    @classmethod
    def from_root(cls, root):
        """
        Build a Label entity object from a json

        :param dict root: _json representation of a label as it is in host
        :return: Label object
        """
        children = list()
        if 'children' in root and root['children'] is not None:
            children = [Label.from_root(child) for child in root['children']]

        root = root.get("value", root)
        if "tag" in root:
            label_name = root["tag"]
        elif "label_name" in root:
            label_name = root["label_name"]
        else:
            raise PlatformException("400", "Invalid input - each label must have a tag")

        display_label = root.get("displayLabel", None)
        if display_label is None:
            display_label = root.get("display_label", None)

        display_data = root.get("displayData", dict())
        return cls(
            tag=label_name,
            display_data=display_data,
            color=root.get("color", None),
            display_label=display_label,
            attributes=root.get("attributes", None),
            children=children
        )

    def to_root(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        """
        value = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Label).children,
                                                              attr.fields(Label).color,
                                                              attr.fields(Label).display_label,
                                                              attr.fields(Label).display_data))
        value['displayLabel'] = self.display_label
        value['displayData'] = self.display_data
        if self.color:
            value['color'] = self.hex
        children = [child.to_root() for child in self.children]
        _json = {
            'value': value,
            'children': children
        }
        return _json

    @property
    def rgb(self):
        """
        Return label's color in RBG format

        :return: label's color in RBG format
        """
        if self.color is None:
            color = None
        elif isinstance(self.color, str) and self.color.startswith('rgb'):
            color = tuple(eval(self.color.lstrip('rgb')))
        elif isinstance(self.color, str) and self.color.startswith('#'):
            color = tuple(int(self.color.lstrip('#')[i:i + 2], 16) for i in (0, 2, 4))
        elif isinstance(self.color, tuple) or isinstance(self.color, list):
            color = self.color
        else:
            logger.warning('Unknown color scheme: {}'.format(self.color))
            color = (255, 0, 0)
        return color

    @property
    def hex(self):
        """
        Return label's color in HEX format

        :return: label's color in HEX format
        """
        if isinstance(self.color, tuple) or isinstance(self.color, list):
            return '#%02x%02x%02x' % self.color
        elif self.color.startswith('rgb'):
            rgb = tuple(eval(self.color.lstrip('rgb')))
            return '#%02x%02x%02x' % rgb
        elif self.color.startswith('#'):
            return self.color


================================================
File: dtlpy/entities/links.py
================================================
from .. import entities
from .. import PlatformException
from enum import Enum
import os
import mimetypes


class LinkTypeEnum(str, Enum):
    """
    State enum
    """
    ID = "id"
    URL = "url"


class Link:

    # noinspection PyShadowingBuiltins
    def __init__(self, name, type: LinkTypeEnum, ref, mimetype=None, dataset_id=None):
        self.type = type
        self.ref = ref
        self.name = name
        self.dataset_id = dataset_id
        self.mimetype = mimetype


class UrlLink(Link):

    def __init__(self, ref, mimetype=None, name=None):
        if name is None:
            name = os.path.split(ref)[-1].replace('/', "").replace("\\", '').replace('.', '')
            name = name.split('?')[0]
        # noinspection PyShadowingBuiltins
        type = LinkTypeEnum.URL
        if not mimetype:
            mimetype = mimetypes.guess_type(ref.split('?')[0])[0] or 'image'
        super().__init__(name=name, type=type, mimetype=mimetype, ref=ref)

    @staticmethod
    def from_list(url_list):
        url_links = list()
        for url in url_list:
            url_links.append(UrlLink(ref=url))
        return url_links


class ItemLink(Link):

    def __init__(self, ref=None, name=None, dataset_id=None, item=None):
        if ref is None and item is None:
            raise PlatformException('400', 'Must provide either ref or item_id')

        if item is not None and not isinstance(item, entities.Item):
            raise PlatformException('400', 'Param item must be of type Item')

        if ref is None:
            ref = item.id

        if name is None:
            if item is not None:
                name = item.name
            else:
                name = ref

        if dataset_id is None and item is not None and isinstance(item, entities.Item):
            dataset_id = item.dataset_id

        # noinspection PyShadowingBuiltins
        type = LinkTypeEnum.ID
        super().__init__(name=name, type=type, ref=ref, dataset_id=dataset_id)

    @staticmethod
    def from_list(items):
        if not isinstance(items, list):
            items = [items]

        item_links = list()
        for item in items:
            if isinstance(item, str):
                item_links.append(ItemLink(ref=item))
            elif isinstance(item, entities.Item):
                item_links.append(ItemLink(item=item))
            else:
                raise PlatformException('400', 'Unknown item type. items should be a list of item entities/item ids')
        return item_links


================================================
File: dtlpy/entities/message.py
================================================
import traceback
import logging
import attr
from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class NotificationEventContext:

    def __init__(self, context: dict):
        self.project: str = context.get('project', None)
        self.org: str = context.get('org', None)
        self.pipeline: str = context.get('pipeline', None)
        self.service: str = context.get('service', None)
        self.node: str = context.get('node', None)

    @classmethod
    def from_json(cls, _json: dict):
        return cls(context=_json)

    def to_json(self):
        _json = dict()
        if self.project:
            _json['project'] = self.project
        if self.org:
            _json['org'] = self.org
        if self.pipeline:
            _json['pipeline'] = self.pipeline
        if self.service:
            _json['service'] = self.service
        if self.node:
            _json['node'] = self.node
        return _json


@attr.s
class Message(entities.BaseEntity):
    """
    Message object
    """
    title = attr.ib(type=str)
    description = attr.ib(type=str)
    context = attr.ib(type=NotificationEventContext)
    read = attr.ib(type=int)
    dismissed = attr.ib(type=int)
    new = attr.ib(type=int)

    # from camel case to snake case
    user_id = attr.ib(type=str)
    notification_id = attr.ib(type=str)
    resource_action = attr.ib(type=str)
    notification_code = attr.ib(type=str)
    resource_type = attr.ib(type=str)
    resource_id = attr.ib(type=str)
    resource_name = attr.ib(type=str)

    # api
    _client_api = attr.ib(type=ApiClient, repr=False)

    # entities
    _project = attr.ib(default=None, repr=False)

    @staticmethod
    def _protected_from_json(
            _json: dict,
            client_api: ApiClient,
            project: entities.Project = None,
            is_fetched=True
    ):
        """
        Same as from_json but with try-except to catch if error

        :param project: message's project
        :param _json: _json response from host
        :param client_api: ApiClient entity
        :param is_fetched: is Entity fetched from Platform
        :return: Message object
        """
        try:
            message = Message.from_json(project=project,
                                        _json=_json,
                                        client_api=client_api,
                                        is_fetched=is_fetched)
            status = True
        except Exception:
            message = traceback.format_exc()
            status = False
        return status, message

    @classmethod
    def from_json(cls,
                  _json: dict,
                  client_api: ApiClient,
                  project: entities.Project = None,
                  is_fetched=True
                  ):
        """
        Build a Message entity object from a json

        :param project: message's project
        :param dict _json: _json response from host
        :param client_api: ApiClient entityÏ
        :param bool is_fetched: is Entity fetched from Platform
        :return: Dataset object
        :rtype: dtlpy.entities.message.Message
        """

        inst = cls(
            title=_json.get('title', None),
            description=_json.get('description', None),
            context=NotificationEventContext.from_json(_json=_json.get('context', None)),
            read=_json.get('read', None),
            dismissed=_json.get('dismissed', None),
            new=_json.get('new', None),
            user_id=_json.get('userId', None),
            notification_id=_json.get('notificationId', None),
            resource_action=_json.get('resourceAction', None),
            notification_code=_json.get('notificationCode', None),
            resource_type=_json.get('resourceType', None),
            resource_id=_json.get('resourceId', None),
            resource_name=_json.get('resourceName', None),
            client_api=client_api,
            project=project
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(
            self, filter=attr.filters.exclude(
                attr.fields(Message)._client_api,
                attr.fields(Message)._project,
                attr.fields(Message).is_fetched,
                attr.fields(Message).context,
                attr.fields(Message).user_id,
                attr.fields(Message).notification_id,
                attr.fields(Message).resource_action,
                attr.fields(Message).notification_code,
                attr.fields(Message).resource_type,
                attr.fields(Message).resource_id,
                attr.fields(Message).resource_name
            )
        )

        _json['context'] = self.context.to_json()
        _json['userId'] = self.user_id
        _json['notificationId'] = self.notification_id
        _json['resourceAction'] = self.resource_action
        _json['notificationCode'] = self.notification_code
        _json['resourceType'] = self.resource_type
        _json['resourceId'] = self.resource_id
        _json['resourceName'] = self.resource_name

        return _json

    @property
    def project(self):
        if self._project is None and self.context.project is not None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.context.project)
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project


================================================
File: dtlpy/entities/model.py
================================================
from collections import namedtuple
from enum import Enum
import traceback
import logging
import attr
from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class DatasetSubsetType(str, Enum):
    """Available types for dataset subsets"""
    TRAIN = 'train'
    VALIDATION = 'validation'
    TEST = 'test'


class ModelStatus(str, Enum):
    """Available types for model status"""
    CREATED = "created",
    PRE_TRAINED = "pre-trained",
    PENDING = "pending",
    TRAINING = "training",
    TRAINED = "trained",
    DEPLOYED = "deployed",
    FAILED = "failed",
    CLONING = "cloning"


class PlotSample:
    def __init__(self, figure, legend, x, y):
        """
        Create a single metric sample for Model

        :param figure: figure name identifier
        :param legend: line name identifier
        :param x: x value for the current sample
        :param y: y value for the current sample
        """
        self.figure = figure
        self.legend = legend
        self.x = x
        self.y = y

    def to_json(self) -> dict:
        _json = {'figure': self.figure,
                 'legend': self.legend,
                 'data': {'x': self.x,
                          'y': self.y}}
        return _json


# class MatrixSample:
#     def __init__(self, figure, legend, x, y):
#         """
#         Create a single metric sample for Model
#
#         :param figure: figure name identifier
#         :param legend: line name identifier
#         :param x: x value for the current sample
#         :param y: y value for the current sample
#         """
#         self.figure = figure
#         self.legend = legend
#         self.x = x
#         self.y = y
#
#     def to_json(self) -> dict:
#         _json = {'figure': self.figure,
#                  'legend': self.legend,
#                  'data': {'x': self.x,
#                           'y': self.y}}
#         return _json


@attr.s
class Model(entities.BaseEntity):
    """
    Model object
    """
    # platform
    id = attr.ib()
    creator = attr.ib()
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    model_artifacts = attr.ib()
    name = attr.ib()
    description = attr.ib()
    ontology_id = attr.ib(repr=False)
    labels = attr.ib()
    status = attr.ib()
    tags = attr.ib()
    configuration = attr.ib()
    metadata = attr.ib()
    input_type = attr.ib()
    output_type = attr.ib()
    module_name = attr.ib()

    url = attr.ib()
    scope = attr.ib()
    version = attr.ib()
    context = attr.ib()

    # name change
    package_id = attr.ib(repr=False)
    project_id = attr.ib()
    dataset_id = attr.ib(repr=False)

    # sdk
    _project = attr.ib(repr=False)
    _package = attr.ib(repr=False)
    _dataset = attr.ib(repr=False)
    _feature_set = attr.ib(repr=False)
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)
    _ontology = attr.ib(repr=False, default=None)
    updated_by = attr.ib(default=None)
    app = attr.ib(default=None)

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, package=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform representation of Model
        :param client_api: ApiClient entity
        :param project: project that owns the model
        :param package: package entity of the model
        :param is_fetched: is Entity fetched from Platform
        :return: Model entity
        """
        try:
            model = Model.from_json(_json=_json,
                                    client_api=client_api,
                                    project=project,
                                    package=package,
                                    is_fetched=is_fetched)
            status = True
        except Exception:
            model = traceback.format_exc()
            status = False
        return status, model

    @classmethod
    def from_json(cls, _json, client_api, project=None, package=None, is_fetched=True):
        """
        Turn platform representation of model into a model entity

        :param _json: platform representation of model
        :param client_api: ApiClient entity
        :param project: project that owns the model
        :param package: package entity of the model
        :param is_fetched: is Entity fetched from Platform
        :return: Model entity
        """
        if project is not None:
            if project.id != _json.get('context', {}).get('project', None):
                logger.warning("Model's project is different then the input project")
                project = None

        if package is not None:
            if package.id != _json.get('packageId', None):
                logger.warning("Model's package is different then the input package")
                package = None

        model_artifacts = [entities.Artifact.from_json(_json=artifact,
                                                       client_api=client_api,
                                                       project=project)
                           for artifact in _json.get('artifacts', list())]

        inst = cls(
            configuration=_json.get('configuration', None),
            description=_json.get('description', None),
            status=_json.get('status', None),
            tags=_json.get('tags', None),
            metadata=_json.get('metadata', dict()),
            project_id=_json.get('context', {}).get('project', None),
            dataset_id=_json.get('datasetId', None),
            package_id=_json.get('packageId', None),
            model_artifacts=model_artifacts,
            labels=_json.get('labels', None),
            ontology_id=_json.get('ontology_id', None),
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            creator=_json.get('context', {}).get('creator', None),
            client_api=client_api,
            name=_json.get('name', None),
            project=project,
            package=package,
            dataset=None,
            feature_set=None,
            id=_json.get('id', None),
            url=_json.get('url', None),
            scope=_json.get('scope', entities.EntityScopeLevel.PROJECT),
            version=_json.get('version', '1.0.0'),
            context=_json.get('context', {}),
            input_type=_json.get('inputType', None),
            output_type=_json.get('outputType', None),
            module_name=_json.get('moduleName', None),
            updated_by=_json.get('updatedBy', None),
            app=_json.get('app', None)
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Get the dict of Model

        :return: platform json of model
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(Model)._project,
                                                        attr.fields(Model)._package,
                                                        attr.fields(Model)._dataset,
                                                        attr.fields(Model)._ontology,
                                                        attr.fields(Model)._repositories,
                                                        attr.fields(Model)._feature_set,
                                                        attr.fields(Model)._client_api,
                                                        attr.fields(Model).package_id,
                                                        attr.fields(Model).project_id,
                                                        attr.fields(Model).dataset_id,
                                                        attr.fields(Model).ontology_id,
                                                        attr.fields(Model).model_artifacts,
                                                        attr.fields(Model).created_at,
                                                        attr.fields(Model).updated_at,
                                                        attr.fields(Model).input_type,
                                                        attr.fields(Model).output_type,
                                                        attr.fields(Model).updated_by,
                                                        attr.fields(Model).app
                                                        ))
        _json['packageId'] = self.package_id
        _json['datasetId'] = self.dataset_id
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        _json['inputType'] = self.input_type
        _json['outputType'] = self.output_type
        _json['moduleName'] = self.module_name

        model_artifacts = list()
        for artifact in self.model_artifacts:
            if artifact.type in ['file', 'dir']:
                artifact = {'type': 'item',
                            'itemId': artifact.id}
            else:
                artifact = artifact.to_json(as_artifact=True)
            model_artifacts.append(artifact)
        _json['artifacts'] = model_artifacts

        if self.updated_by:
            _json['updatedBy'] = self.updated_by
        if self.app:
            _json['app'] = self.app

        return _json

    ############
    # entities #
    ############
    @property
    def project(self):
        if self._project is None:
            self._project = self.projects.get(project_id=self.project_id, fetch=None)
            self._repositories = self.set_repositories()  # update the repos with the new fetched entity
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def feature_set(self) -> 'entities.FeatureSet':
        if self._feature_set is None:
            filters = entities.Filters(field='modelId',
                                       values=self.id,
                                       resource=entities.FiltersResource.FEATURE_SET)
            feature_sets = self.project.feature_sets.list(filters=filters)
            if feature_sets.items_count > 1:
                logger.warning("Found more than one feature set associated with model entity. Returning first result."
                               "Set feature_set if other feature set entity is needed.")
                self._feature_set = feature_sets.items[0]
            elif feature_sets.items_count == 1:
                self._feature_set = feature_sets.items[0]
            else:
                self._feature_set = None
        return self._feature_set

    @feature_set.setter
    def feature_set(self, feature_set: 'entities.FeatureSet'):
        if not isinstance(feature_set, entities.FeatureSet):
            raise ValueError("feature_set must be of type dl.FeatureSet")
        else:
            self._feature_set = feature_set

    @property
    def package(self):
        if self._package is None:
            try:
                self._package = self.packages.get(package_id=self.package_id)
            except Exception as e:
                error = e
                try:
                    self._package = self.dpks.get(dpk_id=self.package_id)
                except Exception:
                    raise error
            self._repositories = self.set_repositories()  # update the repos with the new fetched entity
        assert isinstance(self._package, (entities.Package, entities.Dpk))
        return self._package

    @property
    def dataset(self):
        if self._dataset is None:
            if self.dataset_id is None:
                raise RuntimeError("Model {!r} has no dataset. Can be used only for inference".format(self.id))
            self._dataset = self.datasets.get(dataset_id=self.dataset_id, fetch=None)
            self._repositories = self.set_repositories()  # update the repos with the new fetched entity
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @property
    def ontology(self):
        if self._ontology is None:
            if self.ontology_id is None:
                raise RuntimeError("Model {!r} has no ontology.".format(self.id))
            self._ontology = self.ontologies.get(ontology_id=self.ontology_id)
        assert isinstance(self._ontology, entities.Ontology)
        return self._ontology

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['projects', 'datasets', 'models', 'packages', 'ontologies', 'artifacts',
                                       'metrics', 'dpks', 'services'])

        r = reps(projects=repositories.Projects(client_api=self._client_api),
                 datasets=repositories.Datasets(client_api=self._client_api,
                                                project=self._project),
                 models=repositories.Models(client_api=self._client_api,
                                            project=self._project,
                                            project_id=self.project_id,
                                            package=self._package),
                 packages=repositories.Packages(client_api=self._client_api,
                                                project=self._project),
                 ontologies=repositories.Ontologies(client_api=self._client_api,
                                                    project=self._project,
                                                    dataset=self._dataset),
                 artifacts=repositories.Artifacts(client_api=self._client_api,
                                                  project=self._project,
                                                  project_id=self.project_id,
                                                  model=self),
                 metrics=repositories.Metrics(client_api=self._client_api,
                                              model=self),
                 dpks=repositories.Dpks(client_api=self._client_api),
                 services=repositories.Services(client_api=self._client_api,
                                                project=self._project,
                                                project_id=self.project_id,
                                                model_id=self.id,
                                                model=self),
                 )
        return r

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/model/{}".format(self.project_id, self.id))

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def datasets(self):
        assert isinstance(self._repositories.datasets, repositories.Datasets)
        return self._repositories.datasets

    @property
    def models(self):
        assert isinstance(self._repositories.models, repositories.Models)
        return self._repositories.models

    @property
    def packages(self):
        assert isinstance(self._repositories.packages, repositories.Packages)
        return self._repositories.packages

    @property
    def dpks(self):
        assert isinstance(self._repositories.dpks, repositories.Dpks)
        return self._repositories.dpks

    @property
    def ontologies(self):
        assert isinstance(self._repositories.ontologies, repositories.Ontologies)
        return self._repositories.ontologies

    @property
    def artifacts(self):
        assert isinstance(self._repositories.artifacts, repositories.Artifacts)
        return self._repositories.artifacts

    @property
    def metrics(self):
        assert isinstance(self._repositories.metrics, repositories.Metrics)
        return self._repositories.metrics

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    @property
    def id_to_label_map(self):
        # default
        if 'id_to_label_map' not in self.configuration:
            if not (self.dataset_id == 'null' or self.dataset_id is None):
                self.labels = [label.tag for label in self.dataset.labels]
            self.configuration['id_to_label_map'] = {int(idx): lbl for idx, lbl in enumerate(self.labels)}
        # use existing
        else:
            self.configuration['id_to_label_map'] = {int(idx): lbl for idx, lbl in
                                                     self.configuration['id_to_label_map'].items()}
        return self.configuration['id_to_label_map']

    @id_to_label_map.setter
    def id_to_label_map(self, mapping: dict):
        self.configuration['id_to_label_map'] = {int(idx): lbl for idx, lbl in mapping.items()}

    @property
    def label_to_id_map(self):
        if 'label_to_id_map' not in self.configuration:
            self.configuration['label_to_id_map'] = {v: int(k) for k, v in self.id_to_label_map.items()}
        return self.configuration['label_to_id_map']

    @label_to_id_map.setter
    def label_to_id_map(self, mapping: dict):
        self.configuration['label_to_id_map'] = {v: int(k) for k, v in mapping.items()}

    ###########
    # methods #
    ###########

    def add_subset(self, subset_name: str, subset_filter: entities.Filters):
        """
        Adds a subset for the model, specifying a subset of the model's dataset that could be used for training or
        validation.

        :param str subset_name: the name of the subset
        :param dtlpy.entities.Filters subset_filter: the filtering operation that this subset performs in the dataset.

        **Example**

        .. code-block:: python

            model.add_subset(subset_name='train', subset_filter=dtlpy.Filters(field='dir', values='/train'))
            model.metadata['system']['subsets']
                {'train': <dtlpy.entities.filters.Filters object at 0x1501dfe20>}

        """
        self.models.add_subset(self, subset_name, subset_filter)

    def delete_subset(self, subset_name: str):
        """
        Removes a subset from the model's metadata.

        :param str subset_name: the name of the subset

        **Example**

        .. code-block:: python

            model.add_subset(subset_name='train', subset_filter=dtlpy.Filters(field='dir', values='/train'))
            model.metadata['system']['subsets']
                {'train': <dtlpy.entities.filters.Filters object at 0x1501dfe20>}
            models.delete_subset(subset_name='train')
            metadata['system']['subsets']
                {}

        """
        self.models.delete_subset(self, subset_name)

    def update(self, system_metadata=False):
        """
        Update Models changes to platform

        :param bool system_metadata: bool - True, if you want to change metadata system
        :return: Models entity
        """
        return self.models.update(model=self,
                                  system_metadata=system_metadata)

    def open_in_web(self):
        """
        Open the model in web platform

        :return:
        """
        self._client_api._open_in_web(url=self.platform_url)

    def delete(self):
        """
        Delete Model object

        :return: True
        """
        return self.models.delete(model=self)

    def clone(self,
              model_name: str,
              dataset: entities.Dataset = None,
              configuration: dict = None,
              status=None,
              scope=None,
              project_id: str = None,
              labels: list = None,
              description: str = None,
              tags: list = None,
              train_filter: entities.Filters = None,
              validation_filter: entities.Filters = None,
              wait=True
              ):
        """
        Clones and creates a new model out of existing one

        :param str model_name: `str` new model name
        :param str dataset: dataset object for the cloned model
        :param dict configuration: `dict` (optional) if passed replaces the current configuration
        :param str status: `str` (optional) set the new status
        :param str scope: `str` (optional) set the new scope. default is "project"
        :param str project_id: `str` specify the project id to create the new model on (if other than the source model)
        :param list labels:  `list` of `str` - label of the model
        :param str description: `str` description of the new model
        :param list tags:  `list` of `str` - label of the model
        :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
        :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
        :param bool wait: `bool` wait for the model to be ready before returning

        :return: dl.Model which is a clone version of the existing model
        """
        return self.models.clone(from_model=self,
                                 model_name=model_name,
                                 project_id=project_id,
                                 dataset=dataset,
                                 scope=scope,
                                 status=status,
                                 configuration=configuration,
                                 labels=labels,
                                 description=description,
                                 tags=tags,
                                 train_filter=train_filter,
                                 validation_filter=validation_filter,
                                 wait=wait
                                 )

    def train(self, service_config=None):
        """
        Train the model in the cloud. This will create a service and will run the adapter's train function as an execution

        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
        :return:
        """
        return self.models.train(model_id=self.id, service_config=service_config)

    def evaluate(self, dataset_id, filters: entities.Filters = None, service_config=None):
        """
        Evaluate Model, provide data to evaluate the model on You can also provide specific config for the deployed service

        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
        :param str dataset_id: ID of the dataset to evaluate
        :param entities.Filters filters: dl.Filter entity to run the predictions on
        :return:
        """
        return self.models.evaluate(model_id=self.id,
                                    dataset_id=dataset_id,
                                    filters=filters,
                                    service_config=service_config)

    def predict(self, item_ids=None, dataset_id=None):
        """
        Run model prediction with items

        :param item_ids: a list of item id to run the prediction.
        :param dataset_id: dataset id to run the prediction on
        :return:
        """
        return self.models.predict(model=self, item_ids=item_ids, dataset_id=dataset_id)

    def embed(self, item_ids):
        """
        Run model embed with items

        :param item_ids: a list of item id to run the embed.
        :return:
        """
        return self.models.embed(model=self, item_ids=item_ids)

    def embed_datasets(self, dataset_ids, attach_trigger=False):
        """
        Run model embed with datasets

        :param dataset_ids: a list of dataset id to run the embed.
        :param attach_trigger: bool - True, if you want to activate the trigger
        :return:
        """
        return self.models.embed_datasets(model=self, dataset_ids=dataset_ids, attach_trigger=attach_trigger)

    def deploy(self, service_config=None) -> entities.Service:
        """
        Deploy a trained model. This will create a service that will execute predictions

        :param dict service_config : Service object as dict. Contains the spec of the default service to create.

        :return: dl.Service: The deployed service
        """
        return self.models.deploy(model_id=self.id, service_config=service_config)

    def wait_for_model_ready(self):
        """
        Wait for model to be ready

        :return:
        """
        return self.models.wait_for_model_ready(model=self)

    def log(self,
            service=None,
            size=None,
            checkpoint=None,
            start=None,
            end=None,
            follow=False,
            text=None,
            execution_id=None,
            function_name=None,
            replica_id=None,
            system=False,
            view=True,
            until_completed=True,
            model_operation: str = None,
            ):
        """
        Get service logs

        :param service: service object
        :param int size: size
        :param dict checkpoint: the information from the lst point checked in the service
        :param str start: iso format time
        :param str end: iso format time
        :param bool follow: if true, keep stream future logs
        :param str text: text
        :param str execution_id: execution id
        :param str function_name: function name
        :param str replica_id: replica id
        :param bool system: system
        :param bool view: if true, print out all the logs
        :param bool until_completed: wait until completed
        :param str model_operation: model operation action
        :return: ServiceLog entity
        :rtype: ServiceLog

        **Example**:

        .. code-block:: python

            service_log = service.log()
        """
        return self.services.log(service=service,
                                 size=size,
                                 checkpoint=checkpoint,
                                 start=start,
                                 end=end,
                                 follow=follow,
                                 execution_id=execution_id,
                                 function_name=function_name,
                                 replica_id=replica_id,
                                 system=system,
                                 text=text,
                                 view=view,
                                 until_completed=until_completed,
                                 model_id=self.id,
                                 model_operation=model_operation,
                                 project_id=self.project_id)


================================================
File: dtlpy/entities/node.py
================================================
import warnings

import inspect
import json
import logging
import uuid
from typing import Callable
from enum import Enum
from typing import List
import datetime

from .. import entities, assets, repositories, PlatformException

NODE_SIZE = (200, 87)

logger = logging.getLogger(name='dtlpy')


class PipelineConnectionPort:
    def __init__(self, node_id: str, port_id: str):
        self.node_id = node_id
        self.port_id = port_id

    @staticmethod
    def from_json(_json: dict):
        return PipelineConnectionPort(
            node_id=_json.get('nodeId', None),
            port_id=_json.get('portId', None),
        )

    def to_json(self):
        _json = {
            'nodeId': self.node_id,
            'portId': self.port_id,
        }
        return _json


class PipelineConnection:
    def __init__(self,
                 source: PipelineConnectionPort,
                 target: PipelineConnectionPort,
                 filters: entities.Filters,
                 action: str = None
                 ):
        """
        :param PipelineConnectionPort source: the source pipeline connection
        :param PipelineConnectionPort target: the target pipeline connection
        :param entities.Filters filters: condition for the connection between the nodes
        :param str action: the action that move the input when it happen
        """
        self.source = source
        self.target = target
        self.filters = filters
        self.action = action

    @staticmethod
    def from_json(_json: dict):
        condition = _json.get('condition', None)
        if condition:
            condition = json.loads(condition)
        return PipelineConnection(
            source=PipelineConnectionPort.from_json(_json=_json.get('src', None)),
            target=PipelineConnectionPort.from_json(_json=_json.get('tgt', None)),
            filters=condition,
            action=_json.get('action', None)
        )

    def to_json(self):
        _json = {
            'src': self.source.to_json(),
            'tgt': self.target.to_json(),
        }
        if self.action:
            _json['action'] = self.action
        if self.filters:
            if isinstance(self.filters, entities.Filters):
                filters = self.filters.prepare(query_only=True).get('filter', dict())
            else:
                filters = self.filters

            _json['condition'] = json.dumps(filters)
        return _json


class PipelineNodeIO:
    def __init__(self,
                 input_type: entities.PackageInputType,
                 name: str,
                 display_name: str,
                 port_id: str = None,
                 color: tuple = None,
                 port_percentage: int = None,
                 action: str = None,
                 default_value=None,
                 variable_name: str = None,
                 actions: list = None,
                 description: str = None):
        """
        Pipeline Node

        :param entities.PackageInputType input_type: entities.PackageInputType of the input type of the pipeline
        :param str name: name of the input
        :param str display_name: of the input
        :param str port_id: port id
        :param tuple color: tuple the display the color
        :param int port_percentage: port percentage
        :param str action: the action that move the input when it happen
        :param default_value: default value of the input
        :param list actions: the actions list that move the input when it happen
        """
        self.port_id = port_id if port_id else str(uuid.uuid4())
        self.input_type = input_type
        self.name = name
        self.color = color
        self.display_name = display_name
        self.port_percentage = port_percentage
        self.default_value = default_value
        self.variable_name = variable_name
        self.description = description

        if action is not None:
            warnings.warn('action param has been deprecated in version 1.95', DeprecationWarning)
            if actions is None:
                actions = []
            actions.append(action)
        self.actions = actions

    @property
    def action(self):
        warnings.warn('action attribute has been deprecated in version 1.95', DeprecationWarning)
        return None

    @staticmethod
    def from_json(_json: dict):
        return PipelineNodeIO(
            port_id=_json.get('portId', None),
            input_type=_json.get('type', None),
            name=_json.get('name', None),
            color=_json.get('color', None),
            display_name=_json.get('displayName', None),
            port_percentage=_json.get('portPercentage', None),
            default_value=_json.get('defaultValue', None),
            variable_name=_json.get('variableName', None),
            actions=_json.get('actions', None),
            description=_json.get('description', None),
        )

    def to_json(self):
        _json = {
            'portId': self.port_id,
            'type': self.input_type,
            'name': self.name,
            'color': self.color,
            'displayName': self.display_name,
            'variableName': self.variable_name,
            'portPercentage': self.port_percentage,
        }

        if self.actions:
            _json['actions'] = self.actions
        if self.default_value:
            _json['defaultValue'] = self.default_value
        if self.description:
            _json['description'] = self.description
        return _json


class PipelineNodeType(str, Enum):
    TASK = 'task'
    CODE = 'code'
    FUNCTION = 'function'
    STORAGE = 'storage'
    ML = 'ml'


class PipelineNameSpace:
    def __init__(self, function_name, project_name, module_name=None, service_name=None, package_name=None):
        self.function_name = function_name
        self.project_name = project_name
        self.module_name = module_name
        self.service_name = service_name
        self.package_name = package_name

    def to_json(self):
        _json = {
            "functionName": self.function_name,
            "projectName": self.project_name
        }
        if self.module_name:
            _json['moduleName'] = self.module_name

        if self.service_name:
            _json['serviceName'] = self.service_name

        if self.package_name:
            _json['packageName'] = self.package_name
        return _json

    @staticmethod
    def from_json(_json: dict):
        return PipelineNameSpace(
            function_name=_json.get('functionName'),
            project_name=_json.get('projectName'),
            module_name=_json.get('moduleName', None),
            service_name=_json.get('serviceName', None),
            package_name=_json.get('packageName', None)
        )


class PipelineNode:
    def __init__(self,
                 name: str,
                 node_id: str,
                 outputs: list,
                 inputs: list,
                 node_type: PipelineNodeType,
                 namespace: PipelineNameSpace,
                 project_id: str,
                 metadata: dict = None,
                 config: dict = None,
                 position: tuple = (1, 1),
                 app_id: str = None,
                 dpk_name: str = None,
                 app_name: str = None,
                 ):
        """
        :param str name: node name
        :param str node_id: node id
        :param list outputs: list of PipelineNodeIO outputs
        :param list inputs: list of PipelineNodeIO inputs
        :param dict metadata: dict of the metadata of the node
        :param PipelineNodeType node_type: task, code, function
        :param PipelineNameSpace namespace: PipelineNameSpace of the node space
        :param str project_id: project id
        :param dict config: for the code node dict in format { package: {code : the_code}}
        :param tuple position: tuple of the node place
        :param str app_id: app id
        :param str dpk_name: dpk name
        :param str app_name: app name
        """
        self.name = name
        self.node_id = node_id
        self.outputs = outputs
        self.inputs = inputs
        self.metadata = metadata if metadata is not None else {}
        self.node_type = node_type
        self.namespace = namespace
        self.project_id = project_id
        self.config = config
        self.position = position
        self.app_id = app_id
        self.dpk_name = dpk_name
        self.app_name = app_name
        self._pipeline = None

    @property
    def position(self):
        position_tuple = (self.metadata['position']['x'],
                          self.metadata['position']['y'])
        return position_tuple

    @position.setter
    def position(self, position):
        self.metadata['position'] = \
            {
                "x": position[0] * 1.7 * NODE_SIZE[0] + NODE_SIZE[0] / 2,
                "y": position[1] * 1.5 * NODE_SIZE[1] + NODE_SIZE[1],
                "z": 0
            }

    def _default_io(self, actions: list = None) -> PipelineNodeIO:
        """
        Create a default item pipeline input

        :param str actions:  the action that move the input when it happen
        :return PipelineNodeIO: the default item PipelineNodeIO
        """
        default_io = PipelineNodeIO(port_id=str(uuid.uuid4()),
                                    input_type=entities.PackageInputType.ITEM,
                                    name='item',
                                    color=None,
                                    display_name=actions[0] if actions else 'item',
                                    actions=actions)
        return default_io

    @staticmethod
    def from_json(_json: dict):
        inputs = [PipelineNodeIO.from_json(_json=i_input) for i_input in _json.get('inputs', list())]
        outputs = [PipelineNodeIO.from_json(_json=i_output) for i_output in _json.get('outputs', list())]
        namespace = PipelineNameSpace.from_json(_json.get('namespace', {}))
        metadata = _json.get('metadata', {})
        position = ((metadata['position']['x'] - NODE_SIZE[0] / 2) / (1.7 * NODE_SIZE[0]),
                    (metadata['position']['y'] - NODE_SIZE[1]) / (1.5 * NODE_SIZE[1]))
        return PipelineNode(
            name=_json.get('name', None),
            node_id=_json.get('id', None),
            outputs=outputs,
            inputs=inputs,
            metadata=metadata,
            node_type=_json.get('type', None),
            namespace=namespace,
            project_id=_json.get('projectId', None),
            config=_json.get('config', None),
            position=position,
            app_id=_json.get('appId', None),
            dpk_name=_json.get('dpkName', None),
            app_name=_json.get('appName', None),
        )

    def to_json(self):
        _json = {
            'name': self.name,
            'id': self.node_id,
            'outputs': [_io.to_json() for _io in self.outputs],
            'inputs': [_io.to_json() for _io in self.inputs],
            'metadata': self.metadata,
            'type': self.node_type,
            'namespace': self.namespace.to_json(),
            'projectId': self.project_id,
            'dpkName': self.dpk_name,
            'appName': self.app_name,
        }
        if self.config is not None:
            _json['config'] = self.config
        if self.app_id is not None:
            _json['appId'] = self.app_id
        return _json

    def is_root(self):
        if self._pipeline is not None:
            for node in self._pipeline.start_nodes:
                if self.node_id == node.get('nodeId', None) and node.get('type', None) == 'root':
                    return True
        return False

    def _build_connection(self,
                          node,
                          source_port: PipelineNodeIO = None,
                          target_port: PipelineNodeIO = None,
                          filters: entities.Filters = None,
                          action: str = None) -> PipelineConnection:
        """
        Build connection between the current node and the target node use the given ports

        :param PipelineNode node: the node to connect to it
        :param PipelineNodeIO source_port: the source PipelineNodeIO input port
        :param PipelineNodeIO target_port: the target PipelineNodeIO output port
        :param entities.Filters filters: condition for the connection between the nodes
        :param str action:  the action that move the input when it happen
        :return: the connection between the nodes
        """
        if source_port is None and self.outputs:
            source_port = self.outputs[0]

        if target_port is None and node.inputs:
            target_port = node.inputs[0]

        if node.is_root():
            self._pipeline.set_start_node(self)

        source_connection = PipelineConnectionPort(node_id=self.node_id, port_id=source_port.port_id)
        target_connection = PipelineConnectionPort(node_id=node.node_id, port_id=target_port.port_id)
        if action is None and source_port.actions is not None and source_port.actions != []:
            action = source_port.actions[0]
        connection = PipelineConnection(source=source_connection, target=target_connection, filters=filters,
                                        action=action)
        return connection

    def connect(self,
                node,
                source_port: PipelineNodeIO = None,
                target_port: PipelineNodeIO = None,
                filters=None,
                action: str = None):
        """
        Build connection between the current node and the target node use the given ports

        :param PipelineNode node: the node to connect to it
        :param PipelineNodeIO source_port: the source PipelineNodeIO input port
        :param PipelineNodeIO target_port: the target PipelineNodeIO output port
        :param entities.Filters filters: condition for the connection between the nodes
        :param str action:  the action that move the input when it happen
        :return: the connected node
        """
        if self._pipeline is None:
            raise Exception("must add the node to the pipeline first, e.g pipeline.nodes.add(node)")
        connection = self._build_connection(node=node,
                                            source_port=source_port,
                                            target_port=target_port,
                                            filters=filters,
                                            action=action)
        self._pipeline.connections.append(connection)
        self._pipeline.nodes.add(node)
        return node

    def disconnect(self,
                   node,
                   source_port: PipelineNodeIO = None,
                   target_port: PipelineNodeIO = None) -> bool:
        """
        remove connection between the current node and the target node use the given ports

        :param PipelineNode node: the node to connect to it
        :param PipelineNodeIO source_port: the source PipelineNodeIO input port
        :param PipelineNodeIO target_port: the target PipelineNodeIO output port
        :return: true if success and false if not
        """
        if self._pipeline is None:
            raise Exception("must add the node to the pipeline first, e.g pipeline.nodes.add(node)")
        connection = self._build_connection(node=node,
                                            source_port=source_port,
                                            target_port=target_port,
                                            filters=None)

        current_connection = connection.to_json()
        if 'condition' in current_connection:
            current_connection = current_connection.pop('condition')

        for connection_index in range(len(self._pipeline.connections)):
            pipeline_connection = self._pipeline.connections[connection_index].to_json()
            if 'condition' in pipeline_connection:
                pipeline_connection = pipeline_connection.pop('condition')

            if current_connection == pipeline_connection:
                self._pipeline.connections.pop(connection_index)
                return True
        logger.warning('do not found a connection')
        return False

    def add_trigger(self,
                    trigger_type: entities.TriggerType = entities.TriggerType.EVENT,
                    filters=None,
                    resource: entities.TriggerResource = entities.TriggerResource.ITEM,
                    actions: entities.TriggerAction = entities.TriggerAction.CREATED,
                    execution_mode: entities.TriggerExecutionMode = entities.TriggerExecutionMode.ONCE,
                    cron: str = None,
                    ):
        """
        Create a Trigger. Can create two types: a cron trigger or an event trigger.
        Inputs are different for each type

        Inputs for all types:

        :param trigger_type: can be cron or event. use enum dl.TriggerType for the full list

        Inputs for event trigger:
        :param filters: optional - Item/Annotation metadata filters, default = none
        :param resource: optional - Dataset/Item/Annotation/ItemStatus, default = Item
        :param actions: optional - Created/Updated/Deleted, default = create
        :param execution_mode: how many time trigger should be activate. default is "Once". enum dl.TriggerExecutionMode

        Inputs for cron trigger:
        :param str cron: cron spec specifying when it should run. more information: https://en.wikipedia.org/wiki/Cron

        :return: Trigger entity
        """
        if self._pipeline is None:
            raise Exception("must add the node to the pipeline first, e.g pipeline.nodes.add(node)")

        if not isinstance(actions, list):
            actions = [actions]

        if filters is None:
            filters = {}
        else:
            filters = json.dumps(filters.prepare(query_only=True).get('filter', dict()))

        if trigger_type == entities.TriggerType.EVENT:
            spec = {
                'filter': filters,
                'resource': resource,
                'executionMode': execution_mode,
                'actions': actions
            }
        elif trigger_type == entities.TriggerType.CRON:
            spec = {
                'cron': cron,
            }
        else:
            raise ValueError('Unknown trigger type: "{}". Use dl.TriggerType for known types'.format(trigger_type))

        trigger = {
            "type": trigger_type,
            "spec": spec
        }

        set_trigger = False
        for pipe_node in self._pipeline.start_nodes:
            if pipe_node['nodeId'] == self.node_id:
                set_trigger = True
                pipe_node['trigger'] = trigger

        if not set_trigger:
            self._pipeline.start_nodes.append(
                {
                    "nodeId": self.node_id,
                    "type": "trigger",
                    'trigger': trigger
                }
            )


class CodeNode(PipelineNode):
    def __init__(self,
                 name: str,
                 project_id: str,
                 project_name: str,
                 method: Callable,
                 outputs: List[PipelineNodeIO] = None,
                 inputs: List[PipelineNodeIO] = None,
                 position: tuple = (1, 1),
                 ):
        """
        :param str name: node name
        :param str project_id: project id
        :param str project_name: project name
        :param Callable method: function to deploy
        :param list outputs: list of PipelineNodeIO outputs
        :param list inputs: list of PipelineNodeIO inputs
        :param tuple position: tuple of the node place
        """
        if inputs is None:
            inputs = [self._default_io()]
        if outputs is None:
            outputs = [self._default_io()]

        if method is None or not isinstance(method, Callable):
            raise Exception('must provide a function as input')
        else:
            function_code = self._build_code_from_func(method)
            function_name = method.__name__

        super().__init__(name=name,
                         node_id=str(uuid.uuid4()),
                         outputs=outputs,
                         inputs=inputs,
                         metadata={},
                         node_type=PipelineNodeType.CODE,
                         namespace=PipelineNameSpace(function_name=function_name, project_name=project_name),
                         project_id=project_id,
                         position=position)

        self.config = {
            "package":
                {
                    "code": function_code,
                    "name": function_name,
                    "type": "code"
                }
        }

    def _build_code_from_func(self, func: Callable) -> str:
        """
        Build a code format from the given function

        :param Callable func: function to deploy
        :return: a string the display the code with the package format
        """
        with open(assets.paths.PARTIAL_MAIN_FILEPATH, 'r') as f:
            main_string = f.read()
        lines = inspect.getsourcelines(func)

        tabs_diff = lines[0][0].count('    ') - 1
        for line_index in range(len(lines[0])):
            line_tabs = lines[0][line_index].count('    ') - tabs_diff
            lines[0][line_index] = ('    ' * line_tabs) + lines[0][line_index].strip() + '\n'

        method_func_string = "".join(lines[0])

        code = '{}\n{}\n    @staticmethod\n{}'.format('', main_string,
                                                      method_func_string)
        return code

    @staticmethod
    def from_json(_json: dict):
        parent = PipelineNode.from_json(_json)
        parent.__class__ = CodeNode
        return parent


class TaskNode(PipelineNode):
    def __init__(self,
                 name: str,
                 project_id: str,
                 dataset_id: str,
                 recipe_title: str,
                 recipe_id: str,
                 task_owner: str,
                 workload: List[entities.WorkloadUnit],
                 task_type: str = 'annotation',
                 position: tuple = (1, 1),
                 actions: list = None,
                 repeatable: bool = True,
                 batch_size=None,
                 max_batch_workload=None,
                 priority=entities.TaskPriority.MEDIUM,
                 due_date=None,
                 consensus_task_type=None,
                 consensus_percentage=None,
                 consensus_assignees=None,
                 groups=None
                 ):
        """
        :param str name: node name
        :param str project_id: project id
        :param str dataset_id: dataset id
        :param str recipe_title: recipe title
        :param str recipe_id: recipe id
        :param str task_owner: email of task owner
        :param List[WorkloadUnit] workload: list of WorkloadUnit
        :param str task_type: 'annotation' or 'qa'
        :param tuple position: tuple of the node place
        :param list actions: list of task actions
        :param bool repeatable: can repeat in the item
        :param int groups: groups to assign the task to
        :param int batch_size: Pulling batch size (items) . Restrictions - Min 3, max 100 - for create pulling task
        :param int max_batch_workload: Max items in assignment . Restrictions - Min batchSize + 2 , max batchSize * 2 - for create pulling task
        :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
        :param float due_date: date by which the task should be finished; for example, due_date = datetime.datetime(day= 1, month= 1, year= 2029).timestamp()
        :param entities.ConsensusTaskType consensus_task_type: consensus_task_type of the task options in entities.ConsensusTaskType
        :param int consensus_percentage: percentage of items to be copied to multiple annotators (consensus items)
        :param int consensus_assignees: the number of different annotators per item (number of copies per item)
        """
        if actions is None or actions == []:
            actions = []
            if task_type == 'qa':
                if 'approve' not in actions:
                    actions.append('approve')
            else:
                if 'complete' not in actions:
                    actions.append('complete')
            actions.append('discard')
        else:
            logger.warning(
                "The 'actions' field was updated to override the system default actions for task (complete/approve, discard) if provided, due to a bug fix.")

        inputs = [self._default_io()]

        outputs = [self._default_io(actions=actions)]

        if groups is not None:
            if not isinstance(groups, list) or not all(isinstance(group, str) for group in groups):
                raise ValueError('groups must be a list of strings')

        super().__init__(name=name,
                         node_id=str(uuid.uuid4()),
                         outputs=outputs,
                         inputs=inputs,
                         metadata=dict(),
                         node_type=PipelineNodeType.TASK,
                         namespace=PipelineNameSpace(function_name="move_to_task",
                                                     project_name="DataloopTasks",
                                                     service_name="pipeline-utils"),
                         project_id=project_id,
                         position=position)

        self.dataset_id = dataset_id
        self.recipe_title = recipe_title
        self.recipe_id = recipe_id
        self.task_owner = task_owner
        self.task_type = task_type
        if not isinstance(workload, list):
            workload = [workload]
        self.workload = workload
        self.repeatable = repeatable
        if max_batch_workload:
            self.max_batch_workload = max_batch_workload
        if batch_size:
            self.batch_size = batch_size
        if consensus_task_type:
            self.consensus_task_type = consensus_task_type
        if consensus_percentage:
            self.consensus_percentage = consensus_percentage
        if consensus_assignees:
            self.consensus_assignees = consensus_assignees
        self.priority = priority
        if due_date is None:
            due_date = (datetime.datetime.now() + datetime.timedelta(days=7)).timestamp() * 1000
        self.due_date = due_date
        self.groups = groups

    @property
    def dataset_id(self):
        return self.metadata['datasetId']

    @dataset_id.setter
    def dataset_id(self, dataset_id: str):
        if not isinstance(dataset_id, str):
            raise PlatformException('400', 'Param dataset_id must be of type string')
        self.metadata['datasetId'] = dataset_id

    @property
    def groups(self):
        return self.metadata.get('groups')

    @groups.setter
    def groups(self, groups: List[str]):
        if groups is not None:
            self.metadata['groups'] = groups

    @property
    def repeatable(self):
        return self.metadata['repeatable']

    @repeatable.setter
    def repeatable(self, repeatable: bool):
        if not isinstance(repeatable, bool):
            raise PlatformException('400', 'Param repeatable must be of type bool')
        self.metadata['repeatable'] = repeatable

    @property
    def recipe_title(self):
        return self.metadata['recipeTitle']

    @recipe_title.setter
    def recipe_title(self, recipe_title: str):
        if not isinstance(recipe_title, str):
            raise PlatformException('400', 'Param recipe_title must be of type string')
        self.metadata['recipeTitle'] = recipe_title

    @property
    def recipe_id(self):
        return self.metadata['recipeId']

    @recipe_id.setter
    def recipe_id(self, recipe_id: str):
        if not isinstance(recipe_id, str):
            raise PlatformException('400', 'Param recipe_id must be of type string')
        self.metadata['recipeId'] = recipe_id

    @property
    def task_owner(self):
        return self.metadata['taskOwner']

    @task_owner.setter
    def task_owner(self, task_owner: str):
        if not isinstance(task_owner, str):
            raise PlatformException('400', 'Param task_owner must be of type string')
        self.metadata['taskOwner'] = task_owner

    @property
    def task_type(self):
        return self.metadata['taskType']

    @task_type.setter
    def task_type(self, task_type: str):
        if not isinstance(task_type, str):
            raise PlatformException('400', 'Param task_type must be of type string')
        self.metadata['taskType'] = task_type

    @property
    def workload(self):
        return self.metadata['workload']

    @workload.setter
    def workload(self, workload: list):
        if not isinstance(workload, list):
            workload = [workload]
        self.metadata['workload'] = [val.to_json() for val in workload]

    @property
    def batch_size(self):
        return self.metadata['batchSize']

    @batch_size.setter
    def batch_size(self, batch_size: int):
        if not isinstance(batch_size, int):
            raise PlatformException('400', 'Param batch_size must be of type int')
        self.metadata['batchSize'] = batch_size

    @property
    def max_batch_workload(self):
        return self.metadata['maxBatchWorkload']

    @max_batch_workload.setter
    def max_batch_workload(self, max_batch_workload: int):
        if not isinstance(max_batch_workload, int):
            raise PlatformException('400', 'Param max_batch_workload must be of type int')
        self.metadata['maxBatchWorkload'] = max_batch_workload

    @property
    def consensus_task_type(self):
        return self.metadata['consensusTaskType']

    @consensus_task_type.setter
    def consensus_task_type(self, consensus_task_type: entities.ConsensusTaskType):
        if not isinstance(consensus_task_type, str) and not isinstance(consensus_task_type, entities.ConsensusTaskType):
            raise PlatformException('400', 'Param consensus_task_type must be of type entities.ConsensusTaskType')
        self.metadata['consensusTaskType'] = consensus_task_type

    @property
    def consensus_percentage(self):
        return self.metadata['consensusPercentage']

    @consensus_percentage.setter
    def consensus_percentage(self, consensus_percentage: int):
        if not isinstance(consensus_percentage, int):
            raise PlatformException('400', 'Param consensus_percentage must be of type int')
        self.metadata['consensusPercentage'] = consensus_percentage

    @property
    def consensus_assignees(self):
        return self.metadata['consensusAssignees']

    @consensus_assignees.setter
    def consensus_assignees(self, consensus_assignees: int):
        if not isinstance(consensus_assignees, int):
            raise PlatformException('400', 'Param consensus_assignees must be of type int')
        self.metadata['consensusAssignees'] = consensus_assignees

    @property
    def priority(self):
        return self.metadata['priority']

    @priority.setter
    def priority(self, priority: entities.TaskPriority):
        if not isinstance(priority, int) and not isinstance(priority, entities.TaskPriority):
            raise PlatformException('400', 'Param priority must be of type entities.TaskPriority')
        self.metadata['priority'] = priority

    @property
    def due_date(self):
        return self.metadata['dueDate']

    @due_date.setter
    def due_date(self, due_date: float):
        if not isinstance(due_date, float) and not isinstance(due_date, int):
            raise PlatformException('400', 'Param due_date must be of type float or int')
        self.metadata['dueDate'] = due_date

    @staticmethod
    def from_json(_json: dict):
        parent = PipelineNode.from_json(_json)
        parent.__class__ = TaskNode
        return parent


class FunctionNode(PipelineNode):
    def __init__(self,
                 name: str,
                 service: entities.Service,
                 function_name,
                 position: tuple = (1, 1),
                 project_id=None,
                 project_name=None
                 ):
        """
        :param str name: node name
        :param entities.Service service: service to deploy
        :param str function_name: function name
        :param tuple position: tuple of the node place
        """
        self.service = service

        if project_id is None:
            project_id = service.project_id
        if project_id != service.project_id:
            logger.warning("the project id that provide different from the service project id")

        if project_name is None:
            try:
                project = repositories.Projects(client_api=self.service._client_api).get(project_id=project_id,
                                                                                         log_error=False)
                project_name = project.name
            except:
                logger.warning(
                    'Service project not found using DataloopTasks project.'
                    ' If this is incorrect please provide project_name param.')
                project_name = 'DataloopTasks'
        inputs = []
        outputs = []
        package = self.service.package
        modules = []
        if isinstance(package, entities.Package):
            modules = package.modules
        elif isinstance(package, entities.Dpk):
            modules = package.components.modules
        for model in modules:
            if model.name == self.service.module_name:
                for func in model.functions:
                    if func.name == function_name:
                        inputs = self._convert_from_function_io_to_pipeline_io(func.inputs)
                        outputs = self._convert_from_function_io_to_pipeline_io(func.outputs)

        namespace = PipelineNameSpace(
            function_name=function_name,
            service_name=self.service.name,
            module_name=self.service.module_name,
            package_name=self.service.package.name,
            project_name=project_name
        )
        super().__init__(name=name,
                         node_id=str(uuid.uuid4()),
                         outputs=outputs,
                         inputs=inputs,
                         metadata={},
                         node_type=PipelineNodeType.FUNCTION,
                         namespace=namespace,
                         project_id=service.project_id,
                         position=position)

    def _convert_from_function_io_to_pipeline_io(self, function_io: List[entities.FunctionIO]) -> List[PipelineNodeIO]:
        """
        Get a list of FunctionIO and convert them to PipelineIO
        :param List[entities.FunctionIO] function_io: list of functionIO
        :return: list of PipelineIO
        """
        pipeline_io = []
        for single_input in function_io:
            pipeline_io.append(
                PipelineNodeIO(port_id=str(uuid.uuid4()),
                               input_type=single_input.type,
                               name=single_input.name,
                               color=None,
                               display_name=single_input.name,
                               default_value=single_input.value,
                               actions=single_input.actions if single_input.actions is not None else []))
        return pipeline_io

    @staticmethod
    def from_json(_json: dict):
        parent = PipelineNode.from_json(_json)
        parent.__class__ = FunctionNode
        return parent


class DatasetNode(PipelineNode):
    def __init__(self,
                 name: str,
                 project_id: str,
                 dataset_id: str,
                 dataset_folder: str = None,
                 load_existing_data: bool = False,
                 data_filters: entities.Filters = None,
                 position: tuple = (1, 1)):
        """
        :param str name: node name
        :param str project_id: project id
        :param str dataset_id: dataset id
        :param str dataset_folder: folder in dataset to work in it
        :param bool load_existing_data: optional - enable to automatically load existing data into the
                                        pipeline (executions) upon activation, based on the defined dataset,
                                        folder, and data_filters.
        :param entities.Filters data_filters: optional - filters entity or a dictionary containing filters parameters.
                                              Use to filter the data items to be loaded when load_existing_data
                                              is enabled.
        :param tuple position: tuple of the node place
        """
        inputs = [self._default_io()]
        outputs = [self._default_io()]
        super().__init__(name=name,
                         node_id=str(uuid.uuid4()),
                         outputs=outputs,
                         inputs=inputs,
                         metadata={},
                         node_type=PipelineNodeType.STORAGE,
                         namespace=PipelineNameSpace(function_name="dataset_handler",
                                                     project_name="DataloopTasks",
                                                     service_name="pipeline-utils"),
                         project_id=project_id,
                         position=position)
        self.dataset_id = dataset_id
        self.dataset_folder = dataset_folder
        self.load_existing_data = load_existing_data
        self.data_filters = data_filters

    @property
    def dataset_id(self):
        return self.metadata['datasetId']

    @dataset_id.setter
    def dataset_id(self, dataset_id: str):
        self.metadata['datasetId'] = dataset_id

    @property
    def dataset_folder(self):
        return self.metadata.get('dir', None)

    @dataset_folder.setter
    def dataset_folder(self, dataset_folder: str):
        if dataset_folder is not None:
            if not dataset_folder.startswith("/"):
                dataset_folder = '/' + dataset_folder
            self.metadata['dir'] = dataset_folder

    @property
    def load_existing_data(self):
        return self.metadata.get('triggerToPipeline', {}).get('active', False)

    @load_existing_data.setter
    def load_existing_data(self, load_existing_data: bool):
        if load_existing_data:
            self.metadata.setdefault('triggerToPipeline', {})['active'] = True
        else:
            self.metadata.pop('triggerToPipeline', None)

    @property
    def data_filters(self):
        data_filters = self.metadata.get('triggerToPipeline', {}).get('filter', None)
        if data_filters:
            data_filters = entities.Filters(custom_filter=json.loads(data_filters))
        return data_filters

    @data_filters.setter
    def data_filters(self, data_filters: entities.Filters):
        if data_filters is None:
            filters = None
        else:
            filters = json.dumps(data_filters.prepare(query_only=True).get('filter'))
        self.metadata.setdefault('triggerToPipeline', {})['filter'] = filters

    @staticmethod
    def from_json(_json: dict):
        parent = PipelineNode.from_json(_json)
        parent.__class__ = DatasetNode
        return parent


================================================
File: dtlpy/entities/ontology.py
================================================
from collections import namedtuple
import traceback
import logging
import random
import uuid
import attr
import os

from .. import entities, PlatformException, repositories, exceptions
from ..services.api_client import ApiClient
from .label import Label

logger = logging.getLogger(name='dtlpy')


class AttributesTypes:
    CHECKBOX = "checkbox"
    RADIO_BUTTON = "radio_button"
    SLIDER = "range"
    YES_NO = "boolean"
    FREE_TEXT = "freeText"


class AttributesRange:
    def __init__(self, min_range, max_range, step):
        self.min_range = min_range
        self.max_range = max_range
        self.step = step

    def to_json(self):
        return {'min': self.min_range, 'max': self.max_range, 'step': self.step}


class LabelHandlerMode:
    ADD = "add"
    UPDATE = "update"
    UPSERT = "upsert"


@attr.s
class Ontology(entities.BaseEntity):
    """
    Ontology object
    """
    # api
    _client_api = attr.ib(type=ApiClient, repr=False)

    # params
    id = attr.ib()
    creator = attr.ib()
    url = attr.ib(repr=False)
    title = attr.ib()
    labels = attr.ib(repr=False)
    metadata = attr.ib(repr=False)
    attributes = attr.ib()

    # entities
    _recipe = attr.ib(repr=False, default=None)
    _dataset = attr.ib(repr=False, default=None)
    _project = attr.ib(repr=False, default=None)

    # repositories
    _repositories = attr.ib(repr=False)

    # defaults
    _instance_map = attr.ib(default=None, repr=False)
    _color_map = attr.ib(default=None, repr=False)

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['ontologies', 'datasets', 'projects'])

        if self._recipe is None:
            ontologies = repositories.Ontologies(client_api=self._client_api, recipe=self._recipe)
        else:
            ontologies = self.recipe.ontologies

        r = reps(ontologies=ontologies, datasets=repositories.Datasets(client_api=self._client_api),
                 projects=repositories.Projects(client_api=self._client_api))
        return r

    @property
    def recipe(self):
        if self._recipe is not None:
            assert isinstance(self._recipe, entities.Recipe)
        return self._recipe

    @property
    def dataset(self):
        if self._dataset is None:
            if self.recipe is not None:
                self._dataset = self.recipe.dataset
        if self._dataset is not None:
            assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @property
    def project(self):
        if self._project is None:
            if 'system' in self.metadata:
                project_id = self.metadata['system'].get('projectIds', None)
                if project_id is not None:
                    self._project = self.projects.get(project_id=project_id[0])
            elif self.dataset is not None:
                self._project = self.dataset.project
        if self._project is not None:
            assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def ontologies(self):
        if self._repositories.ontologies is not None:
            assert isinstance(self._repositories.ontologies, repositories.Ontologies)
        return self._repositories.ontologies

    @property
    def projects(self):
        if self._repositories.projects is not None:
            assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def labels_flat_dict(self):
        flatten_dict = dict()

        def add_to_dict(tag, father):
            flatten_dict[tag] = father
            for child in father.children:
                add_to_dict('{}.{}'.format(tag, child.tag), child)

        for label in self.labels:
            add_to_dict(label.tag, label)
        return flatten_dict

    @property
    def instance_map(self):
        """
         instance mapping for creating instance mask

        :return dictionary {label: map_id}
        :rtype: dict
        """
        if self._instance_map is None:
            labels = [label for label in self.labels_flat_dict]
            labels.sort()
            # each label gets index as instance id
            self._instance_map = {label: (i_label + 1) for i_label, label in enumerate(labels)}
        return self._instance_map

    @instance_map.setter
    def instance_map(self, value: dict):
        """
        instance mapping for creating instance mask

        :param value: dictionary {label: map_id}
        :rtype: dict
        """
        if not isinstance(value, dict):
            raise ValueError('input must be a dictionary of {label_name: instance_id}')
        self._instance_map = value

    @property
    def color_map(self):
        """
        Color mapping of labels, {label: rgb}

        :return: dict
        :rtype: dict
        """
        if self._color_map is None:
            self._color_map = {k: v.rgb for k, v in self.labels_flat_dict.items()}
        return self._color_map

    @color_map.setter
    def color_map(self, values):
        """
        Color mapping of labels, {label: rgb}

        :param values: dict {label: rgb}
        :return:
        """
        if not isinstance(values, dict):
            raise ValueError('input must be a dict. got: {}'.format(type(values)))
        self._color_map = values

    @staticmethod
    def _protected_from_json(_json, client_api, recipe=None, dataset=None, project=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error
        :param _json: platform json
        :param client_api: ApiClient entity
        :return:
        """
        try:
            ontology = Ontology.from_json(_json=_json,
                                          client_api=client_api,
                                          project=project,
                                          dataset=dataset,
                                          recipe=recipe,
                                          is_fetched=is_fetched)
            status = True
        except Exception:
            ontology = traceback.format_exc()
            status = False
        return status, ontology

    @property
    def _use_attributes_2(self):
        if isinstance(self.metadata, dict):
            attributes = self.metadata.get("attributes", None)
            if attributes is not None:
                return True
            else:
                if isinstance(self.attributes, list) and len(self.attributes) > 0:
                    return False
        return True

    @classmethod
    def from_json(cls, _json, client_api, recipe=None, dataset=None, project=None, is_fetched=True):
        """
        Build an Ontology entity object from a json

        :param bool is_fetched: is Entity fetched from Platform
        :param dtlpy.entities.project.Project project: project entity
        :param dtlpy.entities.dataset.Dataset dataset: dataset
        :param dict _json: _json response from host
        :param dtlpy.entities.recipe.Recipe recipe: ontology's recipe
        :param dl.ApiClient client_api: ApiClient entity
        :return: Ontology object
        :rtype: dtlpy.entities.ontology.Ontology
        """
        attributes_v2 = _json.get('metadata', {}).get("attributes", [])
        attributes_v1 = _json.get("attributes", [])
        attributes = attributes_v2 if attributes_v2 else attributes_v1

        labels = list()
        for root in _json["roots"]:
            labels.append(entities.Label.from_root(root=root))

        inst = cls(
            metadata=_json.get("metadata", None),
            creator=_json.get("creator", None),
            url=_json.get("url", None),
            id=_json["id"],
            title=_json.get("title", None),
            attributes=attributes,
            client_api=client_api,
            project=project,
            dataset=dataset,
            recipe=recipe,
            labels=labels,
        )

        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        roots = [label.to_root() for label in self.labels]
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Ontology)._client_api,
                                                              attr.fields(Ontology)._recipe,
                                                              attr.fields(Ontology)._project,
                                                              attr.fields(Ontology)._dataset,
                                                              attr.fields(Ontology)._instance_map,
                                                              attr.fields(Ontology)._color_map,
                                                              attr.fields(Ontology)._repositories))
        _json["roots"] = roots
        return _json

    def delete(self):
        """
        Delete recipe from platform

        :return: True
        """
        return self.ontologies.delete(self.id)

    def update(self, system_metadata=False):
        """
        Update items metadata

        :param bool system_metadata: bool - True, if you want to change metadata system
        :return: Ontology object
        """
        return self.ontologies.update(self, system_metadata=system_metadata)

    def _add_children(self, label_name, children, labels_node, mode):
        for child in children:
            if not isinstance(child, entities.Label):
                if isinstance(child, dict):
                    if "label_name" in child:
                        child = dict(child)
                        child["label_name"] = "{}.{}".format(label_name, child["label_name"])
                        labels_node += self._base_labels_handler(labels=[child], update_ontology=False, mode=mode)
                    else:
                        raise PlatformException("400",
                                                "Invalid parameters - child list must have label name attribute")
                else:
                    raise PlatformException("400", "Invalid parameters - child must be a dict type")
            else:
                child.tag = "{}.{}".format(label_name, child.tag)
                labels_node += self._base_labels_handler(labels=child, update_ontology=False, mode=mode)

        return labels_node

    def _labels_handler_update_mode(self, json_req, upsert=False, log_error=True):
        json_req['upsert'] = upsert
        success, response = self._client_api.gen_request(req_type="PATCH",
                                                         path="/ontologies/%s/labels" % self.id,
                                                         json_req=json_req,
                                                         log_error=log_error)
        if success:
            logger.debug("Labels {} has been added successfully".format(json_req))
        else:
            raise exceptions.PlatformException(response)
        return response

    def _labels_handler_add_mode(self, json_req):
        success, response = self._client_api.gen_request(req_type="PATCH",
                                                         path="/ontologies/%s/addLabels" % self.id,
                                                         json_req=json_req)
        if success:
            logger.debug("Labels {} has been added successfully".format(json_req))
        else:
            raise exceptions.PlatformException(response)
        return response

    def _base_labels_handler(self, labels, update_ontology=True, mode=LabelHandlerMode.UPSERT):
        """
        Add a single label to ontology using add label endpoint , nested label is also supported

        :param labels = list of labels
        :param update_ontology - return json_req if False
        :param mode add, update or upsert, relevant on update_ontology=True only
        :return: Ontology updated entire label entity
        """
        labels_node = list()
        if mode not in [LabelHandlerMode.ADD,
                        LabelHandlerMode.UPDATE,
                        LabelHandlerMode.UPSERT]:
            raise ValueError('mode must be on of: "add", "update", "upsert"')

        if not isinstance(labels, list):  # for case that add label get one label
            labels = [labels]

        for label in labels:
            if isinstance(label, str):
                # Generate label from string
                label = entities.Label(tag=label)
            elif isinstance(label, dict):
                # Generate label from dict
                label = Label.from_root(label)
            elif isinstance(label, entities.Label):
                ...
            else:
                raise ValueError(
                    'Unsupported type for `labels`. Expected a list of (str, dict, dl.Label). Got: {}'.format(
                        type(label)))

            # label entity
            label_node = {"tag": label.tag}
            if label.color is not None:
                label_node["color"] = label.hex
            if label.attributes is not None:
                label_node["attributes"] = label.attributes
            if label.display_label is not None:
                label_node["displayLabel"] = label.display_label
            if label.display_data is not None:
                label_node["displayData"] = label.display_data
            labels_node.append(label_node)
            children = label.children
            self._add_children(label.tag, children, labels_node, mode=mode)

        if not update_ontology or not len(labels_node):
            return labels_node

        json_req = {
            "labelsNode": labels_node
        }

        if mode == LabelHandlerMode.UPDATE:
            response = self._labels_handler_update_mode(json_req)
        else:
            response = self._labels_handler_update_mode(json_req, upsert=True, log_error=False)

        added_label = list()
        if "roots" not in response.json():
            raise exceptions.PlatformException("error fetching updated labels from server")

        for root in response.json()["roots"]:  # to get all labels
            added_label.append(entities.Label.from_root(root=root))

        self.labels = added_label
        return added_label

    def _add_image_label(self, icon_path):
        display_data = dict()
        if self.project is not None:
            dataset = self.project.datasets._get_binaries_dataset()
        elif self.dataset is not None:
            dataset = self.dataset.project.datasets._get_binaries_dataset()
        else:
            raise ValueError('must have project or dataset to create with icon path')
        platform_path = "/.dataloop/ontologies/{}/labelDisplayImages/".format(self.id)
        basename = os.path.basename(icon_path)
        item = dataset.items.upload(local_path=icon_path,
                                    remote_path=platform_path,
                                    remote_name='{}-{}'.format(uuid.uuid4().hex, basename))
        display_data['displayImage'] = dict()
        display_data['displayImage']['itemId'] = item.id
        display_data['displayImage']['datasetId'] = item.dataset_id
        return display_data

    def _label_handler(self, label_name, color=None, children=None, attributes=None, display_label=None, label=None,
                       add=True, icon_path=None, update_ontology=False, mode=LabelHandlerMode.UPSERT):
        """
        Add a single label to ontology

        :param label_name: label name
        :param color: optional - if not given a random color will be selected
        :param children: optional - children
        :param attributes: optional - attributes
        :param display_label: optional - display_label
        :param label: label
        :param add:to add or not
        :param icon_path: path to image to be display on label
        :param update_ontology: update the ontology, default = False for backward compatible
        :param mode add, update or upsert, relevant on update_ontology=True only
        :return: Label entity
        """

        if update_ontology:
            if isinstance(label, entities.Label) or isinstance(label, str):
                return self._base_labels_handler(labels=label,
                                                 update_ontology=update_ontology,
                                                 mode=mode)
            else:
                display_data = dict()
                if icon_path is not None:
                    display_data = self._add_image_label(icon_path=icon_path)
                return self._base_labels_handler(labels={"tag": label_name,
                                                         "displayLabel": display_label,
                                                         "color": color,
                                                         "attributes": attributes,
                                                         "children": children,
                                                         "displayData": display_data
                                                         },
                                                 update_ontology=update_ontology,
                                                 mode=mode)

        if not isinstance(label, entities.Label):
            if "." in label_name:
                raise PlatformException("400",
                                        "Invalid parameters - nested label can work with update_ontology option only")

            if attributes is None:
                attributes = list()
            if not isinstance(attributes, list):
                attributes = [attributes]

            # get random color if none given
            if color is None:
                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))

            if children is None:
                children = list()
            if not isinstance(children, list):
                children = [children]

            # add children    
            added_children = list()
            for child in children:
                if not isinstance(child, entities.Label):
                    added_children.append(self._label_handler(**child, add=False))
                else:
                    added_children.append(child)

            if display_label is None:
                display_label = ""
                if len(label_name.split("_")) == 1:
                    display_label = label_name[0].upper() + label_name[1:]
                else:
                    for word in label_name.split("_"):
                        display_label += word[0].upper() + word[1:] + " "
                    display_label = display_label[0:-1]

            display_data = dict()
            if icon_path is not None:
                display_data = self._add_image_label(icon_path=icon_path)

            root = {
                "value": {
                    "tag": label_name,
                    "displayLabel": display_label,
                    "color": color,
                    "attributes": attributes,
                    "displayData": display_data
                },
                "children": list(),
            }
            added_label = entities.Label.from_root(root)
            added_label.children = added_children
        else:
            added_label = label
        if add and self._validate_label(added_label=added_label, mode=mode, color=color,
                                        children=children, attributes=attributes,
                                        display_label=display_label, display_data=icon_path):
            self.labels.append(added_label)
        self._base_labels_handler(labels=added_label, update_ontology=True, mode=mode)
        return added_label

    def _validate_label(self, added_label, mode=LabelHandlerMode.UPSERT, color=None, children=None, attributes=None,
                        display_label=None, display_data=None):
        """
        check if the label is exist
        """
        for i in range(len(self.labels)):
            if self.labels[i].tag == added_label.tag:
                if mode == LabelHandlerMode.UPDATE:
                    if color:
                        self.labels[i].color = added_label.color
                    if children:
                        self.labels[i].children = added_label.children
                    if attributes:
                        self.labels[i].attributes = added_label.attributes
                    if display_label:
                        self.labels[i].display_label = added_label.display_label
                    if display_data:
                        self.labels[i].display_data = added_label.display_data
                return False
        return True

    def _labels_handler(self, label_list, update_ontology=False, mode=LabelHandlerMode.UPSERT):
        """
        Adds a list of labels to ontology

        :param list label_list: a list of labels to add to the dataset's ontology. each value should be a dict, dl.Label or a string
                        if dictionary, should look like this: {"value": {"tag": "name of the label", "displayLabel": "display name on the platform",
                                            "color": "#hex value", "attributes": [attributes]}, "children": [children]}
        :param update_ontology: update the ontology, default = False for backward compatible
        :param mode add, update or upsert, relevant on update_ontology=True only
        :return: List of label entities added
        """
        if update_ontology:
            return self._base_labels_handler(labels=label_list, mode=mode)
        labels = list()
        for label in label_list:

            if isinstance(label, str):
                label = entities.Label(tag=label)

            if isinstance(label, entities.Label):
                # label entity
                labels.append(label)
            else:
                # dictionary
                labels.append(Label.from_root(label))
        added_labels = list()
        for label in labels:
            added_labels.append(self._label_handler(label.tag, label=label, update_ontology=update_ontology))

        return added_labels

    def delete_labels(self, label_names):
        """
        Delete labels from ontology

        :param label_names: label object/ label name / list of label objects / list of label names
        :return:
        """
        if not isinstance(label_names, list):
            label_names = [label_names]

        if isinstance(label_names[0], entities.Label):
            label_names = [label.tag for label in label_names]

        for label in label_names:
            self.__delete_label(label)

        self.update()

    def __delete_label(self, label_name):
        if label_name in self.instance_map.keys():
            labels = self.labels
            label_chain = label_name.split('.')
            while len(label_chain) > 1:
                label_name = label_chain.pop(0)
                for i_label, label in enumerate(labels):
                    if label.tag == label_name:
                        labels = labels[i_label].children
                        break
            label_name = label_chain[0]
            for i_label, label in enumerate(labels):
                if label.tag == label_name:
                    labels.pop(i_label)

    def add_label(self, label_name, color=None, children=None, attributes=None, display_label=None, label=None,
                  add=True, icon_path=None, update_ontology=False):
        """
        Add a single label to ontology

        :param str label_name: str - label name
        :param tuple color: color
        :param children: children (sub labels)
        :param list attributes: attributes
        :param str display_label: display_label
        :param dtlpy.entities.label.Label label: label
        :param bool add: to add or not
        :param str icon_path: path to image to be display on label
        :param bool update_ontology: update the ontology, default = False for backward compatible
        :return: Label entity
        :rtype: dtlpy.entities.label.Label

        **Example**:

        .. code-block:: python

            label = ontology.add_label(label_name='person', color=(34, 6, 231), attributes=['big', 'small'])
        """
        return self._label_handler(label_name=label_name, color=color, children=children, attributes=attributes,
                                   display_label=display_label, label=label, add=add, icon_path=icon_path,
                                   update_ontology=update_ontology)

    def add_labels(self, label_list, update_ontology=False):
        """
        Adds a list of labels to ontology

        :param list label_list: list of labels [{"value": {"tag": "tag", "displayLabel": "displayLabel",
                                            "color": "#color", "attributes": [attributes]}, "children": [children]}]
        :param bool update_ontology: update the ontology, default = False for backward compatible
        :return: List of label entities added

        **Example**:

        .. code-block:: python

            labels = ontology.add_labels(label_list=label_list)
        """
        self._labels_handler(label_list=label_list, update_ontology=update_ontology, mode=LabelHandlerMode.UPSERT)

    def update_label(self, label_name, color=None, children=None, attributes=None, display_label=None, label=None,
                     add=True, icon_path=None, upsert=False, update_ontology=False):
        """
        Update a single label to ontology

        :param str label_name: str - label name
        :param tuple color: color
        :param children: children (sub labels)
        :param list attributes: attributes
        :param str display_label: display_label
        :param dtlpy.entities.label.Label label: label
        :param bool add: to add or not
        :param str icon_path: path to image to be display on label
        :param bool update_ontology: update the ontology, default = False for backward compatible
        :param bool upsert: if True will add in case it does not existing
        :return: Label entity
        :rtype: dtlpy.entities.label.Label

        **Example**:

        .. code-block:: python

            label = ontology.update_label(label_name='person', color=(34, 6, 231), attributes=['big', 'small'])
        """
        if upsert:
            mode = LabelHandlerMode.UPSERT
        else:
            mode = LabelHandlerMode.UPDATE

        return self._label_handler(label_name=label_name, color=color, children=children,
                                   attributes=attributes, display_label=display_label, label=label,
                                   add=add, icon_path=icon_path, update_ontology=update_ontology, mode=mode)

    def update_labels(self, label_list, upsert=False, update_ontology=False):
        """
        Update a list of labels to ontology

        :param list label_list: list of labels [{"value": {"tag": "tag", "displayLabel": "displayLabel", "color": "#color", "attributes": [attributes]}, "children": [children]}]
        :param bool upsert: if True will add in case it does not existing
        :param bool update_ontology: update the ontology, default = False for backward compatible

        :return: List of label entities added

        **Example**:

        .. code-block:: python

            labels = ontology.update_labels(label_list=label_list)
        """

        if upsert:
            mode = LabelHandlerMode.UPSERT
        else:
            mode = LabelHandlerMode.UPDATE
        self._labels_handler(label_list=label_list, update_ontology=update_ontology, mode=mode)

    def update_attributes(self,
                          title: str,
                          key: str,
                          attribute_type,
                          scope: list = None,
                          optional: bool = None,
                          values: list = None,
                          attribute_range=None):
        """
        ADD a new attribute or update if exist

        :param str title: attribute title
        :param str key: the key of the attribute must br unique
        :param AttributesTypes attribute_type: dl.AttributesTypes your attribute type
        :param list scope: list of the labels or * for all labels
        :param bool optional: optional attribute
        :param list values: list of the attribute values ( for checkbox and radio button)
        :param dict or AttributesRange attribute_range: dl.AttributesRange object
        :return: true in success
        :rtype: bool
        """
        return self.ontologies.update_attributes(
            ontology_id=self.id,
            title=title,
            key=key,
            attribute_type=attribute_type,
            scope=scope,
            optional=optional,
            values=values,
            attribute_range=attribute_range)

    def delete_attributes(self, keys: list):
        """
        Delete a bulk of attributes

        :param list keys: Keys of attributes to delete
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = ontology.delete_attributes(['1'])
        """

        return self.ontologies.delete_attributes(ontology_id=self.id, keys=keys)

    def copy_from(self, ontology_json: dict):
        """
        Import ontology to the platform.\n
        Notice: only the following fields will be updated: `labels`, `attributes`, `instance_map` and `color_map`.

        :param dict ontology_json: The source ontology json to copy from
        :return: Ontology object: The updated ontology entity
        :rtype: dtlpy.entities.ontology.Ontology

        **Example**:

        .. code-block:: python

            ontology = ontology.import_ontology(ontology_json=ontology_json)
        """
        # TODO: Add support for import from ontology entity in the Future
        if not self._use_attributes_2:
            raise ValueError("This method is only supported for attributes 2 mode!")
        new_ontology = self.from_json(_json=ontology_json, client_api=self._client_api)

        # Update 'labels' and 'attributes'
        self.labels = new_ontology.labels
        new_attributes = new_ontology.attributes
        if isinstance(new_attributes, list):
            for new_attribute in new_attributes:
                attribute_range = new_attribute.get("range", None)
                if attribute_range is not None:
                    attribute_range = entities.AttributesRange(
                        min_range=attribute_range.get("min", None),
                        max_range=attribute_range.get("max", None),
                        step=attribute_range.get("step", None)
                    )
                script_data = new_attribute.get("scriptData", None)
                if script_data is None:
                    new_attribute_key = new_attribute.get("key", None)
                    raise Exception(f"Attribute '{new_attribute_key}' scriptData is missing in the ontology json!")
                self.update_attributes(
                    title=script_data.get("title", None),
                    key=new_attribute.get("key", None),
                    attribute_type=new_attribute.get("type", None),
                    scope=new_attribute.get("scope", None),
                    optional=script_data.get("optional", None),
                    values=new_attribute.get("values", None),
                    attribute_range=attribute_range
                )

        # Get remote updated 'attributes'
        self.metadata["attributes"] = self.ontologies.get(ontology_id=self.id).attributes

        # Update 'instance map' and 'color map'
        self._instance_map = new_ontology.instance_map
        self._color_map = new_ontology.color_map
        return self.update(system_metadata=True)


================================================
File: dtlpy/entities/organization.py
================================================
from collections import namedtuple
from enum import Enum
import traceback
import logging
import attr

from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class PodType(str, Enum):
    SMALL = "small"
    MEDIUM = "medium"
    HIGH = "high"


class CacheAction(str, Enum):
    APPLY = "apply"
    DESTROY = "destroy"


class OrganizationsPlans(str, Enum):
    PREMIUM = "premium"
    FREEMIUM = "freemium"


class MemberOrgRole(str, Enum):
    OWNER = "owner"
    ADMIN = "admin"
    MEMBER = "member"
    WORKER = "worker"


@attr.s()
class Organization(entities.BaseEntity):
    """
    Organization entity
    """

    members = attr.ib(type=list)
    groups = attr.ib(type=list)
    account = attr.ib(type=dict)
    created_at = attr.ib()
    updated_at = attr.ib()
    id = attr.ib(repr=False)
    name = attr.ib(repr=False)
    logo_url = attr.ib(repr=False)
    plan = attr.ib(repr=False)
    owner = attr.ib(repr=False)
    creator = attr.ib(repr=False)

    # api
    _client_api = attr.ib(type=ApiClient, repr=False)

    # repositories
    _repositories = attr.ib(repr=False)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @property
    def createdBy(self):
        return self.creator

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['organizations', 'projects', 'integrations', 'services', 'settings'])

        r = reps(projects=repositories.Projects(client_api=self._client_api, org=self),
                 organizations=repositories.Organizations(client_api=self._client_api),
                 integrations=repositories.Integrations(client_api=self._client_api, org=self),
                 services=repositories.Services(client_api=self._client_api),
                 settings=repositories.Settings(client_api=self._client_api,
                                                org=self,
                                                resource=self,
                                                resource_type=entities.PlatformEntityType.ORG)
                 )
        return r

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("iam/{}/members".format(self.id))

    @property
    def accounts(self):
        return [self.account]

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    @property
    def settings(self):
        assert isinstance(self._repositories.settings, repositories.Settings)
        return self._repositories.settings

    @property
    def organizations(self):
        assert isinstance(self._repositories.organizations, repositories.Organizations)
        return self._repositories.organizations

    @property
    def integrations(self):
        assert isinstance(self._repositories.integrations, repositories.Integrations)
        return self._repositories.integrations

    @staticmethod
    def _protected_from_json(_json, client_api):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param client_api: ApiClient entity

        :return: update status: bool, Organization entity
        """
        try:
            organization = Organization.from_json(_json=_json,
                                                  client_api=client_api)
            status = True
        except Exception:
            organization = traceback.format_exc()
            status = False
        return status, organization

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Project entity object from a json

        :param bool is_fetched: is Entity fetched from Platform
        :param dict _json: _json response from host
        :param dl.ApiClient client_api: ApiClient entity
        :return: Organization object
        :rtype: dtlpy.entities.organization.Organization
        """
        inst = cls(members=_json.get('members', None),
                   groups=_json.get('groups', None),
                   account=_json.get('account', None),
                   created_at=_json.get('createdAt', None),
                   updated_at=_json.get('updatedAt', None),
                   id=_json.get('id', None),
                   name=_json.get('name', None),
                   logo_url=_json.get('logoUrl', None),
                   plan=_json.get('plan', None),
                   owner=_json.get('owner', None),
                   creator=_json.get('creator', None),
                   client_api=client_api)
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        output_dict = attr.asdict(self,
                                  filter=attr.filters.exclude(attr.fields(Organization)._client_api,
                                                              attr.fields(Organization)._repositories,
                                                              attr.fields(Organization).created_at,
                                                              attr.fields(Organization).updated_at,
                                                              attr.fields(Organization).creator,
                                                              ))
        output_dict['members'] = self.members
        output_dict['groups'] = self.groups
        output_dict['account'] = self.account
        output_dict['accounts'] = self.accounts
        output_dict['createdAt'] = self.created_at
        output_dict['updatedAt'] = self.updated_at
        output_dict['id'] = self.id
        output_dict['name'] = self.name
        output_dict['logo_url'] = self.logo_url
        output_dict['plan'] = self.plan
        output_dict['owner'] = self.owner
        output_dict['creator'] = self.creator

        return output_dict

    def list_groups(self):
        """
        List all organization groups (groups that were created within the organization).

        Prerequisites: You must be an organization "owner" to use this method.

        :return: groups list
        :rtype: list

        """
        return self.organizations.list_groups(organization=self)

    def list_members(self, role: MemberOrgRole = None):
        """
        List all organization members.

        Prerequisites: You must be an organization "owner" to use this method.

        :param str role: MemberOrgRole.ADMIN, MemberOrgRole.OWNER, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :return: projects list
        :rtype: list
        """
        return self.organizations.list_members(organization=self, role=role)

    def update(self, plan: str):
        """
        Update Organization.

        Prerequisities: You must be an Organization **superuser** to update an organization.

        :param str plan: OrganizationsPlans.FREEMIUM, OrganizationsPlans.PREMIUM

        :return: organization object
        """
        return self.organizations.update(organization=self, plan=plan)

    def add_member(self, email, role: MemberOrgRole = MemberOrgRole):
        """
        Add members to your organization. Read about members and groups [here](https://dataloop.ai/docs/org-members-groups).

        Prerequisities: To add members to an organization, you must be in the role of an "owner" in that organization.

        :param str email: the member's email
        :param str role: MemberOrgRole.ADMIN, MemberOrgRole.OWNER, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :return: True if successful or error if unsuccessful
        :rtype: bool
        """
        return self.organizations.add_member(organization=self, email=email, role=role)

    def delete_member(self, user_id: str, sure: bool = False, really: bool = False):
        """
        Delete member from the Organization.

        Prerequisites: Must be an organization "owner" to delete members.

        :param str user_id: user id
        :param bool sure: Are you sure you want to delete?
        :param bool really: Really really sure?
        :return: True if success and error if not
        :rtype: bool
        """
        return self.organizations.delete_member(organization=self, user_id=user_id, sure=sure, really=really)

    def update_member(self, email: str, role: MemberOrgRole = MemberOrgRole.MEMBER):
        """
        Update member role.

        Prerequisities: You must be an organization "owner" to update a member's role.

        :param str email: the member's email
        :param str role: MemberOrgRole.ADMIN, MemberOrgRole.OWNER, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :return: json of the member fields
        :rtype: dict
        """
        return self.organizations.update_member(organization=self, email=email, role=role)

    def open_in_web(self):
        """
        Open the organizations in web platform

        """
        self._client_api._open_in_web(url=self.platform_url)

    def cache_action(self, mode=CacheAction.APPLY, pod_type=PodType.SMALL):
        """
        Open the organizations in web platform

        :param str mode: dl.CacheAction.APPLY or dl.CacheAction.DESTROY
        :param dl.PodType pod_type:  dl.PodType.SMALL, dl.PodType.MEDIUM, dl.PodType.HIGH
        :return: True if success
        :rtype: bool
        """
        return self.services._cache_action(organization=self, mode=mode, pod_type=pod_type)


================================================
File: dtlpy/entities/package.py
================================================
from collections import namedtuple
from typing import Union
from enum import Enum
import traceback
import logging
import inspect
import typing
import json
import os

from .package_module import PackageModule
from .package_slot import PackageSlot
from .. import repositories, entities, exceptions
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class RequirementOperator(str, Enum):
    EQUAL = '==',
    GREATER_THAN = '>',
    LESS_THAN = '<',
    EQUAL_OR_LESS_THAN = '<=',
    EQUAL_OR_GREATER_THAN = '>='

    @staticmethod
    def keys():
        return [key.value for key in list(RequirementOperator)]


class PackageRequirement:

    def __init__(self, name: str, version: str = None, operator: str = None):
        self.name = name
        self.version = version

        valid_operators = RequirementOperator.keys()
        if operator is not None and operator not in valid_operators:
            raise Exception('Illegal operator: {}. Please select from: {}'.format(operator, valid_operators))

        self.operator = operator

    def to_json(self):
        _json = {'name': self.name}
        if self.version is not None:
            _json['version'] = self.version
        if self.operator is not None:
            _json['operator'] = self.operator
        return _json

    @classmethod
    def from_json(cls, _json: dict):
        return cls(**_json)


class Package(entities.DlEntity):
    """
    Package object
    """
    # platform
    id: str = entities.DlProperty(location=['id'], _type=str)
    url: str = entities.DlProperty(location=['url'], _type=str)
    name: str = entities.DlProperty(location=['name'], _type=str)
    version: str = entities.DlProperty(location=['version'], _type=str)
    created_at: str = entities.DlProperty(location=['createdAt'], _type=str)
    updated_at: str = entities.DlProperty(location=['updatedAt'], _type=str)
    project_id: str = entities.DlProperty(location=['projectId'], _type=str)
    creator: str = entities.DlProperty(location=['creator'], _type=str)
    type: str = entities.DlProperty(location=['type'], _type=str)
    metadata: dict = entities.DlProperty(location=['metadata'], _type=dict)
    ui_hooks: list = entities.DlProperty(location=['uiHooks'], _type=str)
    service_config: dict = entities.DlProperty(location=['serviceConfig'], _type=str)
    is_global: bool = entities.DlProperty(location=['global'], _type=str)

    codebase: typing.Any = entities.DlProperty(location=['codebase'], _kls='Codebase')
    modules: typing.List[PackageModule] = entities.DlProperty(location=['modules'], _kls='PackageModule')
    slots: typing.Union[typing.List[PackageSlot], None] = entities.DlProperty(location=['slots'],
                                                                              _kls='PackageSlot')
    requirements: typing.Union[typing.List[PackageRequirement], None] = entities.DlProperty(location=['requirements'],
                                                                                            _kls='PackageRequirement')

    # sdk
    _client_api: ApiClient
    _revisions = None
    __repositories = None
    _project = None

    def __repr__(self):
        # TODO need to move to DlEntity
        return "Package(id={id}, name={name}, creator={creator}, project_id={project_id}, type={type}, version={version})".format(
            id=self.id,
            name=self.name,
            version=self.version,
            type=self.type,
            project_id=self.project_id,
            creator=self.creator)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @property
    def revisions(self):
        if self._revisions is None:
            self._revisions = self.packages.revisions(package=self)
        return self._revisions

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/packages/{}/main".format(self.project.id, self.id))

    @property
    def codebase_id(self):
        if self.codebase is not None and self.codebase.type == entities.PackageCodebaseType.ITEM:
            return self.codebase.item_id
        return None

    @codebase_id.setter
    def codebase_id(self, item_id: str):
        self.codebase = entities.ItemCodebase(item_id=item_id)

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json:  platform json
        :param client_api: ApiClient entity
        :return:
        """
        try:
            package = Package.from_json(_json=_json,
                                        client_api=client_api,
                                        project=project,
                                        is_fetched=is_fetched)
            status = True
        except Exception:
            package = traceback.format_exc()
            status = False
        return status, package

    @classmethod
    def from_json(cls, _json, client_api, project=None, is_fetched=True):
        """
        Turn platform representation of package into a package entity

        :param dict _json: platform representation of package
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: project entity
        :param is_fetched: is Entity fetched from Platform
        :return: Package entity
        :rtype: dtlpy.entities.package.Package
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Package has been fetched from a project that is not belong to it')
                project = None
        # Entity
        inst = cls(_dict=_json)
        # Platform
        inst._project = project
        inst._client_api = client_api
        inst.is_fetched = is_fetched

        return inst

    def to_json(self):
        """
        Turn Package entity into a platform representation of Package

        :return: platform json of package
        :rtype: dict
        """
        _json = self._dict.copy()
        return _json

    ############
    # entities #
    ############
    @property
    def project(self):
        if self._project is None:
            self._project = self.projects.get(project_id=self.project_id, fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project):
        assert isinstance(self._project, entities.Project), "Unknwon 'project' type: {}".format(type(project))
        self._project = project

    ################
    # repositories #
    ################
    @property
    def _repositories(self):
        if self.__repositories is None:
            reps = namedtuple('repositories',
                              field_names=['executions', 'services', 'projects', 'packages', 'artifacts', 'codebases',
                                           'models'])

            self.__repositories = reps(
                executions=repositories.Executions(client_api=self._client_api,
                                                   project=self._project),
                services=repositories.Services(client_api=self._client_api,
                                               package=self,
                                               project=self._project,
                                               project_id=self.project_id),
                projects=repositories.Projects(client_api=self._client_api),
                packages=repositories.Packages(client_api=self._client_api,
                                               project=self._project),
                artifacts=repositories.Artifacts(client_api=self._client_api,
                                                 project=self._project,
                                                 project_id=self.project_id,
                                                 package=self),
                codebases=repositories.Codebases(client_api=self._client_api, project=self._project,
                                                 project_id=self.project_id),
                models=repositories.Models(client_api=self._client_api,
                                           project=self._project,
                                           package=self,
                                           project_id=self.project_id)
            )
        return self.__repositories

    @property
    def executions(self):
        assert isinstance(self._repositories.executions, repositories.Executions)
        return self._repositories.executions

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def packages(self):
        assert isinstance(self._repositories.packages, repositories.Packages)
        return self._repositories.packages

    @property
    def codebases(self):
        assert isinstance(self._repositories.codebases, repositories.Codebases)
        return self._repositories.codebases

    @property
    def artifacts(self):
        assert isinstance(self._repositories.artifacts, repositories.Artifacts)
        return self._repositories.artifacts

    @property
    def models(self):
        assert isinstance(self._repositories.models, repositories.Models)
        return self._repositories.models

    ##############
    # properties #
    ##############
    @property
    def git_status(self):
        status = 'Git status unavailable'
        try:
            if self.codebase.type == entities.PackageCodebaseType.ITEM:
                if 'git' in self.codebase.item.metadata:
                    status = self.codebase.item.metadata['git'].get('status', status)
        except Exception:
            logging.debug('Error getting codebase')
        return status

    @property
    def git_log(self):
        log = 'Git log unavailable'
        try:
            if self.codebase.type == entities.PackageCodebaseType.ITEM:
                if 'git' in self.codebase.item.metadata:
                    log = self.codebase.item.metadata['git'].get('log', log)
        except Exception:
            logging.debug('Error getting codebase')
        return log

    ###########
    # methods #
    ###########
    def update(self):
        """
        Update Package changes to platform

        :return: Package entity
        """
        return self.packages.update(package=self)

    def deploy(self,
               service_name=None,
               revision=None,
               init_input=None,
               runtime=None,
               sdk_version=None,
               agent_versions=None,
               verify=True,
               bot=None,
               pod_type=None,
               module_name=None,
               run_execution_as_process=None,
               execution_timeout=None,
               drain_time=None,
               on_reset=None,
               max_attempts=None,
               force=False,
               secrets: list = None,
               **kwargs):
        """
        Deploy package

        :param str service_name: service name
        :param str revision: package revision - default=latest
        :param init_input: config to run at startup
        :param dict runtime: runtime resources
        :param str sdk_version:  - optional - string - sdk version
        :param dict agent_versions: - dictionary - - optional -versions of sdk, agent runner and agent proxy
        :param str bot: bot email
        :param str pod_type: pod type dl.InstanceCatalog
        :param bool verify: verify the inputs
        :param str module_name: module name
        :param bool run_execution_as_process: run execution as process
        :param int execution_timeout: execution timeout
        :param int drain_time: drain time
        :param str on_reset: on reset
        :param int max_attempts: Maximum execution retries in-case of a service reset
        :param bool force: optional - terminate old replicas immediately
        :param list secrets: list of the integrations ids
        :return: Service object
        :rtype: dtlpy.entities.service.Service

        **Example**:

        .. code-block:: python
        service: dl.Service = package.deploy(service_name=package_name,
                                             execution_timeout=3 * 60 * 60,
                                             module_name=module.name,
                                             runtime=dl.KubernetesRuntime(
                                                 concurrency=10,
                                                 pod_type=dl.InstanceCatalog.REGULAR_S,
                                                 autoscaler=dl.KubernetesRabbitmqAutoscaler(
                                                     min_replicas=1,
                                                     max_replicas=20,
                                                     queue_length=20
                                                 )
                                             )
                                             )

        """
        return self.project.packages.deploy(package=self,
                                            service_name=service_name,
                                            project_id=self.project_id,
                                            revision=revision,
                                            init_input=init_input,
                                            runtime=runtime,
                                            sdk_version=sdk_version,
                                            agent_versions=agent_versions,
                                            pod_type=pod_type,
                                            bot=bot,
                                            verify=verify,
                                            module_name=module_name,
                                            run_execution_as_process=run_execution_as_process,
                                            execution_timeout=execution_timeout,
                                            drain_time=drain_time,
                                            on_reset=on_reset,
                                            max_attempts=max_attempts,
                                            force=force,
                                            jwt_forward=kwargs.get('jwt_forward', None),
                                            is_global=kwargs.get('is_global', None),
                                            secrets=secrets)

    def checkout(self):
        """
        Checkout as package

        :return:
        """
        return self.packages.checkout(package=self)

    def delete(self) -> bool:
        """
        Delete Package object

        :return: True
        """
        return self.packages.delete(package=self)

    def push(self,
             codebase: Union[entities.GitCodebase, entities.ItemCodebase] = None,
             src_path: str = None,
             package_name: str = None,
             modules: list = None,
             checkout: bool = False,
             revision_increment: str = None,
             service_update: bool = False,
             service_config: dict = None,
             package_type='faas'
             ):
        """
        Push local package

        :param dtlpy.entities.codebase.Codebase codebase: PackageCode object - defines how to store the package code
        :param bool checkout: save package to local checkout
        :param str src_path: location of pacjage codebase folder to zip
        :param str package_name: name of package
        :param list modules: list of PackageModule
        :param str revision_increment: optional - str - version bumping method - major/minor/patch - default = None
        :param  bool service_update: optional - bool - update the service
        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
        :param  str package_type: default is "faas", one of "app", "ml"
        :return: package entity
        :rtype: dtlpy.entities.package.Package
        
        **Example**:

        .. code-block:: python

            package = packages.push(package_name='package_name',
                                    modules=[module],
                                    version='1.0.0',
                                    src_path=os.getcwd())
        """
        return self.project.packages.push(
            package_name=package_name if package_name is not None else self.name,
            modules=modules if modules is not None else self.modules,
            revision_increment=revision_increment,
            codebase=codebase,
            src_path=src_path,
            checkout=checkout,
            service_update=service_update,
            service_config=service_config,
            package_type=package_type
        )

    def pull(self, version=None, local_path=None) -> str:
        """
        Pull local package

        :param str version: version
        :param str local_path: local path

        **Example**:

        .. code-block:: python

            path = package.pull(local_path='local_path')
        """
        return self.packages.pull(package=self,
                                  version=version,
                                  local_path=local_path)

    def build(self, module_name=None, init_inputs=None, local_path=None, from_local=None):
        """
        Instantiate a module from the package code. Returns a loaded instance of the runner class

        :param module_name: Name of the module to build the runner class
        :param str init_inputs: dictionary of the class init variables (if exists). will be used to init the module class
        :param str local_path: local path of the package (if from_local=False - codebase will be downloaded)
        :param bool from_local: bool. if true - codebase will not be downloaded (only use local files)
        :return: dl.BaseServiceRunner
        """
        return self.packages.build(package=self,
                                   module_name=module_name,
                                   local_path=local_path,
                                   init_inputs=init_inputs,
                                   from_local=from_local)

    def open_in_web(self):
        """
        Open the package in web platform

        """
        url = self._client_api._get_resource_url(
            f"projects/{self.project.id}/faas?byCreator=false&byProject=true&byDataloop=false&tab=library&name={self.name}")
        self._client_api._open_in_web(url=url)

    def test(self,
             cwd=None,
             concurrency=None,
             module_name=entities.package_defaults.DEFAULT_PACKAGE_MODULE_NAME,
             function_name=entities.package_defaults.DEFAULT_PACKAGE_FUNCTION_NAME,
             class_name=entities.package_defaults.DEFAULT_PACKAGE_CLASS_NAME,
             entry_point=entities.package_defaults.DEFAULT_PACKAGE_ENTRY_POINT
             ):
        """
        Test local package in local environment.

        :param str cwd: path to the file
        :param int concurrency: the concurrency of the test
        :param str module_name: module name
        :param str function_name: function name
        :param str class_name: class name
        :param str entry_point: the file to run like main.py
        :return: list created by the function that tested the output
        :rtype: list

        **Example**:

        .. code-block:: python

            package.test(cwd='path_to_package',
                        function_name='run')
        """
        return self.project.packages.test_local_package(
            cwd=cwd,
            concurrency=concurrency,
            package=self,
            module_name=module_name,
            function_name=function_name,
            class_name=class_name,
            entry_point=entry_point
        )

    @staticmethod
    def _mockify_input(input_type):
        _json = dict()
        if input_type == 'Dataset':
            _json.update({'dataset_id': 'id'})
        if input_type == 'Item':
            _json.update({'item_id': 'id', 'dataset_id': 'id'})
        if input_type == 'Annotation':
            _json.update({'annotation_id': 'id', 'item_id': 'id', 'dataset_id': 'id'})
        return _json

    def mockify(self, local_path=None, module_name=None, function_name=None):
        if local_path is None:
            local_path = os.getcwd()

        if module_name is None:
            if self.modules:
                module_name = self.modules[0].name
            else:
                raise exceptions.PlatformException('400', 'Package has no modules')

        modules = [module for module in self.modules if module.name == module_name]
        if not modules:
            raise exceptions.PlatformException('404', 'Module not found: {}'.format(module_name))
        module = modules[0]

        if function_name is None:
            funcs = [func for func in module.functions]
            if funcs:
                func = funcs[0]
            else:
                raise exceptions.PlatformException('400', 'Module: {} has no functions'.format(module_name))
        else:
            funcs = [func for func in module.functions if func.name == function_name]
            if not funcs:
                raise exceptions.PlatformException('404', 'Function not found: {}'.format(function_name))
            func = funcs[0]

        mock = dict()
        for module in self.modules:
            mock['module_name'] = module.name
            mock['function_name'] = func.name
            mock['init_params'] = {inpt.name: self._mockify_input(input_type=inpt.type) for inpt in module.init_inputs}
            mock['inputs'] = [{'name': inpt.name, 'value': self._mockify_input(input_type=inpt.type)} for inpt in
                              func.inputs]

        with open(os.path.join(local_path, 'mock.json'), 'w') as f:
            json.dump(mock, f)

    @staticmethod
    def get_ml_metadata(cls=None,
                        available_methods=None,
                        output_type=entities.AnnotationType.CLASSIFICATION,
                        input_type='image',
                        default_configuration: dict = None):
        """
        Create ML metadata for the package
        :param cls: ModelAdapter class, to get the list of available_methods
        :param available_methods: available user function on the adapter.  ['load', 'save', 'predict', 'train']
        :param output_type: annotation type the model create, e.g. dl.AnnotationType.CLASSIFICATION
        :param input_type: input file type the model gets, one of ['image', 'video', 'txt']
        :param default_configuration: default service configuration for the deployed services
        :return:
        """
        user_implemented_methods = ['load', 'save', 'predict', 'train']
        if available_methods is None:
            # default
            available_methods = user_implemented_methods

        if cls is not None:
            # TODO dont check if function is on the adapter - check if the functions is implemented (not raise NotImplemented)
            available_methods = [
                {name: 'BaseModelAdapter' not in getattr(cls, name).__qualname__}
                for name in user_implemented_methods
            ]
        if default_configuration is None:
            default_configuration = dict()
        metadata = {
            'system': {'ml': {'defaultConfiguration': default_configuration,
                              'outputType': output_type,
                              'inputType': input_type,
                              'supportedMethods': available_methods
                              }}}
        return metadata

    class decorators:
        @staticmethod
        def module(name='default-module', description='', init_inputs=None):
            def wrapper(cls: typing.Callable):
                # package_module_dict = package_module.to_json()
                package_module_dict = {"name": name,
                                       "description": description,
                                       "functions": list(),
                                       "className": cls.__name__}
                if init_inputs is not None:
                    package_module_dict.update(initInputs=Package.decorators.parse_io(io_list=init_inputs))
                for member_name, member in inspect.getmembers(cls, predicate=inspect.isfunction):
                    spec = getattr(member, '__dtlpy__', None)
                    if spec is not None:
                        package_module_dict["functions"].append(spec)
                cls.__dtlpy__ = package_module_dict
                return cls

            return wrapper

        @staticmethod
        def function(display_name=None, inputs=None, outputs=None):
            def wrapper(func: typing.Callable):
                if display_name is None:
                    d_name = func.__name__
                else:
                    d_name = display_name
                func.__dtlpy__ = {"name": func.__name__,
                                  "displayName": d_name,
                                  "input": Package.decorators.parse_io(io_list=inputs),
                                  "output": Package.decorators.parse_io(io_list=outputs)}
                return func

            return wrapper

        @staticmethod
        def parse_io(io_list: dict):
            output = list()
            if io_list is not None:
                for io_name, io_type in io_list.items():
                    if isinstance(io_type, Enum):
                        io_type = io_type.name
                    if isinstance(io_type, type):
                        io_type = io_type.__name__
                    else:
                        io_type = str(io_type)
                    output.append({"name": io_name,
                                   "type": str(io_type)})
            return output


================================================
File: dtlpy/entities/package_defaults.py
================================================
DEFAULT_PACKAGE_ENTRY_POINT = 'main.py'
DEFAULT_PACKAGE_CLASS_NAME = 'ServiceRunner'
DEFAULT_PACKAGE_FUNCTION_NAME = 'run'
DEFAULT_PACKAGE_MODULE_NAME = 'default_module'
DEFAULT_PACKAGE_NAME = 'default_package'


================================================
File: dtlpy/entities/package_function.py
================================================
import logging
import typing
import json
from enum import Enum
from .. import exceptions, entities

logger = logging.getLogger(name='dtlpy')


class PackageInputType(str, Enum):
    DATASET = "Dataset"
    ITEM = "Item"
    ANNOTATION = "Annotation"
    EXECUTION = "Execution"
    TASK = "Task"
    ASSIGNMENT = "Assignment"
    SERVICE = "Service"
    PACKAGE = "Package"
    PROJECT = "Project"
    RECIPE = "Recipe"
    JSON = "Json"
    STRING = "String"
    NUMBER = "Number"
    INT = "Integer"
    FLOAT = "Float"
    BOOLEAN = "Boolean"
    MODEL = "Model"
    DATASETS = "Dataset[]"
    ITEMS = "Item[]"
    ANNOTATIONS = "Annotation[]"
    EXECUTIONS = "Execution[]"
    TASKS = "Task[]"
    ASSIGNMENTS = "Assignment[]"
    SERVICES = "Service[]"
    PACKAGES = "Package[]"
    PROJECTS = "Project[]"
    JSONS = "Json[]"
    STRINGS = "String[]"
    NUMBERS = "Number[]"
    INTS = "Integer[]"
    FLOATS = "Float[]"
    BOOLEANS = "Boolean[]"
    MODELS = "Model[]"
    RECIPES = "Recipe[]"


class PackageFunction(entities.DlEntity):
    """
    Webhook object
    """

    # platform
    name: str = entities.DlProperty(location=['name'],
                                    _type=str,
                                    default=entities.package_defaults.DEFAULT_PACKAGE_FUNCTION_NAME)
    description: str = entities.DlProperty(location=['description'],
                                           _type=str)
    display_name: typing.Union[str, None] = entities.DlProperty(location=['displayName'],
                                                                _type=typing.Union[str, None])
    display_icon: typing.Union[str, None] = entities.DlProperty(location=['displayIcon'],
                                                                _type=typing.Union[str, None])

    outputs: typing.Union[typing.List['entities.FunctionIO'], None] = entities.DlProperty(location=['output'],
                                                                                          _kls='FunctionIO')
    inputs: typing.Union[typing.List['entities.FunctionIO'], None] = entities.DlProperty(location=['input'],
                                                                                         _kls='FunctionIO')
    compute_config: str = entities.DlProperty(location=['computeConfig'], _type=str, default=None)

    def __repr__(self):
        # TODO need to move to DlEntity
        return f"PackageFunction(name={self.name}, description={self.description})"

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst

    @outputs.default
    def get_outputs(self):
        outputs = list()
        return outputs

    @inputs.default
    def get_inputs(self):
        inputs = list()
        return inputs

    def to_json(self):
        _json = self._dict.copy()
        return _json


class FunctionIO(entities.DlEntity):
    INPUT_TYPES: list = [val for key, val in PackageInputType.__dict__.items() if not key.startswith('_')]
    type = entities.DlProperty(location=['type'], _type=str)
    value = entities.DlProperty(location=['value'], _type=str)
    name = entities.DlProperty(location=['name'], _type=str)
    actions = entities.DlProperty(location=['actions'], _type=list)
    description = entities.DlProperty(location=['description'], _type=str)
    integration = entities.DlProperty(location=['integration'], _type=dict)
    mandatory = entities.DlProperty(location=['mandatory'], _type=bool)

    def __repr__(self):
        # TODO need to move to DlEntity
        return f"FunctionIO(type={self.type}, name={self.name}, value={self.value})"

    @name.default
    def set_name(self):
        if self.type == PackageInputType.ITEM:
            return 'item'
        elif self.type == PackageInputType.DATASET:
            return 'dataset'
        elif self.type == PackageInputType.ANNOTATION:
            return 'annotation'
        elif self.type == PackageInputType.PROJECT:
            return 'project'
        elif self.type == PackageInputType.PACKAGE:
            return 'package'
        elif self.type == PackageInputType.SERVICE:
            return 'service'
        elif self.type == PackageInputType.EXECUTION:
            return 'execution'
        elif self.type == PackageInputType.MODEL:
            return 'model'
        else:
            return 'config'

    @type.validator
    def check_type(self, value):
        if value not in self.INPUT_TYPES:
            raise exceptions.PlatformException(
                error='400',
                message='Invalid input type: {!r}. Please one from dl.PackageInputType'.format(value))

    @staticmethod
    def is_json_serializable(val):
        try:
            json.dumps(val)
            is_json_serializable = True
        except Exception:
            is_json_serializable = False
        return is_json_serializable

    @value.validator
    def check_value(self, value):
        value_ok = True
        expected_value = 'Expected value should be:'
        if self.type == PackageInputType.JSON:
            expected_value = '{} json serializable'.format(expected_value)
            if not self.is_json_serializable(value):
                value_ok = False
        else:
            expected_value = 'Unknown value type: {}'.format(type(value))
            if type(value) not in [dict, str, float, int, bool, list]:
                value_ok = False

        if not value_ok and value is not None:
            raise exceptions.PlatformException('400', 'Illegal value. {}'.format(expected_value))

    def to_json(self, resource='package'):
        """
        :param resource:
        """
        if resource == 'package':
            _json = {
                'name': self.name,
                'type': self.type,
            }
            if self.actions:
                _json['actions'] = self.actions
            if self.description:
                _json['description'] = self.description
        elif resource in ['execution', 'service']:
            _json = {
                self.name: self.value
            }
        else:
            raise exceptions.PlatformException('400', 'Please select resource from: package, execution')

        return _json

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst


================================================
File: dtlpy/entities/package_module.py
================================================
import importlib.util
import inspect
import logging
import typing

from .. import entities

logger = logging.getLogger(name='dtlpy')


class PackageModule(entities.DlEntity):
    """
    PackageModule object
    """
    # platform
    name: str = entities.DlProperty(location=['name'], _type=str)
    init_inputs: typing.List['entities.FunctionIO'] = entities.DlProperty(location=['initInputs'],
                                                                          _type=typing.Union[list, None],
                                                                          _kls='FunctionIO')

    entry_point = entities.DlProperty(location=['entryPoint'],
                                      _type=str,
                                      default=entities.package_defaults.DEFAULT_PACKAGE_ENTRY_POINT)
    class_name = entities.DlProperty(location=['className'],
                                     _type=dict,
                                     default=entities.package_defaults.DEFAULT_PACKAGE_CLASS_NAME)
    functions: typing.List['entities.PackageFunction'] = entities.DlProperty(location=['functions'],
                                                                             _type=list,
                                                                             default=list(),
                                                                             _kls='PackageFunction')
    compute_config: str = entities.DlProperty(location=['computeConfig'], _type=str, default=None)

    def __repr__(self):
        # TODO need to move to DlEntity
        return f"PackageModule(name={self.name}, entry_point={self.entry_point}, class_name={self.class_name})"

    @functions.validator
    def validate_functions(self, value: list):
        if not isinstance(value, list):
            raise Exception('Module functions must be a list.')
        if not self.unique_functions(value):
            raise Exception('Cannot have 2 functions by the same name in one module.')

    @staticmethod
    def unique_functions(functions: list):
        return len(functions) == len(set([function.name for function in functions]))

    @name.default
    def set_name(self):
        logger.warning('No module name was given. Using default name: {}'.format(
            entities.package_defaults.DEFAULT_PACKAGE_MODULE_NAME))
        return entities.package_defaults.DEFAULT_PACKAGE_MODULE_NAME

    @init_inputs.default
    def set_init_inputs(self):
        return list()

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst

    @classmethod
    def from_entry_point(cls, entry_point):
        """
        Create a dl.PackageModule entity using decorator on the service class.

        :param entry_point: path to the python file with the runner class (relative to the package path)
        :return:
        """
        file_spec = importlib.util.spec_from_file_location(entry_point, entry_point)
        file_module = importlib.util.module_from_spec(file_spec)
        file_spec.loader.exec_module(file_module)
        module = None
        for cls_name, cls_inst in inspect.getmembers(file_module, predicate=inspect.isclass):
            spec = getattr(cls_inst, '__dtlpy__', None)
            if spec is not None:
                functions = spec['functions']
                available_methods = [name for name in ['train', 'predict']
                                     if 'BaseModelAdapter' not in getattr(cls_inst, name).__qualname__]
                if "train" not in available_methods:
                    # remove train_model from functions list if train is not available
                    functions[:] = [d for d in functions if d.get('name') != "train_model"]
                if "predict" not in available_methods:
                    # remove predict_items from functions list if predict is not available
                    functions[:] = [d for d in functions if d.get('name') != "predict_items"]
                if "extract_features" not in available_methods:
                    # remove extract_item_features from functions list if extract_features is not available
                    functions[:] = [d for d in functions if d.get('name') != "extract_item_features"]
                spec['entryPoint'] = entry_point
                spec['functions'] = functions
                module = cls.from_json(spec)
                break
        if module is None:
            raise ValueError('Failed to find a decorated Runner class in file: {}'.format(entry_point))
        return module

    def add_function(self, function):
        """
        :param function:
        """
        if not isinstance(self.functions, list):
            self.functions = [self.functions]
        if isinstance(function, entities.PackageFunction):
            self.functions.append(function)
        elif isinstance(function, dict):
            self.functions.append(entities.PackageFunction.from_json(function))
        else:
            raise ValueError('Unknown function type: {}. Expecting dl.PackageFunction or dict')

    def to_json(self):
        _json = self._dict.copy()
        return _json


================================================
File: dtlpy/entities/package_slot.py
================================================
import logging

import attr
import typing
from enum import Enum
from .. import entities

logger = logging.getLogger(name='dtlpy')


class SlotPostActionType(str, Enum):
    DOWNLOAD = 'download'
    DRAW_ANNOTATION = 'drawAnnotation'
    NO_ACTION = 'noAction'


class SlotDisplayScopeResource(str, Enum):
    ANNOTATION = 'annotation'
    ITEM = 'item'
    DATASET = 'dataset'
    DATASET_QUERY = 'datasetQuery'
    TASK = 'task'


class UiBindingPanel(str, Enum):
    BROWSER = "browser",
    STUDIO = "studio",
    TABLE = "table",
    ALL = "all"


class SlotPostAction(entities.DlEntity):
    type: str = entities.DlProperty(location=['type'],
                                    _type=str,
                                    default=SlotPostActionType.NO_ACTION)

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst

    def to_json(self):
        _json = self._dict.copy()
        return _json


class SlotDisplayScope(entities.DlEntity):
    resource: str = entities.DlProperty(location=['resource'],
                                        _type=str)
    filters: entities.Filters = entities.DlProperty(location=['filter'],
                                                    _type=entities.Filters)
    panel: str = entities.DlProperty(location=['panel'],
                                     _type=str,
                                     default=UiBindingPanel.ALL)

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst

    @staticmethod
    def get_resource(resource: str):
        if resource in [SlotDisplayScopeResource.DATASET, SlotDisplayScopeResource.DATASET_QUERY]:
            return entities.FiltersResource.DATASET
        elif resource == SlotDisplayScopeResource.ITEM:
            return entities.FiltersResource.ITEM
        elif resource == SlotDisplayScopeResource.ANNOTATION:
            return entities.FiltersResource.ANNOTATION

    def to_json(self):
        _json = self._dict.copy()
        if isinstance(self.filters, entities.Filters):
            _json['filter'] = self.filters.prepare(query_only=True)['filter']
        elif isinstance(self.filters, dict):
            _json['filter'] = self.filters
        return _json


class PackageSlot(entities.DlEntity):
    """
    Webhook object
    """
    # platform
    display_name: str = entities.DlProperty(location=['displayName'],
                                            _type=str)
    display_scopes: typing.Union[typing.List['entities.SlotDisplayScope'], None] = entities.DlProperty(
        location=['displayScopes'],
        _kls='SlotDisplayScope')
    module_name: str = entities.DlProperty(location=['moduleName'],
                                           _type=str,
                                           default='default_module')
    function_name: str = entities.DlProperty(location=['functionName'],
                                             _type=str,
                                             default='run')
    display_icon: str = entities.DlProperty(location=['displayIcon'],
                                            _type=str)
    post_action: SlotPostAction = entities.DlProperty(location=['postAction'],
                                                      _type=str,
                                                      _kls='SlotPostAction')
    default_inputs: typing.Union[typing.List['entities.FunctionIO'], None] = entities.DlProperty(
        location=['defaultInputs'],
        _kls='FunctionIO')
    input_options: list = entities.DlProperty(
        location=['inputOptions'],
        _kls='FunctionIO')

    @post_action.default
    def post_action_setter(self):
        return SlotPostAction(type=SlotPostActionType.NO_ACTION)

    @classmethod
    def from_json(cls, _json):
        inst = cls(_dict=_json)
        return inst

    def to_json(self):
        _json = self._dict.copy()
        return _json


================================================
File: dtlpy/entities/paged_entities.py
================================================
import logging
import math
import time
import tqdm
import copy
import sys

import attr
from .. import miscellaneous
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


@attr.s
class PagedEntities:
    """
    Pages object
    """
    # api
    _client_api = attr.ib(type=ApiClient, repr=False)

    # params
    page_offset = attr.ib()
    page_size = attr.ib()
    filters = attr.ib()
    items_repository = attr.ib(repr=False)
    has_next_page = attr.ib(default=False)
    total_pages_count = attr.ib(default=0)
    items_count = attr.ib(default=0)

    # execution attribute
    _service_id = attr.ib(default=None, repr=False)
    _project_id = attr.ib(default=None, repr=False)
    _order_by_type = attr.ib(default=None, repr=False)
    _order_by_direction = attr.ib(default=None, repr=False)
    _execution_status = attr.ib(default=None, repr=False)
    _execution_resource_type = attr.ib(default=None, repr=False)
    _execution_resource_id = attr.ib(default=None, repr=False)
    _execution_function_name = attr.ib(default=None, repr=False)
    _list_function = attr.ib(default=None, repr=False)

    # items list
    items = attr.ib(default=miscellaneous.List(), repr=False)

    def process_result(self, result):
        """
        :param result: json object
        """
        if 'page_offset' in result:
            self.page_offset = result['page_offset']
        if 'page_size' in result:
            self.page_size = result['page_size']
        if 'hasNextPage' in result:
            self.has_next_page = result['hasNextPage']
        if 'totalItemsCount' in result:
            self.items_count = result['totalItemsCount']
        if 'totalPagesCount' in result:
            self.total_pages_count = result['totalPagesCount']
        if 'items' in result:
            items = self.items_repository._build_entities_from_response(response_items=result['items'])
        else:
            items = miscellaneous.List(list())
        return items

    def __getitem__(self, y):
        self.go_to_page(y)
        return self.items

    def __len__(self):
        return self.items_count

    def __iter__(self):
        pbar = tqdm.tqdm(total=self.total_pages_count, disable=self._client_api.verbose.disable_progress_bar_iterate_pages,
                         file=sys.stdout, desc="Iterate Pages")
        if self.page_offset != 0:
            # reset the count for page 0
            self.page_offset = 0
            self.get_page()
        while True:
            yield self.items
            pbar.update()

            if self.has_next_page:
                self.page_offset += 1
                self.get_page()
            else:
                pbar.close()
                break

    def __reversed__(self):
        self.page_offset = self.total_pages_count - 1
        while True:
            self.get_page()
            yield self.items
            if self.page_offset == 0:
                break
            self.page_offset -= 1

    def return_page(self, page_offset=None, page_size=None):
        """
        Return page

        :param page_offset: page offset
        :param page_size: page size
        """
        if page_size is None:
            page_size = self.page_size
        if page_offset is None:
            page_offset = self.page_offset

        if self.filters is not None:
            filters = copy.copy(self.filters)
            filters.page = page_offset
            filters.page_size = page_size
            if self._list_function is None:
                result = self.items_repository._list(filters=filters)
            else:
                result = self._list_function(filters=filters)
            items = self.process_result(result)
            return items
        else:
            raise ValueError('Cant return page. Filters is empty')

    def get_page(self, page_offset=None, page_size=None):
        """
        Get page

        :param page_offset: page offset
        :param page_size: page size
        """
        items = self.return_page(page_offset=page_offset,
                                 page_size=page_size)
        self.items = items

    def next_page(self):
        """
        Brings the next page of items from host

        :return:
        """
        self.page_offset += 1
        self.get_page()

    def prev_page(self):
        """
        Brings the previous page of items from host

        :return:
        """
        self.page_offset -= 1
        self.get_page()

    def go_to_page(self, page=0):
        """
        Brings specified page of items from host

        :param page: page number
        :return:
        """
        self.page_offset = page
        self.get_page()

    def all(self):
        page_offset = 0
        page_size = 100
        pbar = tqdm.tqdm(total=self.items_count, disable=self._client_api.verbose.disable_progress_bar,
                         file=sys.stdout, desc='Iterate Entity')
        total_pages = math.ceil(self.items_count / page_size)
        jobs = list()
        pool = self._client_api.thread_pools('item.page')
        while True:
            time.sleep(0.01)  # to flush the results
            if page_offset <= total_pages:
                jobs.append(pool.submit(self.return_page, **{'page_offset': page_offset,
                                                             'page_size': page_size}))
                page_offset += 1
            for i_job, job in enumerate(jobs):
                if job.done():
                    for item in job.result():
                        pbar.update()
                        yield item
                    jobs.remove(job)
            if len(jobs) == 0:
                pbar.close()
                break

    ########
    # misc #
    ########
    def print(self, columns=None):
        self.items.print(columns=columns)

    def to_df(self, columns=None):
        return self.items.to_df(columns=columns)


================================================
File: dtlpy/entities/pipeline.py
================================================
from collections import namedtuple
import logging
import traceback
from enum import Enum
from typing import List
import attr
from .node import PipelineNode, PipelineConnection, TaskNode, CodeNode, FunctionNode, DatasetNode
from .. import repositories, entities
from ..services.api_client import ApiClient
from .package_function import PackageInputType
import copy

logger = logging.getLogger(name='dtlpy')


class PipelineResumeOption(str, Enum):
    TERMINATE_EXISTING_CYCLES = 'terminateExistingCycles',
    RESUME_EXISTING_CYCLES = 'resumeExistingCycles'


class CompositionStatus(str, Enum):
    CREATED = "Created",
    INITIALIZING = "Initializing",
    INSTALLED = "Installed",
    ACTIVATED = "Activated",
    DEACTIVATED = "Deactivated",
    UNINSTALLED = "Uninstalled",
    TERMINATING = "Terminating",
    TERMINATED = "Terminated",
    UPDATING = "Updating",
    FAILURE = "Failure"


class PipelineSettings:

    def __init__(
            self,
            default_resume_option: PipelineResumeOption = None,
            keep_triggers_active: bool = None,
            active_trigger_ask_again: bool = None,
            last_update: dict = None
    ):
        self.default_resume_option = default_resume_option
        self.keep_triggers_active = keep_triggers_active
        self.active_trigger_ask_again = active_trigger_ask_again
        self.last_update = last_update

    @classmethod
    def from_json(cls, _json: dict = None):
        if _json is None:
            _json = dict()
        return cls(
            default_resume_option=_json.get('defaultResumeOption', None),
            keep_triggers_active=_json.get('keepTriggersActive', None),
            active_trigger_ask_again=_json.get('activeTriggerAskAgain', None),
            last_update=_json.get('lastUpdate', None)
        )

    def to_json(self):
        _json = dict()

        if self.default_resume_option is not None:
            _json['defaultResumeOption'] = self.default_resume_option

        if self.default_resume_option is not None:
            _json['keepTriggersActive'] = self.default_resume_option

        if self.default_resume_option is not None:
            _json['activeTriggerAskAgain'] = self.default_resume_option

        if self.default_resume_option is not None:
            _json['lastUpdate'] = self.default_resume_option

        return _json


class Variable(entities.DlEntity):
    """
    Pipeline Variables
    """
    id: str = entities.DlProperty(location=['id'], _type=str)
    created_at: str = entities.DlProperty(location=['createdAt'], _type=str)
    updated_at: str = entities.DlProperty(location=['updatedAt'], _type=str)
    reference: str = entities.DlProperty(location=['reference'], _type=str)
    creator: str = entities.DlProperty(location=['creator'], _type=str)
    variable_type: PackageInputType = entities.DlProperty(location=['type'], _type=PackageInputType)
    name: str = entities.DlProperty(location=['name'], _type=str)
    value = entities.DlProperty(location=['value'])

    @classmethod
    def from_json(cls, _json):
        """
        Turn platform representation of variable into a pipeline variable entity

        :param dict _json: platform representation of pipeline variable
        :return: pipeline variable entity
        :rtype: dtlpy.entities.pipeline.PipelineVariables
        """

        inst = cls(_dict=_json)
        return inst

    def to_json(self):
        """
        :return: variable of pipeline
        :rtype: dict
        """
        _json = self._dict.copy()
        return _json


class PipelineAverages:
    def __init__(
            self,
            avg_time_per_execution: float,
            avg_execution_per_day: float
    ):
        self.avg_time_per_execution = avg_time_per_execution
        self.avg_execution_per_day = avg_execution_per_day

    @classmethod
    def from_json(cls, _json: dict = None):
        if _json is None:
            _json = dict()
        return cls(
            avg_time_per_execution=_json.get('avgTimePerExecution', 'NA'),
            avg_execution_per_day=_json.get('avgExecutionsPerDay', 'NA')
        )


class NodeAverages:
    def __init__(
            self,
            node_id: str,
            averages: PipelineAverages
    ):
        self.node_id = node_id
        self.averages = averages

    @classmethod
    def from_json(cls, _json: dict):
        return cls(
            node_id=_json.get('nodeId', None),
            averages=PipelineAverages.from_json(_json.get('executionStatistics'))
        )


class PipelineCounter:
    def __init__(
            self,
            status: str,
            count: int
    ):
        self.status = status
        self.count = count


class NodeCounters:
    def __init__(
            self,
            node_id: str,
            counters: List[PipelineCounter]
    ):
        self.node_id = node_id
        self.counters = counters

    @classmethod
    def from_json(cls, _json: dict):
        return cls(
            node_id=_json.get('nodeId', None),
            counters=[PipelineCounter(**c) for c in _json.get('statusCount', list())],
        )


class PipelineStats:
    def __init__(
            self,
            pipeline_counters: List[PipelineCounter],
            node_counters: List[NodeCounters],
            pipeline_averages: PipelineAverages,
            node_averages: List[NodeAverages]
    ):
        self.pipeline_counters = pipeline_counters
        self.node_counters = node_counters
        self.pipeline_averages = pipeline_averages
        self.node_averages = node_averages

    @classmethod
    def from_json(cls, _json: dict):
        return cls(
            pipeline_counters=[PipelineCounter(**c) for c in _json.get('pipelineExecutionCounters', list())],
            node_counters=[NodeCounters.from_json(_json=c) for c in _json.get('nodeExecutionsCounters', list())],
            pipeline_averages=PipelineAverages.from_json(_json.get('pipelineExecutionStatistics', None)),
            node_averages=[NodeAverages.from_json(_json=c) for c in _json.get('nodeExecutionStatistics', list())]
        )


@attr.s
class Pipeline(entities.BaseEntity):
    """
    Pipeline object
    """
    # platform
    id = attr.ib()
    name = attr.ib()
    creator = attr.ib()
    org_id = attr.ib()
    connections = attr.ib()
    settings = attr.ib(type=PipelineSettings)
    variables = attr.ib(type=List[Variable])

    status = attr.ib(type=CompositionStatus)

    # name change
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    start_nodes = attr.ib()
    project_id = attr.ib()
    composition_id = attr.ib()
    url = attr.ib()
    preview = attr.ib()
    description = attr.ib()
    revisions = attr.ib()

    # sdk
    _project = attr.ib(repr=False)
    _client_api = attr.ib(type=ApiClient, repr=False)
    _original_settings = attr.ib(repr=False, type=PipelineSettings)
    _original_variables = attr.ib(repr=False, type=List[Variable])
    _repositories = attr.ib(repr=False)

    updated_by = attr.ib(default=None)

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error
        :param _json: platform json
        :param client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: entity
        :param is_fetched: is Entity fetched from Platform
        :return:
        """
        try:
            pipeline = Pipeline.from_json(
                _json=_json,
                client_api=client_api,
                project=project,
                is_fetched=is_fetched
            )
            status = True
        except Exception:
            pipeline = traceback.format_exc()
            status = False
        return status, pipeline

    @classmethod
    def from_json(cls, _json, client_api, project=None, is_fetched=True):
        """
        Turn platform representation of pipeline into a pipeline entity

        :param dict _json: platform representation of package
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: entity
        :param bool is_fetched: is Entity fetched from Platform
        :return: Pipeline entity
        :rtype: dtlpy.entities.pipeline.Pipeline
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Pipeline has been fetched from a project that is not belong to it')
                project = None

        connections = [PipelineConnection.from_json(_json=con) for con in _json.get('connections', list())]
        json_variables = _json.get('variables', None) or list()
        variables = list()
        if json_variables:
            copy_json_variables = copy.deepcopy(json_variables)
            variables = [Variable.from_json(_json=v) for v in copy_json_variables]

        settings = PipelineSettings.from_json(_json=_json.get('settings', dict()))
        inst = cls(
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            project_id=_json.get('projectId', None),
            org_id=_json.get('orgId', None),
            composition_id=_json.get('compositionId', None),
            creator=_json.get('creator', None),
            client_api=client_api,
            name=_json.get('name', None),
            project=project,
            id=_json.get('id', None),
            connections=connections,
            start_nodes=_json.get('startNodes', None),
            url=_json.get('url', None),
            preview=_json.get('preview', None),
            description=_json.get('description', None),
            revisions=_json.get('revisions', None),
            settings=settings,
            variables=variables,
            status=_json.get('status', None),
            original_settings=settings,
            original_variables=json_variables,
            updated_by=_json.get('updatedBy', None),
        )
        for node in _json.get('nodes', list()):
            inst.nodes.add(node=cls.pipeline_node(node))
        inst.is_fetched = is_fetched
        return inst

    @classmethod
    def pipeline_node(cls, _json):
        node_type = _json.get('type')
        if node_type == 'task':
            return TaskNode.from_json(_json)
        elif node_type == 'code':
            return CodeNode.from_json(_json)
        elif node_type == 'function':
            return FunctionNode.from_json(_json)
        elif node_type == 'storage':
            return DatasetNode.from_json(_json)
        else:
            return PipelineNode.from_json(_json)

    def settings_changed(self) -> bool:
        return self.settings.to_json() != self._original_settings.to_json()

    def variables_changed(self) -> bool:
        new_vars = [var.to_json() for var in self.variables]
        old_vars = self._original_variables or list()
        return new_vars != old_vars

    def to_json(self):
        """
        Turn Package entity into a platform representation of Package

        :return: platform json of package
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(Pipeline)._project,
                                                        attr.fields(Pipeline)._repositories,
                                                        attr.fields(Pipeline)._client_api,
                                                        attr.fields(Pipeline).project_id,
                                                        attr.fields(Pipeline).org_id,
                                                        attr.fields(Pipeline).connections,
                                                        attr.fields(Pipeline).created_at,
                                                        attr.fields(Pipeline).updated_at,
                                                        attr.fields(Pipeline).start_nodes,
                                                        attr.fields(Pipeline).project_id,
                                                        attr.fields(Pipeline).composition_id,
                                                        attr.fields(Pipeline).url,
                                                        attr.fields(Pipeline).preview,
                                                        attr.fields(Pipeline).description,
                                                        attr.fields(Pipeline).revisions,
                                                        attr.fields(Pipeline).settings,
                                                        attr.fields(Pipeline).variables,
                                                        attr.fields(Pipeline)._original_settings,
                                                        attr.fields(Pipeline)._original_variables,
                                                        attr.fields(Pipeline).updated_by,
                                                        ))

        _json['projectId'] = self.project_id
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        _json['compositionId'] = self.composition_id
        _json['startNodes'] = self.start_nodes
        _json['orgId'] = self.org_id
        _json['nodes'] = [node.to_json() for node in self.nodes]
        _json['connections'] = [con.to_json() for con in self.connections]
        if self.variables:
            _json['variables'] = [v.to_json() for v in self.variables]
        _json['url'] = self.url

        settings_json = self.settings.to_json()
        if settings_json:
            _json['settings'] = settings_json

        if self.preview is not None:
            _json['preview'] = self.preview
        if self.description is not None:
            _json['description'] = self.description
        if self.revisions is not None:
            _json['revisions'] = self.revisions
        if self.updated_by is not None:
            _json['updatedBy'] = self.updated_by

        return _json

    #########
    # Props #
    #########

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/pipelines/{}".format(self.project_id, self.id))

    @property
    def project(self):
        if self._project is None:
            self._project = self.projects.get(project_id=self.project_id, fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['projects', 'pipelines', 'pipeline_executions', 'triggers', 'nodes'])

        r = reps(
            projects=repositories.Projects(client_api=self._client_api),
            pipelines=repositories.Pipelines(client_api=self._client_api, project=self._project),
            pipeline_executions=repositories.PipelineExecutions(
                client_api=self._client_api, project=self._project, pipeline=self
            ),
            triggers=repositories.Triggers(client_api=self._client_api, pipeline=self),
            nodes=repositories.Nodes(client_api=self._client_api, pipeline=self)
        )
        return r

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def triggers(self):
        assert isinstance(self._repositories.triggers, repositories.Triggers)
        return self._repositories.triggers

    @property
    def nodes(self):
        assert isinstance(self._repositories.nodes, repositories.Nodes)
        return self._repositories.nodes

    @property
    def pipelines(self):
        assert isinstance(self._repositories.pipelines, repositories.Pipelines)
        return self._repositories.pipelines

    @property
    def pipeline_executions(self):
        assert isinstance(self._repositories.pipeline_executions, repositories.PipelineExecutions)
        return self._repositories.pipeline_executions

    ###########
    # methods #
    ###########
    def update(self):
        """
        Update pipeline changes to platform

        :return: pipeline entity
        """
        return self.pipelines.update(pipeline=self)

    def delete(self):
        """
        Delete pipeline object

        :return: True
        """
        return self.pipelines.delete(pipeline=self)

    def open_in_web(self):
        """
        Open the pipeline in web platform

        :return:
        """
        self._client_api._open_in_web(url=self.platform_url)

    def install(self, resume_option: PipelineResumeOption = None):
        """
        install pipeline

        :return: Composition entity
        """
        return self.pipelines.install(pipeline=self, resume_option=resume_option)

    def pause(self, keep_triggers_active: bool = None):
        """
        pause pipeline

        :return: Composition entity
        """
        return self.pipelines.pause(pipeline=self, keep_triggers_active=keep_triggers_active)

    def execute(self, execution_input=None, node_id: str = None):
        """
        execute a pipeline and return to execute

        :param execution_input: list of the dl.FunctionIO or dict of pipeline input - example {'item': 'item_id'}
        :param str node_id: node id to execute
        :return: entities.PipelineExecution object
        """
        execution = self.pipeline_executions.create(
            pipeline_id=self.id,
            execution_input=execution_input,
            node_id=node_id
        )
        return execution

    def execute_batch(
            self,
            filters,
            execution_inputs=None,
            wait=True,
            node_id: str = None
    ):
        """
        execute a pipeline and return to execute

        :param execution_inputs: list of the dl.FunctionIO or dict of pipeline input - example {'item': 'item_id'}, that represent the extra inputs of the function
        :param filters: Filters entity for a filtering before execute
        :param bool wait: wait until create task finish
        :param str node_id: node id to execute
        :return: entities.PipelineExecution object

        **Example**:

        .. code-block:: python

            command = pipeline.execute_batch(
                        execution_inputs=dl.FunctionIO(type=dl.PackageInputType.STRING, value='test', name='string'),
                        filters=dl.Filters(field='dir', values='/test', context={'datasets': [dataset.id]))
        """
        command = self.pipeline_executions.create_batch(
            pipeline_id=self.id,
            execution_inputs=execution_inputs,
            filters=filters,
            wait=wait,
            node_id=node_id
        )
        return command

    def reset(self, stop_if_running: bool = False):
        """
        Resets pipeline counters

        :param bool stop_if_running: If the pipeline is installed it will stop the pipeline and reset the counters.
        :return: bool
        """
        return self.pipelines.reset(pipeline_id=self.id, stop_if_running=stop_if_running)

    def stats(self):
        """
        Get pipeline counters

        :return: PipelineStats
        :rtype: dtlpy.entities.pipeline.PipelineStats
        """
        return self.pipelines.stats(pipeline_id=self.id)

    def set_start_node(self, node: PipelineNode):
        """
        Set the start node of the pipeline

        :param PipelineNode node: node to be the start node
        """
        connections = [connection for connection in self.connections if connection.target.node_id == node.node_id]
        if connections:
            raise Exception(
                'Connections cannot be added to Pipeline start-node. To add a connection, please reposition the start sign')
        if self.start_nodes:
            for pipe_node in self.start_nodes:
                if pipe_node['type'] == 'root':
                    pipe_node['nodeId'] = node.node_id
        else:
            self.start_nodes = [{"nodeId": node.node_id,
                                 "type": "root", }]

    def update_variables_values(self, **kwargs):
        """
        Update pipeline variables values for the given keyword arguments.

        **Example**:

        .. code-block:: python
            pipeline.update_variables_values(
                dataset=dataset.id,
                model=model.id,
                threshold=0.9
            )
            pipeline.update()
        """
        keys = kwargs.keys()
        for variable in self.variables:
            if variable.name in keys:
                variable.value = kwargs[variable.name]


================================================
File: dtlpy/entities/pipeline_execution.py
================================================
from collections import namedtuple
import logging
import traceback
from enum import Enum

import attr

from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class PipelineExecutionStatus(str, Enum):
    PENDING = "pending"
    IN_PROGRESS = "in-progress"
    FAILED = "failed"
    SUCCESS = "success"
    QUEUE = "queue"
    TERMINATED = "terminated"
    RERUN = "rerun"


class CycleRerunMethod(str, Enum):
    START_FROM_NODES = 'startFromNodes',
    START_FROM_FAILED_EXECUTIONS = 'startFromFailedExecutions',
    START_FROM_BEGINNING = 'startFromBeginning'


class PipelineExecutionNode:
    def __init__(self, name, node_id, ports, metadata, node_type, namespace, project_id, status):
        self.node_id = node_id
        self.namespace = namespace
        self.node_type = node_type
        self.status = status
        self.ports = ports
        self.metadata = metadata
        self.project_id = project_id
        self.name = name

    @staticmethod
    def from_json(_json: dict):
        ports = [entities.PipelineNodeIO.from_json(_json=i_input) for i_input in _json.get('ports', list())]
        return PipelineExecutionNode(
            node_id=_json.get('id', None),
            namespace=_json.get('namespace', None),
            node_type=_json.get('type', None),
            status=_json.get('status', None),
            ports=ports,
            metadata=_json.get('metadata', None),
            project_id=_json.get('projectId', None),
            name=_json.get('name', None),
        )

    def to_json(self):
        _json = {
            'id': self.node_id,
            'namespace': self.namespace,
            'type': self.node_type,
            'status': self.status,
            'ports': [_io.to_json() for _io in self.ports],
            'metadata': self.metadata,
            'projectId': self.project_id,
            'name': self.name,
        }

        return _json


@attr.s
class PipelineExecution(entities.BaseEntity):
    """
    Package object
    """
    # platform
    id = attr.ib()
    nodes = attr.ib(repr=False)
    executions = attr.ib(repr=False)
    status = attr.ib()
    # name change
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    pipeline_id = attr.ib()
    max_attempts = attr.ib()
    creator = attr.ib()

    # sdk
    _pipeline = attr.ib(repr=False)
    _project = attr.ib(repr=False)
    _client_api = attr.ib(type=ApiClient, repr=False)
    _repositories = attr.ib(repr=False)

    @staticmethod
    def _protected_from_json(_json, client_api, pipeline=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error
        :param _json: platform json
        :param client_api: ApiClient entity
        :param pipeline: Pipeline entity
        :param is_fetched: is Entity fetched from Platform
        :return:
        """
        try:
            pipeline = PipelineExecution.from_json(
                _json=_json,
                client_api=client_api,
                pipeline=pipeline,
                is_fetched=is_fetched
            )
            status = True
        except Exception:
            pipeline = traceback.format_exc()
            status = False
        return status, pipeline

    @classmethod
    def from_json(cls, _json, client_api, pipeline=None, is_fetched=True) -> 'PipelineExecution':
        """
        Turn platform representation of pipeline_execution into a pipeline_execution entity

        :param dict _json: platform representation of package
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.pipeline.Pipeline pipeline: Pipeline entity
        :param bool is_fetched: is Entity fetched from Platform
        :return: Pipeline entity
        :rtype: dtlpy.entities.PipelineExecution
        """
        project = None
        if pipeline is not None:
            project = pipeline._project
            if pipeline.id != _json.get('pipelineId', None):
                logger.warning('Pipeline has been fetched from a project that is not belong to it')
                pipeline = None

        nodes = [PipelineExecutionNode.from_json(_json=node) for node in _json.get('nodes', list())]

        inst = cls(
            id=_json.get('id', None),
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            pipeline_id=_json.get('pipelineId', None),
            status=_json.get('status', None),
            max_attempts=_json.get('maxAttempts', None),
            creator=_json.get('creator', None),
            nodes=nodes,
            executions=_json.get('executions', dict()),
            pipeline=pipeline,
            project=project,
            client_api=client_api,
        )

        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Turn Package entity into a platform representation of Package

        :return: platform json of package
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(PipelineExecution)._repositories,
                                                        attr.fields(PipelineExecution)._client_api,
                                                        attr.fields(PipelineExecution)._pipeline,
                                                        attr.fields(PipelineExecution).nodes,
                                                        attr.fields(PipelineExecution).created_at,
                                                        attr.fields(PipelineExecution).updated_at,
                                                        attr.fields(PipelineExecution).pipeline_id,
                                                        attr.fields(PipelineExecution).executions,
                                                        attr.fields(PipelineExecution).max_attempts
                                                        ))
        executions = dict()
        for node_id, executions_list in self.executions.items():
            if len(executions_list) > 0 and isinstance(executions_list[0], entities.Execution):
                executions[node_id] = [e.to_json() for e in executions_list]
            else:
                executions[node_id] = executions_list

        _json['pipelineId'] = self.pipeline_id
        _json['maxAttempts'] = self.max_attempts
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        _json['nodes'] = [node.to_json() for node in self.nodes]
        _json['executions'] = executions
        return _json

    #########
    # Props #
    #########
    @property
    def pipeline(self):
        if self._pipeline is None:
            self._pipeline = self.pipelines.get(pipeline_id=self.pipeline_id, fetch=None)
        assert isinstance(self._pipeline, entities.Pipeline)
        return self._pipeline

    @property
    def project(self):
        if self._project is None:
            self._project = self.pipeline.project
        assert isinstance(self._pipeline.project, entities.Project)
        return self._pipeline.project

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['projects', 'pipelines', 'pipeline_executions'])

        r = reps(
            projects=repositories.Projects(client_api=self._client_api),
            pipelines=repositories.Pipelines(client_api=self._client_api, project=self._project),
            pipeline_executions=repositories.PipelineExecutions(client_api=self._client_api,
                                                                project=self._project,
                                                                pipeline=self._pipeline)
        )
        return r

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def pipelines(self):
        assert isinstance(self._repositories.pipelines, repositories.Pipelines)
        return self._repositories.pipelines

    @property
    def pipeline_executions(self):
        assert isinstance(self._repositories.pipeline_executions, repositories.PipelineExecutions)
        return self._repositories.pipeline_executions

    def rerun(self,
              method: str = None,
              start_nodes_ids: list = None,
              wait: bool = True
              ) -> bool:
        """
        Get Pipeline Execution object

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param str method: method to run
        :param list start_nodes_ids: list of start nodes ids
        :param bool wait: wait until rerun finish
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            pipeline_executions.rerun(method=dl.CycleRerunMethod.START_FROM_BEGINNING,)
        """
        filters = entities.Filters(field='id', values=[self.id], operator=entities.FiltersOperations.IN,
                                   resource=entities.FiltersResource.PIPELINE_EXECUTION)
        return self.pipeline_executions.rerun(
            method=method,
            start_nodes_ids=start_nodes_ids,
            filters=filters,
            wait=wait
        )

    def wait(self):
        """
        Wait for pipeline execution

        :return: Pipeline execution object
        """
        return self.pipeline_executions.wait(pipeline_execution_id=self.id)

    def in_progress(self):
        return self.status not in [PipelineExecutionStatus.FAILED,
                                   PipelineExecutionStatus.SUCCESS,
                                   PipelineExecutionStatus.TERMINATED]


================================================
File: dtlpy/entities/project.py
================================================
from collections import namedtuple
from enum import Enum
import traceback
import logging
import attr

from .. import repositories, miscellaneous, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class MemberRole(str, Enum):
    OWNER = "owner"
    DEVELOPER = "engineer"
    ANNOTATOR = "annotator"
    ANNOTATION_MANAGER = "annotationManager"


@attr.s()
class Project(entities.BaseEntity):
    """
    Project entity
    """

    _contributors = attr.ib(repr=False)
    created_at = attr.ib()
    creator = attr.ib()
    id = attr.ib()
    url = attr.ib()
    name = attr.ib()
    org = attr.ib(repr=False)
    updated_at = attr.ib(repr=False)
    role = attr.ib(repr=False)
    account = attr.ib(repr=False)
    is_blocked = attr.ib(repr=False)
    archived = attr.ib(repr=False)

    # name change
    feature_constraints = attr.ib()

    # api
    _client_api = attr.ib(type=ApiClient, repr=False)

    # repositories
    _repositories = attr.ib(repr=False)

    @property
    def isBlocked(self):
        return self.is_blocked

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple(
            'repositories',
            'projects triggers datasets items recipes packages codebases artifacts times_series services '
            'executions assignments tasks bots webhooks models analytics ontologies '
            'drivers pipelines feature_sets features integrations settings apps dpks compositions'
        )
        datasets = repositories.Datasets(client_api=self._client_api, project=self)
        return reps(
            projects=repositories.Projects(client_api=self._client_api),
            webhooks=repositories.Webhooks(client_api=self._client_api, project=self),
            items=repositories.Items(client_api=self._client_api, datasets=datasets, project=self),
            recipes=repositories.Recipes(client_api=self._client_api, project=self, project_id=self.id),
            datasets=datasets,
            executions=repositories.Executions(client_api=self._client_api, project=self),
            triggers=repositories.Triggers(client_api=self._client_api, project=self),
            packages=repositories.Packages(project=self, client_api=self._client_api),
            models=repositories.Models(project=self, client_api=self._client_api),
            codebases=repositories.Codebases(project=self, client_api=self._client_api),
            artifacts=repositories.Artifacts(project=self, client_api=self._client_api),
            times_series=repositories.TimesSeries(project=self, client_api=self._client_api),
            services=repositories.Services(client_api=self._client_api, project=self),
            assignments=repositories.Assignments(project=self, client_api=self._client_api),
            tasks=repositories.Tasks(client_api=self._client_api, project=self),
            bots=repositories.Bots(client_api=self._client_api, project=self),
            analytics=repositories.Analytics(client_api=self._client_api, project=self),
            ontologies=repositories.Ontologies(client_api=self._client_api, project=self),
            drivers=repositories.Drivers(client_api=self._client_api, project=self),
            pipelines=repositories.Pipelines(client_api=self._client_api, project=self),
            feature_sets=repositories.FeatureSets(client_api=self._client_api, project=self),
            features=repositories.Features(client_api=self._client_api, project=self),
            integrations=repositories.Integrations(client_api=self._client_api, project=self),
            settings=repositories.Settings(client_api=self._client_api,
                                           project=self,
                                           resource=self,
                                           resource_type=entities.PlatformEntityType.PROJECT),
            apps=repositories.Apps(client_api=self._client_api, project=self),
            dpks=repositories.Dpks(client_api=self._client_api, project=self),
            compositions=repositories.Compositions(client_api=self._client_api, project=self)
        )

    @property
    def drivers(self):
        assert isinstance(self._repositories.drivers, repositories.Drivers)
        return self._repositories.drivers

    @property
    def integrations(self):
        assert isinstance(self._repositories.integrations, repositories.Integrations)
        return self._repositories.integrations

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}".format(self.id))

    @property
    def triggers(self):
        assert isinstance(self._repositories.triggers, repositories.Triggers)
        return self._repositories.triggers

    @property
    def ontologies(self):
        assert isinstance(self._repositories.ontologies, repositories.Ontologies)
        return self._repositories.ontologies

    @property
    def items(self):
        assert isinstance(self._repositories.items, repositories.Items)
        return self._repositories.items

    @property
    def recipes(self):
        assert isinstance(self._repositories.recipes, repositories.Recipes)
        return self._repositories.recipes

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    @property
    def executions(self):
        assert isinstance(self._repositories.executions, repositories.Executions)
        return self._repositories.executions

    @property
    def apps(self):
        assert isinstance(self._repositories.apps, repositories.Apps)
        return self._repositories.apps

    @property
    def dpks(self):
        assert isinstance(self._repositories.dpks, repositories.Dpks)
        return self._repositories.dpks

    @property
    def compositions(self):
        assert isinstance(self._repositories.compositions, repositories.Compositions)
        return self._repositories.compositions

    @property
    def projects(self):
        assert isinstance(self._repositories.projects, repositories.Projects)
        return self._repositories.projects

    @property
    def datasets(self):
        assert isinstance(self._repositories.datasets, repositories.Datasets)
        return self._repositories.datasets

    @property
    def pipelines(self):
        assert isinstance(self._repositories.pipelines, repositories.Pipelines)
        return self._repositories.pipelines

    @property
    def packages(self):
        assert isinstance(self._repositories.packages, repositories.Packages)
        return self._repositories.packages

    @property
    def models(self):
        assert isinstance(self._repositories.models, repositories.Models)
        return self._repositories.models

    @property
    def codebases(self):
        assert isinstance(self._repositories.codebases, repositories.Codebases)
        return self._repositories.codebases

    @property
    def webhooks(self):
        assert isinstance(self._repositories.webhooks, repositories.Webhooks)
        return self._repositories.webhooks

    @property
    def artifacts(self):
        assert isinstance(self._repositories.artifacts, repositories.Artifacts)
        return self._repositories.artifacts

    @property
    def times_series(self):
        assert isinstance(self._repositories.times_series, repositories.TimesSeries)
        return self._repositories.times_series

    @property
    def assignments(self):
        assert isinstance(self._repositories.assignments, repositories.Assignments)
        return self._repositories.assignments

    @property
    def tasks(self):
        assert isinstance(self._repositories.tasks, repositories.Tasks)
        return self._repositories.tasks

    @property
    def bots(self):
        assert isinstance(self._repositories.bots, repositories.Bots)
        return self._repositories.bots

    @property
    def analytics(self):
        assert isinstance(self._repositories.analytics, repositories.Analytics)
        return self._repositories.analytics

    @property
    def feature_sets(self):
        assert isinstance(self._repositories.feature_sets, repositories.FeatureSets)
        return self._repositories.feature_sets

    @property
    def features(self):
        assert isinstance(self._repositories.features, repositories.Features)
        return self._repositories.features

    @property
    def settings(self):
        assert isinstance(self._repositories.settings, repositories.Settings)
        return self._repositories.settings

    @property
    def contributors(self):
        return miscellaneous.List([entities.User.from_json(_json=_json,
                                                           client_api=self._client_api,
                                                           project=self) for _json in self._contributors])

    @staticmethod
    def _protected_from_json(_json, client_api):
        """
        Same as from_json but with try-except to catch if error

        :param dict _json: platform json
        :param dl.ApiClient client_api: ApiClient entity
        :return:
        """
        try:
            project = Project.from_json(_json=_json,
                                        client_api=client_api)
            status = True
        except Exception:
            project = traceback.format_exc()
            status = False
        return status, project

    @classmethod
    def from_json(cls, _json, client_api, is_fetched=True):
        """
        Build a Project object from a json

        :param bool is_fetched: is Entity fetched from Platform
        :param dict _json: _json response from host
        :param dl.ApiClient client_api: ApiClient entity
        :return: Project object
        :rtype: dtlpy.entities.project.Project
        """
        inst = cls(feature_constraints=_json.get('featureConstraints', None),
                   contributors=_json.get('contributors', None),
                   is_blocked=_json.get('isBlocked', None),
                   created_at=_json.get('createdAt', None),
                   updated_at=_json.get('updatedAt', None),
                   creator=_json.get('creator', None),
                   account=_json.get('account', None),
                   name=_json.get('name', None),
                   role=_json.get('role', None),
                   org=_json.get('org', None),
                   id=_json.get('id', None),
                   url=_json.get('url', None),
                   archived=_json.get('archived', None),
                   client_api=client_api)
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of project object

        :return: platform json format of project object
        :rtype: dict
        """
        output_dict = attr.asdict(self,
                                  filter=attr.filters.exclude(attr.fields(Project)._client_api,
                                                              attr.fields(Project)._repositories,
                                                              attr.fields(Project).feature_constraints,
                                                              attr.fields(Project)._contributors,
                                                              attr.fields(Project).created_at,
                                                              attr.fields(Project).updated_at,
                                                              attr.fields(Project).is_blocked,
                                                              ))
        output_dict['contributors'] = self._contributors
        output_dict['featureConstraints'] = self.feature_constraints
        output_dict['createdAt'] = self.created_at
        output_dict['updatedAt'] = self.updated_at
        output_dict['isBlocked'] = self.is_blocked

        return output_dict

    def delete(self, sure=False, really=False):
        """
        Delete the project forever!

        :param bool sure: Are you sure you want to delete?
        :param bool really: Really really sure?
        :return: True if success, error if not
        :rtype: bool
        """
        return self.projects.delete(project_id=self.id,
                                    sure=sure,
                                    really=really)

    def update(self, system_metadata=False):
        """
        Update the project

        :param bool system_metadata: optional - True, if you want to change metadata system
        :return: Project object
        :rtype: dtlpy.entities.project.Project
        """
        return self.projects.update(project=self, system_metadata=system_metadata)

    def checkout(self):
        """
        Checkout (switch) to a project to work on.

        """
        self.projects.checkout(project=self)

    def open_in_web(self):
        """
        Open the project in web platform

        """
        self._client_api._open_in_web(url=self.platform_url)

    def add_member(self, email, role: MemberRole = MemberRole.DEVELOPER):
        """
        Add a member to the project.

        :param str email: member email
        :param role: The required role for the user. Use the enum dl.MemberRole
        :return: dict that represent the user
        :rtype: dict
        """
        return self.projects.add_member(email=email, role=role, project_id=self.id)

    def update_member(self, email, role: MemberRole = MemberRole.DEVELOPER):
        """
        Update member's information/details from the project.

        :param str email: member email
        :param role: The required role for the user. Use the enum dl.MemberRole
        :return: dict that represent the user
        :rtype: dict
        """
        return self.projects.update_member(email=email, role=role, project_id=self.id)

    def remove_member(self, email):
        """
        Remove a member from the project.

        :param str email: member email
        :return: dict that represents the user
        :rtype: dict
        """
        return self.projects.remove_member(email=email, project_id=self.id)

    def list_members(self, role: MemberRole = None):
        """
        List the project members.

        :param role: The required role for the user. Use the enum dl.MemberRole
        :return: list of the project members
        :rtype: list
        """
        return self.projects.list_members(project=self, role=role)


================================================
File: dtlpy/entities/prompt_item.py
================================================
import requests
import logging
import base64
import enum
import json
import io
import os

from concurrent.futures import ThreadPoolExecutor
from .. import entities, repositories
from dtlpy.services.api_client import client as client_api

logger = logging.getLogger(name='dtlpy')


class PromptType(str, enum.Enum):
    TEXT = 'application/text'
    IMAGE = 'image/*'
    AUDIO = 'audio/*'
    VIDEO = 'video/*'
    METADATA = 'metadata'


class Prompt:
    def __init__(self, key, role='user'):
        """
        Create a single Prompt. Prompt can contain multiple mimetype elements, e.g. text sentence and an image.
        :param key: unique identifier of the prompt in the item
        """
        self.key = key
        self.elements = list()
        # to avoid broken stream of json files - DAT-75653
        client_api.default_headers['x-dl-sanitize'] = '0'
        self._items = repositories.Items(client_api=client_api)
        self.metadata = {'role': role}

    def add_element(self, value, mimetype='application/text'):
        """

        :param value: url or string of the input
        :param mimetype: mimetype of the input. options: `text`, `image/*`, `video/*`, `audio/*`
        :return:
        """
        allowed_prompt_types = [prompt_type for prompt_type in PromptType]
        if mimetype not in allowed_prompt_types:
            raise ValueError(f'Invalid mimetype: {mimetype}. Allowed values: {allowed_prompt_types}')
        if mimetype == PromptType.METADATA and isinstance(value, dict):
            self.metadata.update(value)
        else:
            self.elements.append({'mimetype': mimetype,
                                  'value': value})

    def to_json(self):
        """
        Convert Prompt entity to the item json

        :return:
        """
        elements_json = [
            {
                "mimetype": e['mimetype'],
                "value": e['value'],
            } for e in self.elements if not e['mimetype'] == PromptType.METADATA
        ]
        elements_json.append({
            "mimetype": PromptType.METADATA,
            "value": self.metadata
        })
        return {
            self.key: elements_json
        }

    def _convert_stream_to_binary(self, image_url: str):
        """
        Convert a stream to binary
        :param image_url: dataloop image stream url
        :return: binary object
        """
        image_buffer = None
        if '.' in image_url and 'dataloop.ai' not in image_url:
            # URL and not DL item stream
            try:
                response = requests.get(image_url, stream=True)
                response.raise_for_status()  # Raise an exception for bad status codes

                # Check for valid image content type
                if response.headers["Content-Type"].startswith("image/"):
                    # Read the image data in chunks to avoid loading large images in memory
                    image_buffer = b"".join(chunk for chunk in response.iter_content(1024))
            except requests.exceptions.RequestException as e:
                logger.error(f"Failed to download image from URL: {image_url}, error: {e}")

        elif '.' in image_url and 'stream' in image_url:
            # DL Stream URL
            item_id = image_url.split("/stream")[0].split("/items/")[-1]
            image_buffer = self._items.get(item_id=item_id).download(save_locally=False).getvalue()
        else:
            # DL item ID
            image_buffer = self._items.get(item_id=image_url).download(save_locally=False).getvalue()

        if image_buffer is not None:
            encoded_image = base64.b64encode(image_buffer).decode()
        else:
            logger.error(f'Invalid image url: {image_url}')
            return None

        return f'data:image/jpeg;base64,{encoded_image}'

    def messages(self):
        """
        return a list of messages in the prompt item,
        messages are returned following the openai SDK format https://platform.openai.com/docs/guides/vision
        """
        messages = []
        for element in self.elements:
            if element['mimetype'] == PromptType.TEXT:
                data = {
                    "type": "text",
                    "text": element['value']
                }
                messages.append(data)
            elif element['mimetype'] == PromptType.IMAGE:
                image_url = self._convert_stream_to_binary(element['value'])
                data = {
                    "type": "image_url",
                    "image_url": {
                        "url": image_url
                    }
                }
                messages.append(data)
            elif element['mimetype'] == PromptType.AUDIO:
                raise NotImplementedError('Audio prompt is not supported yet')
            elif element['mimetype'] == PromptType.VIDEO:
                raise NotImplementedError('Video prompt is not supported yet')
            else:
                raise ValueError(f'Invalid mimetype: {element["mimetype"]}')
        return messages, self.key


class PromptItem:
    def __init__(self, name, item: entities.Item = None, role_mapping=None):
        if role_mapping is None:
            role_mapping = {'user': 'item',
                            'assistant': 'annotation'}
        if not isinstance(role_mapping, dict):
            raise ValueError(f'input role_mapping must be dict. type: {type(role_mapping)}')
        self.role_mapping = role_mapping
        # prompt item name
        self.name = name
        # list of user prompts in the prompt item
        self.prompts = list()
        self.assistant_prompts = list()
        # list of assistant (annotations) prompts in the prompt item
        # Dataloop Item
        self._messages = []
        self._item: entities.Item = item
        self._annotations: entities.AnnotationCollection = None
        if item is not None:
            self._items = item.items
            self.fetch()
        else:
            self._items = repositories.Items(client_api=client_api)
        # to avoid broken stream of json files - DAT-75653
        self._items._client_api.default_headers['x-dl-sanitize'] = '0'

    @classmethod
    def from_messages(cls, messages: list):
        ...

    @classmethod
    def from_item(cls, item: entities.Item):
        """
        Load a prompt item from the platform
        :param item : Item object
        :return: PromptItem object
        """
        if 'json' not in item.mimetype or item.system.get('shebang', dict()).get('dltype') != 'prompt':
            raise ValueError('Expecting a json item with system.shebang.dltype = prompt')
        return cls(name=item.name, item=item)

    @classmethod
    def from_local_file(cls, filepath):
        """
        Create a new prompt item from a file
        :param filepath: path to the file
        :return: PromptItem object
        """
        if os.path.exists(filepath) is False:
            raise FileNotFoundError(f'File does not exists: {filepath}')
        if 'json' not in os.path.splitext(filepath)[-1]:
            raise ValueError(f'Expected path to json item, got {os.path.splitext(filepath)[-1]}')
        prompt_item = cls(name=filepath)
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        prompt_item.prompts = prompt_item._load_item_prompts(data=data)
        return prompt_item

    @staticmethod
    def _load_item_prompts(data):
        prompts = list()
        for prompt_key, prompt_elements in data.get('prompts', dict()).items():
            content = list()
            for element in prompt_elements:
                content.append({'value': element.get('value', dict()),
                                'mimetype': element['mimetype']})
            prompt = Prompt(key=prompt_key, role="user")
            for element in content:
                prompt.add_element(value=element.get('value', ''),
                                   mimetype=element.get('mimetype', PromptType.TEXT))
            prompts.append(prompt)
        return prompts

    @staticmethod
    def _load_annotations_prompts(annotations: entities.AnnotationCollection):
        """
        Get all the annotations in the item for the assistant messages
        """
        # clearing the assistant prompts from previous annotations that might not belong
        assistant_prompts = list()
        for annotation in annotations:
            prompt_id = annotation.metadata.get('system', dict()).get('promptId', None)
            model_info = annotation.metadata.get('user', dict()).get('model', dict())
            annotation_id = annotation.id
            if annotation.type == 'ref_image':
                prompt = Prompt(key=prompt_id, role='assistant')
                prompt.add_element(value=annotation.annotation_definition.coordinates.get('ref'),
                                   mimetype=PromptType.IMAGE)
            elif annotation.type == 'text':
                prompt = Prompt(key=prompt_id, role='assistant')
                prompt.add_element(value=annotation.annotation_definition.coordinates,
                                   mimetype=PromptType.TEXT)
            else:
                raise ValueError(f"Unsupported annotation type: {annotation.type}")

            prompt.add_element(value={'id': annotation_id,
                                      'model_info': model_info},
                               mimetype=PromptType.METADATA)
            assistant_prompts.append(prompt)
        return assistant_prompts

    def to_json(self):
        """
        Convert the entity to a platform item.

        :return:
        """
        prompts_json = {
            "shebang": "dataloop",
            "metadata": {
                "dltype": 'prompt'
            },
            "prompts": {}
        }
        for prompt in self.prompts:
            for prompt_key, prompt_values in prompt.to_json().items():
                prompts_json["prompts"][prompt_key] = prompt_values
        return prompts_json

    def to_messages(self, model_name=None, include_assistant=True):
        all_prompts_messages = dict()
        for prompt in self.prompts:
            if prompt.key not in all_prompts_messages:
                all_prompts_messages[prompt.key] = list()
            prompt_messages, prompt_key = prompt.messages()
            messages = {
                'role': prompt.metadata.get('role', 'user'),
                'content': prompt_messages
            }
            all_prompts_messages[prompt.key].append(messages)
        if include_assistant is True:
            # reload to filer model annotations
            for prompt in self.assistant_prompts:
                prompt_model_name = prompt.metadata.get('model_info', dict()).get('name')
                if model_name is not None and prompt_model_name != model_name:
                    continue
                if prompt.key not in all_prompts_messages:
                    logger.warning(
                        f'Prompt key {prompt.key} is not found in the user prompts, skipping Assistant prompt')
                    continue
                prompt_messages, prompt_key = prompt.messages()
                assistant_messages = {
                    'role': 'assistant',
                    'content': prompt_messages
                }
                all_prompts_messages[prompt.key].append(assistant_messages)
        res = list()
        for prompts in all_prompts_messages.values():
            for prompt in prompts:
                res.append(prompt)
        self._messages = res
        return self._messages

    def to_bytes_io(self):
        # Used for item upload, do not delete
        byte_io = io.BytesIO()
        byte_io.name = self.name
        byte_io.write(json.dumps(self.to_json()).encode())
        byte_io.seek(0)
        return byte_io

    def fetch(self):
        if self._item is None:
            raise ValueError('Missing item, nothing to fetch..')
        self._item = self._items.get(item_id=self._item.id)
        self._annotations = self._item.annotations.list()
        self.prompts = self._load_item_prompts(data=json.load(self._item.download(save_locally=False)))
        self.assistant_prompts = self._load_annotations_prompts(self._annotations)

    def build_context(self, nearest_items, add_metadata=None) -> str:
        """
        Create a context stream from nearest items list.
        add_metadata is a list of location in the item.metadata to add to the context, for instance ['system.document.source']
        :param nearest_items: list of item ids
        :param add_metadata: list of metadata location to add metadata to context
        :return:
        """
        if add_metadata is None:
            add_metadata = list()

        def stream_single(w_id):
            context_item = self._items.get(item_id=w_id)
            buf = context_item.download(save_locally=False)
            text = buf.read().decode(encoding='utf-8')
            m = ""
            for path in add_metadata:
                parts = path.split('.')
                value = context_item.metadata
                part = ""
                for part in parts:
                    if isinstance(value, dict):
                        value = value.get(part)
                    else:
                        value = ""

                m += f"{part}:{value}\n"
            return text, m

        pool = ThreadPoolExecutor(max_workers=32)
        context = ""
        if len(nearest_items) > 0:
            # build context
            results = pool.map(stream_single, nearest_items)
            for res in results:
                context += f"\n<source>\n{res[1]}\n</source>\n<text>\n{res[0]}\n</text>"
        return context

    def add(self,
            message: dict,
            prompt_key: str = None,
            model_info: dict = None):
        """
        add a prompt to the prompt item
        prompt: a dictionary. keys are prompt message id, values are prompt messages
        responses: a list of annotations representing responses to the prompt

        :param message:
        :param prompt_key:
        :param model_info:
        :return:
        """
        role = message.get('role', 'user')
        content = message.get('content', list())

        if self.role_mapping.get(role, 'item') == 'item':
            if prompt_key is None:
                prompt_key = str(len(self.prompts) + 1)
            # for new prompt we need a new key
            prompt = Prompt(key=prompt_key, role=role)
            for element in content:
                prompt.add_element(value=element.get('value', ''),
                                   mimetype=element.get('mimetype', PromptType.TEXT))

            # create new prompt and add to prompts
            self.prompts.append(prompt)
            if self._item is not None:
                self._item._Item__update_item_binary(_json=self.to_json())
        else:
            if prompt_key is None:
                prompt_key = str(len(self.prompts))
            assistant_message = content[0]
            assistant_mimetype = assistant_message.get('mimetype', PromptType.TEXT)
            uploaded_annotation = None

            # find if prompt
            if model_info is None:
                # dont search for existing if there's no model information
                existing_prompt = None
            else:
                existing_prompts = list()
                for prompt in self.assistant_prompts:
                    prompt_id = prompt.key
                    model_name = prompt.metadata.get('model_info', dict()).get('name')
                    if prompt_id == prompt_key and model_name == model_info.get('name'):
                        # TODO how to handle multiple annotations
                        existing_prompts.append(prompt)
                if len(existing_prompts) > 1:
                    assert False, "shouldn't be here! more than 1 annotation for a single model"
                elif len(existing_prompts) == 1:
                    # found model annotation to upload
                    existing_prompt = existing_prompts[0]
                else:
                    # no annotation found
                    existing_prompt = None

            if existing_prompt is None:
                prompt = Prompt(key=prompt_key)
                if assistant_mimetype == PromptType.TEXT:
                    annotation_definition = entities.FreeText(text=assistant_message.get('value'))
                    prompt.add_element(value=annotation_definition.to_coordinates(None),
                                       mimetype=PromptType.TEXT)
                elif assistant_mimetype == PromptType.IMAGE:
                    annotation_definition = entities.RefImage(ref=assistant_message.get('value'))
                    prompt.add_element(value=annotation_definition.to_coordinates(None).get('ref'),
                                       mimetype=PromptType.IMAGE)
                else:
                    raise NotImplementedError('Only images of mimetype image and text are supported')
                metadata = {'system': {'promptId': prompt_key},
                            'user': {'model': model_info}}
                prompt.add_element(mimetype=PromptType.METADATA,
                                   value={"model_info": model_info})

                existing_annotation = entities.Annotation.new(item=self._item,
                                                              metadata=metadata,
                                                              annotation_definition=annotation_definition)
                uploaded_annotation = existing_annotation.upload()
                prompt.add_element(mimetype=PromptType.METADATA,
                                   value={"id": uploaded_annotation.id})
                existing_prompt = prompt
                self.assistant_prompts.append(prompt)

            existing_prompt_element = [element for element in existing_prompt.elements if
                                       element['mimetype'] != PromptType.METADATA][-1]
            existing_prompt_element['value'] = assistant_message.get('value')
            if uploaded_annotation is None:
                # Creating annotation with old dict to match platform dict
                annotation_definition = entities.FreeText(text='')
                metadata = {'system': {'promptId': prompt_key},
                            'user': {'model': existing_prompt.metadata.get('model_info')}}
                annotation = entities.Annotation.new(item=self._item,
                                                     metadata=metadata,
                                                     annotation_definition=annotation_definition
                                                     )
                annotation.id = existing_prompt.metadata['id']
                # set the platform dict to match the old annotation for the dict difference check, otherwise it won't
                # update
                annotation._platform_dict = annotation.to_json()
                # update the annotation with the new text
                annotation.annotation_definition.text = existing_prompt_element['value']
                self._item.annotations.update(annotation)


================================================
File: dtlpy/entities/recipe.py
================================================
from collections import namedtuple
import traceback
import logging
import uuid
import attr
import os

from .. import repositories, entities, exceptions
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


@attr.s
class Recipe(entities.BaseEntity):
    """
    Recipe object
    """
    id = attr.ib()
    creator = attr.ib()
    url = attr.ib(repr=False)
    title = attr.ib()
    project_ids = attr.ib()
    description = attr.ib()
    ontology_ids = attr.ib(repr=False)
    instructions = attr.ib(repr=False)
    examples = attr.ib(repr=False)
    custom_actions = attr.ib(repr=False)
    metadata = attr.ib()
    created_at = attr.ib()
    updated_at = attr.ib()
    updated_by = attr.ib()

    # name change
    ui_settings = attr.ib()

    # platform
    _client_api = attr.ib(type=ApiClient, repr=False)
    # entities
    _dataset = attr.ib(repr=False, default=None)
    _project = attr.ib(repr=False, default=None)
    # repositories
    _repositories = attr.ib(repr=False)

    @property
    def customActions(self):
        return self.custom_actions

    @property
    def ontologyIds(self):
        logger.warning('Deprecation Warning - ontologyIds will be Deprecation from version 1.52.0 use ontology_ids')
        return self.ontology_ids

    @classmethod
    def from_json(cls, _json, client_api, dataset=None, project=None, is_fetched=True):
        """
        Build a Recipe entity object from a json

        :param dict _json: _json response from host
        :param dtlpy.entities.dataset.Dataset dataset: Dataset entity
        :param dtlpy.entities.project.Project project: project entity
        :param dl.ApiClient client_api: ApiClient entity
        :param bool is_fetched: is Entity fetched from Platform
        :return: Recipe object
        """
        project_ids = _json.get('projectIds', None)
        if project is not None and project_ids is not None:
            if project.id not in project_ids:
                logger.warning('Recipe has been fetched from a project that is not belong to it')
                project = None

        inst = cls(
            client_api=client_api,
            dataset=dataset,
            project=project,
            id=_json['id'],
            creator=_json.get('creator', None),
            url=_json.get('url', None),
            title=_json.get('title', None),
            project_ids=project_ids,
            description=_json.get('description', None),
            ontology_ids=_json.get('ontologyIds', None),
            instructions=_json.get('instructions', None),
            ui_settings=_json.get('uiSettings', None),
            metadata=_json.get('metadata', None),
            examples=_json.get('examples', None),
            custom_actions=_json.get('customActions', None),
            created_at=_json.get("createdAt", None),
            updated_at=_json.get("updatedAt", None),
            updated_by=_json.get("updatedBy", None))
        inst.is_fetched = is_fetched
        return inst

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, dataset=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param client_api: ApiClient entity
        :param project: project entity
        :param dataset:  dataset entity
        :param is_fetched:  is Entity fetched from Platform
        :return:
        """
        try:
            recipe = Recipe.from_json(_json=_json,
                                      client_api=client_api,
                                      project=project,
                                      dataset=dataset,
                                      is_fetched=is_fetched)
            status = True
        except Exception:
            recipe = traceback.format_exc()
            status = False
        return status, recipe

    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['ontologies', 'recipes'])
        if self._dataset is None and self._project is None:
            recipes = repositories.Recipes(client_api=self._client_api, dataset=self._dataset, project=self._project)
        elif self._dataset is not None:
            recipes = self.dataset.recipes
        else:
            recipes = self.project.recipes
        r = reps(ontologies=repositories.Ontologies(recipe=self, client_api=self._client_api),
                 recipes=recipes)
        return r

    @property
    def dataset(self):
        if self._dataset is not None:
            assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @property
    def project(self):
        if self._project is not None:
            assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def recipes(self):
        assert isinstance(self._repositories.recipes, repositories.Recipes)
        return self._repositories.recipes

    @property
    def ontologies(self):
        assert isinstance(self._repositories.ontologies, repositories.Ontologies)
        return self._repositories.ontologies

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(Recipe)._client_api,
                                                              attr.fields(Recipe)._dataset,
                                                              attr.fields(Recipe)._project,
                                                              attr.fields(Recipe).project_ids,
                                                              attr.fields(Recipe).ui_settings,
                                                              attr.fields(Recipe)._repositories,
                                                              attr.fields(Recipe).custom_actions,
                                                              attr.fields(Recipe).ontology_ids,
                                                              attr.fields(Recipe).created_at,
                                                              attr.fields(Recipe).updated_at,
                                                              attr.fields(Recipe).updated_by,
                                                              ))
        _json['uiSettings'] = self.ui_settings
        _json['projectIds'] = self.project_ids
        _json['customActions'] = self.custom_actions
        _json['ontologyIds'] = self.ontology_ids
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        _json['updatedBy'] = self.updated_by
        return _json

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/recipes/{}".format(self.project_ids[0], self.id))

    def open_in_web(self):
        """
        Open the recipes in web platform

        :return:
        """
        self._client_api._open_in_web(url=self.platform_url)

    def delete(self, force: bool = False):
        """
        Delete recipe from platform

        :param bool force: force delete recipe

        :return: True
        :rtype: bool
        """
        return self.recipes.delete(self.id, force=force)

    def update(self, system_metadata=False):
        """
        Update Recipe

        :param bool system_metadata: bool - True, if you want to change metadata system
        :return: Recipe object
        :rtype: dtlpy.entities.recipe.Recipe
        """
        return self.recipes.update(recipe=self, system_metadata=system_metadata)

    def clone(self, shallow=False):
        """
        Clone Recipe

       :param bool shallow: If True, link ot existing ontology, clones all ontology that are link to the recipe as well
       :return: Cloned ontology object
       :rtype: dtlpy.entities.recipe.Recipe
        """
        return self.recipes.clone(recipe=self,
                                  shallow=shallow)

    def get_annotation_template_id(self, template_name):
        """
        Get annotation template id by template name

       :param str template_name:
       :return: template id or None if does not exist
        """
        collection_templates = list()
        if 'system' in self.metadata and 'collectionTemplates' in self.metadata['system']:
            collection_templates = self.metadata['system']['collectionTemplates']

        for template in collection_templates:
            if "name" and 'id' in template:
                if template_name == template['name']:
                    return template['id']
        raise exceptions.NotFound('404', "annotation template {!r} not found".format(template_name))

    def add_instruction(self, annotation_instruction_file):
        """
        Add instruction to recipe

        :param str annotation_instruction_file: file path or url of the recipe instruction
        """
        _, ext = os.path.splitext(annotation_instruction_file)
        if ext != '.pdf':
            raise exceptions.PlatformException(error='400',
                                               message='file Must be pdf')
        for project_id in self.project_ids:
            project = repositories.Projects(client_api=self._client_api).get(project_id=project_id)
            dataset = project.datasets._get_binaries_dataset()
            remote_path = '/.dataloop/recipes/{}/instructions'.format(self.id)
            instruction_item = dataset.items.upload(local_path=annotation_instruction_file,
                                                    remote_path=remote_path,
                                                    remote_name=str(uuid.uuid4()) + '.pdf',
                                                    overwrite=True)
            self.metadata['system']['instructionDocument'] = {'itemId': instruction_item.id,
                                                              'datasetId': dataset.id,
                                                              'name': instruction_item.name}
            self.update(True)

    def upload_annotations_verification_file(self, local_path: str, overwrite: bool = False) -> entities.Item:
        """
        Add Annotations Verification js file to the recipe.

        :param str local_path: file path of the annotations verification js file.
        :param bool overwrite: overwrite exiting file if the local and the remote names are matching
        :return: annotations verification js item.
        """

        validation_file_metadata = self.metadata.get("system", dict()).get("validationFile", None)
        if validation_file_metadata is None:
            validation_file_metadata = dict()

        remote_name = validation_file_metadata.get("name", None)
        local_name = os.path.basename(local_path)
        binaries_dataset = self._project.datasets._get_binaries_dataset()
        remote_path = f"/.dataloop/recipes/{self.id}/verification/"

        if remote_name is None or overwrite or remote_name != local_name:
            validation_item = binaries_dataset.items.upload(
                local_path=local_path,
                remote_path=remote_path,
                remote_name=local_name,
                overwrite=True
            )
            self.metadata["system"]["validationFile"] = {
                "itemId": validation_item.id,
                "datasetId": binaries_dataset.id,
                "name": local_name
            }
            self.update(system_metadata=True)
        else:
            logger.debug(f"Existing Annotations Validation Script was found.")
            validation_item_id = self.metadata["system"]["validationFile"]["itemId"]
            validation_item = binaries_dataset.items.get(item_id=validation_item_id)
        return validation_item


================================================
File: dtlpy/entities/reflect_dict.py
================================================
class ReflectDict(dict):

    def __init__(self, value_type: type, start: int = None, end: int = None, on_access: callable = None):
        super(ReflectDict, self).__init__()
        self.value_type = value_type
        self.on_access = on_access
        self._start = int(start) if start is not None else 0
        self._end = int(end) if end is not None else 0

    @property
    def start(self):
        return self._start

    @start.setter
    def start(self, start):
        if not isinstance(start, float) and not isinstance(start, int):
            raise ValueError('Must input a valid number')
        self._start = int(start) if start is not None else 0

    @property
    def end(self):
        return self._end

    @end.setter
    def end(self, end):
        if not isinstance(end, float) and not isinstance(end, int):
            raise ValueError('Must input a valid number')
        self._end = int(end) if end is not None else 0

    def actual_keys(self):
        return super(ReflectDict, self).keys()

    def keys(self):
        sorted_keys = list(super(ReflectDict, self).keys())
        sorted_keys.sort()

        yield self._start
        last_yielded = self._start

        for key in sorted_keys:
            if last_yielded == key - 1:
                last_yielded = key
                yield key
            else:
                while last_yielded < key:
                    last_yielded = last_yielded + 1
                    yield last_yielded

        while last_yielded < self._end:
            last_yielded = last_yielded + 1
            yield last_yielded

    def values(self):
        for key in self.keys():
            yield self[key]

    def items(self):
        for key in self.keys():
            yield key, self[key]

    def __iter__(self):
        return self.keys()

    def __contains__(self, key):
        return key in list(self.keys())

    def __setitem__(self, key, value):
        if not isinstance(key, int):
            raise Exception('Key Error - key must be an integer')

        if key > self._end:
            self._end = key

        if key < self._start:
            self._start = key

        super(ReflectDict, self).__setitem__(key, value)

    def __getitem__(self, key):
        requested_key = key
        if not isinstance(key, int):
            raise Exception('Key Error - key must be an integer')

        if key < self._start or key > self._end:
            raise KeyError(key)
        elif super(ReflectDict, self).__contains__(key):
            return super(ReflectDict, self).__getitem__(key)
        else:
            while key > self._start:
                key = key - 1
                if super(ReflectDict, self).__contains__(key):
                    item = super(ReflectDict, self).__getitem__(key)
                    if isinstance(item, self.value_type):
                        if self.on_access is not None:
                            item = self.on_access(self, actual_key=key, requested_key=requested_key, val=item)
                        return item
                    else:
                        raise Exception('Unknown value type, dict must be of type: {}'.format(
                            self.value_type))

    def __len__(self):
        return len(list(self.keys()))


================================================
File: dtlpy/entities/resource_execution.py
================================================
import attr
import logging
import traceback

from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


@attr.s
class ResourceExecution(entities.BaseEntity):
    """
    Resource execution entity
    """
    # platform
    resource_id = attr.ib()
    resource_type = attr.ib()
    execution_id = attr.ib()
    status = attr.ib()
    timestamp = attr.ib()
    progress = attr.ib()
    message = attr.ib()
    error = attr.ib()
    service_name = attr.ib()
    function_name = attr.ib()
    module_name = attr.ib()
    package_name = attr.ib()
    org_name = attr.ib()
    creator = attr.ib()
    project_id = attr.ib()

    # sdk
    _client_api = attr.ib(type=ApiClient, repr=False)
    _project = attr.ib()
    resource = attr.ib()

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, resource=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param client_api: ApiClient entity
        :return:
        """
        try:
            execution = ResourceExecution.from_json(_json=_json,
                                                    client_api=client_api,
                                                    project=project,
                                                    resource=resource,
                                                    is_fetched=is_fetched)
            status = True
        except Exception:
            execution = traceback.format_exc()
            status = False
        return status, execution

    @classmethod
    def from_json(cls, _json, client_api, project=None, resource=None, is_fetched=True):
        """
        :param dict _json: platform json
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: project entity
        :param entity resource: an entity object (item, dataset, ...)
        :param is_fetched: is Entity fetched from Platform
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Execution has been fetched from a project that is not belong to it')
                project = None
        inst = cls(
            resource_id=_json.get('resourceId', None),
            resource_type=_json.get('resourceType', None),
            execution_id=_json.get('executionId', None),
            status=_json.get('status', None),
            timestamp=_json.get('timestamp', None),
            progress=_json.get('progress', None),
            message=_json.get('message', None),
            error=_json.get('error', None),
            service_name=_json.get('serviceName', None),
            function_name=_json.get('functionName', None),
            module_name=_json.get('moduleName', None),
            package_name=_json.get('packageName', None),
            org_name=_json.get('orgName', None),
            creator=_json.get('creator', None),
            project_id=_json.get('projectId', None),
            client_api=client_api,
            project=project,
            resource=resource,
        )
        inst.is_fetched = is_fetched
        return inst

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        # get excluded
        _json = attr.asdict(
            self, filter=attr.filters.exclude(
                attr.fields(ResourceExecution)._client_api,
                attr.fields(ResourceExecution)._project,
                attr.fields(ResourceExecution).resource_id,
                attr.fields(ResourceExecution).resource_type,
                attr.fields(ResourceExecution).execution_id,
                attr.fields(ResourceExecution).function_name,
                attr.fields(ResourceExecution).service_name,
                attr.fields(ResourceExecution).module_name,
                attr.fields(ResourceExecution).package_name,
                attr.fields(ResourceExecution).org_name,
                attr.fields(ResourceExecution).project_id,
            )
        )

        # rename
        _json['projectId'] = self.project_id
        _json['resourceId'] = self.resource_id
        _json['resourceType'] = self.resource_type
        _json['functionName'] = self.function_name
        _json['executionId'] = self.execution_id
        _json['serviceName'] = self.service_name
        _json['moduleName'] = self.module_name
        _json['packageName'] = self.package_name
        _json['orgName'] = self.org_name

        return _json

    @property
    def project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id,
                                                                                   fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project


================================================
File: dtlpy/entities/service.py
================================================
import warnings
from collections import namedtuple
from enum import Enum
import traceback
import logging
from typing import List
from urllib.parse import urlsplit
import attr
from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class ServiceType(str, Enum):
    """ The type of the service (SYSTEM).

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - SYSTEM
         - Dataloop internal service
    """
    SYSTEM = 'system'
    REGULAR = 'regular'


class ServiceModeType(str, Enum):
    """ The type of the service mode.

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - REGULAR
         - Service regular mode type
       * - DEBUG
         - Service debug mode type
    """
    REGULAR = 'regular'
    DEBUG = 'debug'


class OnResetAction(str, Enum):
    """ The Execution action when the service reset (RERUN, FAILED).

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - RERUN
         - When the service resting rerun the execution
       * - FAILED
         - When the service resting fail the execution
    """
    RERUN = 'rerun'
    FAILED = 'failed'


class InstanceCatalog(str, Enum):
    """ The Service Pode size.

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - REGULAR_XS
         - regular pod with extra small size
       * - REGULAR_S
         - regular pod with small size
       * - REGULAR_M
         - regular pod with medium size
       * - REGULAR_L
         - regular pod with large size
       * - HIGHMEM_XS
         - highmem pod with extra small size
       * - HIGHMEM_S
         - highmem pod with small size
       * - HIGHMEM_M
         - highmem pod with medium size
       * - HIGHMEM_L
         - highmem pod with large size
       * - GPU_K80_S
         - GPU NVIDIA K80 pod with small size
       * - GPU_K80_M
         - GPU NVIDIA K80 pod with medium size
       * - GPU_T4_S
         - GPU NVIDIA T4 pod with regular memory
       * - GPU_T4_M
         - GPU NVIDIA T4 pod with highmem
    """
    REGULAR_XS = "regular-xs"
    REGULAR_S = "regular-s"
    REGULAR_M = "regular-m"
    REGULAR_L = "regular-l"
    HIGHMEM_XS = "highmem-xs"
    HIGHMEM_S = "highmem-s"
    HIGHMEM_M = "highmem-m"
    HIGHMEM_L = "highmem-l"
    GPU_K80_S = "gpu-k80-s"
    GPU_K80_M = "gpu-k80-m"
    GPU_T4_S = "gpu-t4"
    GPU_T4_M = "gpu-t4-m"


class RuntimeType(str, Enum):
    """ Service culture Runtime (KUBERNETES).

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - KUBERNETES
         - Service run in kubernetes culture
    """
    KUBERNETES = 'kubernetes'


class ServiceRuntime(entities.BaseEntity):
    def __init__(self, service_type: RuntimeType = RuntimeType.KUBERNETES):
        self.service_type = service_type


class KubernetesRuntime(ServiceRuntime):
    DEFAULT_POD_TYPE = InstanceCatalog.REGULAR_S
    DEFAULT_NUM_REPLICAS = 1
    DEFAULT_CONCURRENCY = 10

    def __init__(self,
                 pod_type: InstanceCatalog = DEFAULT_POD_TYPE,
                 num_replicas=DEFAULT_NUM_REPLICAS,
                 concurrency=DEFAULT_CONCURRENCY,
                 runner_image=None,
                 autoscaler=None,
                 **kwargs):

        super().__init__(service_type=RuntimeType.KUBERNETES)
        self.pod_type = kwargs.get('podType', pod_type)
        self.num_replicas = kwargs.get('numReplicas', num_replicas)
        self.concurrency = kwargs.get('concurrency', concurrency)
        self.runner_image = kwargs.get('runnerImage', runner_image)
        self._proxy_image = kwargs.get('proxyImage', None)
        self.single_agent = kwargs.get('singleAgent', None)
        self.preemptible = kwargs.get('preemptible', None)

        self.autoscaler = kwargs.get('autoscaler', autoscaler)
        if self.autoscaler is not None and isinstance(self.autoscaler, dict):
            if self.autoscaler['type'] == KubernetesAutoscalerType.RABBITMQ:
                self.autoscaler = KubernetesRabbitmqAutoscaler(**self.autoscaler)
            elif self.autoscaler['type'] == KubernetesAutoscalerType.RPS:
                self.autoscaler = KubernetesRPSAutoscaler(**self.autoscaler)
            else:
                raise NotImplementedError(
                    'Unknown kubernetes autoscaler type: {}'.format(self.autoscaler['type']))

    def to_json(self):
        _json = {
            'podType': self.pod_type,
            'numReplicas': self.num_replicas,
            'concurrency': self.concurrency,
            'autoscaler': None if self.autoscaler is None else self.autoscaler.to_json()
        }

        if self.single_agent is not None:
            _json['singleAgent'] = self.single_agent

        if self.runner_image is not None:
            _json['runnerImage'] = self.runner_image

        if self._proxy_image is not None:
            _json['proxyImage'] = self._proxy_image

        if self.preemptible is not None:
            _json['preemptible'] = self.preemptible

        return _json


@attr.s
class Service(entities.BaseEntity):
    """
    Service object
    """
    # platform
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    creator = attr.ib()
    version = attr.ib()

    package_id = attr.ib()
    package_revision = attr.ib()

    bot = attr.ib()
    use_user_jwt = attr.ib(repr=False)
    init_input = attr.ib()
    versions = attr.ib(repr=False)
    module_name = attr.ib()
    name = attr.ib()
    url = attr.ib()
    id = attr.ib()
    active = attr.ib()
    driver_id = attr.ib(repr=False)
    secrets = attr.ib(repr=False)

    # name change
    runtime = attr.ib(repr=False, type=KubernetesRuntime)
    queue_length_limit = attr.ib()
    run_execution_as_process = attr.ib(type=bool)
    execution_timeout = attr.ib()
    drain_time = attr.ib()
    on_reset = attr.ib(type=OnResetAction)
    _type = attr.ib(type=ServiceType)
    project_id = attr.ib()
    org_id = attr.ib()
    is_global = attr.ib()
    max_attempts = attr.ib()
    mode = attr.ib(repr=False)
    metadata = attr.ib()
    archive = attr.ib(repr=False)
    config = attr.ib(repr=False)
    settings = attr.ib(repr=False)
    panels = attr.ib(repr=False)

    # SDK
    _package = attr.ib(repr=False)
    _client_api = attr.ib(type=ApiClient, repr=False)
    _revisions = attr.ib(default=None, repr=False)
    # repositories
    _project = attr.ib(default=None, repr=False)
    _repositories = attr.ib(repr=False)
    updated_by = attr.ib(default=None)
    app = attr.ib(default=None)
    integrations = attr.ib(default=None)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @staticmethod
    def _protected_from_json(_json: dict, client_api: ApiClient, package=None, project=None, is_fetched=True):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param client_api: ApiClient entity
        :param package:
        :param project: project entity
        :param is_fetched: is Entity fetched from Platform
        :return:
        """
        try:
            service = Service.from_json(_json=_json,
                                        client_api=client_api,
                                        package=package,
                                        project=project,
                                        is_fetched=is_fetched)
            status = True
        except Exception:
            service = traceback.format_exc()
            status = False
        return status, service

    @classmethod
    def from_json(cls, _json: dict, client_api: ApiClient = None, package=None, project=None, is_fetched=True):
        """
        Build a service entity object from a json

        :param dict _json: platform json
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.package.Package package: package entity
        :param dtlpy.entities.project.Project project: project entity
        :param bool is_fetched: is Entity fetched from Platform
        :return: service object
        :rtype: dtlpy.entities.service.Service
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Service has been fetched from a project that is not belong to it')
                project = None

        if package is not None:
            if package.id != _json.get('packageId', None):
                logger.warning('Service has been fetched from a package that is not belong to it')
                package = None

        versions = _json.get('versions', dict())
        runtime = _json.get("runtime", None)
        if runtime:
            runtime = KubernetesRuntime(**runtime)

        inst = cls(
            package_revision=_json.get("packageRevision", None),
            bot=_json.get("botUserName", None),
            use_user_jwt=_json.get("useUserJwt", False),
            created_at=_json.get("createdAt", None),
            updated_at=_json.get("updatedAt", None),
            project_id=_json.get('projectId', None),
            package_id=_json.get('packageId', None),
            driver_id=_json.get('driverId', None),
            max_attempts=_json.get('maxAttempts', None),
            version=_json.get('version', None),
            creator=_json.get('creator', None),
            revisions=_json.get('revisions', None),
            queue_length_limit=_json.get('queueLengthLimit', None),
            active=_json.get('active', None),
            runtime=runtime,
            is_global=_json.get("global", False),
            init_input=_json.get("initParams", dict()),
            module_name=_json.get("moduleName", None),
            run_execution_as_process=_json.get('runExecutionAsProcess', False),
            execution_timeout=_json.get('executionTimeout', 60 * 60),
            drain_time=_json.get('drainTime', 60 * 10),
            on_reset=_json.get('onReset', OnResetAction.FAILED),
            name=_json.get("name", None),
            url=_json.get("url", None),
            id=_json.get("id", None),
            versions=versions,
            client_api=client_api,
            package=package,
            project=project,
            secrets=_json.get("secrets", None),
            type=_json.get("type", None),
            mode=_json.get('mode', dict()),
            metadata=_json.get('metadata', None),
            archive=_json.get('archive', None),
            updated_by=_json.get('updatedBy', None),
            config=_json.get('config', None),
            settings=_json.get('settings', None),
            app=_json.get('app', None),
            integrations=_json.get('integrations', None),
            org_id=_json.get('orgId', None),
            panels=_json.get('panels', None)
        )
        inst.is_fetched = is_fetched
        return inst

    ############
    # Entities #
    ############
    @property
    def revisions(self):
        if self._revisions is None:
            self._revisions = self.services.revisions(service=self)
        return self._revisions

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/services/{}/main".format(self.project.id, self.id))

    @property
    def project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id,
                                                                                   fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def package(self):
        if self._package is None:
            try:
                dpk_id = None
                dpk_version = None
                if self.app and isinstance(self.app, dict):
                    dpk_id = self.app.get('dpkId', None)
                    dpk_version = self.app.get('dpkVersion', None)
                if dpk_id is None:
                    self._package = repositories.Dpks(client_api=self._client_api, project=self.project).get(
                        dpk_id=self.package_id)
                else:
                    self._package = repositories.Dpks(client_api=self._client_api, project=self.project).get_revisions(
                        dpk_id=dpk_id,
                        version=dpk_version)

                assert isinstance(self._package, entities.Dpk)
            except:
                self._package = repositories.Packages(client_api=self._client_api).get(package_id=self.package_id,
                                                                                       fetch=None,
                                                                                       log_error=False)
                assert isinstance(self._package, entities.Package)
        return self._package

    @property
    def execution_url(self):
        return 'CURL -X POST' \
               '\nauthorization: Bearer <token>' \
               '\nContent-Type: application/json" -d {' \
               '\n"input": {<input json>}, ' \
               '"projectId": "{<project_id>}", ' \
               '"functionName": "<function_name>"}'

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['executions', 'services', 'triggers'])

        if self._package is None:
            services_repo = repositories.Services(client_api=self._client_api,
                                                  package=self._package,
                                                  project=self._project)
        else:
            services_repo = self._package.services

        triggers = repositories.Triggers(client_api=self._client_api,
                                         project=self._project,
                                         service=self)

        r = reps(executions=repositories.Executions(client_api=self._client_api, service=self),
                 services=services_repo, triggers=triggers)
        return r

    @property
    def executions(self):
        assert isinstance(self._repositories.executions, repositories.Executions)
        return self._repositories.executions

    @property
    def triggers(self):
        assert isinstance(self._repositories.triggers, repositories.Triggers)
        return self._repositories.triggers

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    ###########
    # methods #
    ###########
    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(
            self,
            filter=attr.filters.exclude(
                attr.fields(Service)._project,
                attr.fields(Service)._package,
                attr.fields(Service)._revisions,
                attr.fields(Service)._client_api,
                attr.fields(Service)._repositories,
                attr.fields(Service).project_id,
                attr.fields(Service).init_input,
                attr.fields(Service).module_name,
                attr.fields(Service).bot,
                attr.fields(Service).package_id,
                attr.fields(Service).is_global,
                attr.fields(Service).use_user_jwt,
                attr.fields(Service).package_revision,
                attr.fields(Service).driver_id,
                attr.fields(Service).run_execution_as_process,
                attr.fields(Service).execution_timeout,
                attr.fields(Service).drain_time,
                attr.fields(Service).runtime,
                attr.fields(Service).queue_length_limit,
                attr.fields(Service).max_attempts,
                attr.fields(Service).on_reset,
                attr.fields(Service).created_at,
                attr.fields(Service).updated_at,
                attr.fields(Service).secrets,
                attr.fields(Service)._type,
                attr.fields(Service).mode,
                attr.fields(Service).metadata,
                attr.fields(Service).archive,
                attr.fields(Service).updated_by,
                attr.fields(Service).config,
                attr.fields(Service).settings,
                attr.fields(Service).app,
                attr.fields(Service).integrations,
                attr.fields(Service).org_id,
                attr.fields(Service).panels
            )
        )

        _json['projectId'] = self.project_id
        _json['orgId'] = self.org_id
        _json['packageId'] = self.package_id
        _json['initParams'] = self.init_input
        _json['moduleName'] = self.module_name
        _json['botUserName'] = self.bot
        _json['useUserJwt'] = self.use_user_jwt
        _json['global'] = self.is_global
        _json['driverId'] = self.driver_id
        _json['packageRevision'] = self.package_revision
        _json['runExecutionAsProcess'] = self.run_execution_as_process
        _json['executionTimeout'] = self.execution_timeout
        _json['drainTime'] = self.drain_time
        _json['onReset'] = self.on_reset
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at

        if self.updated_by is not None:
            _json['updatedBy'] = self.updated_by

        if self.panels is not None:
            _json['panels'] = self.panels

        if self.max_attempts is not None:
            _json['maxAttempts'] = self.max_attempts

        if self.is_global is not None:
            _json['global'] = self.is_global

        if self.runtime:
            _json['runtime'] = self.runtime if isinstance(self.runtime, dict) else self.runtime.to_json()

        if self.queue_length_limit is not None:
            _json['queueLengthLimit'] = self.queue_length_limit

        if self.secrets is not None:
            _json['secrets'] = self.secrets

        if self._type is not None:
            _json['type'] = self._type

        if self.mode:
            _json['mode'] = self.mode

        if self.metadata:
            _json['metadata'] = self.metadata

        if self.archive is not None:
            _json['archive'] = self.archive

        if self.config is not None:
            _json['config'] = self.config

        if self.settings is not None:
            _json['settings'] = self.settings

        if self.app is not None:
            _json['app'] = self.app

        if self.integrations is not None:
            _json['integrations'] = self.integrations

        return _json

    def update(self, force=False):
        """
        Update Service changes to platform

        :param bool force: force update
        :return: Service entity
        :rtype: dtlpy.entities.service.Service
        """
        return self.services.update(service=self, force=force)

    def delete(self, force: bool = False):
        """
        Delete Service object

        :return: True
        :rtype: bool
        """
        return self.services.delete(service_id=self.id, force=force)

    def status(self):
        """
        Get Service status

        :return: status json
        :rtype: dict
        """
        return self.services.status(service_id=self.id)

    def log(self,
            size=None,
            checkpoint=None,
            start=None,
            end=None,
            follow=False,
            text=None,
            execution_id=None,
            function_name=None,
            replica_id=None,
            system=False,
            view=True,
            until_completed=True,
            model_id: str = None,
            model_operation: str = None,
            ):
        """
        Get service logs

        :param int size: size
        :param dict checkpoint: the information from the lst point checked in the service
        :param str start: iso format time
        :param str end: iso format time
        :param bool follow: if true, keep stream future logs
        :param str text: text
        :param str execution_id: execution id
        :param str function_name: function name
        :param str replica_id: replica id
        :param bool system: system
        :param bool view: if true, print out all the logs
        :param bool until_completed: wait until completed
        :param str model_id: model id
        :param str model_operation: model operation action
        :return: ServiceLog entity
        :rtype: ServiceLog

        **Example**:

        .. code-block:: python

            service_log = service.log()
        """
        return self.services.log(service=self,
                                 size=size,
                                 checkpoint=checkpoint,
                                 start=start,
                                 end=end,
                                 follow=follow,
                                 execution_id=execution_id,
                                 function_name=function_name,
                                 replica_id=replica_id,
                                 system=system,
                                 text=text,
                                 view=view,
                                 until_completed=until_completed,
                                 model_id=model_id,
                                 model_operation=model_operation)

    def open_in_web(self):
        """
        Open the service in web platform

        :return:
        """
        parsed_url = urlsplit(self.platform_url)
        base_url = parsed_url.scheme + "://" + parsed_url.netloc
        url = '{}/projects/{}/services/{}'.format(base_url, self.project_id, self.id)
        self._client_api._open_in_web(url=url)

    def checkout(self):
        """
        Checkout

        :return:
        """
        return self.services.checkout(service=self)

    def pause(self):
        """
        pause

        :return:
        """
        return self.services.pause(service_id=self.id)

    def resume(self):
        """
        resume

        :return:
        """
        return self.services.resume(service_id=self.id)

    def execute(
            self,
            execution_input=None,
            function_name=None,
            resource=None,
            item_id=None,
            dataset_id=None,
            annotation_id=None,
            project_id=None,
            sync=False,
            stream_logs=True,
            return_output=True
    ):
        """
        Execute a function on an existing service

        :param List[FunctionIO] or dict execution_input: input dictionary or list of FunctionIO entities
        :param str function_name: function name to run
        :param str resource: input type.
        :param str item_id: optional - item id as input to function
        :param str dataset_id: optional - dataset id as input to function
        :param str annotation_id: optional - annotation id as input to function
        :param str project_id: resource's project
        :param bool sync: if true, wait for function to end
        :param bool stream_logs: prints logs of the new execution. only works with sync=True
        :param bool return_output: if True and sync is True - will return the output directly
        :return: execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            execution = service.execute(function_name='function_name', item_id='item_id', project_id='project_id')
        """
        execution = self.executions.create(sync=sync,
                                           execution_input=execution_input,
                                           function_name=function_name,
                                           resource=resource,
                                           item_id=item_id,
                                           dataset_id=dataset_id,
                                           annotation_id=annotation_id,
                                           stream_logs=stream_logs,
                                           project_id=project_id,
                                           return_output=return_output)
        return execution

    def execute_batch(self,
                      filters,
                      function_name: str = None,
                      execution_inputs: list = None,
                      wait=True
                      ):
        """
        Execute a function on an existing service

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param filters: Filters entity for a filtering before execute
        :param str function_name: function name to run
        :param List[FunctionIO] or dict execution_inputs: input dictionary or list of FunctionIO entities, that represent the extra inputs of the function
        :param bool wait: wait until create task finish
        :return: execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            command = service.execute_batch(
                        execution_inputs=dl.FunctionIO(type=dl.PackageInputType.STRING, value='test', name='string'),
                        filters=dl.Filters(field='dir', values='/test', context={"datasets": [dataset.id]),
                        function_name='run')
        """
        execution = self.executions.create_batch(service_id=self.id,
                                                 execution_inputs=execution_inputs,
                                                 filters=filters,
                                                 function_name=function_name,
                                                 wait=wait)
        return execution

    def rerun_batch(self,
                    filters,
                    wait=True
                    ):
        """
        rerun a executions on an existing service

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a Filter.

        :param filters: Filters entity for a filtering before rerun
        :param bool wait: wait until create task finish
        :return: rerun command
        :rtype: dtlpy.entities.command.Command

        **Example**:

        .. code-block:: python

            command = service.executions.rerun_batch(
                        filters=dl.Filters(field='id', values=['executionId'], operator=dl.FiltersOperations.IN, resource=dl.FiltersResource.EXECUTION))
        """
        execution = self.executions.rerun_batch(service_id=self.id,
                                                filters=filters,
                                                wait=wait)
        return execution

    def activate_slots(
            self,
            project_id: str = None,
            task_id: str = None,
            dataset_id: str = None,
            org_id: str = None,
            user_email: str = None,
            slots=None,
            role=None,
            prevent_override: bool = True,
            visible: bool = True,
            icon: str = 'fas fa-magic',
            **kwargs
    ) -> object:
        """
        Activate service slots

        :param str project_id: project id
        :param str task_id: task id
        :param str dataset_id: dataset id
        :param str org_id: org id
        :param str user_email: user email
        :param list slots: list of entities.PackageSlot
        :param str role: user role MemberOrgRole.ADMIN, MemberOrgRole.owner, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :param bool prevent_override: True to prevent override
        :param bool visible: visible
        :param str icon: icon
        :param kwargs: all additional arguments
        :return: list of user setting for activated slots
        :rtype: list

        **Example**:

        .. code-block:: python

            setting = service.activate_slots(project_id='project_id',
                                    slots=List[entities.PackageSlot],
                                    icon='fas fa-magic')
        """
        return self.services.activate_slots(
            service=self,
            project_id=project_id,
            task_id=task_id,
            dataset_id=dataset_id,
            org_id=org_id,
            user_email=user_email,
            slots=slots,
            role=role,
            prevent_override=prevent_override,
            visible=visible,
            icon=icon,
            **kwargs
        )

    def restart(self, replica_name: str = None):
        """
        Restart service

        :param str replica_name: replica name
        :return: True
        :rtype: bool
        """
        return self.services.restart(service=self, replica_name=replica_name)


class KubernetesAutoscalerType(str, Enum):
    """ The Service Autoscaler Type (RABBITMQ, CPU).

    .. list-table::
       :widths: 15 150
       :header-rows: 1

       * - State
         - Description
       * - RABBITMQ
         - Service Autoscaler based on service queue length
       * - CPU
         - Service Autoscaler based on service CPU usage
       * - RPS
            - Service Autoscaler based on service RPS
    """
    RABBITMQ = 'rabbitmq'
    CPU = 'cpu'
    RPS = 'rps'


# added this class to avoid breaking changes after fixing a spelling mistake in KubernetesAutoscalerType
class KubernetesAutuscalerTypeMeta(type):
    def __getattribute__(cls, item):
        if hasattr(KubernetesAutoscalerType, item):
            warnings.warn(
                'KubernetesAutuscalerType is deprecated and will be removed in version 1.97.0, '
                'use KubernetesAutoscalerType instead',
                DeprecationWarning
            )
            return getattr(KubernetesAutoscalerType, item)
        else:
            raise AttributeError(f"KubernetesAutuscalerType has no attribute '{item}'")


class KubernetesAutuscalerType(metaclass=KubernetesAutuscalerTypeMeta):
    pass


class KubernetesAutoscaler(entities.BaseEntity):
    MIN_REPLICA_DEFAULT = 0
    MAX_REPLICA_DEFAULT = 1
    AUTOSCALER_TYPE_DEFAULT = KubernetesAutoscalerType.RABBITMQ

    def __init__(self,
                 autoscaler_type: KubernetesAutoscalerType.RABBITMQ = AUTOSCALER_TYPE_DEFAULT,
                 min_replicas=MIN_REPLICA_DEFAULT,
                 max_replicas=MAX_REPLICA_DEFAULT,
                 cooldown_period=None,
                 polling_interval=None,
                 **kwargs):
        self.autoscaler_type = kwargs.get('type', autoscaler_type)
        self.min_replicas = kwargs.get('minReplicas', min_replicas)
        self.max_replicas = kwargs.get('maxReplicas', max_replicas)
        self.cooldown_period = kwargs.get('cooldownPeriod', cooldown_period)
        self.polling_interval = kwargs.get('pollingInterval', polling_interval)

    def to_json(self):
        _json = {
            'type': self.autoscaler_type,
            'minReplicas': self.min_replicas,
            'maxReplicas': self.max_replicas
        }

        if self.cooldown_period is not None:
            _json['cooldownPeriod'] = self.cooldown_period

        if self.polling_interval is not None:
            _json['pollingInterval'] = self.polling_interval

        return _json


class KubernetesRabbitmqAutoscaler(KubernetesAutoscaler):
    QUEUE_LENGTH_DEFAULT = 1000

    def __init__(self,
                 min_replicas=KubernetesAutoscaler.MIN_REPLICA_DEFAULT,
                 max_replicas=KubernetesAutoscaler.MAX_REPLICA_DEFAULT,
                 queue_length=QUEUE_LENGTH_DEFAULT,
                 cooldown_period=None,
                 polling_interval=None,
                 **kwargs):
        super().__init__(min_replicas=min_replicas,
                         max_replicas=max_replicas,
                         autoscaler_type=KubernetesAutoscalerType.RABBITMQ,
                         cooldown_period=cooldown_period,
                         polling_interval=polling_interval, **kwargs)
        self.queue_length = kwargs.get('queueLength', queue_length)

    def to_json(self):
        _json = super().to_json()
        _json['queueLength'] = self.queue_length
        return _json


class KubernetesRPSAutoscaler(KubernetesAutoscaler):
    THRESHOLD_DEFAULT = 10
    RATE_SECONDS_DEFAULT = 30

    def __init__(self,
                 min_replicas=KubernetesAutoscaler.MIN_REPLICA_DEFAULT,
                 max_replicas=KubernetesAutoscaler.MAX_REPLICA_DEFAULT,
                 threshold=THRESHOLD_DEFAULT,
                 rate_seconds=RATE_SECONDS_DEFAULT,
                 cooldown_period=None,
                 polling_interval=None,
                 **kwargs):
        super().__init__(min_replicas=min_replicas,
                         max_replicas=max_replicas,
                         autoscaler_type=KubernetesAutoscalerType.RPS,
                         cooldown_period=cooldown_period,
                         polling_interval=polling_interval, **kwargs)
        self.threshold = kwargs.get('threshold', threshold)
        self.rate_seconds = kwargs.get('rateSeconds', rate_seconds)

    def to_json(self):
        _json = super().to_json()
        _json['rateSeconds'] = self.rate_seconds
        _json['threshold'] = self.threshold
        return _json


================================================
File: dtlpy/entities/setting.py
================================================
import json
import warnings
from enum import Enum
from .. import repositories


class Role(str, Enum):
    OWNER = "owner",
    ADMIN = "admin",
    MEMBER = "member",
    ANNOTATOR = "annotator",
    DEVELOPER = "engineer",
    ANNOTATION_MANAGER = "annotationManager"
    ALL = "*"


class PlatformEntityType(str, Enum):
    USER = "user",
    TASK = "task",
    PROJECT = "project",
    ORG = "org",
    DATASET = "dataset"
    DATALOOP = "DATALOOP"


class SettingsValueTypes(str, Enum):
    BOOLEAN = "boolean",
    NUMBER = "number",
    SELECT = "select",
    MULTI_SELECT = "multi-select"
    STRING = "string"


class SettingsTypes(str, Enum):
    FEATURE_FLAG = "feature_flag",
    USER_SETTINGS = "user_settings"


class SettingsSectionNames(str, Enum):
    ACCOUNT = "Account",
    CONTACT = "Contact",
    APPLICATIONS = "Applications",
    STUDIO = "Studio",
    PLATFORM = "Platform"
    SDK = "SDK"


class SettingScope:
    def __init__(
            self,
            type: PlatformEntityType,
            id: str,
            role: Role,
            prevent_override: bool,
            visible: bool,
    ):
        self.type = type
        self.id = id
        self.role = role
        self.prevent_override = prevent_override
        self.visible = visible

    @staticmethod
    def from_json(_json):
        return SettingScope(
            type=_json.get('type', None),
            id=_json.get('id', None),
            role=_json.get('role', None),
            prevent_override=_json.get('preventOverride', None),
            visible=_json.get('visible', None)
        )

    def to_json(self):
        _json = dict()
        _json['type'] = self.type
        _json['id'] = self.id
        _json['role'] = self.role
        _json['preventOverride'] = self.prevent_override
        _json['visible'] = self.visible

        return _json


class BaseSetting:

    def __init__(
            self,
            default_value,
            value,
            name: str,
            value_type: SettingsValueTypes,
            scope: SettingScope,
            metadata: dict,
            setting_type: SettingsTypes,
            id: str = None,
            client_api=None,
            project=None,
            org=None
    ):

        self.default_value = default_value
        self.name = name
        self.value = value
        self.value_type = value_type
        self.scope = scope
        self.metadata = metadata
        self.setting_type = setting_type
        self.client_api = client_api
        self.project = project
        self.org = org
        self.id = id
        self.settings = repositories.Settings(client_api=self.client_api, project=self.project, org=self.org)

    @staticmethod
    def from_json(_json: dict, client_api, project=None, org=None):
        scope = SettingScope.from_json(_json.get('scope', None))
        return BaseSetting(
            default_value=_json.get('defaultValue', None),
            name=_json.get('name', None),
            value=_json.get('value', None),
            value_type=_json.get('valueType', None),
            scope=scope,
            metadata=_json.get('metadata', None),
            id=_json.get('id', None),
            setting_type=_json.get('settingType', None),
            client_api=client_api,
            project=project,
            org=org
        )

    @staticmethod
    def __slot_to_db_slot(slot: dict):
        if 'displayScopes' in slot:
            scopes = slot['displayScopes']
            if scopes:
                for display_scope in scopes:
                    if 'filter' in display_scope and isinstance(display_scope['filter'], dict):
                        display_scope['filter'] = json.dumps(display_scope['filter'])

    def to_json(self):
        if self.metadata is not None and 'slots' in self.metadata and isinstance(self.metadata['slots'], list):
            for slot in self.metadata['slots']:
                self.__slot_to_db_slot(slot)

        _json = {
            'name': self.name,
            'valueType': self.value_type,
            'scope': self.scope.to_json(),
            'settingType': self.setting_type,
            'id': self.id,
            'metadata': self.metadata,
            'value': self.value,
            'defaultValue': self.default_value
        }

        return _json

    def delete(self):
        """
        Delete a setting

        :return: True if success exceptions if not
        """
        return self.settings.delete(setting_id=self.id)

    def update(self):
        """
        Update a setting

        :return: setting entity
        """
        return self.settings.update(setting=self)


class Setting(BaseSetting):
    def __init__(
            self,
            value,
            name: str,
            value_type: SettingsValueTypes,
            scope: SettingScope,
            section_name: SettingsSectionNames,
            default_value=None,
            inputs=None,
            metadata: dict = None,
            description: str = None,
            icon: str = None,
            id: str = None,
            sub_section_name: str = None,
            hint=None,
            client_api=None,
            project=None,
            org=None,
            setting_type=SettingsTypes.USER_SETTINGS
    ):
        super().__init__(
            default_value=default_value,
            value=value,
            name=name,
            value_type=value_type,
            scope=scope,
            metadata=metadata,
            setting_type=setting_type,
            client_api=client_api,
            project=project,
            org=org,
            id=id
        )
        self.description = description
        self.inputs = inputs
        self.icon = icon
        self.section_name = section_name
        self.hint = hint
        self.sub_section_name = sub_section_name

    @staticmethod
    def from_json(_json: dict, client_api, project=None, org=None):
        scope = SettingScope.from_json(_json.get('scope', None))
        setting = Setting(
            default_value=_json.get('defaultValue', None),
            name=_json.get('name', None),
            value=_json.get('value', None),
            value_type=_json.get('valueType', None),
            scope=scope,
            metadata=_json.get('metadata', None),
            description=_json.get('description', None),
            inputs=_json.get('inputs', None),
            icon=_json.get('icon', None),
            section_name=_json.get('sectionName', None),
            hint=_json.get('hint', None),
            sub_section_name=_json.get('subSectionName', None),
            id=_json.get('id', None),
            client_api=client_api,
            project=project,
            org=org
        )
        if setting.metadata is not None and 'slots' in setting.metadata:
            for slot in setting.metadata.get('slots', []):
                for scope in slot.get('displayScopes', []):
                    if 'filter' in scope:
                        scope['filter'] = json.loads(scope['filter']) if isinstance(scope['filter'], str) else scope['filter']

        return setting

    def to_json(self):
        _json = super().to_json()
        _json['description'] = self.description
        _json['inputs'] = self.inputs
        _json['icon'] = self.icon
        _json['sectionName'] = self.section_name
        _json['hint'] = self.hint
        _json['subSectionName'] = self.sub_section_name
        _json['id'] = self.id
        return _json


class UserSetting(Setting):
    def __init__(
            self,
            default_value,
            value,
            name: str,
            value_type: SettingsValueTypes,
            scope: SettingScope,
            section_name: SettingsSectionNames,
            inputs=None,
            metadata: dict = None,
            description: str = None,
            icon: str = None,
            id: str = None,
            sub_section_name: str = None,
            hint=None,
            client_api=None,
            project=None,
            org=None
    ):
        super().__init__(
            default_value=default_value,
            value=value,
            name=name,
            value_type=value_type,
            scope=scope,
            metadata=metadata,
            client_api=client_api,
            project=project,
            org=org,
            id=id,
            section_name=section_name,
            inputs=inputs,
            description=description,
            icon=icon,
            sub_section_name=sub_section_name,
            hint=hint
        )


================================================
File: dtlpy/entities/task.py
================================================
import traceback
from enum import Enum
from typing import Union, List
import attr
import logging

from .. import repositories, entities, exceptions

logger = logging.getLogger(name='dtlpy')


class ConsensusTaskType(str, Enum):
    CONSENSUS = 'consensus'
    QUALIFICATION = 'qualification'
    HONEYPOT = 'honeypot'

class TaskPriority(int, Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3


class ItemAction:
    def __init__(self, action, display_name=None, color='#FFFFFF', icon=None):
        if not action or not isinstance(action, str):
            raise ValueError('action should be a non-empty string')
        self.action = action
        if not display_name:
            display_name = action
        self.display_name = display_name
        self.color = color
        self.icon = icon

    @classmethod
    def from_json(cls, _json: dict):
        kwarg = {
            'action': _json.get('action')
        }

        if _json.get('displayName', False):
            kwarg['display_name'] = _json['displayName']

        if _json.get('color', False):
            kwarg['color'] = _json['color']

        if _json.get('icon', False):
            kwarg['icon'] = _json['icon']

        return cls(**kwarg)

    def to_json(self) -> dict:
        _json = {
            'action': self.action,
            'color': self.color,
            'displayName': self.display_name if self.display_name is not None else self.action
        }

        if self.icon is not None:
            _json['icon'] = self.icon

        return _json


@attr.s
class Task:
    """
    Task object
    """

    # platform
    name = attr.ib()
    status = attr.ib()
    project_id = attr.ib()
    metadata = attr.ib(repr=False)
    id = attr.ib()
    url = attr.ib(repr=False)
    task_owner = attr.ib(repr=False)
    item_status = attr.ib(repr=False)
    creator = attr.ib()
    due_date = attr.ib()
    dataset_id = attr.ib()
    spec = attr.ib()
    recipe_id = attr.ib(repr=False)
    query = attr.ib(repr=False)
    assignmentIds = attr.ib(repr=False)
    annotation_status = attr.ib(repr=False)
    progress = attr.ib()
    for_review = attr.ib()
    issues = attr.ib()
    updated_at = attr.ib()
    created_at = attr.ib()
    available_actions = attr.ib()
    total_items = attr.ib()
    priority = attr.ib()
    _description = attr.ib()

    # sdk
    _client_api = attr.ib(repr=False)
    _current_assignments = attr.ib(default=None, repr=False)
    _assignments = attr.ib(default=None, repr=False)
    _project = attr.ib(default=None, repr=False)
    _dataset = attr.ib(default=None, repr=False)
    _tasks = attr.ib(default=None, repr=False)
    _settings = attr.ib(default=None, repr=False)

    @property
    def description(self):
        return self._description

    @description.setter
    def description(self, description):
        if not isinstance(description, str):
            raise ValueError('description should be a string')
        if self._description is None:
            self._description = {}
        self._description['content'] = description

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, dataset=None):
        """
        Same as from_json but with try-except to catch if error

        :param dict _json: platform json that describe the task
        :param client_api: ApiClient object
        :param dtlpy.entities.project.Project project: project object where task will create
        :param dtlpy.entities.dataset.Dataset dataset: dataset object that refer to the task
        :return:
        """
        try:
            task = Task.from_json(
                _json=_json,
                client_api=client_api,
                project=project,
                dataset=dataset
            )
            status = True
        except Exception:
            task = traceback.format_exc()
            status = False
        return status, task

    @classmethod
    def from_json(cls, _json, client_api, project=None, dataset=None):
        """
        Return the task object form the json

        :param dict _json: platform json that describe the task
        :param client_api: ApiClient object
        :param dtlpy.entities.project.Project project: project object where task will create
        :param dtlpy.entities.dataset.Dataset dataset: dataset object that refer to the task
        :return:
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Task has been fetched from a project that is not belong to it')
                project = None

        if dataset is not None:
            if dataset.id != _json.get('datasetId', None):
                logger.warning('Task has been fetched from a dataset that is not belong to it')
                dataset = None

        actions = [ItemAction.from_json(_json=action) for action in _json.get('availableActions', list())]

        return cls(
            name=_json.get('name', None),
            status=_json.get('status', None),
            project_id=_json.get('projectId', None),
            metadata=_json.get('metadata', dict()),
            url=_json.get('url', None),
            spec=_json.get('spec', None),
            id=_json['id'],
            creator=_json.get('creator', None),
            due_date=_json.get('dueDate', 0),
            dataset_id=_json.get('datasetId', None),
            recipe_id=_json.get('recipeId', None),
            query=_json.get('query', None),
            task_owner=_json.get('taskOwner', None),
            item_status=_json.get('itemStatus', None),
            assignmentIds=_json.get('assignmentIds', list()),
            dataset=dataset,
            project=project,
            client_api=client_api,
            annotation_status=_json.get('annotationStatus', None),
            progress=_json.get('progress', None),
            for_review=_json.get('forReview', None),
            issues=_json.get('issues', None),
            updated_at=_json.get('updatedAt', None),
            created_at=_json.get('createdAt', None),
            available_actions=actions,
            total_items=_json.get('totalItems', None),
            priority=_json.get('priority', None),
            description=_json.get('description', None)
        )

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(
            self, filter=attr.filters.exclude(
                attr.fields(Task)._client_api,
                attr.fields(Task)._project,
                attr.fields(Task).project_id,
                attr.fields(Task).dataset_id,
                attr.fields(Task).recipe_id,
                attr.fields(Task).task_owner,
                attr.fields(Task).available_actions,
                attr.fields(Task).item_status,
                attr.fields(Task).due_date,
                attr.fields(Task)._tasks,
                attr.fields(Task)._dataset,
                attr.fields(Task)._current_assignments,
                attr.fields(Task)._assignments,
                attr.fields(Task).annotation_status,
                attr.fields(Task).for_review,
                attr.fields(Task).issues,
                attr.fields(Task).updated_at,
                attr.fields(Task).created_at,
                attr.fields(Task).total_items,
                attr.fields(Task)._settings,
                attr.fields(Task)._description
            )
        )
        _json['projectId'] = self.project_id
        _json['datasetId'] = self.dataset_id
        _json['recipeId'] = self.recipe_id
        _json['taskOwner'] = self.task_owner
        _json['dueDate'] = self.due_date
        _json['totalItems'] = self.total_items
        _json['forReview'] = self.for_review
        _json['description'] = self.description

        if self.available_actions is not None:
            _json['availableActions'] = [action.to_json() for action in self.available_actions]

        return _json

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/tasks/{}".format(self.project.id, self.id))

    @property
    def current_assignments(self):
        if self._current_assignments is None:
            self._current_assignments = list()
            for assignment in self.assignmentIds:
                self._current_assignments.append(self.assignments.get(assignment_id=assignment))
        return self._current_assignments

    @property
    def assignments(self):
        if self._assignments is None:
            self._assignments = repositories.Assignments(client_api=self._client_api, dataset=self._dataset,
                                                         project=self.project, task=self, project_id=self.project_id)
        assert isinstance(self._assignments, repositories.Assignments)
        return self._assignments

    @property
    def tasks(self):
        if self._tasks is None:
            self._tasks = repositories.Tasks(client_api=self._client_api, project=self.project, dataset=self.dataset)
        assert isinstance(self._tasks, repositories.Tasks)
        return self._tasks

    @property
    def settings(self):
        if self._settings is None:
            self._settings = repositories.Settings(
                client_api=self._client_api,
                project=self.project,
                dataset=self.dataset,
                task=self
            )
        assert isinstance(self._settings, repositories.Settings)
        return self._settings

    @property
    def project(self):
        if self._project is None:
            self.get_project()
            if self._project is None:
                raise exceptions.PlatformException(error='2001',
                                                   message='Missing entity "project". need to "get_project()" ')
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def dataset(self):
        if self._dataset is None:
            self.get_dataset()
            if self._dataset is None:
                raise exceptions.PlatformException(error='2001',
                                                   message='Missing entity "dataset". need to "get_dataset()" ')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    def get_project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id)

    def get_dataset(self):
        if self._dataset is None:
            self._dataset = repositories.Datasets(client_api=self._client_api, project=self._project).get(
                dataset_id=self.dataset_id)

    def open_in_web(self):
        """
        Open the task in web platform

        :return:
        """
        self._client_api._open_in_web(url=self.platform_url)

    def delete(self, wait=True):
        """
        Delete task from platform

        :param bool wait: wait until delete task finish
        :return: True
        :rtype: bool
        """
        return self.tasks.delete(task_id=self.id, wait=wait)

    def update(self, system_metadata=False):
        """
        Update an Annotation Task

        :param bool system_metadata: True, if you want to change metadata system
        """
        return self.tasks.update(task=self, system_metadata=system_metadata)

    def create_qa_task(self,
                       due_date,
                       assignee_ids,
                       filters=None,
                       items=None,
                       query=None,
                       workload=None,
                       metadata=None,
                       available_actions=None,
                       wait=True,
                       batch_size=None,
                       max_batch_workload=None,
                       allowed_assignees=None,
                       priority=TaskPriority.MEDIUM
                       ):
        """
        Create a new QA Task

        :param float due_date: date by which the QA task should be finished; for example, due_date=datetime.datetime(day=1, month=1, year=2029).timestamp()
        :param list assignee_ids: list the QA task assignees (contributors) that should be working on the task. Provide a list of users' emails
        :param entities.Filters filters: dl.Filters entity to filter items for the task
        :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
        :param dict DQL query: filter items for the task
        :param List[WorkloadUnit] workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param dict metadata: metadata for the task
        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "approved" and "discard"
        :param bool wait: wait until create task finish
        :param int batch_size: Pulling batch size (items), use with pulling allocation method. Restrictions - Min 3, max 100
        :param int max_batch_workload: Max items in assignment, use with pulling allocation method. Restrictions - Min batchSize + 2, max batchSize * 2
        :param list allowed_assignees: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
        :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
        :return: task object
        :rtype: dtlpy.entities.task.Task

        **Example**:

        .. code-block:: python

            task = task.create_qa_task(due_date = datetime.datetime(day= 1, month= 1, year= 2029).timestamp(),
                                assignee_ids =[ 'annotator1@dataloop.ai', 'annotator2@dataloop.ai'])
        """
        return self.tasks.create_qa_task(task=self,
                                         due_date=due_date,
                                         assignee_ids=assignee_ids,
                                         filters=filters,
                                         items=items,
                                         query=query,
                                         workload=workload,
                                         metadata=metadata,
                                         available_actions=available_actions,
                                         wait=wait,
                                         batch_size=batch_size,
                                         max_batch_workload=max_batch_workload,
                                         allowed_assignees=allowed_assignees,
                                         priority=priority
                                         )

    def create_assignment(self, assignment_name, assignee_id, items=None, filters=None):
        """
        Create a new assignment

        :param str assignment_name: assignment name
        :param str assignee_id: the assignment assignees (contributors) that should be working on the task. Provide a user email
        :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment assignment

        **Example**:

        .. code-block:: python

            assignment = task.create_assignment(assignee_id='annotator1@dataloop.ai')
        """
        assignment = self.assignments.create(assignee_id=assignee_id,
                                             filters=filters,
                                             items=items)

        assignment.metadata['system']['taskId'] = self.id
        assignment.update(system_metadata=True)
        self.assignmentIds.append(assignment.id)
        self.update()
        self.add_items(filters=filters, items=items)
        return assignment

    def add_items(self, filters=None, items=None, assignee_ids=None, workload=None, limit=None, wait=True, query=None):
        """
        Add items to Task

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param list items: list of items (item Ids or objects) to add to the task
        :param list assignee_ids: list to assignee who works in the task
        :param list workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param int limit: the limit items that task can include
        :param bool wait: wait until add items will to finish
        :param dict query: query to filter the items for the task

        :return: task entity
        :rtype: dtlpy.entities.task.Task
        """
        return self.tasks.add_items(task=self,
                                    filters=filters,
                                    items=items,
                                    assignee_ids=assignee_ids,
                                    workload=workload,
                                    limit=limit,
                                    wait=wait,
                                    query=query)

    def remove_items(self,
                     filters: entities.Filters = None,
                     query=None,
                     items=None,
                     wait=True):
        """
        remove items from Task.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param dict query: query to filter the items use it
        :param list items: list of items to add to the task
        :param bool wait: wait until remove items finish

        :return: True if success and an error if failed
        :rtype: bool
        """
        return self.tasks.remove_items(task=self,
                                       query=query,
                                       filters=filters,
                                       items=items,
                                       wait=wait)

    def get_items(self, filters=None, get_consensus_items: bool = False):
        """
        Get the task items

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: list of the items or PagedEntity output of items
        :rtype: list or dtlpy.entities.paged_entities.PagedEntities
        """
        return self.tasks.get_items(task_id=self.id, dataset=self.dataset, filters=filters, get_consensus_items=get_consensus_items)

    def set_status(self, status: str, operation: str, item_ids: List[str]):
        """
        Update item status within task

        :param str status: string the describes the status
        :param str operation: the status action need 'create' or 'delete'
        :param list item_ids: List[str] id items ids

        :return: True if success
        :rtype: bool
        """
        return self.tasks.set_status(status=status, operation=operation, item_ids=item_ids, task_id=self.id)
    


================================================
File: dtlpy/entities/time_series.py
================================================
import attr
from .. import entities
import logging

logger = logging.getLogger(name='dtlpy')


@attr.s
class TimeSeries(entities.BaseEntity):
    """
    Time Series object
    """
    # platform
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    owner = attr.ib()
    name = attr.ib()
    id = attr.ib()
    # entities
    _project = attr.ib(repr=False)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @property
    def project(self):
        assert isinstance(self._project, entities.Project)
        return self._project

    @classmethod
    def from_json(cls, _json, project):
        """
        Build a TimeSeries entity object from a json

        :param _json: _json response from host
        :param project: project id
        :return: Time Series object
        """
        return cls(
            created_at=_json.get('createdAt', None),
            updated_at=_json.get('updatedAt', None),
            owner=_json.get('owner', None),
            name=_json.get('name', None),
            id=_json.get('id', None),
            project=project,
        )

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(TimeSeries)._project,
                                                        attr.fields(TimeSeries).created_at,
                                                        attr.fields(TimeSeries).updated_at,
                                                        ))
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        return _json

    ##########
    # Entity #
    ##########
    def delete(self):
        """
        delete the time series

        :return:
        """
        return self.project.times_series.delete(series=self)

    ##########
    # Series #
    ##########
    def samples(self, filters=None):
        """
        get the time table according to filters

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return:
        """
        return self.project.times_series.get_samples(series_id=self.id, filters=filters)

    def add_samples(self, data):
        """
        add data to time series table

        :param data: data
        :return:
        """
        return self.project.times_series.add_samples(series_id=self.id, data=data)

    def delete_samples(self, filters):
        """
        add data to time series table

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return:
        """
        return self.project.times_series.delete_samples(series_id=self.id, filters=filters)

    ###########
    # Samples #
    ###########
    def sample(self, sample_id):
        """
        get a sample line by id

        :param sample_id: sample id
        :return:
        """
        return self.project.times_series.get_sample(series_id=self.id,
                                                    sample_id=sample_id)

    def update_sample(self, sample_id, data):
        """
        Update a sample line by id

        :param sample_id: sample id
        :param data: data
        :return:
        """
        return self.project.times_series.update_sample(series_id=self.id,
                                                       sample_id=sample_id,
                                                       data=data)

    def delete_sample(self, sample_id):
        """
        Delete a single sample line from time series

        :param sample_id:sample id
        :return:
        """
        return self.project.times_series.delete_sample(series_id=self.id,
                                                       sample_id=sample_id)


================================================
File: dtlpy/entities/trigger.py
================================================
import attr
import traceback
import logging
from enum import Enum
from collections import namedtuple

from .. import entities, exceptions, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class TriggerResource(str, Enum):
    ITEM = "Item"
    DATASET = "Dataset"
    ANNOTATION = "Annotation"
    TASK = 'Task',
    ASSIGNMENT = 'Assignment',
    ITEM_STATUS = "ItemStatus"


class TriggerAction(str, Enum):
    CREATED = "Created"
    UPDATED = "Updated"
    DELETED = "Deleted"
    STATUS_CHANGED = 'statusChanged'
    CLONE = 'Clone'


class TriggerExecutionMode(str, Enum):
    ONCE = "Once"
    ALWAYS = "Always"


class TriggerType(str, Enum):
    EVENT = "Event"
    CRON = "Cron"


@attr.s
class BaseTrigger(entities.BaseEntity):
    """
    Trigger Entity
    """
    #######################
    # Platform attributes #
    #######################
    id = attr.ib()
    url = attr.ib(repr=False)
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    creator = attr.ib()
    name = attr.ib()
    active = attr.ib()
    type = attr.ib()
    scope = attr.ib()
    is_global = attr.ib()
    input = attr.ib()

    # name change
    function_name = attr.ib()
    service_id = attr.ib()
    webhook_id = attr.ib()
    pipeline_id = attr.ib()

    ########
    # temp #
    ########
    special = attr.ib(repr=False)

    ##############################
    # different name in platform #
    ##############################
    project_id = attr.ib()
    _spec = attr.ib()
    operation = attr.ib()

    ##################
    # SDK attributes #
    ##################
    _service = attr.ib(repr=False)
    _project = attr.ib(repr=False)
    _client_api = attr.ib(type=ApiClient, repr=False)
    _op_type = attr.ib(default='service')
    _repositories = attr.ib(repr=False)

    updated_by = attr.ib(default=None)

    @staticmethod
    def _get_operation(operation):
        op_type = operation.get('type', None)
        if op_type == 'function':
            service_id = operation.get('serviceId', None)
            webhook_id = None
            pipeline_id = None
        elif op_type == 'webhook':
            webhook_id = operation.get('webhookId', None)
            service_id = None
            pipeline_id = None
        elif op_type == 'rabbitmq':
            webhook_id = None
            service_id = None
            pipeline_id = None
        elif op_type == 'pipeline':
            webhook_id = None
            service_id = None
            pipeline_id = operation.get('id', None)
        else:
            raise exceptions.PlatformException('400', 'unknown trigger operation type: {}'.format(op_type))

        return service_id, webhook_id, pipeline_id

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @staticmethod
    def _protected_from_json(_json, client_api, project=None, service=None):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param client_api: ApiClient entity
        :param project: project entity
        :param service: service entity
        :return:
        """
        try:
            trigger = BaseTrigger.from_json(_json=_json,
                                            client_api=client_api,
                                            project=project,
                                            service=service)
            status = True
        except Exception:
            trigger = traceback.format_exc()
            status = False
        return status, trigger

    @classmethod
    def from_json(cls, _json, client_api, project=None, service=None):
        """
        Build a trigger entity object from a json

        :param dict _json: platform json
        :param dl.ApiClient client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: project entity
        :param dtlpy.entities.service.Service service: service entity
        :return:
        """
        if project is not None:
            if project.id != _json.get('projectId', None):
                logger.warning('Trigger has been fetched from a project that is not belong to it')
                project = None

        if service is not None:
            spec = _json.get('spec', dict())
            operation = spec.get('operation', dict())
            if service.id != operation.get('serviceId', None):
                logger.warning('Trigger has been fetched from a service that is not belong to it')
                service = None

        trigger_type = _json.get('type', None)

        if trigger_type == TriggerType.CRON:
            ent = CronTrigger.from_json(_json, client_api, project, service)
        else:
            ent = Trigger.from_json(_json, client_api, project, service)
        return ent

    ################
    # repositories #
    ################
    @_repositories.default
    def set_repositories(self):
        reps = namedtuple('repositories',
                          field_names=['services', 'triggers'])

        if self._project is None:
            services_repo = repositories.Services(client_api=self._client_api, project=self._project)
        else:
            services_repo = self._project.services

        triggers = repositories.Triggers(client_api=self._client_api,
                                         project=self._project)

        r = reps(services=services_repo, triggers=triggers)
        return r

    @property
    def triggers(self):
        assert isinstance(self._repositories.triggers, repositories.Triggers)
        return self._repositories.triggers

    @property
    def services(self):
        assert isinstance(self._repositories.services, repositories.Services)
        return self._repositories.services

    ############
    # entities #
    ############
    @property
    def project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id,
                                                                                   fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def service(self):
        if self._service is None:
            self._service = self.services.get(service_id=self.service_id, fetch=None)
        assert isinstance(self._service, entities.Service)
        return self._service

    ###########
    # methods #
    ###########
    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        # get excluded
        _json = attr.asdict(self, filter=attr.filters.exclude(attr.fields(BaseTrigger)._client_api,
                                                              attr.fields(BaseTrigger).project_id,
                                                              attr.fields(BaseTrigger)._project,
                                                              attr.fields(BaseTrigger)._service,
                                                              attr.fields(BaseTrigger).special,
                                                              attr.fields(BaseTrigger)._op_type,
                                                              attr.fields(BaseTrigger)._spec,
                                                              attr.fields(BaseTrigger)._repositories,
                                                              attr.fields(BaseTrigger).service_id,
                                                              attr.fields(BaseTrigger).webhook_id,
                                                              attr.fields(BaseTrigger).pipeline_id,
                                                              attr.fields(BaseTrigger).function_name,
                                                              attr.fields(BaseTrigger).is_global,
                                                              attr.fields(BaseTrigger).created_at,
                                                              attr.fields(BaseTrigger).updated_at,
                                                              attr.fields(BaseTrigger).operation,
                                                              attr.fields(BaseTrigger).updated_by,
                                                              ))

        # rename
        _json['projectId'] = self.project_id
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        if self.is_global is not None:
            _json['global'] = self.is_global
        if self.updated_by is not None:
            _json['updatedBy'] = self.updated_by
        return _json

    def delete(self):
        """
        Delete Trigger object

        :return: True
        """
        return self.project.triggers.delete(trigger_id=self.id)

    def update(self):
        """
        Update Trigger object

        :return: Trigger entity
        """
        return self.project.triggers.update(trigger=self)


@attr.s
class Trigger(BaseTrigger):
    """
    Trigger Entity
    """
    filters = attr.ib(default=None, repr=False)
    execution_mode = attr.ib(default=TriggerExecutionMode.ONCE, repr=False)
    actions = attr.ib(default=TriggerAction.CREATED, repr=False)
    resource = attr.ib(default=TriggerResource.ITEM, repr=False)

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = super().to_json()

        _json['spec'] = {
            'filter': _json.pop('filters'),
            'executionMode': _json.pop('execution_mode'),
            'resource': _json.pop('resource'),
            'actions': _json.pop('actions'),
            'input': _json.pop('input', None),
            'operation': self.operation,
        }
        return _json

    @classmethod
    def from_json(cls, _json, client_api, project=None, service=None):
        """
        Build a trigger entity object from a json

        :param _json: platform json
        :param client_api: ApiClient entity
        :param dtlpy.entities.project.Project project: project entity
        :param dtlpy.entities.service.Service service: service entity
        :return:
        """
        spec = _json.get('spec', dict())
        operation = spec.get('operation', dict())

        service_id, webhook_id, pipeline_id = cls._get_operation(operation=operation)

        return cls(
            execution_mode=spec.get('executionMode', None),
            updated_at=_json.get('updatedAt', None),
            created_at=_json.get('createdAt', None),
            resource=spec.get('resource', None),
            creator=_json.get('creator', None),
            special=_json.get('special', None),
            actions=spec.get('actions', None),
            active=_json.get('active', None),
            function_name=operation.get('functionName', None),
            scope=_json.get('scope', None),
            is_global=_json.get('global', None),
            type=_json.get('type', None),
            name=_json.get('name', None),
            url=_json.get('url', None),
            service_id=service_id,
            project_id=_json.get('projectId', None),
            input=spec.get('input', None),
            webhook_id=webhook_id,
            client_api=client_api,
            filters=spec.get('filter', dict()),
            project=project,
            service=service,
            id=_json['id'],
            op_type=operation.get('type', None),
            spec=spec,
            pipeline_id=pipeline_id,
            operation=operation,
            updated_by=_json.get('updatedBy', None),
        )


@attr.s
class CronTrigger(BaseTrigger):
    start_at = attr.ib(default=None)
    end_at = attr.ib(default=None)
    cron = attr.ib(default=None)

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = super().to_json()

        _json['spec'] = {
            'startAt': _json.pop('start_at'),
            'endAt': _json.pop('end_at'),
            'cron': _json.pop('cron'),
            'input': _json.pop('input'),
            'operation': self.operation,
        }
        return _json

    @classmethod
    def from_json(cls, _json, client_api, project=None, service=None):
        """
        Build a trigger entity object from a json

        :param _json: platform json
        :param client_api: ApiClient entity
        :param project: project entity
        :param service: service entity
        :return:
        """
        spec = _json.get('spec', dict())
        operation = spec.get('operation', dict())

        project_id = _json.get('projectId', None)
        if project_id is not None and project is not None:
            if project_id != project.id:
                project = None

        service_id, webhook_id, pipeline_id = cls._get_operation(operation=operation)
        return cls(
            updated_at=_json.get('updatedAt', None),
            created_at=_json.get('createdAt', None),
            creator=_json.get('creator', None),
            special=_json.get('special', None),
            active=_json.get('active', None),
            function_name=operation.get('functionName', None),
            scope=_json.get('scope', None),
            is_global=_json.get('global', None),
            type=_json.get('type', None),
            name=_json.get('name', None),
            input=spec.get('input', None),
            end_at=spec.get('endAt', None),
            start_at=spec.get('startAt', None),
            cron=spec.get('cron', None),
            url=_json.get('url', None),
            service_id=service_id,
            project_id=project_id,
            webhook_id=webhook_id,
            client_api=client_api,
            project=project,
            service=service,
            id=_json['id'],
            op_type=operation.get('type', None),
            spec=spec,
            pipeline_id=pipeline_id,
            operation=operation
        )


================================================
File: dtlpy/entities/user.py
================================================
import traceback
import logging
import attr

from .. import entities, exceptions

logger = logging.getLogger(name='dtlpy')


@attr.s
class User(entities.BaseEntity):
    """
    User entity
    """
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    name = attr.ib()
    last_name = attr.ib()
    username = attr.ib()
    avatar = attr.ib(repr=False)
    email = attr.ib()
    role = attr.ib()
    type = attr.ib()
    org = attr.ib()
    id = attr.ib()

    # api
    _project = attr.ib(repr=False)
    _client_api = attr.ib(default=None, repr=False)
    _users = attr.ib(repr=False, default=None)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @staticmethod
    def _protected_from_json(_json, project, client_api, users=None):
        """
        Same as from_json but with try-except to catch if error

        :param _json: platform json
        :param project: project entity
        :param client_api: ApiClient entity
        :param users: Users repository
        :return:
        """
        try:
            user = User.from_json(_json=_json,
                                  project=project,
                                  users=users,
                                  client_api=client_api)
            status = True
        except Exception:
            user = traceback.format_exc()
            status = False
        return status, user

    @property
    def project(self):
        if self._project is None:
            raise exceptions.PlatformException(error='2001',
                                               message='Missing entity "project".')
        assert isinstance(self._project, entities.Project)
        return self._project

    @classmethod
    def from_json(cls, _json, project, client_api, users=None):
        """
        Build a User entity object from a json

        :param dict _json: _json response from host
        :param dtlpy.entities.project.Project project: project entity
        :param client_api: ApiClient entity
        :param users: Users repository
        :return: User object
        :rtype: dtlpy.entities.user.User
        """
        return cls(
            created_at=_json.get('createdAt', None),
            name=_json.get('firstName', None),
            updated_at=_json.get('updatedAt', None),
            last_name=_json.get('lastName', None),
            username=_json.get('username', None),
            avatar=_json.get('avatar', None),
            email=_json.get('email', None),
            role=_json.get('role', None),
            type=_json.get('type', None),
            org=_json.get('org', None),
            id=_json.get('id', None),
            project=project,
            users=users,
            client_api=client_api)

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(self,
                            filter=attr.filters.exclude(attr.fields(User)._project,
                                                        attr.fields(User).name,
                                                        attr.fields(User)._client_api,
                                                        attr.fields(User)._users,
                                                        attr.fields(User).last_name,
                                                        attr.fields(User).created_at,
                                                        attr.fields(User).updated_at,
                                                        ))
        _json['firstName'] = self.name
        _json['lastName'] = self.last_name
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at
        return _json


================================================
File: dtlpy/entities/webhook.py
================================================
import logging
import attr
from enum import Enum

from .. import repositories, entities
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class HttpMethod(str, Enum):
    GET = "GET"
    POST = "POST"
    DELETE = "DELETE"
    PATCH = "PATCH"


@attr.s
class Webhook(entities.BaseEntity):
    """
    Webhook object
    """
    # platform
    id = attr.ib()
    url = attr.ib()
    created_at = attr.ib()
    updated_at = attr.ib(repr=False)
    creator = attr.ib()
    name = attr.ib()

    # name change
    project_id = attr.ib()
    http_method = attr.ib()
    hook_url = attr.ib()

    # SDK
    _client_api = attr.ib(type=ApiClient, repr=False)
    _project = attr.ib()

    # repos
    _webhooks = attr.ib(default=None)

    @property
    def createdAt(self):
        return self.created_at

    @property
    def updatedAt(self):
        return self.updated_at

    @classmethod
    def from_json(cls, _json: dict, client_api: ApiClient, project=None):
        """
        :param _json: platform json
        :param client_api: ApiClient entity
        :param project: project entity
        :return:
        """
        if project is not None:
            if project.id != _json.get('project', None):
                logger.warning('Webhook has been fetched from a project that is not in it projects list')
                project = None
        return cls(
            http_method=_json.get('httpMethod', None),
            created_at=_json.get("createdAt", None),
            updated_at=_json.get("updatedAt", None),
            project_id=_json.get('project', None),
            hook_url=_json.get('hookUrl', None),
            creator=_json.get("creator", None),
            name=_json.get("name", None),
            url=_json.get("url", None),
            id=_json.get("id", None),
            client_api=client_api,
            project=project
        )

    @property
    def project(self):
        if self._project is None:
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id,
                                                                                   fetch=None)
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def webhooks(self):
        if self._webhooks is None:
            self._webhooks = repositories.Webhooks(client_api=self._client_api, project=self._project)
        assert isinstance(self._webhooks, repositories.Webhooks)
        return self._webhooks

    def to_json(self):
        """
        Returns platform _json format of object

        :return: platform json format of object
        :rtype: dict
        """
        _json = attr.asdict(
            self,
            filter=attr.filters.exclude(
                attr.fields(Webhook)._project,
                attr.fields(Webhook)._client_api,
                attr.fields(Webhook).project_id,
                attr.fields(Webhook).hook_url,
                attr.fields(Webhook).http_method,
                attr.fields(Webhook).created_at,
                attr.fields(Webhook).updated_at,
            ),
        )

        _json['project'] = self.project_id
        _json['hookUrl'] = self.hook_url
        _json['httpMethod'] = self.http_method
        _json['createdAt'] = self.created_at
        _json['updatedAt'] = self.updated_at

        return _json

    def delete(self):
        return self.webhooks.delete(self)

    def update(self):
        return self.webhooks.update(self)


================================================
File: dtlpy/entities/annotation_definitions/__init__.py
================================================
from .base_annotation_definition import BaseAnnotationDefinition
from .box import Box
from .cube import Cube
from .cube_3d import Cube3d
from .note import Note, Message
from .classification import Classification
from .comparison import Comparison
from .ellipse import Ellipse
from .point import Point
from .polygon import Polygon
from .polyline import Polyline
from .segmentation import Segmentation
from .subtitle import Subtitle
from .description import Description
from .undefined_annotation import UndefinedAnnotationType
from .pose import Pose
from .text import Text
from .free_text import FreeText
from .ref_image import RefImage
from .gis import Gis, GisType


================================================
File: dtlpy/entities/annotation_definitions/base_annotation_definition.py
================================================
import logging
import numpy as np
import warnings

logger = logging.getLogger(name='dtlpy')


class BaseAnnotationDefinition:
    def __init__(self, description=None, attributes=None):
        self.description = description
        self._top = 0
        self._left = 0
        self._bottom = 0
        self._right = 0
        self._annotation = None

        if isinstance(attributes, list) and len(attributes) > 0:
            warnings.warn("List attributes are deprecated and will be removed in version 1.109. Use Attribute 2.0 (Dictionary) instead."
                "For more details, refer to the documentation: "
                "https://developers.dataloop.ai/tutorials/data_management/upload_and_manage_annotations/chapter/#set-attributes-on-annotations",
                DeprecationWarning,
            )
        self._attributes = attributes

    @property
    def attributes(self):
        return self._attributes

    @attributes.setter
    def attributes(self, v):
        if isinstance(v, list):
            warnings.warn("List attributes are deprecated and will be removed in version 1.109. Use Attribute 2.0 (Dictionary) instead. "
                "For more details, refer to the documentation: "
                "https://developers.dataloop.ai/tutorials/data_management/upload_and_manage_annotations/chapter/#set-attributes-on-annotations",
                DeprecationWarning,
            )
        self._attributes = v
    @property
    def top(self):
        return self._top

    @top.setter
    def top(self, v):
        self._top = v

    @property
    def left(self):
        return self._left

    @left.setter
    def left(self, v):
        self._left = v

    @property
    def bottom(self):
        return self._bottom

    @bottom.setter
    def bottom(self, v):
        self._bottom = v

    @property
    def right(self):
        return self._right

    @right.setter
    def right(self, v):
        self._right = v

    @property
    def height(self):
        return np.round(self.bottom - self.top)

    @property
    def width(self):
        return np.round(self.right - self.left)

    @staticmethod
    def add_text_to_image(image, annotation):
        """
        :param image:
        :param annotation:
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        text = '{label}-{attributes}'.format(label=annotation.label, attributes=','.join(annotation.attributes))
        top = annotation.top
        left = annotation.left
        if top == 0:
            top = image.shape[0] / 10
        if left == 0:
            left = image.shape[1] / 10
        return cv2.putText(img=image,
                           text=text,
                           org=tuple([int(np.round(left)), int(np.round(top))]),
                           color=(255, 0, 0),
                           fontFace=cv2.FONT_HERSHEY_DUPLEX,
                           fontScale=1,
                           thickness=2)

    @property
    def logger(self):
        return logger


================================================
File: dtlpy/entities/annotation_definitions/box.py
================================================
import copy

import numpy as np
from . import BaseAnnotationDefinition
from .polygon import Polygon
import warnings


class Box(BaseAnnotationDefinition):
    """
        Box annotation object
        Can create a box using 2 point using: "top", "left", "bottom", "right" (to form a box [(left, top), (right, bottom)])
        For rotated box add the "angel"
    """
    type = "box"

    def __init__(self,
                 left=None, top=None, right=None, bottom=None,
                 label=None, attributes=None, description=None, angle=None):
        """
        Can create a box using 2 point using:
         "top", "left", "bottom", "right" (to form a box [(left, top), (right, bottom)])
        And for create a rotated box need to add the angle of the rotation

        :param left: left x coordinate of the box
        :param top: top Y coordinate of the box
        :param right: right x coordinate of the box
        :param bottom: bottom Y coordinate of the box
        :param label: annotation label
        :param attributes: a list of attributes for the annotation
        :param description:
        :param angle: the angle of the rotation in degrees

        :return:
        """
        super().__init__(description=description, attributes=attributes)

        self.angle = angle
        self.left = left
        self.top = top
        self.right = right
        self.bottom = bottom
        self.top_left = [left, top]
        self.top_right = [right, top]
        self.bottom_left = [left, bottom]
        self.bottom_right = [right, bottom]
        self.label = label
        self._four_points = self._rotate_around_point() if self.is_rotated else [self.top_left,
                                                                                 self.bottom_left,
                                                                                 self.bottom_right,
                                                                                 self.top_right]

    @property
    def is_rotated(self):
        return self.angle is not None and self.angle != 0

    @property
    def x(self):
        if self._box_points_setting():
            return [x_point[0] for x_point in self._four_points]
        return [self.left, self.right]

    @property
    def y(self):
        if self._box_points_setting():
            return [y_point[1] for y_point in self._four_points]
        return [self.top, self.bottom]

    @property
    def geo(self):
        if self._box_points_setting():
            res = self._four_points
        else:
            res = [
                [self.left, self.top],
                [self.right, self.bottom]
            ]
        return res

    def _box_points_setting(self):
        res = False
        if self._annotation and self._annotation.item:
            item = self._annotation.item
            project_id = item.project_id if item.project_id else item.project.id
            settings_dict = item._client_api.platform_settings.settings.get('4ptBox', None)
            if settings_dict is not None:
                if project_id in settings_dict:
                    res = settings_dict.get(project_id, None)
                elif '*' in settings_dict:
                    res = settings_dict.get('*', None)
        return res

    def _rotate_points(self, points):
        angle = np.radians(self.angle)
        rotation_matrix = np.asarray([[np.cos(angle), -np.sin(angle)],
                                      [np.sin(angle), np.cos(angle)]])
        pts2 = np.asarray([rotation_matrix.dot(pt)[:2] for pt in points])
        return pts2

    def _translate(self, points, translate_x, translate_y=None):
        translation_matrix = np.asarray([[1, 0, translate_x],
                                         [0, 1, translate_y],
                                         [0, 0, 1]])
        pts2 = np.asarray([translation_matrix.dot(list(pt) + [1])[:2] for pt in points])
        return pts2

    def _rotate_around_point(self):
        points = copy.deepcopy(self.four_points)
        center = [((self.left + self.right) / 2), ((self.top + self.bottom) / 2)]
        centerized = self._translate(points, -center[0], -center[1])
        rotated = self._rotate_points(centerized)
        moved = self._translate(rotated, center[0], center[1])
        return moved

    @property
    def four_points(self):
        return [self.top_left, self.bottom_left, self.bottom_right, self.top_right]

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        if thickness is None:
            thickness = 2

        # draw annotation
        if self.is_rotated:
            points = self._rotate_around_point()
        else:
            points = self.four_points

        # create image to draw on
        if alpha != 1:
            overlay = image.copy()
        else:
            overlay = image

        # draw annotation
        overlay = cv2.drawContours(
            image=overlay,
            contours=[np.round(points).astype(int)],
            contourIdx=-1,
            color=color,
            thickness=thickness,
            lineType=cv2.LINE_AA
        )

        if not isinstance(color, int) and len(color) == 4 and color[3] != 255:
            # add with opacity
            image = cv2.addWeighted(src1=overlay,
                                    alpha=alpha,
                                    src2=image,
                                    beta=1 - alpha,
                                    gamma=0)
        else:
            image = overlay

        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        points = [
            [self.left, self.top],
            [self.right, self.bottom]
        ]
        pts = [{"x": float(x), "y": float(y), "z": 0} for x, y in points]

        if self.angle is not None and self.angle != 0:
            pts.append(self.angle)

        return pts

    @staticmethod
    def from_coordinates(coordinates):
        return np.asarray([[pt["x"], pt["y"]] for pt in coordinates[:2]])

    @classmethod
    def from_json(cls, _json):
        coordinates = _json.get("coordinates", None) if "coordinates" in _json else _json.get("data", None)
        if coordinates is None:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        geo = cls.from_coordinates(coordinates=coordinates)

        left = np.min(geo[:, 0])
        top = np.min(geo[:, 1])
        right = np.max(geo[:, 0])
        bottom = np.max(geo[:, 1])

        angel = coordinates[2] if len(coordinates) > 2 and (
                isinstance(coordinates[2], float) or isinstance(coordinates[2], int)) else None

        return cls(
            left=left,
            top=top,
            right=right,
            bottom=bottom,
            label=_json["label"],
            attributes=_json.get("attributes", None),
            angle=angel
        )

    @classmethod
    def from_segmentation(cls, mask, label, attributes=None):
        """
        Convert binary mask to Polygon

        :param mask: binary mask (0,1)
        :param label: annotation label
        :param attributes: annotations list of attributes

        :return: Box annotations list  to each separated  segmentation
        """
        polygons = Polygon.from_segmentation(
            mask=mask,
            label=label,
            attributes=attributes,
            max_instances=None
        )

        if not isinstance(polygons, list):
            polygons = [polygons]

        boxes = [
            cls(
                left=polygon.left,
                top=polygon.top,
                right=polygon.right,
                bottom=polygon.bottom,
                label=label,
                attributes=attributes
            ) for polygon in polygons
        ]

        return boxes


================================================
File: dtlpy/entities/annotation_definitions/classification.py
================================================
from . import BaseAnnotationDefinition


class Classification(BaseAnnotationDefinition):
    """
        Classification annotation object
    """
    type = "class"

    def __init__(self, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.label = label

    @property
    def x(self):
        return 0

    @property
    def y(self):
        return 0

    @property
    def geo(self):
        return list()

    @property
    def left(self):
        return 0

    @property
    def top(self):
        return 0

    @property
    def right(self):
        return 0

    @property
    def bottom(self):
        return 0

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        return list()

    @classmethod
    def from_json(cls, _json):
        return cls(
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/comparison.py
================================================
from . import BaseAnnotationDefinition


class Comparison(BaseAnnotationDefinition):
    """
    Comparison annotation object
    """
    type = "comparison"

    def __init__(self, coordinates, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.label = label
        self.coordinates = coordinates

    @property
    def geo(self):
        return 0

    @property
    def left(self):
        return 0

    @property
    def top(self):
        return 0

    @property
    def right(self):
        return 0

    @property
    def bottom(self):
        return 0

    @property
    def z(self):
        return 0

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        return self.coordinates

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            coordinates = _json["coordinates"]
        elif "data" in _json:
            coordinates = _json["data"]
        else:
            coordinates = dict()

        return cls(
            coordinates=coordinates,
            label=_json.get("label", None),
            attributes=_json.get("attributes", None)
        )


================================================
File: dtlpy/entities/annotation_definitions/cube.py
================================================
import math
import numpy as np

from . import BaseAnnotationDefinition


class Cube(BaseAnnotationDefinition):
    """
        Cube annotation object
    """
    type = "cube"

    def __init__(self, label, front_tl, front_tr, front_br, front_bl,
                 back_tl, back_tr, back_br, back_bl, angle=None,
                 attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.front_bl = front_bl
        self.front_br = front_br
        self.front_tr = front_tr
        self.front_tl = front_tl
        self.back_bl = back_bl
        self.back_br = back_br
        self.back_tr = back_tr
        self.back_tl = back_tl

        self._angle = angle
        self.label = label

        self.keys = ["front_tl", "front_tr", "front_br", "front_bl",
                     "back_tl", "back_tr", "back_br", "back_bl"]

    @staticmethod
    def calculate_angle(b, c):
        a = [b[0] + 200, b[1]]
        if b in (a, c):
            return 0

        ang = math.degrees(
            math.atan2(c[1] - b[1], c[0] - b[0]) - math.atan2(a[1] - b[1], a[0] - b[0]))
        return ang + 360 if ang < 0 else ang

    @property
    def angle(self):
        if self._angle is None:
            self._angle = Cube.calculate_angle(self.front_tl, self.front_tr)
        return self._angle

    @property
    def x(self):
        return self.geo[:, 0]

    @property
    def y(self):
        return self.geo[:, 1]

    @property
    def geo(self):
        return np.asarray([self.front_tl,
                           self.front_tr,
                           self.front_br,
                           self.front_bl,
                           self.back_tl,
                           self.back_tr,
                           self.back_br,
                           self.back_bl
                           ])

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        image = cv2.polylines(image,
                              pts=[np.asarray(
                                  [self.front_bl, self.front_br, self.front_tr, self.front_tl]).round().astype(int)],
                              isClosed=True,
                              color=color,
                              thickness=thickness)
        image = cv2.polylines(image,
                              pts=[np.asarray([self.back_bl, self.back_br, self.back_tr, self.back_tl]).round().astype(
                                  int)],
                              isClosed=True,
                              color=color,
                              thickness=thickness)
        image = cv2.line(image,
                         pt1=tuple(np.asarray(self.front_bl).round().astype(int)),
                         pt2=tuple(np.asarray(self.back_bl).round().astype(int)),
                         color=color,
                         thickness=thickness)
        image = cv2.line(image,
                         pt1=tuple(np.asarray(self.front_br).round().astype(int)),
                         pt2=tuple(np.asarray(self.back_br).round().astype(int)),
                         color=color,
                         thickness=thickness)
        image = cv2.line(image,
                         pt1=tuple(np.asarray(self.front_tl).round().astype(int)),
                         pt2=tuple(np.asarray(self.back_tl).round().astype(int)),
                         color=color,
                         thickness=thickness)
        image = cv2.line(image,
                         pt1=tuple(np.asarray(self.front_tr).round().astype(int)),
                         pt2=tuple(np.asarray(self.back_tr).round().astype(int)),
                         color=color,
                         thickness=thickness)
        return image

    def to_coordinates(self, color):
        coordinates = {self.keys[idx]: {"x": float(x), "y": float(y), "z": 0}
                       for idx, [x, y] in enumerate(self.geo)}
        coordinates['angle'] = self.angle
        return coordinates

    @staticmethod
    def from_coordinates(coordinates):
        geo = list()
        for key, pt in enumerate(coordinates):
            geo.append([pt["x"], pt["y"]])
        return np.asarray(geo)

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            key = "coordinates"
        elif "data" in _json:
            key = "data"
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        return cls(
            front_bl=np.asarray([_json[key]["front_bl"]['x'], _json[key]["front_bl"]['y']]),
            front_br=np.asarray([_json[key]["front_br"]['x'], _json[key]["front_br"]['y']]),
            front_tl=np.asarray([_json[key]["front_tl"]['x'], _json[key]["front_tl"]['y']]),
            front_tr=np.asarray([_json[key]["front_tr"]['x'], _json[key]["front_tr"]['y']]),
            back_bl=np.asarray([_json[key]["back_bl"]['x'], _json[key]["back_bl"]['y']]),
            back_br=np.asarray([_json[key]["back_br"]['x'], _json[key]["back_br"]['y']]),
            back_tl=np.asarray([_json[key]["back_tl"]['x'], _json[key]["back_tl"]['y']]),
            back_tr=np.asarray([_json[key]["back_tr"]['x'], _json[key]["back_tr"]['y']]),
            label=_json["label"],
            angle=_json[key]["angle"],
            attributes=_json.get("attributes", None)
        )

    @staticmethod
    def rotate(center, point, angle):
        angle = math.radians(angle)
        cx, cy = center
        px, py = point

        qx = cx + math.cos(angle) * (px - cx) - math.sin(angle) * (py - cy)
        qy = cy + math.sin(angle) * (px - cx) + math.cos(angle) * (py - cy)
        return [qx, qy]

    @classmethod
    def from_boxes_and_angle(cls,
                             front_left, front_top, front_right, front_bottom,
                             back_left, back_top, back_right, back_bottom,
                             label, angle=0, attributes=None):
        """
        Create cuboid by given front and back boxes with angle
        the angle calculate fom the center of each box
        """
        if angle != 0:
            front_center = [front_left + (front_right - front_left) / 2, front_top + (front_bottom - front_top) / 2]
            back_center = [back_left + (back_right - back_left) / 2, back_top + (back_bottom - back_top) / 2]
            front_tl = Cube.rotate(center=front_center, point=[front_left, front_top], angle=angle)
            front_tr = Cube.rotate(center=front_center, point=[front_right, front_top], angle=angle)
            front_br = Cube.rotate(center=front_center, point=[front_right, front_bottom], angle=angle)
            front_bl = Cube.rotate(center=front_center, point=[front_left, front_bottom], angle=angle)
            back_tl = Cube.rotate(center=back_center, point=[back_left, back_top], angle=angle)
            back_tr = Cube.rotate(center=back_center, point=[back_right, back_top], angle=angle)
            back_br = Cube.rotate(center=back_center, point=[back_right, back_bottom], angle=angle)
            back_bl = Cube.rotate(center=back_center, point=[back_left, back_bottom], angle=angle)
        else:
            front_tl = [front_left, front_top]
            front_tr = [front_right, front_top]
            front_br = [front_right, front_bottom]
            front_bl = [front_left, front_bottom]
            back_tl = [back_left, back_top]
            back_tr = [back_right, back_top]
            back_br = [back_right, back_bottom]
            back_bl = [back_left, back_bottom]

        return cls(
            front_tl=front_tl, front_tr=front_tr, front_br=front_br, front_bl=front_bl,
            back_tl=back_tl, back_tr=back_tr, back_br=back_br, back_bl=back_bl,
            label=label,
            angle=angle,
            attributes=attributes
        )


================================================
File: dtlpy/entities/annotation_definitions/cube_3d.py
================================================
import numpy as np
# import open3d as o3d
from . import BaseAnnotationDefinition
# from scipy.spatial.transform import Rotation as R
import logging

logger = logging.getLogger(name='dtlpy')


class Cube3d(BaseAnnotationDefinition):
    """
        Cube annotation object
    """
    type = "cube_3d"

    def __init__(self, label, position, scale, rotation,
                 attributes=None, description=None):
        """
        :param label:
        :param position: the XYZ position of the ‘center’ of the annotation.
        :param scale: the scale of the object by each axis (XYZ).
        :param rotation: an euler representation of the object rotation on each axis (with rotation order ‘XYZ’). (rotation in radians)
        :param attributes:
        :param description:
        """
        super().__init__(description=description, attributes=attributes)

        self.position = position
        self.scale = scale
        self.rotation = rotation
        self.label = label

    def _translate(self, points, translate_x, translate_y, translate_z):
        translation_matrix = np.array([[1, 0, 0, 0],
                                       [0, 1, 0, 0],
                                       [0, 0, 1, 0],
                                       [translate_x, translate_y, translate_z, 1]])

        matrix = [(list(i) + [1]) for i in points]
        pts2 = np.dot(matrix, translation_matrix)
        return [pt[:3] for pt in pts2]

    # def make_points(self):
    #     simple = [
    #         [self.scale[0] / 2, self.scale[1] / 2, self.scale[2] / 2],
    #         [-self.scale[0] / 2, self.scale[1] / 2, self.scale[2] / 2],
    #         [self.scale[0] / 2, -self.scale[1] / 2, self.scale[2] / 2],
    #         [self.scale[0] / 2, self.scale[1] / 2, -self.scale[2] / 2],
    #         [-self.scale[0] / 2, -self.scale[1] / 2, self.scale[2] / 2],
    #         [self.scale[0] / 2, -self.scale[1] / 2, -self.scale[2] / 2],
    #         [-self.scale[0] / 2, self.scale[1] / 2, -self.scale[2] / 2],
    #         [-self.scale[0] / 2, -self.scale[1] / 2, -self.scale[2] / 2],
    #     ]
    #
    #     # matrix = R.from_euler('xyz', self.rotation, degrees=False)
    #
    #     vecs = [np.array(p) for p in simple]
    #     rotated = matrix.apply(vecs)
    #     translation = np.array(self.position)
    #     dX = translation[0]
    #     dY = translation[1]
    #     dZ = translation[2]
    #     points = self._translate(rotated, dX, dY, dZ)
    #     return points

    @property
    def geo(self):
        return np.asarray([
            list(self.position),
            list(self.scale),
            list(self.rotation)
        ])

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise
        points = self.make_points()
        front_bl = points[0]
        front_br = points[1]
        front_tr = points[2]
        front_tl = points[3]
        back_bl = points[4]
        back_br = points[5]
        back_tr = points[6]
        back_tl = points[7]
        logger.warning('the show for 3d_cube is not supported.')
        return image

        # image = np.zeros((100, 100, 100), dtype=np.uint8)
        # pcd = o3d.io.read_point_cloud(r"C:\Users\97250\PycharmProjects\tt\qw\3D\D34049418_0000635.las.pcd")
        # # o3d.visualization.draw_geometries([pcd])
        # # points = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1],
        # #           [0, 1, 1], [1, 1, 1]]
        # lines = [[0, 1], [0, 2], [1, 3], [2, 3], [4, 5], [4, 6], [5, 7], [6, 7],
        #          [0, 4], [1, 5], [2, 6], [3, 7]]
        # colors = [[1, 0, 0] for i in range(len(lines))]
        # points = [back_bl, back_br, back_tl, back_tr, front_bl, front_br, front_tl, front_tr]
        # line_set = o3d.geometry.LineSet()
        # line_set.points = o3d.utility.Vector3dVector(points)
        # line_set.lines = o3d.utility.Vector2iVector(lines)
        # line_set.colors = o3d.utility.Vector3dVector(colors)
        # o3d.visualization.draw_geometries([line_set])
        # return image

    def to_coordinates(self, color=None):
        keys = ["position", "scale", "rotation"]
        coordinates = {keys[idx]: {"x": float(x), "y": float(y), "z": float(z)}
                       for idx, [x, y, z] in enumerate(self.geo)}
        return coordinates

    @staticmethod
    def from_coordinates(coordinates):
        geo = list()
        for key, pt in coordinates.items():
            geo.append([pt["x"], pt["y"], pt["z"]])
        return np.asarray(geo)

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            key = "coordinates"
        elif "data" in _json:
            key = "data"
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        return cls(
            position=list(_json[key]['position'].values()),
            scale=list(_json[key]['scale'].values()),
            rotation=list(_json[key]['rotation'].values()),
            label=_json["label"],
            attributes=_json.get("attributes", None)
        )


================================================
File: dtlpy/entities/annotation_definitions/description.py
================================================
from . import BaseAnnotationDefinition


class Description(BaseAnnotationDefinition):
    """
        Subtitle annotation object
    """

    def __init__(self, text, description=None):
        super().__init__(description=description)
        self.type = "item_description"
        self.text = text
        self.label = "item.description"

    def to_coordinates(self, color):
        return {"text": self.text}

    @staticmethod
    def from_coordinates(coordinates):
        return coordinates["text"]

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            text = cls.from_coordinates(coordinates=_json["coordinates"])
        elif "data" in _json:
            text = cls.from_coordinates(coordinates=_json["data"])
        else:
            raise ValueError('Bad json, "coordinates", "data" or "description" not found')
        return cls(
            text=text,
        )


================================================
File: dtlpy/entities/annotation_definitions/ellipse.py
================================================
import numpy as np

from . import BaseAnnotationDefinition


class Ellipse(BaseAnnotationDefinition):
    """
        Ellipse annotation object
    """
    type = "ellipse"

    def __init__(self, x, y, rx, ry, angle, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.label = label
        self.angle = angle
        self.x = x
        self.y = y
        self.rx = rx
        self.ry = ry

    @property
    def geo(self):
        return np.asarray([[self.x, self.y],
                           [self.rx, self.ry],
                           [self.angle, self.rad]])

    @property
    def rad(self):
        return np.deg2rad(self.angle)

    @property
    def left(self):
        return self.x - np.sqrt(np.power(self.rx, 2) * np.power(np.cos(-self.rad), 2)
                                + np.power(self.ry, 2) * np.power(np.sin(-self.rad), 2))

    @property
    def top(self):
        return self.y - np.sqrt(np.power(self.rx, 2) * np.power(np.sin(-self.rad), 2)
                                + np.power(self.ry, 2) * np.power(np.cos(-self.rad), 2))

    @property
    def right(self):
        return self.x + np.sqrt(np.power(self.rx, 2) * np.power(np.cos(-self.rad), 2)
                                + np.power(self.ry, 2) * np.power(np.sin(-self.rad), 2))

    @property
    def bottom(self):
        return self.y + np.sqrt(np.power(self.rx, 2) * np.power(np.sin(-self.rad), 2)
                                + np.power(self.ry, 2) * np.power(np.cos(-self.rad), 2))

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        if thickness is None:
            thickness = 2

        # draw annotation
        image = cv2.ellipse(
            image,
            center=(int(np.round(self.x)), int(np.round(self.y))),
            axes=(int(np.round(self.rx)), int(np.round(self.ry))),
            angle=self.angle,
            startAngle=0,
            endAngle=360,
            color=color,
            thickness=thickness,
            lineType=-1,
        )
        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        return {'angle': float(self.angle),
                'center': {'x': float(self.x),
                           'y': float(self.y),
                           'z': 0},
                'rx': float(self.rx),
                'ry': float(self.ry)}

    @staticmethod
    def from_coordinates(coordinates):
        angle = coordinates["angle"]
        x = coordinates["center"]["x"]
        y = coordinates["center"]["y"]
        rx = coordinates["rx"]
        ry = coordinates["ry"]
        return x, y, rx, ry, angle

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            x, y, rx, ry, angle = cls.from_coordinates(_json["coordinates"])
        elif "data" in _json:
            x, y, rx, ry, angle = cls.from_coordinates(_json["data"])
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        return cls(
            angle=angle,
            x=x,
            y=y,
            rx=rx,
            ry=ry,
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/free_text.py
================================================
from . import BaseAnnotationDefinition


class FreeText(BaseAnnotationDefinition):
    """
    Free text annotation type (e.g. response for a prompt)
    """
    type = "text"

    def __init__(self, text, label='free-text', attributes=None, description=None):
        """
        Create a free text annotation
        :param label: annotation label
        :param text: string of the annotation
        :param attributes: annotation attributes
        :param description:

        :return:
        """
        super().__init__(description=description, attributes=attributes)
        self.text = text
        self.label = label

    @property
    def x(self):
        return 0

    @property
    def y(self):
        return 0

    @property
    def geo(self):
        return list()

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        return self.text

    def to_coordinates(self, color):
        return self.text

    @classmethod
    def from_json(cls, _json):
        coordinates = _json["coordinates"]

        return cls(
            text=coordinates,
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/gis.py
================================================
from . import BaseAnnotationDefinition


class GisType:
    """
    State enum
    """
    BOX = 'box'
    POLYGON = 'polygon'
    POLYLINE = 'polyline'
    POINT = 'point'


class Gis(BaseAnnotationDefinition):
    """
        Box annotation object
        Can create a box using 2 point using: "top", "left", "bottom", "right" (to form a box [(left, top), (right, bottom)])
        For rotated box add the "angel"
    """
    type = "gis"

    def __init__(self,
                 annotation_type: GisType,
                 geo,
                 label=None,
                 attributes=None,
                 description=None,
                 ):
        """
        Can create gis annotation using points:

        :param geo: list of points
        :param label: annotation label
        :param attributes: a list of attributes for the annotation
        :param description:

        :return:
        """
        super().__init__(description=description, attributes=attributes)

        if geo is None:
            raise ValueError('geo must be provided')
        if annotation_type is None:
            raise ValueError('annotation_type must be provided')
        self.label = label
        self.annotation = None
        self.geo = geo
        self.annotation_type = annotation_type

    def to_coordinates(self, color):
        return {
            "geo_type": self.annotation_type,
            "wgs84_geo_coordinates": self.geo
        }

    @classmethod
    def from_json(cls, _json):
        json_coordinates = _json.get("coordinates", {}) if "coordinates" in _json else _json.get("data", {})
        coordinates = json_coordinates.get("wgs84_geo_coordinates", None)
        annotations_type = json_coordinates.get("geo_type", None)
        if coordinates is None:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        return cls(
            annotation_type=annotations_type,
            geo=coordinates,
            label=_json["label"],
            attributes=_json.get("attributes", None)
        )


================================================
File: dtlpy/entities/annotation_definitions/note.py
================================================
import numpy as np
import time

from . import Box
from ...services.api_client import client as api_client


class Note(Box):
    """
        Note annotation object
    """

    def __init__(
            self,
            left,
            top,
            right,
            bottom,
            label,
            attributes=None,
            messages=None,
            status='issue',
            assignee=None,
            create_time=None,
            creator=None,
            description=None
    ):
        super(Note, self).__init__(
            left=left,
            top=top,
            right=right,
            bottom=bottom,
            label=label,
            attributes=attributes,
            description=description
        )
        self.type = "note"
        if messages is None:
            messages = []
        if not isinstance(messages, list):
            messages = [messages]
        for msg_index in range(len(messages)):
            if not isinstance(messages[msg_index], Message):
                messages[msg_index] = Message(body=messages[msg_index])
        self.messages = messages
        self.status = status
        self.create_time = create_time
        self.creator = creator
        if self.creator is None:
            self.creator = api_client.info()['user_email']
        self.assignee = assignee
        if self.assignee is None:
            self.assignee = self.creator

    def to_coordinates(self, color):
        box = super(Note, self).to_coordinates(color=color)
        note = {
            'messages': [msg.to_json() for msg in self.messages],
            'status': self.status,
            'createTime': self.create_time,
            'creator': self.creator,
            'assignee': self.assignee
        }
        coordinates = {
            'box': box,
            'note': note
        }

        return coordinates

    @staticmethod
    def from_coordinates(coordinates):
        return Box.from_coordinates(coordinates['box'])

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            geo = cls.from_coordinates(_json["coordinates"])
            note_data = _json["coordinates"].get('note', dict())
        elif "data" in _json:
            geo = cls.from_coordinates(_json["data"])
            note_data = _json["data"].get('note', dict())
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        left = np.min(geo[:, 0])
        top = np.min(geo[:, 1])
        right = np.max(geo[:, 0])
        bottom = np.max(geo[:, 1])
        messages = [Message.from_json(msg) for msg in note_data.get('messages', list())]

        return cls(
            left=left,
            top=top,
            right=right,
            bottom=bottom,
            label=_json["label"],
            attributes=_json.get("attributes", None),
            messages=messages,
            status=note_data.get('status', 'open'),
            creator=note_data.get('creator', 'me'),
            assignee=note_data.get('assignee', 'me'),
            create_time=note_data.get('createTime', 0),
        )

    def add_message(self, body: str = None):
        self.messages.append(Message(body=body))


class Message:
    """
    Note message object
    """

    def __init__(self, msg_id: str = None, creator: str = None, msg_time=None, body: str = None):
        self.id = msg_id
        self.time = msg_time if msg_time is not None else int(time.time() * 1000)
        self.body = body
        self.creator = creator
        if self.creator is None:
            self.creator = api_client.info()['user_email']

    def to_json(self):
        _json = {
            "id": self.id,
            "creator": self.creator,
            "time": self.time,
            "body": self.body
        }
        return _json

    @staticmethod
    def from_json(_json):
        return Message(
            msg_id=_json.get('id', None),
            msg_time=_json.get('time', None),
            body=_json.get('body', None),
            creator=_json.get('creator', None)
        )


================================================
File: dtlpy/entities/annotation_definitions/point.py
================================================
import numpy as np

from . import BaseAnnotationDefinition


class Point(BaseAnnotationDefinition):
    """
    Point annotation object
    """
    type = "point"

    def __init__(self, x, y, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.y = y
        self.x = x
        self.label = label

    @property
    def geo(self):
        return [self.x, self.y]

    @property
    def left(self):
        return self.x

    @property
    def top(self):
        return self.y

    @property
    def right(self):
        return self.y

    @property
    def bottom(self):
        return self.x

    @property
    def z(self):
        return 0

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        # point cant have thickness 1
        if thickness is None or thickness == -1:
            thickness = 5

        # create image to draw on
        if alpha != 1:
            overlay = image.copy()
        else:
            overlay = image

        # draw annotation
        overlay = cv2.circle(
            img=overlay,
            center=(int(np.round(self.x)), int(np.round(self.y))),
            radius=thickness,
            color=color,
            thickness=thickness,
            lineType=cv2.LINE_AA,
        )

        if not isinstance(color, int) and len(color) == 4 and color[3] != 255:
            # add with opacity
            image = cv2.addWeighted(src1=overlay,
                                    alpha=alpha,
                                    src2=image,
                                    beta=1 - alpha,
                                    gamma=0)
        else:
            image = overlay

        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        return {"x": float(self.x), "y": float(self.y), "z": float(self.z)}

    @staticmethod
    def from_coordinates(coordinates):
        return coordinates["x"], coordinates["y"]

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            x, y = cls.from_coordinates(_json["coordinates"])
        elif "data" in _json:
            x, y = cls.from_coordinates(_json["data"])
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))

        return cls(
            x=x,
            y=y,
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/polygon.py
================================================
import numpy as np
import logging

from . import BaseAnnotationDefinition

logger = logging.getLogger(name='dtlpy')


class Polygon(BaseAnnotationDefinition):
    """
    Polygon annotation object
    """
    type = "segment"

    def __init__(self, geo, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.geo = geo
        self.label = label

    @property
    def x(self):
        return self.geo[:, 0]

    @property
    def y(self):
        return self.geo[:, 1]

    @property
    def left(self):
        return np.min(self.x)

    @property
    def top(self):
        return np.min(self.y)

    @property
    def right(self):
        return np.max(self.x)

    @property
    def bottom(self):
        return np.max(self.y)

    def to_coordinates(self, color):
        return [[{"x": float(x), "y": float(y)} for x, y in self.geo]]

    @staticmethod
    def from_coordinates(coordinates):
        return np.asarray([[pt["x"], pt["y"]] for pt in coordinates])

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        if thickness is None:
            thickness = 2

        # create image to draw on
        if alpha != 1:
            overlay = image.copy()
        else:
            overlay = image

        overlay = cv2.drawContours(
            image=overlay,
            contours=[np.round(self.geo).astype(int)],
            contourIdx=-1,
            color=color,
            thickness=thickness,
        )

        if not isinstance(color, int) and len(color) == 4 and color[3] != 255:
            # add with opacity
            image = cv2.addWeighted(src1=overlay,
                                    alpha=alpha,
                                    src2=image,
                                    beta=1 - alpha,
                                    gamma=0)
        else:
            image = overlay

        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    @classmethod
    def from_segmentation(cls, mask, label, attributes=None, epsilon=None, max_instances=1, min_area=0):
        """
        Convert binary mask to Polygon

        :param mask: binary mask (0,1)
        :param label: annotation label
        :param attributes: annotations list of attributes
        :param epsilon: from opencv: specifying the approximation accuracy. This is the maximum distance between the original curve and its approximation. if 0 all points are returns
        :param max_instances: number of max instances to return. if None all wil be returned
        :param min_area: remove polygons with area lower thn this threshold (pixels)

        :return: Polygon annotation
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            raise ImportError('opencv not found. Must install to perform this function')

        # mask float
        mask = 1. * mask
        # normalize to 1
        mask /= np.max(mask)
        # threshold the mask
        ret, thresh = cv2.threshold(mask, 0.5, 255, 0)
        # find contours
        major, minor, _ = cv2.__version__.split(".")
        if int(major) > 3:
            contours, hierarchy = cv2.findContours(thresh.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        else:
            _, contours, hierarchy = cv2.findContours(thresh.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

        if len(contours) == 0:
            # no contours were found
            new_pts_list = []
        else:
            # calculate contours area
            areas = np.asarray([cv2.contourArea(cnt) for cnt in contours])
            # take onr contour with maximum area
            sorted_areas_inds = areas.argsort()[::-1]
            filtered_contours = [contours[s_ind] for s_ind in
                                 sorted_areas_inds if areas[s_ind] > min_area]  # filter by area size
            filtered_contours = filtered_contours[:max_instances]  # take only the first max_instance of the results
            # estimate contour to reduce number of points
            new_pts_list = list()
            for curve in filtered_contours:
                if epsilon is None:
                    epsilon = 0.0005 * cv2.arcLength(curve=curve,
                                                     closed=True)
                estimated_polygon = np.squeeze(cv2.approxPolyDP(curve=curve,
                                                                epsilon=epsilon,
                                                                closed=True))
                if len(estimated_polygon.shape) == 1:
                    new_pts_list.append(np.squeeze(curve))
                else:
                    new_pts_list.append(estimated_polygon)
        polygons = [cls(geo=new_pts,
                        label=label,
                        attributes=attributes,
                        ) for new_pts in new_pts_list]

        if len(polygons) == 1:
            polygons = polygons[0]
        return polygons

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            geo = cls.from_coordinates(coordinates=_json["coordinates"][0])
        elif "data" in _json:
            geo = cls.from_coordinates(coordinates=_json["data"][0])
        else:
            raise ValueError(
                'can not find "coordinates" or "data" in annotation. id: %s'
                % _json["id"]
            )
        return cls(
            geo=geo,
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/polyline.py
================================================
import numpy as np

from . import BaseAnnotationDefinition


class Polyline(BaseAnnotationDefinition):
    """
    Polyline annotation object
    """

    def __init__(self, geo, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.type = "polyline"
        self.geo = geo
        self.label = label

    @property
    def x(self):
        return self.geo[:, 0]

    @property
    def y(self):
        return self.geo[:, 1]

    @property
    def left(self):
        return np.min(self.x)

    @property
    def top(self):
        return np.min(self.y)

    @property
    def right(self):
        return np.max(self.x)

    @property
    def bottom(self):
        return np.max(self.y)

    def to_coordinates(self, color):
        return [[{"x": float(x), "y": float(y)} for x, y in self.geo]]

    @staticmethod
    def from_coordinates(coordinates):
        return np.asarray([[pt["x"], pt["y"]] for pt in coordinates])

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            self.logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        # polyline cant have thickness -1
        if thickness is None or thickness == -1:
            thickness = 2

        if alpha != 1:
            overlay = image.copy()
        else:
            overlay = image

        overlay = cv2.polylines(
            img=overlay,
            pts=[np.round(self.geo).astype(int)],
            color=color,
            isClosed=False,
            thickness=thickness,
        )

        if not isinstance(color, int) and len(color) == 4 and color[3] != 255:
            # add with opacity
            image = cv2.addWeighted(src1=overlay,
                                    alpha=alpha,
                                    src2=image,
                                    beta=1 - alpha,
                                    gamma=0)
        else:
            image = overlay

        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            geo = cls.from_coordinates(coordinates=_json["coordinates"][0])
        elif "data" in _json:
            geo = cls.from_coordinates(coordinates=_json["data"][0])
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))
        return cls(
            geo=geo,
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/pose.py
================================================
import numpy as np
import uuid

from . import BaseAnnotationDefinition


class Pose(BaseAnnotationDefinition):
    """
        Classification annotation object
    """

    def __init__(self, label, template_id, instance_id=None, attributes=None, points=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.type = "pose"
        self.label = label
        self.template_id = template_id
        if instance_id is None:
            instance_id = str(uuid.uuid1())
        self.instance_id = instance_id
        if points is None:
            points = list()
        self.points = points

    @property
    def x(self):
        return [point.x for point in self.points]

    @property
    def y(self):
        return [point.y for point in self.points]

    @property
    def geo(self):
        return list()

    @property
    def left(self):
        if not len(self.points):
            return 0
        return np.min(self.x)

    @property
    def top(self):
        if not len(self.points):
            return 0
        return np.min(self.y)

    @property
    def right(self):
        if not len(self.points):
            return 0
        return np.max(self.x)

    @property
    def bottom(self):
        if not len(self.points):
            return 0
        return np.max(self.y)

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        return {'templateId': self.template_id,
                'instanceId': self.instance_id}

    @staticmethod
    def from_coordinates(coordinates):
        return coordinates

    @classmethod
    def from_json(cls, _json):
        return cls(
            label=_json["label"],
            attributes=_json.get("attributes", None),
            template_id=cls.from_coordinates(_json["coordinates"])['templateId'],
            instance_id=cls.from_coordinates(_json["coordinates"])['instanceId']
        )


================================================
File: dtlpy/entities/annotation_definitions/ref_image.py
================================================
from . import BaseAnnotationDefinition


class RefImage(BaseAnnotationDefinition):
    """
    Create an image annotation. Reference the url or item id in this annotation type
    """
    type = "ref_image"

    def __init__(self, ref, ref_type=None, mimetype=None, label='ref-image', attributes=None, description=None):

        """
        Create an image annotation. Used for generative model and any other algorithm where and image is the output

        For type 'id', need to upload the image as item in the platform and reference the item id in the annotation.
        For type 'url', mimetype must be provided to load the ref correctly in the platform

        :param str ref: the reference to the image annotation, represented by an ‘itemId’ or ‘url’
        :param str ref_type: one of ‘id’ | ‘url’
        :param str mimetype: optional. in case the refType is URL, e.g. image/jpeg, video/mpeg
        :param label: annotation label
        :param attributes: annotation attributes
        :param description:

        :return:
        """
        super().__init__(description=description, attributes=attributes)
        if ref_type is None:
            if ref.startswith('http'):
                ref_type = 'url'
            else:
                ref_type = 'id'
        self.ref = ref
        self.ref_type = ref_type
        self.mimetype = mimetype
        self.label = label

    @property
    def x(self):
        return 0

    @property
    def y(self):
        return 0

    @property
    def geo(self):
        return list()

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        # TODO over or show the image annotations
        return self.add_text_to_image(image=image, annotation=self)

    def to_coordinates(self, color):
        coordinates = {
            "ref": self.ref,
            "refType": self.ref_type,
            "mimetype": self.mimetype,
        }
        return coordinates

    @classmethod
    def from_json(cls, _json):
        coordinates = _json["coordinates"]
        ref = coordinates.get('ref')
        ref_type = coordinates.get('refType')
        mimetype = coordinates.get('mimetype')
        return cls(
            ref=ref,
            ref_type=ref_type,
            mimetype=mimetype,
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/segmentation.py
================================================
import numpy as np
import base64
import io
from PIL import Image

from . import BaseAnnotationDefinition

from .box import Box
from .polygon import Polygon


class Segmentation(BaseAnnotationDefinition):
    """
    Segmentation annotation object
    """
    type = "binary"

    def __init__(self, geo: np.ndarray, label: str, attributes=None, description=None, color=None):
        super().__init__(description=description, attributes=attributes)
        self._geo = geo
        self._coordinates = None
        self.label = label
        self._color = color

    @property
    def geo(self) -> np.ndarray:
        if self._geo is None:
            self._geo = self.from_coordinates(self._coordinates)
            if self._color is None:
                color = None
                fill_coordinates = self._geo.nonzero()
                if len(fill_coordinates) > 0 and len(fill_coordinates[0]) > 0 and len(fill_coordinates[1]) > 0:
                    color = self._geo[fill_coordinates[0][0]][fill_coordinates[1][0]]
                self._color = color
            self._geo = (self._geo[:, :, 3] > 127).astype(float)
        return self._geo

    @geo.setter
    def geo(self, geo: np.ndarray):
        self._geo = geo
        self._coordinates = None

    @property
    def x(self):
        return

    @property
    def y(self):
        return

    @property
    def pts(self):
        return np.where(self.geo > 0)

    @property
    def left(self):
        left = 0
        if len(self.pts[1]) > 0:
            left = np.min(self.pts[1])
        return left

    @property
    def top(self):
        top = 0
        if len(self.pts[0]) > 0:
            top = np.min(self.pts[0])
        return top

    @property
    def right(self):
        right = 0
        if len(self.pts[1]) > 0:
            right = np.max(self.pts[1])
        return right

    @property
    def bottom(self):
        bottom = 0
        if len(self.pts[0]) > 0:
            bottom = np.max(self.pts[0])
        return bottom

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            raise ImportError('opencv not found. Must install to perform this function')

        if alpha != 1:
            overlay = image.copy()
        else:
            overlay = image
        if color is None:
            if self._color:
                color = self._color
            else:
                color = (255, 255, 255)
        # draw annotation
        overlay[np.where(self.geo)] = color

        if not isinstance(color, int) and len(color) == 4 and color[3] != 255:
            # add with opacity
            image = cv2.addWeighted(src1=overlay,
                                    alpha=alpha,
                                    src2=image,
                                    beta=1 - alpha,
                                    gamma=0)
        else:
            image = overlay

        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color=None):
        need_encode = False
        if color is not None and self._color is not None:
            # if input color is not the same as the annotation's color - need to re-encode
            if self._color != color:
                need_encode = True

        if need_encode or self._coordinates is None:
            if self._color is not None:
                color = self._color
            else:
                color = (255, 255, 255)
            max_val = np.max(self.geo)
            if max_val > 1:
                self.geo = self.geo / max_val
            png_ann = np.stack((color[0] * self.geo,
                                color[1] * self.geo,
                                color[2] * self.geo,
                                255 * self.geo),
                               axis=2).astype(np.uint8)
            pil_img = Image.fromarray(png_ann)
            buff = io.BytesIO()
            pil_img.save(buff, format="PNG")
            new_image_string = base64.b64encode(buff.getvalue()).decode("utf-8")
            self._coordinates = "data:image/png;base64,%s" % new_image_string
        return self._coordinates

    def to_box(self):
        """

        :return: Box annotations list  to each separated  segmentation
        """
        polygons = Polygon.from_segmentation(mask=self.geo, label=self.label,
                                             attributes=self.attributes, max_instances=None)

        if not isinstance(polygons, list):
            polygons = [polygons]

        boxes = [Box(left=polygon.left,
                     top=polygon.top,
                     right=polygon.right,
                     bottom=polygon.bottom,
                     label=polygon.label,
                     attributes=polygon.attributes) for polygon in polygons]

        return boxes

    @classmethod
    def from_polygon(cls, geo, label, shape, attributes=None):
        """

        :param geo: list of x,y coordinates of the polygon ([[x,y],[x,y]...]
        :param label: annotation's label
        :param shape: image shape (h,w)
        :param attributes:
        :return:
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            raise ImportError('opencv not found. Must install to perform this function')

        thickness = -1

        # plot polygon on a blank mask with thickness -1 to fill the polyline
        mask = np.zeros(shape=shape, dtype=np.uint8)
        mask = cv2.drawContours(image=mask,
                                contours=[np.asarray(geo).astype('int')],
                                contourIdx=-1,
                                color=1,
                                thickness=thickness)

        return cls(
            geo=mask,
            label=label,
            attributes=attributes,
        )

    @staticmethod
    def from_coordinates(coordinates):
        if isinstance(coordinates, dict):
            data = coordinates["data"][22:]
        elif isinstance(coordinates, str):
            data = coordinates[22:]
        else:
            raise TypeError('unknown binary data type')
        decode = base64.b64decode(data)
        mask = np.array(Image.open(io.BytesIO(decode)))
        return mask

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            coordinates = _json["coordinates"]
        elif "data" in _json:
            coordinates = _json["data"]
        else:
            raise ValueError('can not find "coordinates" or "data" in annotation. id: {}'.format(_json["id"]))
        inst = cls(
            geo=None,
            label=_json["label"],
            attributes=_json.get("attributes", None),
            color=None
        )
        inst._coordinates = coordinates
        return inst


================================================
File: dtlpy/entities/annotation_definitions/subtitle.py
================================================
from . import BaseAnnotationDefinition


class Subtitle(BaseAnnotationDefinition):
    """
        Subtitle annotation object
    """
    type = "subtitle"

    def __init__(self, text, label, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.text = text
        self.label = label

    def to_coordinates(self, color):
        return {"text": self.text}

    @staticmethod
    def from_coordinates(coordinates):
        return coordinates["text"]

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            text = cls.from_coordinates(coordinates=_json["coordinates"])
        elif "data" in _json:
            text = cls.from_coordinates(coordinates=_json["data"])
        else:
            raise ValueError('Bad json, "coordinates" or "data" not found')
        return cls(
            text=text,
            label=_json["label"],
            attributes=_json.get("attributes", None)
        )


================================================
File: dtlpy/entities/annotation_definitions/text.py
================================================
from . import BaseAnnotationDefinition


class Text(BaseAnnotationDefinition):
    """
        can create a text annotation having two types (paragraph, block)
    """
    type = "text_mark"

    def __init__(self, text_type, start, end, label,
                 top=None, left=None, attributes=None, description=None):
        """
        can create a text annotation having two types (paragraph, block)

        :param text_type: text type (paragraph, block)
        :param start: start position in characters
        :param end: end position in characters
        :param label: annotation label
        :param top: top box pixel
        :param left: left box pixel
        :param attributes: annotation attributes
        :param description:

        :return:
        """
        super().__init__(description=description, attributes=attributes)
        self.text_type = text_type
        self.start = start
        self.end = end
        self.top = top
        self.left = left
        self.label = label

    @property
    def x(self):
        return 0

    @property
    def y(self):
        return 0

    @property
    def geo(self):
        return list()

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        return self.add_text_to_image(image=image, annotation=self)

    def to_coordinates(self, color):
        coordinates = {
            "type": self.text_type,
            "label": self.label,
            "start": self.start,
            "end": self.end
        }
        if self.top is not None and self.left is not None:
            coordinates['top'] = self.top
            coordinates['left'] = self.left
        return coordinates

    @classmethod
    def from_json(cls, _json):
        coordinates = _json["coordinates"]

        return cls(
            text_type=coordinates.get("type"),
            start=coordinates.get("start"),
            end=coordinates.get("end"),
            top=coordinates.get("top", None),
            left=coordinates.get("left", None),
            label=_json["label"],
            attributes=_json.get("attributes", None),
        )


================================================
File: dtlpy/entities/annotation_definitions/undefined_annotation.py
================================================
from . import BaseAnnotationDefinition


class UndefinedAnnotationType(BaseAnnotationDefinition):
    """
    UndefinedAnnotationType annotation object
    """

    def __init__(self, type, label, coordinates, attributes=None, description=None):
        super().__init__(description=description, attributes=attributes)
        self.type = type

        self.label = label
        self.coordinates = coordinates

    @property
    def geo(self):
        return 0

    @property
    def left(self):
        return 0

    @property
    def top(self):
        return 0

    @property
    def right(self):
        return 0

    @property
    def bottom(self):
        return 0

    @property
    def z(self):
        return 0

    def show(self, image, thickness, with_text, height, width, annotation_format, color, alpha=1):
        """
        Show annotation as ndarray
        :param image: empty or image to draw on
        :param thickness:
        :param with_text: not required
        :param height: item height
        :param width: item width
        :param annotation_format: options: list(dl.ViewAnnotationOptions)
        :param color: color
        :param alpha: opacity value [0 1], default 1
        :return: ndarray
        """
        if with_text:
            image = self.add_text_to_image(image=image, annotation=self)
        return image

    def to_coordinates(self, color):
        return self.coordinates

    @classmethod
    def from_json(cls, _json):
        if "coordinates" in _json:
            coordinates = _json["coordinates"]
        elif "data" in _json:
            coordinates = _json["data"]
        else:
            coordinates = dict()

        return cls(
            coordinates=coordinates,
            label=_json.get("label", None),
            type=_json.get('type', None),
            attributes=_json.get("attributes", None)
        )


================================================
File: dtlpy/examples/__init__.py
================================================
from . import add_labels
from . import add_metadata_to_item
from . import annotate_items_using_model
from . import annotate_video_using_model_and_tracker
from . import annotations_convert_to_voc
from . import annotations_convert_to_yolo
from . import convert_annotation_types
from . import copy_annotations
from . import copy_folder
from . import create_annotations
from . import create_video_annotations
from . import move_item
from . import play_video_annotation
from . import show_item_and_mask
from . import upload_batch_of_items
from . import upload_items_and_custom_format_annotations
from . import upload_segmentation_annotations_from_mask_image
from . import upload_yolo_format_annotations
from . import upload_items_with_modalities


================================================
File: dtlpy/examples/add_labels.py
================================================
def main(project_name, dataset_name):
    # Imports the SDK package
    import dtlpy as dl

    """
    Label dictionary format:
    
    {
        'label_name': 'dog',
        'displayLabel': 'Dog',
        'attributes': ['list of attributes'],
        'color': (34, 6, 231),
        'children': ['list of label dictionaries']
    }

    """

    # prep
    project = dl.projects.get(project_name=project_name)
    dataset = project.datasets.get(dataset_name=dataset_name)

    #########################
    # View dataset's labels #
    #########################
    # as objects
    labels = dataset.labels

    # as instance map
    labels = dataset.instance_map

    ###############################
    # add label to dataset entity #
    ###############################
    dataset.add_label(label_name='Horse', color=(2, 43, 123))

    #############################
    # add label with sub-labels #
    #############################
    dataset.add_label(label_name='CEO', color=(2, 43, 123),
                      children=[{'label_name': 'Manager',
                                 'children': [{'label_name': 'Employee'}]}])

    ################################
    # add labels to dataset entity #
    ################################
    labels = [
        {'label_name': 'Dog',
         'color': (34, 6, 231),
         'children': [{'label_name': 'Puppy',
                       'color': (24, 16, 130)}]},
        {'label_name': 'Cat',
         'color': (24, 25, 31),
         'children': [{'label_name': 'Kitten',
                       'color': (124, 116, 140)}]}
    ]
    dataset.add_labels(label_list=labels)

    #################
    # Delete Labels #
    #################
    dataset.delete_labels(label_names=['Cat', 'Dog'])

    ########################################
    # Copy dataset labels to a new dataset #
    ########################################
    new_dataset = project.datasets.create(dataset_name='new_dataset_with_labels',
                                          labels=dataset.labels)

    ##########################################
    # Copy dataset ontology to a new dataset #
    ##########################################
    new_dataset = project.datasets.create(dataset_name='new_dataset_with_ontology',
                                          ontology_ids=dataset.ontology_ids)

    ##############################################
    # Copy dataset labels to an existing dataset #
    ##############################################
    new_dataset = project.datasets.create(dataset_name='new_dataset_without_labels')

    # Get from a list or recipes
    recipe = new_dataset.recipes.list()[0]
    # Or get recipe by id
    # recipe = new_dataset.recipes.get(recipe_id='recipe_id')

    # Get from the list of ontologies
    ontology = recipe.ontologies.list()[0]
    # Or get ontology by id
    # ontology = recipe.ontologies.get(ontology='ontology_id')

    # Add the labels to the dataset
    ontology.add_labels(label_list=dataset.labels)
    ontology.update()

    #########################################################
    # Copy dataset ontology to an existing dataset's recipe #
    #########################################################
    new_dataset = project.datasets.create(dataset_name='new_dataset_without_ontology')
    # get recipe
    new_dataset_recipe = new_dataset.recipes.list()[0]
    # Copy from a different dataset
    new_dataset_recipe.ontology_ids = dataset.ontology_ids
    # Update the new dataset
    new_dataset_recipe.update()

    ##########################
    # update existing recipe #
    ##########################
    # Get recipe from list
    recipe = dataset.recipes.list()[0]
    # Or get specific recipe:
    # recipe = dataset.recipes.get(recipe_id='id')

    # Get ontology from list
    ontology = recipe.ontologies.list()[0]
    # Or get specific ontology:
    # ontology = recipe.ontologies.get(ontology_id='id')

    # Add one label
    ontology.add_label(label_name='Lion', color=(35, 234, 123))

    # Add a list of labels
    labels = [{'label_name': 'Shark', 'color': (1, 1, 1)}, {'label_name': 'Whale', 'color': (34, 56, 7)},
              {'label_name': 'Dolphin', 'color': (100, 14, 150)}]
    ontology.add_labels(label_list=labels)

    # After adding - update ontology
    ontology.update()

    #####################
    # Create new recipe #
    #####################
    # Label list
    labels = [{'tag': 'Donkey', 'color': (1, 1, 1)}, {'tag': 'Mammoth', 'color': (34, 56, 7)},
              {'tag': 'Bird', 'color': (100, 14, 150)}]
    recipe = dataset.recipes.create(recipe_name='My Recipe', labels=labels)


================================================
File: dtlpy/examples/add_metadata_to_item.py
================================================
def main(project_name, dataset_name, item_path):
    """
    Add any metadata to item
    :return:
    """
    # import Dataloop SDK
    import dtlpy as dl

    # get dataset
    dataset = dl.projects.get(project_name=project_name).datasets.get(dataset_name=dataset_name)

    # upload and claim item
    item = dataset.items.upload(local_path=item_path)

    # modify metadata
    item.metadata['user'] = dict()
    item.metadata['user']['MyKey'] = 'MyVal'
    # update and reclaim item
    item = item.update()

    # item in platform should have section 'user' in metadata with field 'MyKey' and value 'MyVal'


================================================
File: dtlpy/examples/annotate_items_using_model.py
================================================
def main():
    """
        Annotate a batch of images using a model and upload to platform
        :return:
        """
    import numpy as np
    from PIL import Image
    from keras.applications.imagenet_utils import decode_predictions
    from keras.applications.inception_v3 import InceptionV3, preprocess_input
    import dtlpy as dl

    ##############
    # load model #
    ##############
    model = InceptionV3()

    ##########################
    # init platform instance #
    ##########################
    project = dl.projects.get(project_name='ImageNet')
    dataset = project.datasets.get(dataset_name='sample')

    # get pages of images from dataset
    pages = dataset.items.list()
    ####################
    # start annotating #
    ####################
    for page in pages:
        for item in page:
            if item.type == 'dir':
                continue
            img_batch = [item.download(save_locally=False)]
            # load images
            img_batch = [Image.open(buf) for buf in img_batch]
            # get original images shapes before reshaping for model
            orig_img_shape = [img.size[::-1] for img in img_batch]
            # reshape and load images
            batch = np.array([np.array(img.resize((299, 299))) for img in img_batch])
            # preprocess batch
            batch = preprocess_input(batch)
            # inference the model
            predictions = model.predict(batch)
            # get ImageNet labels
            labels = decode_predictions(predictions, top=1)
            # create platform annotations instance
            builder = item.annotations.builder()
            for i_pred, label in enumerate(labels):

                # add the class labels
                ##############################
                # If model is classification #
                ##############################
                builder.add(annotation_definition=dl.Classification(label=label[0][1]))
                #############################
                # If model outputs polygons #
                #############################
                builder.add(annotation_definition=dl.Polyline(geo=pred['polygon_pts'],
                                                              label=labels[i_pred][0][1]))
                #########################
                # If model outputs mask #
                #########################
                builder.add(annotation_definition=dl.Segmentation(geo=pred['mask'],
                                                                  label=labels[i_pred][0][1]))
            # upload a annotations to matching items in platform
            builder.upload()


================================================
File: dtlpy/examples/annotate_video_using_model_and_tracker.py
================================================
def main():
    """
    Detect and track (using model and some tracker) and upload annotation to platform
    :return:
    """
    import cv2
    import dtlpy as dl

    ##########################
    # Load model and tracker #
    ##########################
    # load your model for detection
    model = load_some_model()
    # load any tracking algorithm to track detected elements
    tracker = load_some_tracker()

    ##############
    # load video #
    ##############
    video_path = 'some/video/path'

    vid = cv2.VideoCapture(video_path)
    if not vid.isOpened():
        raise IOError("Couldn't open webcam or video")
    video_fps = vid.get(cv2.CAP_PROP_FPS)
    video_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),
                  int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))
    video_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))

    ############
    # Platform #
    ###########
    # get the item from platform
    item = dl.projects.get(project_name='MyProject') \
        .datasets.get(dataset_name='MyDataset') \
        .items.get(filepath='/path/to/video.mp4')
    builder = item.annotations.builder()

    #######
    # Run #
    #######
    frame_num = 0
    while True:
        # get new frame from video
        return_value, frame = vid.read()
        if not return_value:
            break

        # get detection
        detections = model.predict(frame)

        # update tracker
        tracked_elements = tracker.update(detections, frame)

        # update annotations object
        for element in tracked_elements:
            # element.bb - format of the bounding box is 2 points in 1 array - [x_left, y_top, x_right, y_bottom])
            # tracking id of each element is in element.id. to keep the ids of the detected elements
            left, top, bottom, right = element.bb  # points bounding box annotation
            builder.add(annotation_definition=dl.Box(top=top,
                                                     left=left,
                                                     right=right,
                                                     bottom=bottom,
                                                     label=element.label),
                        object_id=element.id,
                        frame_num=frame_num)
        # increase frame number
        frame_num += 1
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    ##################################
    # Upload annotations to platform #
    ##################################
    item.annotations.upload(builder.to_platform())


================================================
File: dtlpy/examples/annotations_convert_to_voc.py
================================================
def main():
    import dtlpy as dl

    project = dl.projects.get(project_name='Ocean')
    dataset = project.datasets.get(dataset_name='Sharks')

    converter = dl.Converter()
    converter.convert_dataset(dataset=dataset, to_format='voc',
                              local_path='home/voc_annotations/sharks')


================================================
File: dtlpy/examples/annotations_convert_to_yolo.py
================================================
def main():
    import dtlpy as dl
    
    project = dl.projects.get(project_name='Jungle')
    dataset = project.datasets.get(dataset_name='Tigers')

    converter = dl.Converter()
    converter.convert_dataset(dataset=dataset, to_format='yolo',
                              local_path='home/yolo_annotations/tigers')


================================================
File: dtlpy/examples/convert_annotation_types.py
================================================
def main():
    """
    Convert annotation types
    :return:
    """
    from PIL import Image
    import matplotlib.pyplot as plt
    import dtlpy as dl

    # Get project and dataset
    project = dl.projects.get(project_name='Toilet Paper')
    dataset = project.datasets.get(dataset_name='Extra Soft')

    # Get item and binaries
    item = dataset.items.get(filepath='/with_puppies.jpg')
    buffer = item.download(save_locally=False)

    #######################################
    # Convert mask annotations to polygon #
    #######################################
    img = Image.open(buffer)
    builder = item.annotations.builder()
    builder.add(dl.Polygon.from_segmentation(mask=mask,  # binary mask of the annotation
                                             label='roll'))
    # plot the annotation
    plt.figure()
    plt.imshow(builder.show())
    # plot annotation on the image
    plt.figure()
    plt.imshow(builder.show(image=img))

    # upload annotation to platform
    item.annotations.update(builder)

    #######################################
    # Convert polygon annotations to mask #
    #######################################
    img = Image.open(buffer)
    builder = item.annotations.builder()
    builder.add(dl.Segmentation.from_polygon(geo=[[x1, y1], [x2, y2], [x3, y3]],  # list of coordinates
                                             shape=img.size[::-1],  # (h,w)
                                             label='roll'))
    # plot the annotation
    plt.figure()
    plt.imshow(builder.show())
    # plot annotation on the image
    plt.figure()
    plt.imshow(builder.show(image=img))

    # upload annotation to platform
    item.annotations.update(builder)


================================================
File: dtlpy/examples/converter.py
================================================
def main():
    import dtlpy as dl

    dataset = dl.projects.get("project_name").datasets.get("dataset_name")

    ############################
    # convert annotations list #
    ############################

    # convert from dataloop to other formats #
    ##########################################

    # known format #

    item = dataset.items.get(item_id="item_id")
    annotations = item.annotations.list()
    converter = dl.Converter()
    converted_annotations = converter.convert(annotations=annotations,
                                              from_format='dataloop',
                                              to_format='yolo')

    # custom format #

    # custom conversion function
    # converts 1 dataloop annotation to custom format annotation
    # returns converted annotation
    def my_converter(annotation):
        """
        :param annotation: dataloop Annotation object
        :return: format of new annotation
        """
        ann = {'label': annotation.label, 'type': annotation.type}
        return ann

    converted_annotations = converter.convert(annotations=annotations,
                                              from_format='dataloop',
                                              to_format='my_format',
                                              conversion_func=my_converter)

    # convert from other formats to dataloop format #
    #################################################

    # known format #

    # load yolo annotations from file
    annotations = list()
    with open('annotations_filepath.txt', 'r') as f:
        line = f.readline()
        while line:
            annotations.append(line)
            line = f.readline()

    # create converter object
    converter = dl.Converter()

    # convert
    converted_annotations = converter.convert(annotations=annotations,
                                              from_format='yolo',
                                              to_format='dataloop')

    # custom format #

    # load yolo annotations from file
    annotations = list()
    with open('annotations_filepath.txt', 'r') as f:
        line = f.readline()
        while line:
            annotations.append(line)
            line = f.readline()

    # create converter object
    converter = dl.Converter()

    # custom conversion function
    # converts 1 custom annotation to dataloop annotation
    # dataloop annotation
    def my_converter(annotation):
        """
        :param annotation: custom annotation format
        :type annotation: dict
        :return: dataloop Annotation object
        """
        annotations = dl.Annotation.new(annotation_definition=dl.Box(top=annotation['top'],
                                                                     bottom=annotation['bottom'],
                                                                     left=annotation['left'],
                                                                     right=annotation['right'],
                                                                     label=annotation['label']))

        return annotation

    converted_annotations = converter.convert(annotations=annotations,
                                              from_format='my_format',
                                              to_format='dataloop',
                                              conversion_func=my_converter)

    ###############################
    # convert dataset annotations #
    ###############################

    # known format #
    ################

    converter = dl.Converter()

    converter.save_to_format = 'json'
    converter.convert_dataset(dataset=dataset,
                              to_format='coco',
                              local_path='some/local/path/to/download/converted/annotations')

    # custom format #
    #################

    def my_converter(annotation):
        """
        :param annotation: dataloop Annotation object
        :return: format of new annotation
        """
        ann = {'label': annotation.label, 'type': annotation.type}
        return ann

    converter = dl.Converter()

    converter.save_to_format = 'json'
    converter.convert_dataset(dataset=dataset,
                              to_format='my_format',
                              conversion_func=my_converter,
                              local_path='some/local/path/to/download/converted/annotations')

    ################
    # save to file #
    ################

    item = dataset.items.get(item_id="item_id")
    annotations = item.annotations.list()
    converter = dl.Converter()
    converter.convert(annotations=annotations,
                      from_format='dataloop',
                      to_format='yolo')

    # what file format to save to
    converter.save_to_format = '.txt'
    # save
    converter.save_to_file(save_to='local_path', to_format='yolo')


================================================
File: dtlpy/examples/copy_annotations.py
================================================
def main(first_project_name, second_project_name, first_dataset_name, second_dataset_name, first_remote_filepath, second_remote_filepath):
    """
    Copy annotations between items
    :return:
    """
    import dtlpy as dl

    # FROM get the annotations from item
    project = dl.projects.get(project_name=first_project_name)
    dataset = project.datasets.get(dataset_name=first_dataset_name)
    item = dataset.items.get(filepath=first_remote_filepath)

    # get annotations
    annotations = item.annotations.list()

    # TO post annotations to other item
    project = dl.projects.get(project_name=second_project_name)
    dataset = project.datasets.get(dataset_name=second_dataset_name)
    item = dataset.items.get(filepath=second_remote_filepath)

    # post
    item.annotations.upload(annotations=annotations)


================================================
File: dtlpy/examples/copy_folder.py
================================================
def main(first_project_name, second_project_name, first_dataset_name, second_dataset_name):
    """
    Copy folder from project/dataset/folder
    :return:
    """
    import dtlpy as dl

    # Get source project and dataset
    project = dl.projects.get(project_name=first_project_name)
    dataset_from = project.datasets.get(dataset_name=first_dataset_name)
    # filter to get all files of a specific folder
    filters = dl.Filters()
    filters.add(field='type', values='file')  # get only files
    filters.add(field='filename', values='/source_folder/**')  # get all items in folder (recursive)
    pages = dataset_from.items.list(filters=filters)

    # Get destination project and annotations
    project = dl.projects.get(project_name=second_project_name)
    dataset_to = project.datasets.get(dataset_name=second_dataset_name)

    # go over all projects and copy file from src to dst
    for page in pages:
        for item in page:
            # download item (without save to disk)
            buffer = item.download(save_locally=False)
            # give the items name to the buffer
            buffer.name = item.name
            # upload item
            new_item = dataset_to.items.upload(local_path=buffer,
                                               remote_path='/destination_folder')
            print(new_item.filename)


================================================
File: dtlpy/examples/create_annotations.py
================================================
def main():
    import dtlpy as dl
    import numpy as np
    import matplotlib.pyplot as plt

    # Get project and dataset
    project = dl.projects.get(project_name='Food')
    dataset = project.datasets.get(dataset_name='BeansDataset')

    # get item from platform
    item = dataset.items.get(filepath='/image.jpg')

    # Create a builder instance
    builder = item.annotations.builder()

    # add annotations of type box and label person
    builder.add(annotation_definition=dl.Box(top=10,
                                             left=10,
                                             bottom=100,
                                             right=100,
                                             label='black_bean'))

    # add annotations of type point with attribute
    builder.add(annotation_definition=dl.Point(x=80,
                                               y=80,
                                               label='pea'),
                attribute=['large'])

    # add annotations of type polygon
    builder.add(annotation_definition=dl.Polyline(geo=[[80, 40],
                                                       [100, 120],
                                                       [110, 130]],
                                                  label='beans_can'))

    # create a mask
    mask = np.zeros(shape=(item.height, item.width), dtype=np.uint8)
    # mark some part in the middle
    mask[50:100, 200:250] = 1
    # add annotations of type segmentation
    builder.add(annotation_definition=dl.Segmentation(geo=mask,
                                                      label='tomato_sauce'))

    # plot the all of the annotations you created
    plt.figure()
    plt.imshow(builder.show())
    for annotation in builder:
        plt.figure()
        plt.imshow(annotation.show())
        plt.title(annotation.label)
    # upload annotations to the item
    item.annotations.upload(builder)


================================================
File: dtlpy/examples/create_video_annotations.py
================================================
def main():
    import dtlpy as dl
    import matplotlib.pyplot as plt

    # Get project and dataset
    project = dl.projects.get(project_name='Food')
    dataset = project.datasets.get(dataset_name='BeansDataset')
    item = dataset.items.get(filepath='/flying boxes.mp4')

    ############################
    # using annotation builder #
    ############################
    # create annotation builder
    builder = item.annotations.builder()

    for i_frame in range(100):
        # go over 100 frame
        for i_detection in range(10):
            # for each frame we have 10 different detections (location is just for the example)
            builder.add(annotation_definition=dl.Box(top=2 * i_frame,
                                                     left=2 * i_detection,
                                                     bottom=2 * i_frame + 10,
                                                     right=2 * i_detection + 100,
                                                     label="moving box"),
                        frame_num=i_frame,  # set the frame for the annotation
                        object_id=i_detection + 1
                        # need to input the element id to create the connection between frames
                        )
            # starting from frame 50 add another 10 new annotations of a moving point
            if i_frame > 50:
                builder.add(annotation_definition=dl.Point(x=2 * i_frame,
                                                           y=2 * i_detection,
                                                           label="moving point"),
                            frame_num=i_frame,
                            object_id=20 + (i_detection + 1))
    # get frame annotations
    frame_annotations = builder.get_frame(frame_num=55)
    # Plot the annotations in frame 55 of the created annotations
    plt.figure()
    plt.imshow(frame_annotations.show())

    # plot each annotations separately
    for annotation in frame_annotations:
        plt.figure()
        plt.imshow(annotation.show())
        plt.title(annotation.label)

    # Add the annotations to platform
    item.annotations.upload(builder)

    #####################
    # single annotation #
    #####################
    # create one annotations for a video (without using tracker and object ids)
    annotation = dl.Annotation.new(item=item)

    for i_frame in range(100):
        # go over 100 frame
        annotation.add_frame(annotation_definition=dl.Box(top=2 * i_frame,
                                                          left=2 * (i_frame + 10),
                                                          bottom=2 * (i_frame + 50),
                                                          right=2 * (i_frame + 100),
                                                          label="moving box",
                                                          ),
                             frame_num=i_frame,  # set the frame for the annotation
                             )

    # upload to platform
    annotation.upload()

    ##############################################
    # show annotation state in a specified frame #
    ##############################################

    # Get from platform
    annotations = item.annotations.list()

    # Plot the annotations in frame 55 of the created annotations
    plt.figure()
    plt.imshow(annotations.get_frame(frame_num=55).show())

    # Play video with the Dataloop video player
    annotations.video_player()


================================================
File: dtlpy/examples/delete_annotations.py
================================================
def main(project_name, dataset_name, remote_filepath):
    """
    Copy annotations between items
    :return:
    """
    import dtlpy as dl

    # FROM get the annotations from item
    project = dl.projects.get(project_name=project_name)
    dataset = project.datasets.get(dataset_name=dataset_name)
    item = dataset.items.get(filepath=remote_filepath)

    # get annotations
    annotations = item.annotations.list()

    # delete first annotation
    ann = annotations[0]
    ann.delete()

    ### Or - to delete all annotations ###

    # get annotations
    annotations = item.annotations.list()

    # delete first annotation
    annotations.delete()


================================================
File: dtlpy/examples/filters.py
================================================
def main():
    import dtlpy as dl

    ########
    # prep #
    ########
    project = dl.projects.get(project_name='RQL')
    dataset = project.datasets.get(dataset_name='Dataset')

    #################
    ###   Items   ###
    #################

    ##################
    # create filters #
    ##################
    filters = dl.Filters(resource=dl.FiltersResource.ITEM)  # set resource - optional - default is item
    # add filter - only files
    filters.add(field='type', values='file')
    # add filter - only annotated items
    filters.add(field='annotated', values=True)
    # add filter - filename includes 'dog'
    filters.add(field='filename', values='/dog.jpg')
    # -- time filters -- must be in ISO format and in UTC (offset from local time). converting using datetime package as follows:
    import datetime, time
    timestamp = datetime.datetime(year=2019, month=10, day=27, hour=15, minute=39, second=6,
                                  tzinfo=datetime.timezone(datetime.timedelta(seconds=-time.timezone))).isoformat()
    filters.add(field='createdAt', values=timestamp, operator=dl.FiltersOperations.GREATER_THAN)

    ######################
    # get filtered items #
    ######################
    # return results sorted by ascending id 
    filters.sort_by(field='filename')
    pages = dataset.items.list(filters=filters)

    #########################
    # update filtered items #
    #########################
    # to add filed annotatedDogs to all filtered items and give value True
    # this field will be added to user metadata
    # create update order
    update_values = {'annotatedDogsSingJune2019': True}

    # update
    pages = dataset.items.update(filters=filters, update_values=update_values)

    # #########################
    # # delete filtered items #
    # #########################
    # dataset.items.delete(filters=filters)

    #####################################
    # filter items by their annotations #
    #####################################
    filters = dl.Filters()
    # set resource
    filters.resource = 'items'
    # add filter - only files
    filters.add(field='type', values='file')

    # add annotation filters - only items with 'box' annotations
    filters.add_join(field='type', values='box')

    # get results
    pages = dataset.items.list(filters=filters)

    #######################
    ###   Annotations   ###
    #######################

    ##################
    # create filters #
    ##################
    filters = dl.Filters()
    # set resource
    filters.resource = dl.FiltersResource.ANNOTATION
    # add filter - only box annotations
    filters.add(field='type', values='box')
    # add filter - only dogs
    filters.add(field='label', values=['Dog', 'cat'], operator=dl.FiltersOperations.IN)
    # add filter - annotated by Joe and David
    filters.add(field='creator',
                values=['Joe@dataloop.ai', 'David@dataloop.ai', 'oa-test-1@dataloop.ai'],
                operator=dl.FiltersOperations.IN
                )

    ############################
    # get filtered annotations #
    ############################
    # return results sorted by descending id 
    filters.sort_by(field='id', value=dl.FiltersOrderByDirection.DESCENDING)
    pages = dataset.items.list(filters=filters)

    ###############################
    # update filtered annotations #
    ###############################
    # to add filed annotation_quality to all filtered annotations and give them value 'high'
    # this field will be added to user metadata
    # create update order
    update_values = {'annotation_quality': 'high'}

    # update
    pages = dataset.items.update(filters=filters, update_values=update_values)

    ###############################
    # delete filtered annotations #
    ###############################
    dataset.items.delete(filters=filters)


if __name__ == '__main__':
    main()


================================================
File: dtlpy/examples/move_item.py
================================================
def main():
    """
    Move items to another folder in platform
    :return:
    """
    import dtlpy as dl

    # Get project and dataset
    project = dl.projects.get(project_name='Ninja Turtles')
    dataset = project.datasets.get(dataset_name='Splinter')

    # Get all items from the source folder
    filters = dl.Filters()
    filters.add(field='filename', values='/fighting/**')  # take files from the directory only (recursive)
    filters.add(field='type', values='file')  # only files
    pages = dataset.items.list(filters=filters)

    dst_folder = '/fighting_shredder'
    # iterate through items
    for page in pages:
        for item in page:
            # move item
            item.move(new_path=dst_folder)


================================================
File: dtlpy/examples/play_video_annotation.py
================================================
def run():
    """
    Play a video from platform with annotations in the Dataloop python player
    :return:
    """
    from dtlpy.utilities.videos.video_player import VideoPlayer
    project_name = 'Dancing'
    dataset_name = 'Flossing'
    item_name = '/first_try.mp4'

    VideoPlayer(project_name=project_name,
                dataset_name=dataset_name,
                item_filepath=item_name)


================================================
File: dtlpy/examples/show_item_and_mask.py
================================================
def main(project_name, dataset_name, item_remote_path):
    """
    Download and show an image with it's annotations
    :return:
    """
    import dtlpy as dl
    import numpy as np
    from PIL import Image

    # Get project and dataset
    project = dl.projects.get(project_name=project_name)
    dataset = project.datasets.get(dataset_name=dataset_name)
    # Get item
    item = dataset.items.get(filepath=item_remote_path)

    # download item as a buffer
    buffer = item.download(save_locally=False)

    # open image
    image = Image.open(buffer)

    # download annotations
    annotations = item.annotations.show(width=image.size[0],
                                        height=image.size[1],
                                        thickness=3)
    annotations = Image.fromarray(annotations.astype(np.uint8))

    # show separate
    annotations.show()
    image.show()

    # plot on top
    image.paste(annotations, (0, 0), annotations)
    image.show()

    #####################################################
    # show annotations from json file (Dataloop format) #
    #####################################################
    # import dtlpy as dl
    # from PIL import Image
    # import json
    #
    # with open('annotations.json', 'r') as f:
    #     data = json.load(f)
    #
    # for annotation in data['annotations']:
    #     annotations = dl.Annotation.from_json(annotation)
    #     mask = annotations.show(width=640,
    #                             height=480,
    #                             thickness=3,
    #                             color=(255, 0, 0))
    #     mask = Image.fromarray(mask.astype(np.uint8))
    #     mask.show()


================================================
File: dtlpy/examples/triggers.py
================================================
def main():
    import dtlpy as dl

    project = dl.projects.get(project_name='project_name')

    ##################
    # create trigger #
    ##################
    # create Item trigger
    # that will trigger service with given id
    # when item is created
    trigger = project.triggers.create(service_ids=['some_service_id'],
                                      resource=dl.FiltersResource.ITEM,
                                      actions=dl.TriggerAction.CREATED,
                                      active=True)

    # create Annotation trigger
    # that will trigger service with given id
    # when Annotation is deleted
    trigger = project.triggers.create(service_ids=['some_service_id'],
                                      resource=dl.FiltersResource.ANNOTATION,
                                      actions=dl.TriggerAction.DELETED,
                                      active=True)

    ##################
    # update trigger #
    ##################
    # if we want trigger to be triggered on Created
    trigger.actions = ['Annotation']
    trigger.update()

    # update trigger filters
    # to work only on specific items
    # trigger will be triggered only on
    # items with string: dog in their name
    filters = dl.Filters()
    filters.add(field='filename', values='*dog*')
    trigger.filters = filters
    trigger.update()

    ##################
    # delete trigger #
    ##################
    trigger.delete()

    ############################
    # list of project triggers #
    ############################
    project.triggers.list()


================================================
File: dtlpy/examples/upload_batch_of_items.py
================================================
def main():
    """

    :return:
    """
    import dtlpy as dl

    # Get project and dataset
    project = dl.projects.get(project_name='Curling')
    dataset = project.datasets.get(dataset_name='Practice')

    # upload specific files
    dataset.items.upload(local_path=['/home/project/images/John Morris.jpg',
                                     '/home/project/images/John Benton.jpg',
                                     '/home/project/images/Liu Jinli.jpg'],
                         remote_path='/first_batch')

    # upload all files in a folder
    dataset.items.upload(local_path='/home/project/images',
                         remote_path='/first_batch')


================================================
File: dtlpy/examples/upload_items_and_custom_format_annotations.py
================================================
def main():
    """
    This is an example how to upload files and annotations to Dataloop platform.
    Image folder contains the images to upload.
    Annotations folder contains json file of the annotations. Same name as the image.
    We read the images one by one and create the Dataloop annotations using the annotation builder.
    Finally, we upload both the image and the matching annotations

    :return:
    """
    import json
    import os
    import dtlpy as dl

    # Get project and dataset
    project = dl.projects.get(project_name='Yachts')
    dataset = project.datasets.get(dataset_name='Open Seas')

    images_folder = '/home/local/images'
    annotations_folder = '/home/local/annotations'

    for img_filename in os.listdir(images_folder):
        # get the matching annotations json
        _, ext = os.path.splitext(img_filename)
        ann_filename = os.path.join(annotations_folder, img_filename.replace(ext, '.json'))
        img_filename = os.path.join(images_folder, img_filename)

        # Upload or get annotations from platform (if already exists)
        item = dataset.items.upload(local_path=img_filename,
                                    remote_path='/in_storm',
                                    overwrite=False)
        assert isinstance(item, dl.Item)

        # read annotations from file
        with open(ann_filename, 'r') as f:
            annotations = json.load(f)

        # create a Builder instance and add all annotations to it
        builder = item.annotations.builder()
        for annotation in annotations:
            # line format if 4 points of bbox
            # this is where you need to update according to your annotation format
            label_id = annotation['label']
            left = annotation['left']
            top = annotation['top']
            right = annotation['right']
            bottom = annotation['bottom']
            builder.add(annotation_definition=dl.Box(top=top,
                                                     left=left,
                                                     bottom=bottom,
                                                     right=right,
                                                     label=str(label_id)))

        # upload annotations
        item.annotations.upload(builder)


================================================
File: dtlpy/examples/upload_items_with_modalities.py
================================================
def main(project_name, dataset_name):
    import traceback
    import random
    import string
    import dtlpy as dl
    from concurrent.futures import ThreadPoolExecutor

    def upload_single(w_url, w_metadata):
        try:
            item = dataset.items.upload(local_path=dl.UrlLink(
                ref=w_url,
                name='.{}.json'.format(
                    ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(10)))),
                remote_path='/cats',
                item_metadata=w_metadata)
            return item.id
        except:
            print(traceback.format_exc())
            return None

    primary_url = 'https://images.unsplash.com/photo-1518020382113-a7e8fc38eac9'
    secondary_urls = [
        'https://images.unsplash.com/photo-1543852786-1cf6624b9987',
        'https://images.unsplash.com/photo-1561948955-570b270e7c36'
    ]
    project = dl.projects.get(project_name=project_name)
    dataset = project.datasets.get(dataset_name=dataset_name)

    pool = ThreadPoolExecutor(max_workers=32)
    jobs = list()
    for i_url, url in enumerate(secondary_urls):
        jobs.append(pool.submit(upload_single, **{'w_url': url,
                                                  'w_metadata': {'user': {'num': i_url}}}))
    pool.shutdown()
    secondary_ids = [j.result() for j in jobs]
    modalities = list()
    for i_secondary_id, secondary_id in enumerate(secondary_ids):
        modalities.append(dl.Modality(modality_type=dl.ModalityTypeEnum.OVERLAY,
                                      ref=secondary_id,
                                      name='cat_num:{}'.format(i_secondary_id)).to_json())

    primary_item = dataset.items.upload(local_path=dl.UrlLink(ref=primary_url),
                                        item_metadata={'system': {'modalities': modalities}})


================================================
File: dtlpy/examples/upload_segmentation_annotations_from_mask_image.py
================================================
"""

"""


def main():
    from PIL import Image
    import numpy as np
    import dtlpy as dl

    # Get project and dataset
    project = dl.projects.get(project_name='Presidents')
    dataset = project.datasets.get(dataset_name='William Henry Harrison')

    # image filepath
    image_filepath = '/home/images/with_family.png'
    # annotations filepath - RGB with color for each label
    annotations_filepath = '/home/masks/with_family.png'

    # upload item to root directory
    item = dataset.items.upload(local_path=image_filepath,
                                remote_path='/')

    # read mask from file
    mask = np.array(Image.open(annotations_filepath))

    # get unique color (labels)
    unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)

    # init dataloop annotations builder
    builder = item.annotations.builder()
    # for each label - create a dataloop mask annotation
    for i, color in enumerate(unique_colors):
        print(color)
        if i == 0:
            # ignore background
            continue
        # get mask of same color
        class_mask = np.all(color == mask, axis=2)
        # add annotation to builder
        builder.add(annotation_definition=dl.Segmentation(geo=class_mask,
                                                          label=str(i)))
    # upload all annotations
    item.annotations.upload(builder)


================================================
File: dtlpy/examples/upload_yolo_format_annotations.py
================================================
def main():
    """
    Convert yolo annotation and images to Dataloop
    Yolo annotations format:
    For each image there is a text file with same name that has a list of box location and label index
    """
    import dtlpy as dl
    from PIL import Image
    import os

    # Get project and dataset
    project = dl.projects.get(project_name='Fruits')
    dataset = project.datasets.get(dataset_name='Rambutan')

    images_and_annotations_path = '/home/fruits/data'

    # read all images from local dataset
    img_filepaths = list()
    for path, subdirs, files in os.walk(images_and_annotations_path):
        for filename in files:
            striped, ext = os.path.splitext(filename)
            if ext in ['.jpeg']:
                img_filepaths.append(os.path.join(path, filename))

    classes_filepath = '/home/fruits/classes.txt'
    # get labels from file
    with open(classes_filepath, 'r') as f:
        labels = {i_line: label.strip() for i_line, label in enumerate(f.readlines())}

    for filepath in img_filepaths:
        # get image height and width
        img = Image.open(filepath)
        width, height = img.size()
        # upload item to platform
        item = dataset.items.update(filepath=filepath)

        # get YOLO annotations
        _, ext = os.path.splitext(filepath)
        annotation_filepath = filepath.replace(ext, '.txt')
        with open(annotation_filepath, 'r') as f:
            annotations = f.read().split('\n')

        builder = item.annotations.builder()
        # convert to Dataloop annotations
        for annotation in annotations:
            if not annotation:
                continue
            # convert annotation format
            elements = annotation.split(" ")
            label_id = elements[0]

            xmin_add_xmax = float(elements[1]) * (2.0 * float(width))
            ymin_add_ymax = float(elements[2]) * (2.0 * float(height))

            w = float(elements[3]) * float(width)
            h = float(elements[4]) * float(height)

            left = (xmin_add_xmax - w) / 2
            top = (ymin_add_ymax - h) / 2
            right = left + w
            bottom = top + h

            # add to annotations
            builder.add(annotation_definition=dl.Box(top=top,
                                                     left=left,
                                                     bottom=bottom,
                                                     right=right,
                                                     label=labels[label_id]))
            # upload all annotations of an item
            builder.upload()


================================================
File: dtlpy/miscellaneous/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .dict_differ import DictDiffer
from .git_utils import GitUtils
from .zipping import Zipping
from .list_print import List
from .json_utils import JsonUtils


================================================
File: dtlpy/miscellaneous/dict_differ.py
================================================
import dictdiffer
import logging
from typing import Union

logger = logging.getLogger(name='dtlpy')
TYPE = 0
FIELD = 1
LIST = 2


class DictDiffer:

    @staticmethod
    def diff(origin, modified):
        diffs = dict()

        dict_diff = list(dictdiffer.diff(origin, modified))
        for i_diff, diff in enumerate(dict_diff):
            modified_field_pointer = modified
            root_change = False
            if len(diff[FIELD]) > 0:
                field_pointer, modified_field_pointer, is_list = DictDiffer.get_field_path(
                    diffs=diffs,
                    path=diff[FIELD],
                    diff_type=diff[TYPE],
                    values=diff[LIST],
                    modified_field_pointer=modified_field_pointer
                )
            else:
                field_pointer, modified_field_pointer, is_list, root_change = origin, modified, False, True

            if is_list:
                for i in modified_field_pointer:
                    if i not in field_pointer:
                        field_pointer.append(i)
                continue
            else:
                if diff[TYPE] == 'add':
                    for addition in diff[LIST]:
                        field_pointer[addition[0]] = addition[1]
                        if root_change is True:
                            diffs[addition[0]] = addition[1]

                elif diff[TYPE] == 'remove':
                    for deletion in diff[LIST]:
                        field_pointer[deletion[0]] = None

                elif diff[TYPE] == 'change':
                    change = diff[LIST]
                    field = diff[FIELD]
                    if not isinstance(field, list):
                        field = field.split('.')
                    field_pointer[field[-1]] = change[1]
        return diffs

    @staticmethod
    def next_is_list(path: list, i_level: Union[str, int], values):
        if len(path) > (i_level + 1):
            if isinstance(path[i_level + 1], int):
                return True
        elif isinstance(values[0][0], int):
            return True
        return False

    @staticmethod
    def get_field_path(diffs, path, diff_type, modified_field_pointer, values):
        field_pointer = diffs
        if not isinstance(path, list):
            path = path.split('.')

        next_is_list = False
        if len(path) > 1 or diff_type != 'change':
            for i_level, level in enumerate(path):
                next_is_list = DictDiffer.next_is_list(i_level=i_level, path=path, values=values)
                if diff_type == 'change' and level == path[-2]:
                    if level not in field_pointer:
                        if next_is_list:
                            field_pointer[level] = list()
                        else:
                            field_pointer[level] = dict()
                    field_pointer = field_pointer[level]
                    modified_field_pointer = modified_field_pointer[level]
                    break
                if level not in field_pointer:
                    if next_is_list:
                        field_pointer[level] = list()
                    else:
                        field_pointer[level] = dict()
                field_pointer = field_pointer[level]
                modified_field_pointer = modified_field_pointer[level]

                if next_is_list:
                    break

        return field_pointer, modified_field_pointer, next_is_list


================================================
File: dtlpy/miscellaneous/git_utils.py
================================================
import subprocess
import logging
import os

logger = logging.getLogger(name='dtlpy')


class GitUtils:
    """
    Performs git related methods
    """

    @staticmethod
    def git_included(path):
        """
        Get only included git repo files based on .gitignore file

        :param path: directory - str
        :return: list()
        """
        included_files = list()

        try:
            p = subprocess.Popen(['git', '--git-dir', os.path.join(path, '.git'), 'ls-files'],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
            output, err = p.communicate()
            string_output = str(output, 'utf-8')
            included_files = string_output.split('\n')
        except Exception:
            logging.warning('Error getting git info for git repository in: {}'.format(path))
            # include all files
            for r, d, f in os.walk(path):
                for folder in d:
                    included_files.append(os.path.join(r, folder))

        return included_files

    @staticmethod
    def is_git_repo(path):
        """
        Check if directory is a git repo

        :param path: directory - str
        :return: True/False
        """
        try:
            p = subprocess.Popen(['git', '--git-dir', os.path.join(path, '.git'), '--work-tree', path, 'status'],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
            output, err = p.communicate()
            if p.returncode != 0 and 'not a git repository' in str(err):
                response = False
            elif p.returncode == 0 and 'On branch' in str(output):
                response = True
            else:
                response = False
        except Exception:
            response = False
            logging.warning('Error getting git info for git repository in: {}'.format(path))
        return response

    @staticmethod
    def git_status(path):
        """
        Get git repository git status

        :param path: directory - str
        :return: String
        """
        status = dict()
        try:
            p = subprocess.Popen(['git', '--git-dir', os.path.join(path, '.git'), '--work-tree', path, 'status'],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
            output, err = p.communicate()
            output_lines = str(output, 'utf-8').splitlines()
            branch = output_lines[0].replace('On branch ', '').strip()

            logs = GitUtils.git_log(path)
            status = {'branch': branch,
                      'commit_id': logs[0]['commit'],
                      'commit_author': logs[0]['Author'],
                      'commit_message': logs[0]['message']}
        except Exception:
            logging.warning('Error getting git info for git repository in: {}'.format(path))

        return status

    @staticmethod
    def git_url(path):
        """
        Get git remote url

        :param path: directory - str
        :return: String
        """
        url = ''
        try:
            p = subprocess.Popen(
                ['git', '--git-dir', os.path.join(path, '.git'), 'config', '--get', 'remote.origin.url'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE)
            output, err = p.communicate()
            url = str(output, 'utf-8').splitlines()[0]

        except Exception:
            logging.warning('Error getting git info for git repository in: {}'.format(path))

        return url

    @staticmethod
    def git_log(path):
        """
        Get git repository git log

        :param path: directory - str
        :return: log as list()
        """
        log = list()
        try:
            log_limit = 100
            p = subprocess.Popen(['git', '--git-dir', os.path.join(path, '.git'), '--work-tree', path, 'log'],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
            output, err = p.communicate()
            string_output = str(output, 'utf-8').split('\ncommit')
            for output in string_output:
                output = output.split('\n')
                if output[0].startswith('commit'):
                    output[0] = output[0].replace('commit', '')
                log_line = {
                    'commit': output[0].strip(),
                    'Author': output[1].replace('Author:', '').strip(),
                    'Date': output[2].replace('Date:', '').strip(),
                    'message': output[4].strip(),
                }
                log.append(log_line)
            log = log[0:log_limit]
        except Exception:
            logging.warning('Error getting git log for git repository in: {}'.format(path))

        return log

    @staticmethod
    def git_clone(path, git_url, **kwargs):
        """
        Clone git repo to local path
        :param path: `str` local path to clone to
        :param git_url: `str` git url to clone from
        :return `bool` for successful clone
        """
        cmd = ''
        if not os.path.isdir(path):
            os.makedirs(path)
        try:
            username = kwargs.get('username')
            password = kwargs.get('password')
            if username is not None and password is not None:
                if git_url.startswith('https://'):
                    git_url = git_url.replace('https://', 'https://{}:{}@'.format(username, password))
                elif git_url.startswith('http://'):
                    git_url = git_url.replace('http://', 'http://{}:{}@'.format(username, password))
                else:
                    git_url = 'https://{}:{}@{}'.format(username, password, git_url)
            tag = kwargs.get('tag')
            branch = kwargs.get('branch')
            if tag is not None:
                branch_cmd = ['--branch', tag]
            elif branch is not None:
                branch_cmd = ['--branch', branch]
            else:
                branch_cmd = []

            cmd = ['git', 'clone'] + branch_cmd + [git_url, path]
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            output, err = [std.decode() for std in p.communicate()]
            exit_code = p.returncode
            response = not exit_code
            if exit_code:
                logging.error('Error executing:  {ps1} $ {cmd}\n{err}'.format(ps1=path, cmd=' '.join(cmd), err=err))
        except Exception:
            response = False
            logging.exception('Error cloning git with cmd: {}'.format(cmd))
        return response

    @staticmethod
    def git_command(path, cmd, show=False):
        """
        Execute command for a local git repo in path
        :param path: `str` local path for the git repo
        :param cmd:  `str` or `list` of `str` specifying the command to run
        :param show: `bool` if True, prints the stdout of the process (default=False)
        :return:  `bool` if command was successful or not
        """
        prev_dir = os.getcwd()
        if isinstance(cmd, str):
            cmd = cmd.split()

        try:
            os.chdir(path)
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            output, err = p.communicate()
            output, err = output.decode(), err.decode()
            exit_code = p.returncode
            response = not exit_code
            # Print the process stdout / stderr
            if show:
                print(output)
            if exit_code:
                logging.error('Error executing:  {ps1} $ {cmd}\n{err}'.format(ps1=path, cmd=' '.join(cmd), err=err))
        except Exception:
            response = False
            logging.critical('Error executing:  {ps1} $ {cmd}'.format(ps1=path, cmd=' '.join(cmd)))
        finally:
            os.chdir(prev_dir)
        return response


================================================
File: dtlpy/miscellaneous/json_utils.py
================================================
class JsonUtils:
    @staticmethod
    def get_if_absent(value, default=None):
        if value is None:
            if default is None:
                value_type = type(value)
                if value_type == dict:
                    default = {}
                elif value_type == list:
                    default = []
                else:
                    default = ''
            value = default
        return value


================================================
File: dtlpy/miscellaneous/list_print.py
================================================
import datetime
import tabulate
import typing
import logging
import pandas

from .. import exceptions

logger = logging.getLogger(name='dtlpy')

T = typing.TypeVar('T')


class List(list, typing.MutableSequence[T]):
    def to_df(self, show_all=False, columns=None):
        try:
            to_print = list()
            keys_list = list()
            for element in self.__iter__():
                if hasattr(element, 'to_json'):
                    item_dict = element.to_json()
                else:
                    item_dict = element
                to_print.append(item_dict)
                [keys_list.append(key) for key in list(item_dict.keys()) if key not in keys_list]
            try:
                # try sorting bt creation date
                to_print = sorted(to_print, key=lambda k: k['createdAt'] if k['createdAt'] is not None else "")
            except KeyError:
                pass
            except Exception:
                logger.exception('Error sorting printing:')

            remove_keys_list = ['contributors', 'url', 'annotations', 'items', 'export', 'directoryTree', 'org',
                                '_contributors', 'role', 'account', 'featureConstraints',
                                'attributes', 'partitions', 'metadata', 'stream', 'updatedAt', 'arch',
                                'input', 'revisions', 'pipeline',  # task fields
                                'feedbackQueue',  # session fields
                                '_ontology_ids', '_labels',  # dataset
                                'esInstance', 'esIndex',  # time series fields
                                'thumbnail',  # item thumnail too long
                                # services fields
                                'driverId', 'useUserJwt', 'versions', 'runtime', 'mq', 'global',
                                # triggers
                                'scope',
                                # Package
                                'modules'
                                ]
            if not show_all:
                if columns is not None:
                    # take columns from inputs
                    if not isinstance(columns, list):
                        if not isinstance(columns, str):
                            raise exceptions.PlatformException(
                                error='3002',
                                message='"columns" input must be str or list. found: {}'.format(type(columns)))
                        columns = [columns]
                    keys_list = columns
                else:
                    # take default columns
                    for key in remove_keys_list:
                        if key in keys_list:
                            keys_list.remove(key)

            for element in to_print:
                # handle printing errors for not ascii string when in cli
                if 'name' in element:
                    try:
                        # check if ascii
                        element['name'].encode('ascii')
                    except UnicodeEncodeError:
                        # if not - print bytes instead
                        element['name'] = str(element['name']).encode('utf-8')
                if 'createdAt' in element:
                    try:
                        str_timestamp = str(element['createdAt'])
                        if len(str_timestamp) > 10:
                            str_timestamp = str_timestamp[:10]
                        element['createdAt'] = datetime.datetime.utcfromtimestamp(int(str_timestamp)).isoformat()
                    except Exception:
                        pass
            df = pandas.DataFrame(to_print, columns=keys_list)
            return df
        except Exception:
            raise exceptions.PlatformException(error='3002',
                                               message='Failed converting to DataFrame')

    def print(self, show_all=False, level='print', to_return=False, columns=None):
        try:
            df = self.to_df(show_all=show_all, columns=columns)
            if 'name' in list(df.columns.values):
                df['name'] = df['name'].astype(str)

            if to_return:
                return tabulate.tabulate(df, headers='keys', tablefmt='psql')
            else:
                if level == 'print':
                    print('\n{}'.format(tabulate.tabulate(df, headers='keys', tablefmt='psql')))
                elif level == 'debug':
                    logger.debug('\n{}'.format(tabulate.tabulate(df, headers='keys', tablefmt='psql')))
                else:
                    raise ValueError('unknown log level in printing: {}'.format(level))

        except Exception:
            raise exceptions.PlatformException(error='3002', message='Failed printing entity')


================================================
File: dtlpy/miscellaneous/zipping.py
================================================
import logging
import os
import zipfile
from typing import List

import numpy as np
import pathspec

logger = logging.getLogger(name='dtlpy')

MAX_ZIP_FILE = 100e6  # 100MB


class Zipping:
    def __init__(self):
        pass

    @staticmethod
    def zip_directory(zip_filename, directory=None, ignore_max_file_size=False, ignore_directories: List[str] = None):
        """
        Zip Directory
        Will ignore .gitignore files

        :param directory: the directory to zip
        :param zip_filename: the name of the zipfile.
        :param ignore_max_file_size: ignore the limitation on the zip file size
        :param list[str] ignore_directories: directories to ignore.
        :return: None
        """
        # default path
        if directory is None:
            directory = os.getcwd()
        # check if directory
        assert os.path.isdir(directory), '[ERROR] Directory does not exists: {}'.format(directory)

        if '.gitignore' in os.listdir(directory):
            with open(os.path.join(directory, '.gitignore')) as f:
                spec_src = f.read()
        else:
            spec_src = ''
        ignore_lines = spec_src.splitlines() + ['.git', '.dataloop']
        if ignore_directories is not None:
            ignore_lines += ignore_directories
        spec = pathspec.PathSpec.from_lines(pathspec.patterns.GitWildMatchPattern, ignore_lines)

        # init zip file
        zip_file = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)
        try:
            for root, dirs, files in os.walk(directory):
                # remove dirs to avoid going file by file
                for d in dirs:
                    if spec.match_file(os.path.relpath(os.path.join(root, d), directory)):
                        dirs.remove(d)
                for file in files:
                    filepath = os.path.join(root, file)
                    if not spec.match_file(os.path.relpath(filepath, directory)):
                        Zipping.__add_to_zip_file(directory, filepath, ignore_max_file_size, zip_file)
        finally:
            zip_file.close()

    @staticmethod
    def zip_directory_inclusive(zip_filename, directory=None, ignore_max_file_size=False,
                                subpaths: List[str] = None):
        """
        Zip Directory
        Will ignore .gitignore files

        :param directory: the directory to zip.
        :param zip_filename: the name of the zipfile
        :param ignore_max_file_size: ignore the limitation on the zip file size
        :param list[str] subpaths: paths to include in the final zip (relative path).
        :return: None
        """
        # default path
        if directory is None:
            directory = os.getcwd()
        # check if directory
        assert os.path.isdir(directory), '[ERROR] Directory does not exists: %s' % directory

        if '.gitignore' in os.listdir(directory):
            with open(os.path.join(directory, '.gitignore')) as f:
                spec_src = f.read()
        else:
            spec_src = ''
        ignore_lines = spec_src.splitlines() + ['.git', '.dataloop']
        spec = pathspec.PathSpec.from_lines(pathspec.patterns.GitWildMatchPattern, ignore_lines)

        # init zip file
        zip_file = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    filepath = os.path.join(root, file)
                    if not spec.match_file(os.path.relpath(filepath, directory)) \
                            and Zipping.__check_filepath(os.path.relpath(filepath, directory), subpaths):
                        Zipping.__add_to_zip_file(directory, filepath, ignore_max_file_size, zip_file)
        finally:
            zip_file.close()

    @staticmethod
    def __check_filepath(filepath: str, paths: List[str]):
        """
        Checks whether a specific file is inside one of the subdirectories
        """
        return any(filepath.startswith(directory) for directory in paths)

    @staticmethod
    def __add_to_zip_file(directory, filepath, ignore_max_file_size, zip_file):
        zip_file.write(filepath, arcname=os.path.relpath(filepath, directory))
        if not ignore_max_file_size:
            if np.sum([f.file_size for f in list(zip_file.NameToInfo.values())]) > MAX_ZIP_FILE:
                logger.error('Failed zipping in file: {}'.format(filepath))
                raise ValueError(
                    'Zip file cant be over 100MB. '
                    'Please verify that only code is being uploaded or '
                    'add files to .gitignore so they wont be zipped and uploaded as code.')

    @staticmethod
    def unzip_directory(zip_filename, to_directory=None):
        zipdata = zipfile.ZipFile(zip_filename)
        zipinfos = zipdata.infolist()

        # iterate through each file
        for zipinfo in zipinfos:
            # encode the file names
            # zip package make decode by cp437 for file that have name that not ascii
            # this happen when the flag_bits be different than 0
            # so we encode the name back
            if not zipinfo.flag_bits:
                zipinfo.filename = zipinfo.filename.encode('cp437').decode('utf-8')
            zipdata.extract(zipinfo, to_directory)


================================================
File: dtlpy/ml/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .base_model_adapter import BaseModelAdapter

from . import metrics
from . import predictions_utils
from . import train_utils


================================================
File: dtlpy/ml/base_feature_extractor_adapter.py
================================================
from abc import ABC

from base_model_adapter import BaseModelAdapter
from .. import entities


class BaseFeatureExtractorAdapter(BaseModelAdapter, ABC):
    def __int__(self, model_entity: entities.Model = None):
        super().__init__(model_entity)

    def extract_features(self, batch: list, **kwargs):
        """ Runs inference with the model, but does not predict. Instead, extracts features for the input batch.

            Virtual method - need to implement

        :param batch: `list` a list containing a batch of items whose features will be extracted
        """
        raise NotImplementedError("Please implement 'extract_features' method in {}".format(self.__class__.__name__))

    def extract_dataset_features(self, dataset: entities.Dataset, **kwargs):
        """ Runs inference to extract features for all items in a dataset.

            Virtual method - need to implement

        :param dataset: `entities.Dataset` dataset entity whose items will have their features extracted
        """
        raise NotImplementedError("Please implement 'extract_dataset_features' method in "
                                  "{}".format(self.__class__.__name__))

================================================
File: dtlpy/ml/base_model_adapter.py
================================================
import dataclasses
import tempfile
import datetime
import logging
import string
import shutil
import random
import base64
import tqdm
import sys
import io
import os
from PIL import Image
from functools import partial
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import attr
from .. import entities, utilities, repositories, exceptions
from ..services import service_defaults
from ..services.api_client import ApiClient

logger = logging.getLogger('ModelAdapter')


@dataclasses.dataclass
class AdapterDefaults(dict):
    # for predict items, dataset, evaluate
    upload_annotations: bool = dataclasses.field(default=True)
    clean_annotations: bool = dataclasses.field(default=True)
    # for embeddings
    upload_features: bool = dataclasses.field(default=True)
    # for training
    root_path: str = dataclasses.field(default=None)
    data_path: str = dataclasses.field(default=None)
    output_path: str = dataclasses.field(default=None)

    def __post_init__(self):
        # Initialize the internal dictionary with the dataclass fields
        self.update(**dataclasses.asdict(self))

    def update(self, **kwargs):
        for f in dataclasses.fields(AdapterDefaults):
            if f.name in kwargs:
                setattr(self, f.name, kwargs[f.name])
        super().update(**kwargs)

    def resolve(self, key, *args):

        for arg in args:
            if arg is not None:
                return arg
        return self.get(key, None)


class BaseModelAdapter(utilities.BaseServiceRunner):
    _client_api = attr.ib(type=ApiClient, repr=False)

    def __init__(self, model_entity: entities.Model = None):
        self.adapter_defaults = AdapterDefaults()
        self.logger = logger
        # entities
        self._model_entity = None
        self._package = None
        self._base_configuration = dict()
        self.package_name = None
        self.model = None
        self.bucket_path = None
        # funcs
        self.item_to_batch_mapping = {'text': self._item_to_text,
                                      'image': self._item_to_image}
        if model_entity is not None:
            self.load_from_model(model_entity=model_entity)
        logger.warning(
            "in case of a mismatch between 'model.name' and 'model_info.name' in the model adapter, model_info.name will be updated to align with 'model.name'.")

    ##################
    # Configurations #
    ##################

    @property
    def configuration(self) -> dict:
        # load from model
        if self._model_entity is not None:
            configuration = self.model_entity.configuration
        # else - load the default from the package
        elif self._package is not None:
            configuration = self.package.metadata.get('system', {}).get('ml', {}).get('defaultConfiguration', {})
        else:
            configuration = self._base_configuration
        return configuration

    @configuration.setter
    def configuration(self, d):
        assert isinstance(d, dict)
        if self._model_entity is not None:
            self._model_entity.configuration = d

    ############
    # Entities #
    ############
    @property
    def model_entity(self):
        if self._model_entity is None:
            raise ValueError(
                "No model entity loaded. Please load a model (adapter.load_from_model(<dl.Model>)) or set: 'adapter.model_entity=<dl.Model>'")
        assert isinstance(self._model_entity, entities.Model)
        return self._model_entity

    @model_entity.setter
    def model_entity(self, model_entity):
        assert isinstance(model_entity, entities.Model)
        if self._model_entity is not None and isinstance(self._model_entity, entities.Model):
            if self._model_entity.id != model_entity.id:
                self.logger.warning(
                    'Replacing Model from {!r} to {!r}'.format(self._model_entity.name, model_entity.name))
        self._model_entity = model_entity
        self.package = model_entity.package

    @property
    def package(self):
        if self._model_entity is not None:
            self.package = self.model_entity.package
        if self._package is None:
            raise ValueError('Missing Package entity on adapter. Please set: "adapter.package=package"')
        assert isinstance(self._package, (entities.Package, entities.Dpk))
        return self._package

    @package.setter
    def package(self, package):
        assert isinstance(package, (entities.Package, entities.Dpk))
        self.package_name = package.name
        self._package = package

    ###################################
    # NEED TO IMPLEMENT THESE METHODS #
    ###################################

    def load(self, local_path, **kwargs):
        """ Loads model and populates self.model with a `runnable` model

            Virtual method - need to implement

            This function is called by load_from_model (download to local and then loads)

        :param local_path: `str` directory path in local FileSystem
        """
        raise NotImplementedError("Please implement `load` method in {}".format(self.__class__.__name__))

    def save(self, local_path, **kwargs):
        """ saves configuration and weights locally

            Virtual method - need to implement

            the function is called in save_to_model which first save locally and then uploads to model entity

        :param local_path: `str` directory path in local FileSystem
        """
        raise NotImplementedError("Please implement `save` method in {}".format(self.__class__.__name__))

    def train(self, data_path, output_path, **kwargs):
        """
        Virtual method - need to implement

        Train the model according to data in data_paths and save the train outputs to output_path,
        this include the weights and any other artifacts created during train

        :param data_path: `str` local File System path to where the data was downloaded and converted at
        :param output_path: `str` local File System path where to dump training mid-results (checkpoints, logs...)
        """
        raise NotImplementedError("Please implement `train` method in {}".format(self.__class__.__name__))

    def predict(self, batch, **kwargs):
        """ Model inference (predictions) on batch of items

            Virtual method - need to implement

        :param batch: output of the `prepare_item_func` func
        :return: `list[dl.AnnotationCollection]` each collection is per each image / item in the batch
        """
        raise NotImplementedError("Please implement `predict` method in {}".format(self.__class__.__name__))

    def embed(self, batch, **kwargs):
        """ Extract model embeddings on batch of items

            Virtual method - need to implement

        :param batch: output of the `prepare_item_func` func
        :return: `list[list]` a feature vector per each item in the batch
        """
        raise NotImplementedError("Please implement `embed` method in {}".format(self.__class__.__name__))

    def evaluate(self, model: entities.Model, dataset: entities.Dataset, filters: entities.Filters) -> entities.Model:
        """
        This function evaluates the model prediction on a dataset (with GT annotations).
        The evaluation process will upload the scores and metrics to the platform.

        :param model: The model to evaluate (annotation.metadata.system.model.name
        :param dataset: Dataset where the model predicted and uploaded its annotations
        :param filters: Filters to query items on the dataset
        :return:
        """
        import dtlpymetrics
        compare_types = model.output_type
        if not filters:
            filters = entities.Filters()
        if filters is not None and isinstance(filters, dict):
            filters = entities.Filters(custom_filter=filters)
        model = dtlpymetrics.scoring.create_model_score(model=model,
                                                        dataset=dataset,
                                                        filters=filters,
                                                        compare_types=compare_types)
        return model

    def convert_from_dtlpy(self, data_path, **kwargs):
        """ Convert Dataloop structure data to model structured

            Virtual method - need to implement

            e.g. take dlp dir structure and construct annotation file

        :param data_path: `str` local File System directory path where we already downloaded the data from dataloop platform
        :return:
        """
        raise NotImplementedError("Please implement `convert_from_dtlpy` method in {}".format(self.__class__.__name__))

    #################
    # DTLPY METHODS #
    ################
    def prepare_item_func(self, item: entities.Item):
        """
        Prepare the Dataloop item before calling the `predict` function with a batch.
        A user can override this function to load item differently
        Default will load the item according the input_type (mapping type to function is in self.item_to_batch_mapping)

        :param item:
        :return: preprocessed: the var with the loaded item information (e.g. ndarray for image, dict for json files etc)
        """
        # Item to batch func
        if isinstance(self.model_entity.input_type, list):
            if 'text' in self.model_entity.input_type and 'text' in item.mimetype:
                processed = self._item_to_text(item)
            elif 'image' in self.model_entity.input_type and 'image' in item.mimetype:
                processed = self._item_to_image(item)
            else:
                processed = self._item_to_item(item)

        elif self.model_entity.input_type in self.item_to_batch_mapping:
            processed = self.item_to_batch_mapping[self.model_entity.input_type](item)

        else:
            processed = self._item_to_item(item)

        return processed

    def prepare_data(self,
                     dataset: entities.Dataset,
                     # paths
                     root_path=None,
                     data_path=None,
                     output_path=None,
                     #
                     overwrite=False,
                     **kwargs):
        """
        Prepares dataset locally before training or evaluation.
        download the specific subset selected to data_path and preforms `self.convert` to the data_path dir

        :param dataset: dl.Dataset
        :param root_path: `str` root directory for training. default is "tmp". Can be set using self.adapter_defaults.root_path
        :param data_path: `str` dataset directory. default <root_path>/"data". Can be set using self.adapter_defaults.data_path
        :param output_path: `str` save everything to this folder. default <root_path>/"output". Can be set using self.adapter_defaults.output_path

        :param bool overwrite: overwrite the data path (download again). default is False
        """
        # define paths
        dataloop_path = service_defaults.DATALOOP_PATH
        root_path = self.adapter_defaults.resolve("root_path", root_path)
        data_path = self.adapter_defaults.resolve("data_path", data_path)
        output_path = self.adapter_defaults.resolve("output_path", output_path)

        if root_path is None:
            now = datetime.datetime.now()
            root_path = os.path.join(dataloop_path,
                                     'model_data',
                                     "{s_id}_{s_n}".format(s_id=self.model_entity.id, s_n=self.model_entity.name),
                                     now.strftime('%Y-%m-%d-%H%M%S'),
                                     )
        if data_path is None:
            data_path = os.path.join(root_path, 'datasets', self.model_entity.dataset.id)
            os.makedirs(data_path, exist_ok=True)
        if output_path is None:
            output_path = os.path.join(root_path, 'output')
            os.makedirs(output_path, exist_ok=True)

        if len(os.listdir(data_path)) > 0:
            self.logger.warning("Data path directory ({}) is not empty..".format(data_path))

        annotation_options = entities.ViewAnnotationOptions.JSON
        if self.model_entity.output_type in [entities.AnnotationType.SEGMENTATION]:
            annotation_options = entities.ViewAnnotationOptions.INSTANCE

        # Download the subset items
        subsets = self.model_entity.metadata.get("system", dict()).get("subsets", None)
        if subsets is None:
            raise ValueError("Model (id: {}) must have subsets in metadata.system.subsets".format(self.model_entity.id))
        for subset, filters_dict in subsets.items():
            filters = entities.Filters(custom_filter=filters_dict)
            data_subset_base_path = os.path.join(data_path, subset)
            if os.path.isdir(data_subset_base_path) and not overwrite:
                # existing and dont overwrite
                self.logger.debug("Subset {!r} already exists (and overwrite=False). Skipping.".format(subset))
            else:
                self.logger.debug("Downloading subset {!r} of {}".format(subset,
                                                                         self.model_entity.dataset.name))

                if self.model_entity.output_type is not None:
                    if self.model_entity.output_type in [entities.AnnotationType.SEGMENTATION,
                                                         entities.AnnotationType.POLYGON]:
                        model_output_types = [entities.AnnotationType.SEGMENTATION, entities.AnnotationType.POLYGON]
                    else:
                        model_output_types = [self.model_entity.output_type]
                    annotation_filters = entities.Filters(
                        field=entities.FiltersKnownFields.TYPE,
                        values=model_output_types,
                        resource=entities.FiltersResource.ANNOTATION,
                        operator=entities.FiltersOperations.IN
                    )
                else:
                    annotation_filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION)

                if not self.configuration.get("include_model_annotations", False):
                    annotation_filters.add(
                        field="metadata.system.model.name",
                        values=False,
                        operator=entities.FiltersOperations.EXISTS
                    )

                ret_list = dataset.items.download(filters=filters,
                                                  local_path=data_subset_base_path,
                                                  annotation_options=annotation_options,
                                                  annotation_filters=annotation_filters
                                                  )

        self.convert_from_dtlpy(data_path=data_path, **kwargs)
        return root_path, data_path, output_path

    def load_from_model(self, model_entity=None, local_path=None, overwrite=True, **kwargs):
        """ Loads a model from given `dl.Model`.
            Reads configurations and instantiate self.model_entity
            Downloads the model_entity bucket (if available)

        :param model_entity:  `str` dl.Model entity
        :param local_path:  `str` directory path in local FileSystem to download the model_entity to
        :param overwrite: `bool` (default False) if False does not download files with same name else (True) download all
        """
        if model_entity is not None:
            self.model_entity = model_entity
        if local_path is None:
            local_path = os.path.join(service_defaults.DATALOOP_PATH, "models", self.model_entity.name)
        # Load configuration
        self.configuration = self.model_entity.configuration
        # Update the adapter config with the model config to run over defaults if needed
        self.adapter_defaults.update(**self.configuration)
        # Download
        self.model_entity.artifacts.download(
            local_path=local_path,
            overwrite=overwrite
        )
        self.load(local_path, **kwargs)

    def save_to_model(self, local_path=None, cleanup=False, replace=True, **kwargs):
        """
        Saves the model state to a new bucket and configuration

        Saves configuration and weights to artifacts
        Mark the model as `trained`
        loads only applies for remote buckets

        :param local_path: `str` directory path in local FileSystem to save the current model bucket (weights) (default will create a temp dir)
        :param replace: `bool` will clean the bucket's content before uploading new files
        :param cleanup: `bool` if True (default) remove the data from local FileSystem after upload
        :return:
        """

        if local_path is None:
            local_path = tempfile.mkdtemp(prefix="model_{}".format(self.model_entity.name))
            self.logger.debug("Using temporary dir at {}".format(local_path))

        self.save(local_path=local_path, **kwargs)

        if self.model_entity is None:
            raise ValueError('Missing model entity on the adapter. '
                             'Please set before saving: "adapter.model_entity=model"')

        self.model_entity.artifacts.upload(filepath=os.path.join(local_path, '*'),
                                           overwrite=True)
        if cleanup:
            shutil.rmtree(path=local_path, ignore_errors=True)
            self.logger.info("Clean-up. deleting {}".format(local_path))

    # ===============
    # SERVICE METHODS
    # ===============

    @entities.Package.decorators.function(display_name='Predict Items',
                                          inputs={'items': 'Item[]'},
                                          outputs={'items': 'Item[]', 'annotations': 'Annotation[]'})
    def predict_items(self, items: list, upload_annotations=None, clean_annotations=None, batch_size=None, **kwargs):
        """
        Run the predict function on the input list of items (or single) and return the items and the predictions.
        Each prediction is by the model output type (package.output_type) and model_info in the metadata

        :param items: `List[dl.Item]` list of items to predict
        :param upload_annotations: `bool` uploads the predictions on the given items
        :param clean_annotations: `bool` deletes previous model annotations (predictions) before uploading new ones
        :param batch_size: `int` size of batch to run a single inference

        :return: `List[dl.Item]`, `List[List[dl.Annotation]]`
        """
        if batch_size is None:
            batch_size = self.configuration.get('batch_size', 4)
        upload_annotations = self.adapter_defaults.resolve("upload_annotations", upload_annotations)
        clean_annotations = self.adapter_defaults.resolve("clean_annotations", clean_annotations)
        input_type = self.model_entity.input_type
        self.logger.debug(
            "Predicting {} items, using batch size {}. input type: {}".format(len(items), batch_size, input_type))
        pool = ThreadPoolExecutor(max_workers=16)

        annotations = list()
        for i_batch in tqdm.tqdm(range(0, len(items), batch_size), desc='predicting', unit='bt', leave=None,
                                 file=sys.stdout):
            batch_items = items[i_batch: i_batch + batch_size]
            batch = list(pool.map(self.prepare_item_func, batch_items))
            batch_collections = self.predict(batch, **kwargs)
            _futures = list(pool.map(partial(self._update_predictions_metadata),
                                     batch_items,
                                     batch_collections))
            # Loop over the futures to make sure they are all done to avoid race conditions
            _ = [_f for _f in _futures]
            if upload_annotations is True:
                self.logger.debug(
                    "Uploading items' annotation for model {!r}.".format(self.model_entity.name))
                try:
                    batch_collections = list(pool.map(partial(self._upload_model_annotations,
                                                              clean_annotations=clean_annotations),
                                                      batch_items,
                                                      batch_collections))
                except Exception as err:
                    self.logger.exception("Failed to upload annotations items.")

            for collection in batch_collections:
                # function needs to return `List[List[dl.Annotation]]`
                # convert annotation collection to a list of dl.Annotation for each batch
                if isinstance(collection, entities.AnnotationCollection):
                    annotations.extend([annotation for annotation in collection.annotations])
                else:
                    logger.warning(f'RETURN TYPE MAY BE INVALID: {type(collection)}')
                    annotations.extend(collection)
            # TODO call the callback

        pool.shutdown()
        return items, annotations

    @entities.Package.decorators.function(display_name='Embed Items',
                                          inputs={'items': 'Item[]'},
                                          outputs={'items': 'Item[]', 'features': 'Json[]'})
    def embed_items(self, items: list, upload_features=None, batch_size=None, **kwargs):
        """
        Extract feature from an input list of items (or single) and return the items and the feature vector.

        :param items: `List[dl.Item]` list of items to embed
        :param upload_features: `bool` uploads the features on the given items
        :param batch_size: `int` size of batch to run a single embed

        :return: `List[dl.Item]`, `List[List[vector]]`
        """
        if batch_size is None:
            batch_size = self.configuration.get('batch_size', 4)
        upload_features = self.adapter_defaults.resolve("upload_features", upload_features)
        input_type = self.model_entity.input_type
        self.logger.debug(
            "Embedding {} items, using batch size {}. input type: {}".format(len(items), batch_size, input_type))

        # Search for existing feature set for this model id
        feature_set = self.model_entity.feature_set
        if feature_set is None:
            logger.info('Feature Set not found. creating... ')
            try:
                self.model_entity.project.feature_sets.get(feature_set_name=self.model_entity.name)
                feature_set_name = f"{self.model_entity.name}-{''.join(random.choices(string.ascii_letters + string.digits, k=5))}"
                logger.warning(
                    f"Feature set with the model name already exists. Creating new feature set with name {feature_set_name}")
            except exceptions.NotFound:
                feature_set_name = self.model_entity.name
            feature_set = self.model_entity.project.feature_sets.create(name=feature_set_name,
                                                                        entity_type=entities.FeatureEntityType.ITEM,
                                                                        model_id=self.model_entity.id,
                                                                        project_id=self.model_entity.project_id,
                                                                        set_type=self.model_entity.name,
                                                                        size=self.configuration.get('embeddings_size',
                                                                                                    256))
            logger.info(f'Feature Set created! name: {feature_set.name}, id: {feature_set.id}')
        else:
            logger.info(f'Feature Set found! name: {feature_set.name}, id: {feature_set.id}')

        # upload the feature vectors
        pool = ThreadPoolExecutor(max_workers=16)
        vectors = list()
        for i_batch in tqdm.tqdm(range(0, len(items), batch_size),
                                 desc='embedding',
                                 unit='bt',
                                 leave=None,
                                 file=sys.stdout):
            batch_items = items[i_batch: i_batch + batch_size]
            batch = list(pool.map(self.prepare_item_func, batch_items))
            batch_vectors = self.embed(batch, **kwargs)
            vectors.extend(batch_vectors)
            if upload_features is True:
                self.logger.debug(
                    "Uploading items' feature vectors for model {!r}.".format(self.model_entity.name))
                try:
                    list(pool.map(partial(self._upload_model_features,
                                              feature_set.id,
                                              self.model_entity.project_id),
                                      batch_items,
                                      batch_vectors))
                except Exception as err:
                    self.logger.exception("Failed to upload feature vectors to items.")

        pool.shutdown()
        return items, vectors

    @entities.Package.decorators.function(display_name='Embed Dataset with DQL',
                                          inputs={'dataset': 'Dataset',
                                                  'filters': 'Json'})
    def embed_dataset(self,
                      dataset: entities.Dataset,
                      filters: entities.Filters = None,
                      upload_features=None,
                      batch_size=None,
                      **kwargs):
        """
        Extract feature from all items given

        :param dataset: Dataset entity to predict
        :param filters: Filters entity for a filtering before embedding
        :param upload_features: `bool` uploads the features back to the given items
        :param batch_size: `int` size of batch to run a single embed

        :return: `bool` indicating if the embedding process completed successfully
        """
        if batch_size is None:
            batch_size = self.configuration.get('batch_size', 4)
        upload_features = self.adapter_defaults.resolve("upload_features", upload_features)

        self.logger.debug("Creating embeddings for dataset (name:{}, id:{}), using batch size {}".format(dataset.name,
                                                                                                         dataset.id,
                                                                                                         batch_size))
        if not filters:
            filters = entities.Filters()
        if filters is not None and isinstance(filters, dict):
            filters = entities.Filters(custom_filter=filters)
        pages = dataset.items.list(filters=filters, page_size=batch_size)
        # Item type is 'file' only, can be deleted if default filters are added to custom filters
        items = [item for page in pages for item in page if item.type == 'file']
        self.embed_items(items=items,
                         upload_features=upload_features,
                         batch_size=batch_size,
                         **kwargs)
        return True

    @entities.Package.decorators.function(display_name='Predict Dataset with DQL',
                                          inputs={'dataset': 'Dataset',
                                                  'filters': 'Json'})
    def predict_dataset(self,
                        dataset: entities.Dataset,
                        filters: entities.Filters = None,
                        upload_annotations=None,
                        clean_annotations=None,
                        batch_size=None,
                        **kwargs):
        """
        Predicts all items given

        :param dataset: Dataset entity to predict
        :param filters: Filters entity for a filtering before predicting
        :param upload_annotations: `bool` uploads the predictions back to the given items
        :param clean_annotations: `bool` if set removes existing predictions with the same package-model name (default: False)
        :param batch_size: `int` size of batch to run a single inference

        :return: `bool` indicating if the prediction process completed successfully
        """

        if batch_size is None:
            batch_size = self.configuration.get('batch_size', 4)

        self.logger.debug("Predicting dataset (name:{}, id:{}, using batch size {}".format(dataset.name,
                                                                                           dataset.id,
                                                                                           batch_size))
        if not filters:
            filters = entities.Filters()
        if filters is not None and isinstance(filters, dict):
            filters = entities.Filters(custom_filter=filters)
        pages = dataset.items.list(filters=filters, page_size=batch_size)
        # Item type is 'file' only, can be deleted if default filters are added to custom filters
        items = [item for page in pages for item in page if item.type == 'file']
        self.predict_items(items=items,
                           upload_annotations=upload_annotations,
                           clean_annotations=clean_annotations,
                           batch_size=batch_size,
                           **kwargs)
        return True

    @entities.Package.decorators.function(display_name='Train a Model',
                                          inputs={'model': entities.Model},
                                          outputs={'model': entities.Model})
    def train_model(self,
                    model: entities.Model,
                    cleanup=False,
                    progress: utilities.Progress = None,
                    context: utilities.Context = None):
        """
        Train on existing model.
        data will be taken from dl.Model.datasetId
        configuration is as defined in dl.Model.configuration
        upload the output the model's bucket (model.bucket)
        """
        if isinstance(model, dict):
            model = repositories.Models(client_api=self._client_api).get(model_id=model['id'])
        output_path = None
        try:
            logger.info("Received {s} for training".format(s=model.id))
            model = model.wait_for_model_ready()
            if model.status == 'failed':
                raise ValueError("Model is in failed state, cannot train.")

            ##############
            # Set status #
            ##############
            model.status = 'training'
            if context is not None:
                if 'system' not in model.metadata:
                    model.metadata['system'] = dict()
            model.update()

            ##########################
            # load model and weights #
            ##########################
            logger.info("Loading Adapter with: {n} ({i!r})".format(n=model.name, i=model.id))
            self.load_from_model(model_entity=model)

            ################
            # prepare data #
            ################
            root_path, data_path, output_path = self.prepare_data(
                dataset=self.model_entity.dataset,
                root_path=os.path.join('tmp', model.id)
            )
            # Start the Train
            logger.info("Training {p_name!r} with model {m_name!r} on data {d_path!r}".
                        format(p_name=self.package_name, m_name=model.id, d_path=data_path))
            if progress is not None:
                progress.update(message='starting training')

            def on_epoch_end_callback(i_epoch, n_epoch):
                if progress is not None:
                    progress.update(progress=int(100 * (i_epoch + 1) / n_epoch),
                                    message='finished epoch: {}/{}'.format(i_epoch, n_epoch))

            self.train(data_path=data_path,
                       output_path=output_path,
                       on_epoch_end_callback=on_epoch_end_callback)
            if progress is not None:
                progress.update(message='saving model',
                                progress=99)

            self.save_to_model(local_path=output_path, replace=True)
            model.status = 'trained'
            model.update()
            ###########
            # cleanup #
            ###########
            if cleanup:
                shutil.rmtree(output_path, ignore_errors=True)
        except Exception:
            # save also on fail
            if output_path is not None:
                self.save_to_model(local_path=output_path, replace=True)
            logger.info('Execution failed. Setting model.status to failed')
            model.status = 'failed'
            model.update()
            raise
        return model

    @entities.Package.decorators.function(display_name='Evaluate a Model',
                                          inputs={'model': entities.Model,
                                                  'dataset': entities.Dataset,
                                                  'filters': 'Json'},
                                          outputs={'model': entities.Model,
                                                   'dataset': entities.Dataset
                                                   })
    def evaluate_model(self,
                       model: entities.Model,
                       dataset: entities.Dataset,
                       filters: entities.Filters = None,
                       #
                       progress: utilities.Progress = None,
                       context: utilities.Context = None):
        """
        Evaluate a model.
        data will be downloaded from the dataset and query
        configuration is as defined in dl.Model.configuration
        upload annotations and calculate metrics vs GT

        :param model: Model entity to run prediction
        :param dataset: Dataset to evaluate
        :param filters: Filter for specific items from dataset
        :param progress: dl.Progress for report FaaS progress
        :param context:
        :return:
        """
        logger.info(
            f"Received model: {model.id} for evaluation on dataset (name: {dataset.name}, id: {dataset.id}")
        ##########################
        # load model and weights #
        ##########################
        logger.info(f"Loading Adapter with: {model.name} ({model.id!r})")
        self.load_from_model(dataset=dataset,
                             model_entity=model)

        ##############
        # Predicting #
        ##############
        logger.info(f"Calling prediction, dataset: {dataset.name!r} ({model.id!r}), filters: {filters}")
        if not filters:
            filters = entities.Filters()
        self.predict_dataset(dataset=dataset,
                             filters=filters,
                             with_upload=True)

        ##############
        # Evaluating #
        ##############
        logger.info(f"Starting adapter.evaluate()")
        if progress is not None:
            progress.update(message='calculating metrics',
                            progress=98)
        model = self.evaluate(model=model,
                              dataset=dataset,
                              filters=filters)
        #########
        # Done! #
        #########
        if progress is not None:
            progress.update(message='finishing evaluation',
                            progress=99)
        return model, dataset

    # =============
    # INNER METHODS
    # =============

    @staticmethod
    def _upload_model_features(feature_set_id, project_id, item: entities.Item, vector):
        try:
            if vector is not None:
                item.features.create(value=vector,
                                     project_id=project_id,
                                     feature_set_id=feature_set_id,
                                     entity=item)
        except Exception as e:
            logger.error(f'Failed to upload feature vector of length {len(vector)} to item {item.id}, Error: {e}')

    def _upload_model_annotations(self, item: entities.Item, predictions, clean_annotations):
        """
        Utility function that upload prediction to dlp platform based on the package.output_type
        :param predictions: `dl.AnnotationCollection`
        :param cleanup: `bool` if set removes existing predictions with the same package-model name
        """
        if not (isinstance(predictions, entities.AnnotationCollection) or isinstance(predictions, list)):
            raise TypeError('predictions was expected to be of type {}, but instead it is {}'.
                            format(entities.AnnotationCollection, type(predictions)))
        if clean_annotations:
            clean_filter = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
            clean_filter.add(field='metadata.user.model.name', values=self.model_entity.name)
            # clean_filter.add(field='type', values=self.model_entity.output_type,)
            item.annotations.delete(filters=clean_filter)
        annotations = item.annotations.upload(annotations=predictions)
        return annotations

    @staticmethod
    def _item_to_image(item):
        """
        Preprocess items before calling the `predict` functions.
        Convert item to numpy array

        :param item:
        :return:
        """
        buffer = item.download(save_locally=False)
        image = np.asarray(Image.open(buffer))
        return image

    @staticmethod
    def _item_to_item(item):
        """
        Default item to batch function.
        This function should prepare a single item for the predict function, e.g. for images, it loads the image as numpy array
        :param item:
        :return:
        """
        return item

    @staticmethod
    def _item_to_text(item):
        filename = item.download(overwrite=True)
        text = None
        if item.mimetype == 'text/plain' or item.mimetype == 'text/markdown':
            with open(filename, 'r') as f:
                text = f.read()
                text = text.replace('\n', ' ')
        else:
            logger.warning('Item is not text file. mimetype: {}'.format(item.mimetype))
            text = item
        if os.path.exists(filename):
            os.remove(filename)
        return text

    @staticmethod
    def _uri_to_image(data_uri):
        # data_uri = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAS4AAAEuCAYAAAAwQP9DAAAU80lEQVR4Xu2da+hnRRnHv0qZKV42LDOt1eyGULoSJBGpRBFprBJBQrBJBBWGSm8jld5WroHUCyEXKutNu2IJ1QtXetULL0uQFCu24WoRsV5KpYvGYzM4nv6X8zu/mTnznPkcWP6XPTPzzOf7/L7/OXPmzDlOHBCAAAScETjOWbyECwEIQEAYF0kAAQi4I4BxuZOMgCEAAYyLHIAABNwRwLjcSUbAEIAAxkUOQAAC7ghgXO4kI2AIQADjIgcgAAF3BDAud5IRMAQggHGRAxCAgDsCGJc7yQgYAhDAuMgBCEDAHQGMy51kBAwBCGBc5AAEIOCOAMblTjIChgAEMC5yAAIQcEcA43InGQFDAAIYFzkAAQi4I4BxuZOMgCEAAYyLHIAABNwRwLjcSUbAEIAAxkUOQAAC7ghgXO4kI2AIQADjIgcgAAF3BDAud5IRMAQggHGRAxCAgDsCGJc7yQgYAhDAuMgBCEDAHQGMy51kBAwBCGBc5AAEIOCOAMblTjIChgAEMC5yAAIQcEcA43InGQFDAAIYFzkAAQi4I4BxuZOMgCEAAYyLHIAABNwRwLjcSUbAEIAAxkUOQAAC7ghgXO4kI2AIQADjIgcgAAF3BDAud5IRMAQggHGRAxDwTeDTkr4s6UxJ/5F0QNK3JD3lu1tbR49xLVld+jYXgcskvSTpIkmnS/qgpJMk/Tv8bHHZ7+PXPw6M5kRJx0t6Ijkv9uUsSW+U9Iykczfp4K8lfXiuztdoF+OqQZk2vBEwUzFTsK9mQNFkotGkhvFeSc+G86NRtdDfd0h6tIVASsSAcZWgSp0eCJjJ7JR0SRgZ2SjHDMp+38Jho7PXTAzkBUmvn1jWRTGMy4VMBJmBgBnSpZLsMs7+paOodao3k/hLqCBe8j0cfj4Yvtp8k/1fPLaaf4pxxXPSS8r4/Vsl3SXp5EHgNjo8JukDkg6v06nWy2JcrSvUX3xmKjYSipdqF0h6V/jgp6Mh+2DHf0YpnSd6p6TTkjml7UZRL4bLPasnmo7VHb+PKsQ20rZTQ6ql1lclfXODxr4u6Ru1gpizHYxrTvq0beZkE9cfkXRxxcu0pyXZaMiMKX71dBfua5sY1Psk/baHtMK4elC5rT5eFS7Z7Otmd8VyRDwcRZkxmUlFo8rRxlx13Clpz6Dxn0r61FwB1W4X46pNvM/27PLPPmhmVhvNLUWTiaZil1/xEswMx/7fbv9bWfs5nfcxommdceQU55eWSNxGihcmHbMRZK45Oxe8MK75ZYofaku8MyQ9J+mQpKNJMqbzLfeHkIeTuPP35JUIbCSVToRvNrKyftqCSfs3nE9qqT+txWKT8OmxT9LnWguyZDwYV0m6m9dtH+SbJNlamw+tGIIl7Va6/VPS8xusP4rN2JojG8E8NrhUS+d4ht/bbfkTJP0umGk6ER7PtfkVmwR/wzaXgEck7Q1mNcfE9oq4mzx9aFxXB55NBlsiKIyrBNXt67xB0q3bn7aYM+xSxkZVNjez5Eu4GoLZ5fb+pCFb/mB/LLo6MK555LaRyUPzND251VUWRJpRxTt2cUJ8csMUfBUBG61en/ymu8tE6zvGNd+nwuao7N8PJO0Kz7JZNDbH9aSkv4fQ0su2RyS9VtKD4dJtOClt5+4Il4Fpz+KkdqzLnpuzdrY74vnppWG6ujx9xMXOsUWPjw8WW27XBv+/GgH7Q2Dzh/G4NoxkV6vF+dkYV1sCRoNpKyqiaYmA/TGxxbXxsD963d3YwLhaSkligcDWBIZTDHajo+RauGb1wLialYbAIPB/BO6Q9Pnkt7dJshs93R0YV3eS02HHBGz+8Owk/vN6nU/EuBxnMaF3RWC4DOJ7kr7UFYGksxhXr8rTb28Eho/5dDvaMuEwLm/pS7w9EhiOtu4Oz332yOLlPmNc3UpPx50QsCUytlg5vXvY5RKIVC+My0n2Ema3BG4Oz7VGAN2PthhxdftZoOOOCKQLTu1RKlvL1f3D6Yy4HGUwoXZHwLaq+X7S6xvDzhrdgRh2GOPqPgUA0DCB9LlE27tsu73zG+5K3tAwrrw8qQ0CuQjYZLztmRaP7vbc2gokxpUrzagHAnkJpNvXMNoasMW48iYbtUEgF4F0Up7RFsaVK6+oBwLFCKST8t3uAMGlYrH8omIIFCFg21zvDjV3uwMExlUkt6gUAkUIDCflu34mcTPCzHEVyT0qhcBkAumLVJiU3wQjxjU5vygIgSIE0l0gutxPfgxVjGsMJc6BQB0C9kC1vW4sHvbik/RlKXWicNAKxuVAJELshkC6fY29sdzecs6xAQGMi7SAQDsE7IW5e0I4PJe4hS4YVztJSyQQsF0fdgYM3E3EuPhEQKB5Aumrx7ibuI1cjLiaz2cC7IRAugyCy0SMq5O0p5veCaSr5blMxLi85zPxd0LgGUmnSOIycYTgXCqOgMQpEChMwJY93MfdxPGUMa7xrDgTAqUIxGUQ7Ck/kjDGNRIUp0GgIIG49xaXiSMhY1wjQXEaBAoRSFfLczdxJGSMayQoToNAIQLpannuJo6EjHGNBMVpEChEgMvECWAxrgnQKAKBTAS4TJwIEuOaCI5iEMhAgMvEiRAxrongKAaBDAS4TJwIEeOaCI5iEFiTQPpQNXcTV4SJca0IjNMhkIlA+sJX7iauCBXjWhEYp0MgE4G49xaLTicAxbgmQKMIBNYkkL6CjPcmToCJcU2ARhEIrEkgfVP1Lkn2Zh+OFQhgXCvA4lQIZCIQl0EckWSjL44VCWBcKwLjdAhkIHBY0vmS9kmy0RfHigQwrhWBcToE1iSQLoO4QtK9a9bXZXGMq0vZ6fSMBOLe8rb3ll0m8sLXCWJgXBOgUQQCaxA4KOlStmheg6AkjGs9fpSGwKoEXgoFbpF086qFOf9/BDAuMgEC9Qike8tfLslGXxwTCGBcE6BRBAITCdgI66ZQls/eRIiMuNYAR1EITCAQ57ful2SjL46JBHD9ieAoBoEJBJjfmgBtoyIYVyaQVAOBbQik67eulmRvruaYSADjmgiOYhBYkUBcv2XFdrB+a0V6g9MxrvX4URoCYwnwfOJYUiPOw7hGQOIUCGQgEPff4vnEDDAxrgwQqQIC2xBI99+6VpKNvjjWIIBxrQGPohAYSSDdf4ttmkdC2+o0jCsDRKqAwDYEmN/KnCIYV2agVAeBDQgclfQW9t/KlxsYVz6W1ASBjQiw/1aBvMC4CkClSggkBOLziey/lTEtMK6MMKkKAhsQsBdhXMj+W3lzA+PKy5PaIJASOF3SsfAL3ladMTcwrowwqQoCAwK8hqxQSmBchcBSLQTCg9S7Jdn8lo2+ODIRwLgygaQaCGxAwF6EcRrLIPLnBsaVnyk1QsAIXCVpf0DBNjaZcwLjygyU6iAQCOyVdH34nm1sMqcFxpUZKNVBIBCIu0HcHUZfgMlIAOPKCJOqIBAIpKvl2Q2iQFpgXAWgUmX3BLhMLJwCGFdhwFTfJQEuEwvLjnEVBkz13RHgpRgVJMe4KkCmia4IpA9Vs+i0kPQYVyGwVNstgQcl7WLRaVn9Ma6yfKm9LwLsvVVJb4yrEmia6YJAvJvIs4mF5ca4CgOm+q4I8GxiJbkxrkqgaWbxBNJnE22OyzYQ5ChEAOMqBJZquyMQ124dkWTvUeQoSADjKgiXqrshcJmk+0Jv2em0guwYVwXINLF4Agck2YaBdvDC1wpyY1wVINPEognYZeHvJZ0g6RFJFyy6t410DuNqRAjCcEvgBkm3huhvl3Sd2544ChzjciQWoTZJIL5+zILjbmIliTCuSqBpZpEE0tePsei0osQYV0XYNLU4Aunrx/ZJsp85KhDAuCpAponFErhT0p7QO5ZBVJQZ46oIm6YWR4D5rZkkxbhmAk+z7gkwvzWjhBjXjPBp2jWBz0i6K/TgN5Iucd0bZ8FjXM4EI9xmCMSdTi2gn0gyI+OoRADjqgSaZhZHIH3Mh1eQVZYX46oMnOYWQyDuBmEdulzSwcX0zEFHMC4HIhFikwReSqLiwerKEmFclYHT3CIIpNvYWIf4HFWWFeCVgdPcIgh8R9JXQk/+KulNi+iVo05gXI7EItRmCPxS0kdDNLalzXuaiayTQDCuToSmm9kI2MJT25751FDjLZJsaQRHRQIYV0XYNLUIAvdIujLpCXcUZ5AV45oBOk26JvCMpFNCD+zO4vGue+M0eIzLqXCEPQuBdBsbC+BeSVfMEknnjWJcnScA3V+JwJOS3pyUuFqSraDnqEwA46oMnOZcE0gXnVpH+PzMJCfgZwJPsy4JYFyNyIZxNSIEYbggMDSuHZKechH5woLEuBYmKN0pSoARV1G84yvHuMaz4sy+CQzvKB6VdE7fSObrPcY1H3ta9kVgeEeRt/rMqB/GNSN8mnZFYHiZyIr5GeXDuGaET9NuCFwlaX8SLTtCzCwdxjWzADTvgkC6v7wFfJukG1xEvtAgMa6FCku3shL4s6QzkxpZMZ8V7+qVYVyrM6NEfwSel3Ri0m3Wb82cAxjXzALQfPMEhvNbf5D07uajXniAGNfCBaZ7axN4VNLbk1pulLR37VqpYC0CGNda+Ci8cAK22+mxQR95o08DomNcDYhACM0SGK6Wt3cpmnFxzEwA45pZAJpvmsBwtTyXiY3IhXE1IgRhNElguFqey8RGZMK4GhGCMJojMLybyGViQxJhXA2JQShNEbhT0p4kIlbLNyQPxtWQGITSFAH2l29KjlcHg3E1LA6hzUrgxcGe8nxWZpUD42oIP6E0SuAiSQ8NYtsl6eFG4+0uLP6KdCc5HR5BYKOFp+y/NQJcrVMwrlqkaccTgQckXTwI+DJJ93vqxJJjxbiWrC59m0LgfEmHBwX/JemEKZVRpgwBjKsMV2r1S8BGVvcNwv+spB/67dLyIse4lqcpPVqPwEbGxcaB6zHNXhrjyo6UCp0TuFLSPYM+XCPpx877tajwMa5FyUlnMhCwveRvHdTDjqcZwOasAuPKSZO6lkDggKTdSUeOSDp3CR1bUh8wriWpSV9yEPiHpJOSinhGMQfVzHVgXJmBUp17AsOtbFgx36CkGFeDohDSbASGj/r8TdIZs0VDw5sSwLhIDgi8QmC4VfPdkmxfLo7GCGBcjQlCOLMSGO7BxVbNs8qxeeMYV6PCENYsBGyX051JyzxYPYsM2zeKcW3PiDP6ITCcmGf9VqPaY1yNCkNY1QkMJ+YPSbLfcTRIAONqUBRCmoXA8BlF1m/NIsO4RjGucZw4a/kEhncUebC6Yc0xrobFIbSqBIbPKDK/VRX/ao1hXKvx4uzlEtgr6frQvUckXbDcrvrvGcblX0N6kIdAaly/kPTxPNVSSwkCGFcJqtTpkUC6+JSFp40riHE1LhDhVSNwUNKloTUm5qthn9YQxjWNG6WWRyA1LlbMN64vxtW4QIRXjcBTkk4LrWFc1bBPawjjmsaNUssjkD7ug3E1ri/G1bhAhFeNQGpcbB5YDfu0hjCuadwotTwCqXGdJ8l2iuBolADG1agwhFWdQGpcfC6q41+tQQRajRdnL5dANK6nJZ2+3G4uo2cY1zJ0pBfrEbDXjz0WquB1ZOuxrFIa46qCmUYaJ/AJST8PMf5K0scaj7f78DCu7lMAAJLSnSFul3QdVNomgHG1rQ/R1SGQPmDNGq46zNdqBeNaCx+FF0LgYUkXhr6wFMKBqBiXA5EIsTgB7igWR5y3AYwrL09q80cg3WueF8A60Q/jciIUYRYjcLOkm0Lt7MNVDHPeijGuvDypzR+BdH6LZxSd6IdxORGKMIsQsBXyx0LNLDwtgrhMpRhXGa7U6oNA+kqyfZLsZw4HBDAuByIRYjEC6T7zbNdcDHP+ijGu/Eyp0Q+BuOspD1b70ezlSDEuZ4IRbjYCF0l6KNTGZWI2rHUqwrjqcKaV9gikj/lwmdiePltGhHE5E4xwsxGIyyC4TMyGtF5FGFc91rTUFoEXJL1OEqvl29JlVDQY1yhMnLQwAuljPl+QdMfC+rf47mBci5eYDm5AIJ3fYjcIhymCcTkUjZDXJhDnt1gtvzbKeSrAuObhTqvzEUj3l78t7H46XzS0PIkAxjUJG4UcE0i3aWYZhFMhMS6nwhH2ZAIHJO0Opcn/yRjnLYhw8/Kn9foE4m6nhyTZ6nkOhwQwLoeiEfJkAryGbDK6tgpiXG3pQTRlCaS7nfJ8YlnWRWvHuIripfLGCLCNTWOCTA0H45pKjnIeCaTbNPP+RI8KclfFsWqEPpVAnJi38jsk2X5cHA4JMOJyKBohTyaQGhe5Pxnj/AURb34NiKAOgXTjQLayqcO8WCsYVzG0VNwYgXRHCNZwNSbOquFgXKsS43yvBOxlr98OwT8g6f1eO0Lc7DlPDvRD4LuSvhi6+zNJn+yn68vrKSOu5WlKjzYmkD6jaKMv25OLwykBjMupcIS9MoH4KjIryK4QK+NrqwDG1ZYeRFOGQDoxby2whqsM52q1YlzVUNPQjAR+JOma0P5zkk6eMRaazkAA48oAkSqaJ/CEpLNClM9KOrX5iAlwSwIYFwmydAJnS3p80MlzJB1deseX3D+Ma8nq0rdIwF6K8bbww58k7QSNbwIYl2/9iH4cAdtA0O4k2rFf0r3jinFWqwQwrlaVIS4IQGBTAhgXyQEBCLgjgHG5k4yAIQABjIscgAAE3BHAuNxJRsAQgADGRQ5AAALuCGBc7iQjYAhAAOMiByAAAXcEMC53khEwBCCAcZEDEICAOwIYlzvJCBgCEMC4yAEIQMAdAYzLnWQEDAEIYFzkAAQg4I4AxuVOMgKGAAQwLnIAAhBwRwDjcicZAUMAAhgXOQABCLgjgHG5k4yAIQABjIscgAAE3BHAuNxJRsAQgADGRQ5AAALuCGBc7iQjYAhAAOMiByAAAXcEMC53khEwBCCAcZEDEICAOwIYlzvJCBgCEMC4yAEIQMAdAYzLnWQEDAEIYFzkAAQg4I4AxuVOMgKGAAQwLnIAAhBwRwDjcicZAUMAAhgXOQABCLgjgHG5k4yAIQABjIscgAAE3BHAuNxJRsAQgADGRQ5AAALuCGBc7iQjYAhAAOMiByAAAXcEMC53khEwBCCAcZEDEICAOwIYlzvJCBgCEMC4yAEIQMAdAYzLnWQEDAEIYFzkAAQg4I4AxuVOMgKGAAQwLnIAAhBwR+C/doIhTZIi/uMAAAAASUVORK5CYII="
        image_b64 = data_uri.split(",")[1]
        binary = base64.b64decode(image_b64)
        image = np.asarray(Image.open(io.BytesIO(binary)))
        return image

    def _update_predictions_metadata(self, item: entities.Item, predictions: entities.AnnotationCollection):
        """
        add model_name and model_id to the metadata of the annotations.
        add model_info to the metadata of the system metadata of the annotation.
        Add item id to all the annotations in the AnnotationCollection

        :param item: Entity.Item
        :param predictions: item's AnnotationCollection
        :return:
        """
        for prediction in predictions:
            if prediction.type == entities.AnnotationType.SEGMENTATION:
                color = None
                try:
                    color = item.dataset._get_ontology().color_map.get(prediction.label, None)
                except (exceptions.BadRequest, exceptions.NotFound):
                    ...
                if color is None:
                    if self.model_entity._dataset is not None:
                        try:
                            color = self.model_entity.dataset._get_ontology().color_map.get(prediction.label,
                                                                                            (255, 255, 255))
                        except (exceptions.BadRequest, exceptions.NotFound):
                            ...
                if color is None:
                    logger.warning("Can't get annotation color from model's dataset, using default.")
                    color = prediction.color
                prediction.color = color

            prediction.item_id = item.id
            if 'user' in prediction.metadata and 'model' in prediction.metadata['user']:
                prediction.metadata['user']['model']['model_id'] = self.model_entity.id
                prediction.metadata['user']['model']['name'] = self.model_entity.name
            if 'system' not in prediction.metadata:
                prediction.metadata['system'] = dict()
            if 'model' not in prediction.metadata['system']:
                prediction.metadata['system']['model'] = dict()
            confidence = prediction.metadata.get('user', dict()).get('model', dict()).get('confidence', None)
            prediction.metadata['system']['model'] = {
                'model_id': self.model_entity.id,
                'name': self.model_entity.name,
                'confidence': confidence
            }

    ##############################
    # Callback Factory functions #
    ##############################
    @property
    def dataloop_keras_callback(self):
        """
        Returns the constructor for a keras api dump callback
        The callback is used for dlp platform to show train losses

        :return: DumpHistoryCallback constructor
        """
        try:
            import keras
        except (ImportError, ModuleNotFoundError) as err:
            raise RuntimeError(
                '{} depends on extenral package. Please install '.format(self.__class__.__name__)) from err

        import os
        import time
        import json

        class DumpHistoryCallback(keras.callbacks.Callback):
            def __init__(self, dump_path):
                super().__init__()
                if os.path.isdir(dump_path):
                    dump_path = os.path.join(dump_path,
                                             '__view__training-history__{}.json'.format(time.strftime("%F-%X")))
                self.dump_file = dump_path
                self.data = dict()

            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                for name, val in logs.items():
                    if name not in self.data:
                        self.data[name] = {'x': list(), 'y': list()}
                    self.data[name]['x'].append(float(epoch))
                    self.data[name]['y'].append(float(val))
                self.dump_history()

            def dump_history(self):
                _json = {
                    "query": {},
                    "datasetId": "",
                    "xlabel": "epoch",
                    "title": "training loss",
                    "ylabel": "val",
                    "type": "metric",
                    "data": [{"name": name,
                              "x": values['x'],
                              "y": values['y']} for name, values in self.data.items()]
                }

                with open(self.dump_file, 'w') as f:
                    json.dump(_json, f, indent=2)

        return DumpHistoryCallback


================================================
File: dtlpy/ml/metrics.py
================================================
import numpy as np
import pandas as pd
import logging
import datetime

from .. import entities

logger = logging.getLogger(name='dtlpy')


class Results:
    def __init__(self, matches, annotation_type):
        self.matches = matches
        self.annotation_type = annotation_type

    def to_df(self):
        return self.matches.to_df()

    def summary(self):
        df = self.matches.to_df()
        total_set_one = len(df['first_id'].dropna())
        total_set_two = len(df['second_id'].dropna())
        # each set unmatched is the number of Nones from the other set
        unmatched_set_one = df.shape[0] - total_set_two
        unmatched_set_two = df.shape[0] - total_set_one
        matched_set_one = total_set_one - unmatched_set_one
        matched_set_two = total_set_two - unmatched_set_two
        # sanity
        assert matched_set_one == matched_set_two, 'matched numbers are not the same'
        assert df['annotation_score'].shape[0] == (unmatched_set_one + unmatched_set_two + matched_set_one), \
            'mis-match number if scores and annotations'
        return {
            'annotation_type': self.annotation_type,
            'mean_annotations_scores': df['annotation_score'].mean(),
            'mean_attributes_scores': df['attribute_score'].mean(),
            'mean_labels_scores': df['label_score'].mean(),
            'n_annotations_set_one': total_set_one,
            'n_annotations_set_two': total_set_two,
            'n_annotations_total': total_set_one + total_set_two,
            'n_annotations_unmatched_set_one': unmatched_set_one,
            'n_annotations_unmatched_set_two': unmatched_set_two,
            'n_annotations_unmatched_total': unmatched_set_one + unmatched_set_two,
            'n_annotations_matched_total': matched_set_one,
            'precision': matched_set_one / (matched_set_one + unmatched_set_two),
            'recall': matched_set_one / (matched_set_one + unmatched_set_one)
        }


class Match:
    def __init__(self,
                 first_annotation_id, first_annotation_label, first_annotation_confidence,
                 second_annotation_id, second_annotation_label, second_annotation_confidence,
                 # defaults
                 annotation_score=0, attributes_score=0, geometry_score=0, label_score=0):
        """
        Save a match between two annotations with all relevant scores

        :param first_annotation_id:
        :param second_annotation_id:
        :param annotation_score:
        :param attributes_score:
        :param geometry_score:
        :param label_score:
        """
        self.first_annotation_id = first_annotation_id
        self.first_annotation_label = first_annotation_label
        self.first_annotation_confidence = first_annotation_confidence
        self.second_annotation_id = second_annotation_id
        self.second_annotation_label = second_annotation_label
        self.second_annotation_confidence = second_annotation_confidence
        self.annotation_score = annotation_score
        self.attributes_score = attributes_score
        # Replace the old annotation score
        self.geometry_score = geometry_score
        self.label_score = label_score

    def __repr__(self):
        return 'annotation: {:.2f}, attributes: {:.2f}, geometry: {:.2f}, label: {:.2f}'.format(
            self.annotation_score, self.attributes_score, self.geometry_score, self.label_score)


class Matches:
    def __init__(self):
        self.matches = list()
        self._annotations_raw_df = list()

    def __len__(self):
        return len(self.matches)

    def __repr__(self):
        return self.to_df().to_string()

    def to_df(self):
        results = list()
        for match in self.matches:
            results.append({
                'first_id': match.first_annotation_id,
                'first_label': match.first_annotation_label,
                'first_confidence': match.first_annotation_confidence,
                'second_id': match.second_annotation_id,
                'second_label': match.second_annotation_label,
                'second_confidence': match.second_annotation_confidence,
                'annotation_score': match.annotation_score,
                'attribute_score': match.attributes_score,
                'geometry_score': match.geometry_score,
                'label_score': match.label_score,
            })
        df = pd.DataFrame(results)
        return df

    def add(self, match: Match):
        self.matches.append(match)

    def validate(self):
        first = list()
        second = list()
        for match in self.matches:
            if match.first_annotation_id in first:
                raise ValueError('duplication for annotation id {!r} in FIRST set'.format(match.first_annotation_id))
            if match.first_annotation_id is not None:
                first.append(match.first_annotation_id)
            if match.second_annotation_id in second:
                raise ValueError('duplication for annotation id {!r} in SECOND set'.format(match.second_annotation_id))
            if match.second_annotation_id is not None:
                second.append(match.second_annotation_id)
        return True

    def find(self, annotation_id, loc='first'):
        for match in self.matches:
            if loc == 'first':
                if match.first_annotation_id == annotation_id:
                    return match
            elif loc == 'second':
                if match.second_annotation_id == annotation_id:
                    return match
        raise ValueError('could not find annotation id {!r} in {}'.format(annotation_id, loc))


######################
# Matching functions #
######################
class Matchers:

    @staticmethod
    def calculate_iou_box(pts1, pts2, config):
        """
        Measure the two list of points IoU
        :param pts1: ann.geo coordinates
        :param pts2: ann.geo coordinates
        :return: `float` how Intersection over Union of tho shapes
        """
        try:
            from shapely.geometry import Polygon
        except (ImportError, ModuleNotFoundError) as err:
            raise RuntimeError('dtlpy depends on external package. Please install ') from err
        if len(pts1) == 2:
            # regular box annotation (2 pts)
            pt1_left_top = [pts1[0][0], pts1[0][1]]
            pt1_right_top = [pts1[0][0], pts1[1][1]]
            pt1_right_bottom = [pts1[1][0], pts1[1][1]]
            pt1_left_bottom = [pts1[1][0], pts1[0][1]]
        else:
            # rotated box annotation (4 pts)
            pt1_left_top = pts1[0]
            pt1_right_top = pts1[3]
            pt1_left_bottom = pts1[1]
            pt1_right_bottom = pts1[2]

        poly_1 = Polygon([pt1_left_top,
                          pt1_right_top,
                          pt1_right_bottom,
                          pt1_left_bottom])

        if len(pts2) == 2:
            # regular box annotation (2 pts)
            pt2_left_top = [pts2[0][0], pts2[0][1]]
            pt2_right_top = [pts2[0][0], pts2[1][1]]
            pt2_right_bottom = [pts2[1][0], pts2[1][1]]
            pt2_left_bottom = [pts2[1][0], pts2[0][1]]
        else:
            # rotated box annotation (4 pts)
            pt2_left_top = pts2[0]
            pt2_right_top = pts2[3]
            pt2_left_bottom = pts2[1]
            pt2_right_bottom = pts2[2]

        poly_2 = Polygon([pt2_left_top,
                          pt2_right_top,
                          pt2_right_bottom,
                          pt2_left_bottom])
        iou = poly_1.intersection(poly_2).area / poly_1.union(poly_2).area
        return iou

    @staticmethod
    def calculate_iou_classification(pts1, pts2, config):
        """
        Measure the two list of points IoU
        :param pts1: ann.geo coordinates
        :param pts2: ann.geo coordinates
        :return: `float` how Intersection over Union of tho shapes
        """
        return 1

    @staticmethod
    def calculate_iou_polygon(pts1, pts2, config):
        try:
            # from shapely.geometry import Polygon
            import cv2
        except (ImportError, ModuleNotFoundError) as err:
            raise RuntimeError('dtlpy depends on external package. Please install ') from err
        # # using shapley
        # poly_1 = Polygon(pts1)
        # poly_2 = Polygon(pts2)
        # iou = poly_1.intersection(poly_2).area / poly_1.union(poly_2).area

        # # using opencv
        width = int(np.ceil(np.max(np.concatenate((pts1[:, 0], pts2[:, 0]))))) + 10
        height = int(np.ceil(np.max(np.concatenate((pts1[:, 1], pts2[:, 1]))))) + 10
        mask1 = np.zeros((height, width))
        mask2 = np.zeros((height, width))
        mask1 = cv2.drawContours(
            image=mask1,
            contours=[pts1.round().astype(int)],
            contourIdx=-1,
            color=1,
            thickness=-1,
        )
        mask2 = cv2.drawContours(
            image=mask2,
            contours=[pts2.round().astype(int)],
            contourIdx=-1,
            color=1,
            thickness=-1,
        )
        iou = np.sum((mask1 + mask2) == 2) / np.sum((mask1 + mask2) > 0)
        if np.sum((mask1 + mask2) > 2):
            assert False
        return iou

    @staticmethod
    def calculate_iou_semantic(mask1, mask2, config):
        joint_mask = mask1 + mask2
        return np.sum(np.sum(joint_mask == 2) / np.sum(joint_mask > 0))

    @staticmethod
    def calculate_iou_point(pt1, pt2, config):
        """
        pt is [x,y]
        normalizing  to score  between [0, 1] -> 1 is the exact match
        if same point score is 1
        at about 20 pix distance score is about 0.5, 100 goes to 0
        :param pt1:
        :param pt2:
        :return:
        """
        """
        x = np.arange(int(diag))
        y = np.exp(-1 / diag * 20 * x)
        plt.figure()
        plt.plot(x, y)
        """
        height = config.get('height', 500)
        width = config.get('width', 500)
        diag = np.sqrt(height ** 2 + width ** 2)
        # 20% of the image diagonal tolerance (empirically). need to
        return np.exp(-1 / diag * 20 * np.linalg.norm(np.asarray(pt1) - np.asarray(pt2)))

    @staticmethod
    def match_attributes(attributes1, attributes2):
        """
        Returns IoU of the attributes. if both are empty - its a prefect match (returns 1)
        0: no matching
        1: perfect attributes match
        """
        if type(attributes1) is not type(attributes2):
            logger.warning('attributes are not same type: {}, {}'.format(type(attributes1), type(attributes2)))
            return 0

        if attributes1 is None and attributes2 is None:
            return 1

        if isinstance(attributes1, dict) and isinstance(attributes2, dict):
            # convert to list
            attributes1 = ['{}-{}'.format(key, val) for key, val in attributes1.items()]
            attributes2 = ['{}-{}'.format(key, val) for key, val in attributes2.items()]

        intersection = set(attributes1).intersection(set(attributes2))
        union = set(attributes1).union(attributes2)
        if len(union) == 0:
            # if there is no union - there are no attributes at all
            return 1
        return len(intersection) / len(union)

    @staticmethod
    def match_labels(label1, label2):
        """
        Returns 1 in one of the labels in substring of the second
        """
        return int(label1 in label2 or label2 in label1)

    @staticmethod
    def general_match(matches: Matches,
                      first_set: entities.AnnotationCollection,
                      second_set: entities.AnnotationCollection,
                      match_type,
                      match_threshold: float,
                      ignore_attributes=False,
                      ignore_labels=False):
        """

        :param matches:
        :param first_set:
        :param second_set:
        :param match_type:
        :param match_threshold:
        :param ignore_attributes:
        :param ignore_labels:
        :return:
        """
        annotation_type_to_func = {
            entities.AnnotationType.BOX: Matchers.calculate_iou_box,
            entities.AnnotationType.CLASSIFICATION: Matchers.calculate_iou_classification,
            entities.AnnotationType.SEGMENTATION: Matchers.calculate_iou_semantic,
            entities.AnnotationType.POLYGON: Matchers.calculate_iou_polygon,
            entities.AnnotationType.POINT: Matchers.calculate_iou_point,
        }
        df = pd.DataFrame(data=-1 * np.ones((len(second_set), len(first_set))),
                          columns=[a.id for a in first_set],
                          index=[a.id for a in second_set])
        for annotation_one in first_set:
            for annotation_two in second_set:
                if match_type not in annotation_type_to_func:
                    raise ValueError('unsupported type: {}'.format(match_type))
                if df[annotation_one.id][annotation_two.id] == -1:
                    try:
                        config = {'height': annotation_one._item.height if annotation_one._item is not None else 500,
                                  'width': annotation_one._item.width if annotation_one._item is not None else 500}
                        df[annotation_one.id][annotation_two.id] = annotation_type_to_func[match_type](
                            annotation_one.geo,
                            annotation_two.geo,
                            config)
                    except ZeroDivisionError:
                        logger.warning(
                            'Found annotations with area=0!: annotations ids: {!r}, {!r}'.format(annotation_one.id,
                                                                                                 annotation_two.id))
                        df[annotation_one.id][annotation_two.id] = 0
        # for debug - save the annotations scoring matrix
        matches._annotations_raw_df.append(df.copy())

        # go over all matches
        while True:
            # take max IoU score, list the match and remove annotations' ids from columns and rows
            # keep doing that until no more matches or lower than match threshold
            max_cell = df.max().max()
            if max_cell < match_threshold or np.isnan(max_cell):
                break
            row_index, col_index = np.where(df == max_cell)
            row_index = row_index[0]
            col_index = col_index[0]
            first_annotation_id = df.columns[col_index]
            second_annotation_id = df.index[row_index]
            first_annotation = [a for a in first_set if a.id == first_annotation_id][0]
            second_annotation = [a for a in second_set if a.id == second_annotation_id][0]
            geometry_score = df.iloc[row_index, col_index]
            labels_score = Matchers.match_labels(label1=first_annotation.label,
                                                 label2=second_annotation.label)
            attribute_score = Matchers.match_attributes(attributes1=first_annotation.attributes,
                                                        attributes2=second_annotation.attributes)

            # TODO use ignores for final score
            annotation_score = (geometry_score + attribute_score + labels_score) / 3
            matches.add(Match(first_annotation_id=first_annotation_id,
                              first_annotation_label=first_annotation.label,
                              first_annotation_confidence=
                              first_annotation.metadata.get('user', dict()).get('model', dict()).get('confidence', 1),
                              second_annotation_id=second_annotation_id,
                              second_annotation_label=second_annotation.label,
                              second_annotation_confidence=
                              second_annotation.metadata.get('user', dict()).get('model', dict()).get('confidence', 1),
                              geometry_score=geometry_score,
                              annotation_score=annotation_score,
                              label_score=labels_score,
                              attributes_score=attribute_score))
            df.drop(index=second_annotation_id, inplace=True)
            df.drop(columns=first_annotation_id, inplace=True)
        # add un-matched
        for second_id in df.index:
            second_annotation = [a for a in second_set if a.id == second_id][0]
            matches.add(match=Match(first_annotation_id=None,
                                    first_annotation_label=None,
                                    first_annotation_confidence=None,
                                    second_annotation_id=second_id,
                                    second_annotation_label=second_annotation.label,
                                    second_annotation_confidence=
                                    second_annotation.metadata.get('user', dict()).get('model', dict()).get(
                                        'confidence', 1),
                                    ))
        for first_id in df.columns:
            first_annotation = [a for a in first_set if a.id == first_id][0]
            matches.add(match=Match(first_annotation_id=first_id,
                                    first_annotation_label=first_annotation.label,
                                    first_annotation_confidence=
                                    first_annotation.metadata.get('user', dict()).get('model', dict()).get('confidence',
                                                                                                           1),
                                    second_annotation_id=None,
                                    second_annotation_label=None,
                                    second_annotation_confidence=None))
        return matches


def item_annotation_duration(item: entities.Item = None,
                             dataset: entities.Dataset = None,
                             project: entities.Project = None,
                             task: entities.Task = None,
                             assignment: entities.Assignment = None):
    if all(ent is None for ent in [item, dataset, project, assignment, task]):
        raise ValueError('At least one input to annotation duration must not be None')
    query = {
        "startTime": 0,
        "context": {
            "accountId": [],
            "orgId": [],
            "projectId": [],
            "datasetId": [],
            "taskId": [],
            "assignmentId": [],
            "itemId": [],
            "userId": [],
            "serviceId": [],
            "podId": [],
        },
        "measures": [
            {
                "measureType": "itemAnnotationDuration",
                "pageSize": 1000,
                "page": 0,
            },
        ]
    }
    # add context for analytics
    created_at = list()
    if item is not None:
        query['context']['itemId'].append(item.id)
        created_at.append(int(1000 * datetime.datetime.fromisoformat(item.created_at[:-1]).timestamp()))
    if task is not None:
        query['context']['taskId'].append(task.id)
        created_at.append(int(1000 * datetime.datetime.fromisoformat(task.created_at[:-1]).timestamp()))
    if dataset is not None:
        query['context']['datasetId'].append(dataset.id)
        created_at.append(int(1000 * datetime.datetime.fromisoformat(dataset.created_at[:-1]).timestamp()))
    if assignment is not None:
        query['context']['assignmentId'].append(assignment.id)
        # assignment doesnt have "created_at" attribute
    query['startTime'] = int(np.min(created_at))
    raw = project.analytics.get_samples(query=query, return_field=None, return_raw=True)
    res = {row['itemId']: row['duration'] for row in raw[0]['response']}
    if item.id not in res:
        total_time_s = 0
    else:
        total_time_s = res[item.id] / 1000
    return total_time_s


================================================
File: dtlpy/ml/predictions_utils.py
================================================
import sys

import pandas as pd
import numpy as np
import traceback
import datetime
import logging
import tqdm
import uuid
import os

from .. import entities
from . import BaseModelAdapter, metrics

logger = logging.getLogger(name='dtlpy')


def mean_or_nan(arr):
    if isinstance(arr, list) and len(arr) == 0:
        return np.nan
    else:
        return np.mean(arr)


def model_info_name(model: entities.Model, package: entities.Package):
    if model is None:
        return "{}-no-model".format(package.name)
    else:
        return "{}-{}".format(package.name, model.name)


def measure_annotations(
        annotations_set_one: entities.AnnotationCollection,
        annotations_set_two: entities.AnnotationCollection,
        match_threshold=0.5,
        ignore_labels=False,
        ignore_attributes=False,
        compare_types=None):
    """
    Compares list (or collections) of annotations
    This will also return the precision and recall of the two sets, given that the first that is a GT and the second set
    is the detection (this affects the denominator of the calculation).


    :param annotations_set_one: dl.AnnotationCollection entity with a list of annotations to compare
    :param annotations_set_two: dl.AnnotationCollection entity with a list of annotations to compare
    :param match_threshold: IoU threshold to count as a match
    :param ignore_labels: ignore label when comparing - measure only geometry
    :param ignore_attributes: ignore attribute score for final annotation score
    :param compare_types: list of type to compare. enum dl.AnnotationType

    Returns a dictionary of all the compare data
    """

    if compare_types is None:
        compare_types = [entities.AnnotationType.BOX,
                         entities.AnnotationType.CLASSIFICATION,
                         entities.AnnotationType.POLYGON,
                         entities.AnnotationType.POINT,
                         entities.AnnotationType.SEGMENTATION]
    final_results = dict()
    all_scores = list()
    true_positives = 0
    false_positives = 0
    false_negatives = 0

    # for local annotations - set random id if None
    for annotation in annotations_set_one:
        if annotation.id is None:
            annotation.id = str(uuid.uuid1())
    for annotation in annotations_set_two:
        if annotation.id is None:
            annotation.id = str(uuid.uuid1())

    # start comparing
    for compare_type in compare_types:
        matches = metrics.Matches()
        annotation_subset_one = entities.AnnotationCollection()
        annotation_subset_two = entities.AnnotationCollection()
        annotation_subset_one.annotations = [a for a in annotations_set_one if
                                             a.type == compare_type and not a.metadata.get('system', dict()).get(
                                                 'system', False)]
        annotation_subset_two.annotations = [a for a in annotations_set_two if
                                             a.type == compare_type and not a.metadata.get('system', dict()).get(
                                                 'system', False)]
        # create 2d dataframe with annotation id as names and set all to -1 -> not calculated
        if ignore_labels:
            matches = metrics.Matchers.general_match(matches=matches,
                                                     first_set=annotation_subset_one,
                                                     second_set=annotation_subset_two,
                                                     match_type=compare_type,
                                                     match_threshold=match_threshold,
                                                     ignore_labels=ignore_labels,
                                                     ignore_attributes=ignore_attributes)
        else:
            unique_labels = np.unique([a.label for a in annotation_subset_one] +
                                      [a.label for a in annotation_subset_two])
            for label in unique_labels:
                first_set = [a for a in annotation_subset_one if a.label == label]
                second_set = [a for a in annotation_subset_two if a.label == label]
                matches = metrics.Matchers.general_match(matches=matches,
                                                         first_set=first_set,
                                                         second_set=second_set,
                                                         match_type=compare_type,
                                                         match_threshold=match_threshold,
                                                         ignore_labels=ignore_labels,
                                                         ignore_attributes=ignore_attributes
                                                         )
        if len(matches) == 0:
            continue
        all_scores.extend(matches.to_df()['annotation_score'])
        final_results[compare_type] = metrics.Results(matches=matches,
                                                      annotation_type=compare_type)
        true_positives += final_results[compare_type].summary()['n_annotations_matched_total']
        false_positives += final_results[compare_type].summary()['n_annotations_unmatched_set_two']
        false_negatives += final_results[compare_type].summary()['n_annotations_unmatched_set_one']

    final_results['total_mean_score'] = mean_or_nan(all_scores)
    final_results['precision'] = true_positives / (true_positives + false_positives)
    final_results['recall'] = true_positives / (true_positives + false_negatives)
    return final_results


def measure_item(ref_item: entities.Item, test_item: entities.Item,
                 ref_project: entities.Project = None, test_project: entities.Project = None,
                 ignore_labels=False,
                 ignore_attributes=False,
                 match_threshold=0.5,
                 pbar=None):
    """
    Compare annotations sets between two items

    :param ref_item:
    :param test_item:
    :param ref_project:
    :param test_project:
    :param ignore_labels:
    :param ignore_attributes:
    :param match_threshold:
    :param pbar:
    :return:
    """
    try:
        annotations_set_one = ref_item.annotations.list()
        annotations_set_two = test_item.annotations.list()
        final = measure_annotations(annotations_set_one=annotations_set_one,
                                    annotations_set_two=annotations_set_two,
                                    ignore_labels=ignore_labels,
                                    ignore_attributes=ignore_attributes,
                                    match_threshold=match_threshold)

        # get times
        try:
            ref_item_duration_s = metrics.item_annotation_duration(item=ref_item, project=ref_project)
            ref_item_duration = str(datetime.timedelta(seconds=int(np.round(ref_item_duration_s))))
        except Exception:
            ref_item_duration_s = -1
            ref_item_duration = ''

        try:
            test_item_duration_s = metrics.item_annotation_duration(item=test_item, project=test_project)
            test_item_duration = str(datetime.timedelta(seconds=int(np.round(test_item_duration_s))))
        except Exception:
            test_item_duration_s = -1
            test_item_duration = ''

        final.update({'ref_url': ref_item.platform_url,
                      'test_url': test_item.platform_url,
                      'filename': ref_item.filename,
                      'ref_item_duration[s]': ref_item_duration_s,
                      'test_item_duration[s]': test_item_duration_s,
                      'diff_duration[s]': test_item_duration_s - ref_item_duration_s,
                      # round to sec
                      'ref_item_duration': ref_item_duration,
                      'test_item_duration': test_item_duration,
                      })

        return True, final
    except Exception:
        fail_msg = 'failed measuring. ref_item: {!r}, test_item: {!r}'.format(ref_item.id, test_item.id)
        return False, '{}\n{}'.format(fail_msg, traceback.format_exc())
    finally:
        if pbar is not None:
            pbar.update()


def measure_items(ref_items, ref_project, ref_dataset, ref_name,
                  test_items, test_project, test_dataset, test_name,
                  dump_path=None):
    from multiprocessing.pool import ThreadPool
    ref_items_filepath_dict = {item.filename: item for page in ref_items for item in page}
    test_items_filepath_dict = {item.filename: item for page in test_items for item in page}
    pool = ThreadPool(processes=32)
    pbar = tqdm.tqdm(total=len(ref_items_filepath_dict), file=sys.stdout)
    jobs = dict()
    for filepath, ref_item in ref_items_filepath_dict.items():
        if filepath in test_items_filepath_dict:
            test_item = test_items_filepath_dict[filepath]
            jobs[ref_item.filename] = pool.apply_async(measure_item, kwds={'test_item': test_item,
                                                                           'ref_item': ref_item,
                                                                           'ref_project': ref_project,
                                                                           'test_project': test_project,
                                                                           'pbar': pbar})
    pool.close()
    pool.join()
    _ = [job.wait() for job in jobs.values()]
    raw_items_summary = dict()
    failed_items_errors = dict()
    for filename, job in jobs.items():
        success, result = job.get()
        if success:
            raw_items_summary[filename] = result
        else:
            failed_items_errors[filename] = result
    pool.terminate()
    pbar.close()

    df, raw_items_summary = create_summary(ref_name=ref_name, test_name=test_name, raw_items_summary=raw_items_summary)

    if len(failed_items_errors) != 0:
        logger.error(failed_items_errors)
    if dump_path is not None:
        save_to_file(dump_path=dump_path,
                     df=df,
                     ref_name=ref_name,
                     test_name=test_name)
    return df, raw_items_summary, failed_items_errors


def create_summary(ref_name, test_name, raw_items_summary):
    summary = list()
    ref_column_name = 'Ref-{!r}'.format(ref_name)
    test_column_name = 'Test-{!r}'.format(test_name)
    for filename, scores in raw_items_summary.items():
        line = {'filename': scores['filename'],
                ref_column_name: scores['ref_url'],
                test_column_name: scores['test_url'],
                'total_score': scores['total_mean_score'],
                'ref_duration[s]': scores['ref_item_duration[s]'],
                'test_duration[s]': scores['test_item_duration[s]'],
                'diff_duration[s]': scores['diff_duration[s]']}
        for tool_type in list(entities.AnnotationType):
            if tool_type in scores:
                res = scores[tool_type].summary()
                line['{}_annotation_score'.format(tool_type)] = res['mean_annotations_scores']
                line['{}_attributes_score'.format(tool_type)] = res['mean_attributes_scores']
                line['{}_ref_number'.format(tool_type)] = res['n_annotations_set_one']
                line['{}_test_number'.format(tool_type)] = res['n_annotations_set_two']
                line['{}_match_number'.format(tool_type)] = res['n_annotations_matched_total']
        summary.append(line)
    df = pd.DataFrame(summary)
    # Drop column only if all the values are None
    df = df.dropna(how='all', axis=1)
    ####

    return df, raw_items_summary


def save_to_file(df, dump_path, ref_name, test_name):
    # df = df.sort_values(by='box_score')
    ref_column_name = 'Ref-{!r}'.format(ref_name)
    test_column_name = 'Test-{!r}'.format(test_name)

    def make_clickable(val):
        return '<a href="{}">{}</a>'.format(val, 'item')

    s = df.style.format({ref_column_name: make_clickable,
                         test_column_name: make_clickable}).render()
    os.makedirs(dump_path, exist_ok=True)
    html_filepath = os.path.join(dump_path, '{}-vs-{}.html'.format(ref_name, test_name))
    csv_filepath = os.path.join(dump_path, '{}-vs-{}.csv'.format(ref_name, test_name))
    with open(html_filepath, 'w') as f:
        f.write(s)
    df.to_csv(csv_filepath)


================================================
File: dtlpy/ml/summary_writer.py
================================================
# from torch.utils.tensorboard import SummaryWriter, FileWriter
# from tensorboard.compat.proto.event_pb2 import Event, SessionLog
# import time
#
#
# class DtlpyFileWrite(FileWriter):
#     def __init__(self, snapshot, *args, **kwargs):
#         self.snapshot = snapshot
#         super(DtlpyFileWrite, self).__init__(*args, **kwargs)
#
#     def add_event(self, event, step=None, walltime=None):
#         event: Event
#         event.wall_time = time.time() if walltime is None else walltime
#         if step is not None:
#             # Make sure step is converted from numpy or other formats
#             # since protobuf might not convert depending on version
#             event.step = int(step)
#         self.event_writer.add_event(event)
#         # print(event)
#         # print(event.step)
#         # print(event.SerializeToString())
#         dtlpy_records = list()
#         for v in event.summary.value:
#             dtlpy_records.append({'time': event.wall_time,
#                                   'step': event.step,
#                                   'tag': v.tag,
#                                   'value': v.simple_value,
#                                   'image': v.image.encoded_image_string,
#                                   'string': v.tensor.string_val,
#                                   'plugin_name': v.metadata.plugin_data.plugin_name})
#         print(dtlpy_records)
#         # self.snapshot.add_event(event)
#
#
# class DtlpySummaryWriter(SummaryWriter):
#     def __init__(self, snapshot, *args, **kwargs):
#         self.snapshot = snapshot
#         super(DtlpySummaryWriter, self).__init__(*args, **kwargs)
#
#     def _get_file_writer(self):
#         """Returns the default FileWriter instance. Recreates it if closed."""
#         print(self.snapshot)
#         if self.all_writers is None or self.file_writer is None:
#             self.file_writer = DtlpyFileWrite(snapshot=self.snapshot,
#                                               log_dir=self.log_dir,
#                                               max_queue=self.max_queue,
#                                               flush_secs=self.flush_secs,
#                                               filename_suffix=self.filename_suffix)
#             self.all_writers = {self.file_writer.get_logdir(): self.file_writer}
#             if self.purge_step is not None:
#                 most_recent_step = self.purge_step
#                 self.file_writer.add_event(
#                     Event(step=most_recent_step, file_version='brain.Event:2'))
#                 self.file_writer.add_event(
#                     Event(step=most_recent_step, session_log=SessionLog(status=SessionLog.START)))
#                 self.purge_step = None
#         return self.file_writer


================================================
File: dtlpy/ml/train_utils.py
================================================
import datetime
import logging
import json
import time

import numpy as np

from .. import entities, exceptions

logger = logging.getLogger(name='dtlpy')


def prepare_dataset(dataset: entities.Dataset,
                    subsets: dict,
                    filters: entities.Filters = None,
                    async_clone: bool = False):
    """
    clones the given dataset and locks it to be readonly

    :param dataset:  `dl.Dataset` to clone
    :param dtlpy.entities.filters.Filters filters: `dl.Filters` to use when cloning - which items to clone
    :param subsets: dictionary to set subsets from the data.
                        key is subset name, value is the filter to set, eg.
                                    {dl.DatasetSubsetType.TEST: dl.Filters(field='dir', values='/test')}
    :param async_clone: `bool` if True (default) - waits until the sync is complete
                                     False - return before the cloned dataset is fully populated
    :return: `dl.Dataset` which is readonly
    """

    project = dataset.project
    now = datetime.datetime.utcnow().isoformat(timespec='minutes', sep='T')  # This serves as an id
    today = datetime.datetime.utcnow().strftime('%F')

    # CLONE
    clone_name = 'cloned-{ds_name}-{date_str}'.format(ds_name=dataset.name, date_str=today)
    try:
        cloned_dataset = project.datasets.get(clone_name)
        logger.warning("Cloned dataset already exist. Using it...")
        return cloned_dataset
    except exceptions.NotFound:
        pass

    tic = time.time()
    cloned_dataset = dataset.clone(clone_name=clone_name,
                                   filters=filters)
    toc = time.time()

    orig_recipe = dataset.recipes.list()[0]
    cloned_recipe = orig_recipe.clone(shallow=True)
    cloned_dataset.metadata['system']['recipes'] = [cloned_recipe.id]

    logger.info("clone complete: {!r} in {:1.1f}[s]".format(cloned_dataset.name, toc - tic))

    assert cloned_dataset.name is not None, ('unable to get new ds {}'.format(clone_name))
    cloned_dataset.metadata['system']['clone_info'] = {'date': now,
                                                       'originalDatasetId': dataset.id}
    if filters is not None:
        cloned_dataset.metadata['system']['clone_info'].update({'filters': json.dumps(filters.prepare())})
    cloned_dataset.update(system_metadata=True)
    return cloned_dataset


================================================
File: dtlpy/repositories/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .annotations import Annotations
from .artifacts import Artifacts
from .datasets import Datasets
from .items import Items
from .codebases import Codebases
from .projects import Projects
from .ontologies import Ontologies
from .recipes import Recipes
from .packages import Packages, PackageCatalog
from .downloader import Downloader
from .uploader import Uploader
from .triggers import Triggers
from .times_series import TimesSeries
from .services import Services, FUNCTION_END_LINE
from .executions import Executions
from .assignments import Assignments
from .tasks import Tasks
from .bots import Bots
from .webhooks import Webhooks
from .models import Models, Metrics
from .analytics import Analytics
from .drivers import Drivers
from .commands import Commands
from .pipelines import Pipelines
from .nodes import Nodes
from .pipeline_executions import PipelineExecutions
from .features import Features
from .feature_sets import FeatureSets
from .organizations import Organizations
from .integrations import Integrations
from .drivers import Drivers
from .settings import Settings
from .resource_executions import ResourceExecutions
from .apps import Apps
from .dpks import Dpks
from .messages import Messages
from .compositions import Compositions
from .schema import Schema
from .computes import Computes, ServiceDrivers
from .collections import Collections


================================================
File: dtlpy/repositories/analytics.py
================================================
import logging
import pandas as pd
from dtlpy import entities, exceptions
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Analytics:
    """
    Time series Repository
    """

    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.times_series repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ############
    #  methods #
    ############
    def get_samples(self, query=None, return_field: str = None, return_raw: bool = False) -> pd.DataFrame:
        """
        Get Analytics table

        :param dict query: match filters to get specific data from series
        :param str return_field: name of field to return from response. default: "samples"
        :param bool return_raw: return the response with out converting
        :return: Analytics table
        :rtype: pd.DataFrame
        """
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/analytics/query',
                                                         json_req=query)
        if success:
            if return_field is not None:
                res = response.json()[return_field]
            else:
                res = response.json()
            if return_raw:
                return res
            if isinstance(res, dict):
                df = pd.DataFrame.from_dict(res, orient="index")
            elif isinstance(res, list):
                df = pd.DataFrame(res)
            else:
                raise ValueError('unknown return type for time series: {}'.format(type(res)))
        else:
            raise exceptions.PlatformException(response)
        return df

    def report_metrics(self, samples):
        """
        Report metrics

        :param samples: table samples to report
        :return: True/ False
        :rtype: bool
        """
        if not isinstance(samples, list):
            samples = [samples]

        samples = [s.to_json() if not isinstance(s, dict) else s for s in samples]

        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/analytics/metric',
                                                         json_req=samples)
        return success


================================================
File: dtlpy/repositories/annotations.py
================================================
from copy import deepcopy
import traceback
import logging
import json
import jwt
import os
from PIL import Image
from io import BytesIO
import base64

from .. import entities, exceptions, miscellaneous, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Annotations:
    """
    Annotations Repository

    The Annotation class allows you to manage the annotations of data items. For information on annotations explore our
    documentation at:
    `Classification SDK <https://developers.dataloop.ai/tutorials/annotations_image/classification_point_and_pose/chapter/>`_,
    `Annotation Labels and Attributes <https://developers.dataloop.ai/tutorials/data_management/upload_and_manage_annotations/chapter/#set-attributes-on-annotations>`_,
    `Show Video with Annotations <https://developers.dataloop.ai/tutorials/annotations_video/video_annotations/chapter/>`_.
    """

    def __init__(self, client_api: ApiClient, item=None, dataset=None, dataset_id=None):
        self._client_api = client_api
        self._item = item
        self._dataset = dataset
        self._upload_batch_size = 100
        if dataset_id is None:
            if dataset is not None:
                dataset_id = dataset.id
            elif item is not None:
                dataset_id = item.dataset_id
        self._dataset_id = dataset_id

    ############
    # entities #
    ############
    @property
    def dataset(self):
        if self._dataset is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "dataset". need to set a Dataset entity or use dataset.annotations repository')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @dataset.setter
    def dataset(self, dataset: entities.Dataset):
        if not isinstance(dataset, entities.Dataset):
            raise ValueError('Must input a valid Dataset entity')
        self._dataset = dataset

    @property
    def item(self):
        if self._item is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "item". need to set an Item entity or use item.annotations repository')
        assert isinstance(self._item, entities.Item)
        return self._item

    @item.setter
    def item(self, item: entities.Item):
        if not isinstance(item, entities.Item):
            raise ValueError('Must input a valid Item entity')
        self._item = item

    ###########
    # methods #
    ###########
    @_api_reference.add(path='/annotations/{annotationId}', method='get')
    def get(self, annotation_id: str) -> entities.Annotation:
        """
        Get a single annotation.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
        *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param str annotation_id: The id of the annotation
        :return: Annotation object or None
        :return: Annotation object or None
        :rtype: dtlpy.entities.annotation.Annotation

        **Example**:

        .. code-block:: python

            annotation = item.annotations.get(annotation_id='annotation_id')
        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/annotations/{}'.format(annotation_id))
        if success:
            annotation = entities.Annotation.from_json(_json=response.json(),
                                                       annotations=self,
                                                       dataset=self._dataset,
                                                       client_api=self._client_api,
                                                       item=self._item)
        else:
            raise exceptions.PlatformException(response)
        return annotation

    def _build_entities_from_response(self, response_items):
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_json, _json in enumerate(response_items):
            jobs[i_json] = pool.submit(entities.Annotation._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': _json,
                                          'item': self._item,
                                          'dataset': self._dataset,
                                          'annotations': self})

        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        return miscellaneous.List([r[1] for r in results if r[0] is True])

    def _list(self, filters: entities.Filters):
        """
        Get a dataset's item list. This is a browsing endpoint. For any given path, item count will be returned.
        The user is then expected to perform another request for every folder to actually get its item list.

        :param dtlpy.entities.filters.Filters filters: Filter entity or a dictionary containing filters parameters

        :return: json response
        :rtype: 
        """
        # prepare request
        success, response = self._client_api.gen_request(req_type="POST",
                                                         path="/datasets/{}/query".format(self._dataset_id),
                                                         json_req=filters.prepare(),
                                                         headers={'user_query': filters._user_query}
                                                         )
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @_api_reference.add(path='/datasets/{id}/query', method='post')
    def list(self, filters: entities.Filters = None, page_offset: int = None, page_size: int = None):
        """
        List Annotations of a specific item. You must get the item first and then list the annotations with the desired filters.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
        *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param int page_offset: starting page
        :param int page_size: size of page
        :return: Pages object
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            annotations = item.annotations.list(filters=dl.Filters(
                                         resource=dl.FiltersResource.ANNOTATION,
                                         field='type',
                                         values='box'),
                      page_size=100,
                      page_offset=0)
        """
        if self._dataset_id is not None:
            if filters is None:
                filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
                filters._user_query = 'false'

            if not filters.resource == entities.FiltersResource.ANNOTATION:
                raise exceptions.PlatformException(error='400',
                                                   message='Filters resource must to be FiltersResource.ANNOTATION')

            if self._item is not None and not filters.has_field('itemId'):
                filters = deepcopy(filters)
                filters.page_size = 1000
                filters.add(field='itemId', values=self.item.id, method=entities.FiltersMethod.AND)

            # assert type filters
            if not isinstance(filters, entities.Filters):
                raise exceptions.PlatformException('400', 'Unknown filters type')

            # page size
            if page_size is None:
                # take from default
                page_size = filters.page_size
            else:
                filters.page_size = page_size

            # page offset
            if page_offset is None:
                # take from default
                page_offset = filters.page
            else:
                filters.page = page_offset

            paged = entities.PagedEntities(items_repository=self,
                                           filters=filters,
                                           page_offset=page_offset,
                                           page_size=page_size,
                                           client_api=self._client_api)
            paged.get_page()

            if self._item is not None:
                if paged.total_pages_count > 1:
                    annotations = list()
                    for page in paged:
                        annotations += page
                else:
                    annotations = paged.items
                return entities.AnnotationCollection(annotations=annotations, item=self._item)
            else:
                return paged
        else:
            raise exceptions.PlatformException('400',
                                               'Please use item.annotations.list() or dataset.annotations.list() '
                                               'to perform this action.')

    def show(self,
             image=None,
             thickness: int = 1,
             with_text: bool = False,
             height: float = None,
             width: float = None,
             annotation_format: entities.ViewAnnotationOptions = entities.ViewAnnotationOptions.MASK,
             alpha: float = 1):
        """
        Show annotations. To use this method, you must get the item first and then show the annotations with
        the desired filters. The method returns an array showing all the annotations.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
        *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param ndarray image: empty or image to draw on
        :param int thickness: optional - line thickness, default=1
        :param bool with_text: add label to annotation
        :param float height: item height
        :param float width: item width
        :param str annotation_format: the format that want to show ,options: list(dl.ViewAnnotationOptions)
        :param float alpha: opacity value [0 1], default 1
        :return: ndarray of the annotations
        :rtype: ndarray

        **Example**:

        .. code-block:: python

            image = item.annotations.show(image='nd array',
                      thickness=1,
                      with_text=False,
                      height=100,
                      width=100,
                      annotation_format=dl.ViewAnnotationOptions.MASK,
                      alpha=1)
        """
        # get item's annotations
        annotations = self.list()

        return annotations.show(image=image,
                                width=width,
                                height=height,
                                thickness=thickness,
                                alpha=alpha,
                                with_text=with_text,
                                annotation_format=annotation_format)

    def download(self,
                 filepath: str,
                 annotation_format: entities.ViewAnnotationOptions = entities.ViewAnnotationOptions.JSON,
                 img_filepath: str = None,
                 height: float = None,
                 width: float = None,
                 thickness: int = 1,
                 with_text: bool = False,
                 alpha: float = 1):
        """
        Save annotation to file.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
        *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param str filepath: Target download directory
        :param str annotation_format: the format that want to download ,options: list(dl.ViewAnnotationOptions)
        :param str img_filepath: img file path - needed for img_mask
        :param float height: optional - image height
        :param float width: optional - image width
        :param int thickness: optional - line thickness, default=1
        :param bool with_text: optional - draw annotation with text, default = False
        :param float alpha: opacity value [0 1], default 1
        :return: file path to where save the annotations
        :rtype: str

        **Example**:

        .. code-block:: python

            file_path = item.annotations.download(
                          filepath='file_path',
                          annotation_format=dl.ViewAnnotationOptions.MASK,
                          img_filepath='img_filepath',
                          height=100,
                          width=100,
                          thickness=1,
                          with_text=False,
                          alpha=1)
        """
        # get item's annotations
        annotations = self.list()
        if 'text' in self.item.metadata.get('system').get('mimetype', '') or 'json' in self.item.metadata.get('system').get('mimetype', ''):
            annotation_format = entities.ViewAnnotationOptions.JSON
        elif 'audio' not in self.item.metadata.get('system').get('mimetype', ''):
            # height/weight
            if height is None:
                if self.item.height is None:
                    raise exceptions.PlatformException('400', 'Height must be provided')
                height = self.item.height
            if width is None:
                if self.item.width is None:
                    raise exceptions.PlatformException('400', 'Width must be provided')
                width = self.item.width

        return annotations.download(filepath=filepath,
                                    img_filepath=img_filepath,
                                    width=width,
                                    height=height,
                                    thickness=thickness,
                                    with_text=with_text,
                                    annotation_format=annotation_format,
                                    alpha=alpha)

    def _delete_single_annotation(self, w_annotation_id):
        try:
            creator = jwt.decode(self._client_api.token, algorithms=['HS256'],
                                 verify=False, options={'verify_signature': False})['email']
            payload = {'username': creator}
            success, response = self._client_api.gen_request(req_type='delete',
                                                             path='/annotations/{}'.format(w_annotation_id),
                                                             json_req=payload)

            if not success:
                raise exceptions.PlatformException(response)
            return success
        except Exception:
            logger.exception('Failed to delete annotation')
            raise

    @_api_reference.add(path='/annotations/{annotationId}', method='delete')
    def delete(self, annotation: entities.Annotation = None,
               annotation_id: str = None,
               filters: entities.Filters = None) -> bool:
        """
        Remove an annotation from item.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
        *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param dtlpy.entities.annotation.Annotation annotation: Annotation object
        :param str annotation_id: The id of the annotation
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: True/False
        :rtype: bool

        **Example**:

        .. code-block:: python

            is_deleted = item.annotations.delete(annotation_id='annotation_id')
        """
        if annotation is not None:
            if isinstance(annotation, entities.Annotation):
                annotation_id = annotation.id
            elif isinstance(annotation, str) and annotation.lower() == 'all':
                if self._item is None:
                    raise exceptions.PlatformException(error='400',
                                                       message='To use "all" option repository must have an item')
                filters = entities.Filters(
                    resource=entities.FiltersResource.ANNOTATION,
                    field='itemId',
                    values=self._item.id,
                    method=entities.FiltersMethod.AND
                )
            else:
                raise exceptions.PlatformException(error='400',
                                                   message='Unknown annotation type')

        if annotation_id is not None:
            if not isinstance(annotation_id, list):
                return self._delete_single_annotation(w_annotation_id=annotation_id)
            filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION,
                                       field='annotationId',
                                       values=annotation_id,
                                       operator=entities.FiltersOperations.IN)
            filters.pop(field="type")

        if filters is None:
            raise exceptions.PlatformException(error='400',
                                               message='Must input filter, annotation id or annotation entity')

        if not filters.resource == entities.FiltersResource.ANNOTATION:
            raise exceptions.PlatformException(error='400',
                                               message='Filters resource must to be FiltersResource.ANNOTATION')

        if self._item is not None and not filters.has_field('itemId'):
            filters = deepcopy(filters)
            filters.add(field='itemId', values=self._item.id, method=entities.FiltersMethod.AND)

        if self._dataset is not None:
            items_repo = self._dataset.items
        elif self._item is not None:
            items_repo = self._item.dataset.items
        else:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "dataset". need to set a Dataset entity or use dataset.annotations repository')

        return items_repo.delete(filters=filters)

    @staticmethod
    def _update_snapshots(origin, modified):
        """
        Update the snapshots if a change occurred and return a list of the new snapshots with the flag to update.
        """
        update = False
        origin_snapshots = list()
        origin_metadata = origin['metadata'].get('system', None)
        if origin_metadata:
            origin_snapshots = origin_metadata.get('snapshots_', None)
            if origin_snapshots is not None:
                modified_snapshots = modified['metadata'].get('system', dict()).get('snapshots_', None)
                # if the number of the snapshots change
                if len(origin_snapshots) != len(modified_snapshots):
                    origin_snapshots = modified_snapshots
                    update = True

                i = 0
                # if some snapshot change
                while i < len(origin_snapshots) and not update:
                    if origin_snapshots[i] != modified_snapshots[i]:
                        origin_snapshots = modified_snapshots
                        update = True
                        break
                    i += 1

        return update, origin_snapshots

    def _update_single_annotation(self, w_annotation, system_metadata):
        try:
            if isinstance(w_annotation, entities.Annotation):
                if w_annotation.id is None:
                    raise exceptions.PlatformException(
                        '400',
                        'Cannot update annotation because it was not fetched'
                        ' from platform and therefore does not have an id'
                    )
                annotation_id = w_annotation.id
            else:
                raise exceptions.PlatformException('400',
                                                   'unknown annotations type: {}'.format(type(w_annotation)))

            origin = w_annotation._platform_dict
            modified = w_annotation.to_json()
            # check snapshots
            update, updated_snapshots = self._update_snapshots(origin=origin,
                                                               modified=modified)

            # pop the snapshots to make the diff work with out them
            origin.get('metadata', dict()).get('system', dict()).pop('snapshots_', None)
            modified.get('metadata', dict()).get('system', dict()).pop('snapshots_', None)

            # check diffs in the json
            json_req = miscellaneous.DictDiffer.diff(origin=origin,
                                                     modified=modified)

            # add the new snapshots if exist
            if updated_snapshots and update:
                if 'metadata' not in json_req:
                    json_req['metadata'] = dict()
                if 'system' not in json_req['metadata']:
                    json_req['metadata']['system'] = dict()
                json_req['metadata']['system']['snapshots_'] = updated_snapshots

            # no changes happen
            if not json_req and not updated_snapshots:
                status = True
                result = w_annotation
            else:
                suc, response = self._update_annotation_req(annotation_json=json_req,
                                                            system_metadata=system_metadata,
                                                            annotation_id=annotation_id)
                if suc:
                    result = entities.Annotation.from_json(_json=response.json(),
                                                           annotations=self,
                                                           dataset=self._dataset,
                                                           item=self._item)
                    w_annotation._platform_dict = result._platform_dict
                else:
                    raise exceptions.PlatformException(response)
                status = True
        except Exception:
            status = False
            result = traceback.format_exc()
        return status, result

    def _update_annotation_req(self, annotation_json, system_metadata, annotation_id):
        url_path = '/annotations/{}'.format(annotation_id)
        if system_metadata:
            url_path += '?system=true'
        suc, response = self._client_api.gen_request(req_type='put',
                                                     path=url_path,
                                                     json_req=annotation_json)
        return suc, response

    @_api_reference.add(path='/annotations/{annotationId}', method='put')
    def update(self, annotations, system_metadata=False):
        """
        Update an existing annotation. For example, you may change the annotation's label and then use the update method. 

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
         *developer* or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :param dtlpy.entities.annotation.Annotation annotations: Annotation object
        :param bool system_metadata: bool - True, if you want to change metadata system

        :return: True if successful or error if unsuccessful
        :rtype: bool

         **Example**:

        .. code-block:: python

            annotations = item.annotations.update(annotation='annotation')
        """
        pool = self._client_api.thread_pools(pool_name='annotation.update')
        if not isinstance(annotations, list):
            annotations = [annotations]
        jobs = [None for _ in range(len(annotations))]
        for i_ann, ann in enumerate(annotations):
            jobs[i_ann] = pool.submit(self._update_single_annotation,
                                      **{'w_annotation': ann,
                                         'system_metadata': system_metadata})

        # get all results
        results = [j.result() for j in jobs]
        out_annotations = [r[1] for r in results if r[0] is True]
        out_errors = [r[1] for r in results if r[0] is False]
        if len(out_errors) == 0:
            logger.debug('Annotation/s updated successfully. {}/{}'.format(len(out_annotations), len(results)))
        else:
            logger.error(out_errors)
            logger.error('Annotation/s updated with {} errors'.format(len(out_errors)))
        return out_annotations

    @staticmethod
    def _annotation_encoding(annotation):
        metadata = annotation.get('metadata', dict())
        system = metadata.get('system', dict())
        snapshots = system.get('snapshots_', list())
        last_frame = {
            'label': annotation.get('label', None),
            'attributes': annotation.get('attributes', None),
            'type': annotation.get('type', None),
            'data': annotation.get('coordinates', None),
            'fixed': snapshots[0].get('fixed', None) if (isinstance(snapshots, list) and len(snapshots) > 0) else None,
            'objectVisible': snapshots[0].get('objectVisible', None) if (
                    isinstance(snapshots, list) and len(snapshots) > 0) else None,
        }

        offset = 0
        for idx, frame in enumerate(deepcopy(snapshots)):
            frame.pop("frame", None)
            if frame == last_frame and not frame['fixed']:
                del snapshots[idx - offset]
                offset += 1
            else:
                last_frame = frame
        return annotation

    def _create_batches_for_upload(self, annotations, merge=False):
        """
        receives a list of annotations and split them into batches to optimize the upload

        :param annotations: list of all annotations
        :param merge: bool - merge the new binary annotations with the existing annotations
        :return: batch_annotations: list of list of annotation. each batch with size self._upload_batch_size
        """
        annotation_batches = list()
        single_batch = list()
        for annotation in annotations:
            if isinstance(annotation, str):
                annotation = json.loads(annotation)
            elif isinstance(annotation, entities.Annotation):
                if annotation._item is None and self._item is not None:
                    # if annotation is without item - set one (affects the binary annotation color)
                    annotation._item = self._item
                annotation = annotation.to_json()
            elif isinstance(annotation, dict):
                pass
            else:
                raise exceptions.PlatformException(error='400',
                                                   message='unknown annotations type: {}'.format(type(annotation)))
            annotation = self._annotation_encoding(annotation)
            single_batch.append(annotation)
            if len(single_batch) >= self._upload_batch_size:
                annotation_batches.append(single_batch)
                single_batch = list()
        if len(single_batch) > 0:
            annotation_batches.append(single_batch)
        if merge and self.item:
            annotation_batches = self._merge_new_annotations(annotation_batches)
            annotation_batches = self._merge_to_exits_annotations(annotation_batches)
        return annotation_batches

    def _merge_binary_annotations(self, data_url1, data_url2, item_width, item_height):
        # Decode base64 data
        img_data1 = base64.b64decode(data_url1.split(",")[1])
        img_data2 = base64.b64decode(data_url2.split(",")[1])

        # Convert binary data to images
        img1 = Image.open(BytesIO(img_data1))
        img2 = Image.open(BytesIO(img_data2))

        # Create a new image with the target item size
        merged_img = Image.new('RGBA', (item_width, item_height))

        # Paste both images on the new canvas at their original sizes and positions
        # Adjust positioning logic if needed (assuming top-left corner for both images here)
        merged_img.paste(img1, (0, 0), img1)  # Use img1 as a mask to handle transparency
        merged_img.paste(img2, (0, 0), img2)  # Overlay img2 at the same position

        # Save the merged image to a buffer
        buffer = BytesIO()
        merged_img.save(buffer, format="PNG")
        merged_img_data = buffer.getvalue()

        # Encode the merged image back to a base64 string
        merged_data_url = "data:image/png;base64," + base64.b64encode(merged_img_data).decode()

        return merged_data_url

    def _merge_new_annotations(self, annotations_batch):
        """
        Merge the new binary annotations with the existing annotations
        :param annotations_batch: list of list of annotation. each batch with size self._upload_batch_size
        :return: merged_annotations_batch: list of list of annotation. each batch with size self._upload_batch_size
        """
        for annotations in annotations_batch:
            for annotation in annotations:
                if annotation['type'] == 'binary' and not annotation.get('clean', False):
                    to_merge = [a for a in annotations if
                                not a.get('clean', False) and a.get("metadata", {}).get('system', {}).get('objectId',
                                                                                                          None) ==
                                annotation.get("metadata", {}).get('system', {}).get('objectId', None) and a['label'] ==
                                annotation['label']]
                    if len(to_merge) == 0:
                        # no annotation to merge with
                        continue
                    for a in to_merge:
                        if a['coordinates'] == annotation['coordinates']:
                            continue
                        merged_data_url = self._merge_binary_annotations(a['coordinates'], annotation['coordinates'],
                                                                         self.item.width, self.item.height)
                        annotation['coordinates'] = merged_data_url
                        a['clean'] = True
        return [[a for a in annotations if not a.get('clean', False)] for annotations in annotations_batch]

    def _merge_to_exits_annotations(self, annotations_batch):
        filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION, field='type', values='binary')
        filters.add(field='itemId', values=self.item.id, method=entities.FiltersMethod.AND)
        exist_annotations = self.list(filters=filters).annotations or list()
        to_delete = list()
        for annotations in annotations_batch:
            for ann in annotations:
                if ann['type'] == 'binary':
                    to_merge = [a for a in exist_annotations if
                                a.object_id == ann.get("metadata", {}).get('system', {}).get('objectId',
                                                                                             None) and a.label == ann[
                                    'label']]
                    if len(to_merge) == 0:
                        # no annotation to merge with
                        continue
                    if to_merge[0].coordinates == ann['coordinates']:
                        # same annotation
                        continue
                    if len(to_merge) > 1:
                        raise exceptions.PlatformException('400', 'Multiple annotations with the same label')
                    # merge
                    exist_annotations.remove(to_merge[0])
                    merged_data_url = self._merge_binary_annotations(to_merge[0].coordinates, ann['coordinates'],
                                                                     self.item.width, self.item.height)
                    json_ann = to_merge[0].to_json()
                    json_ann['coordinates'] = merged_data_url
                    suc, response = self._update_annotation_req(annotation_json=json_ann,
                                                                system_metadata=True,
                                                                annotation_id=to_merge[0].id)
                    if not suc:
                        raise exceptions.PlatformException(response)
                    if suc:
                        result = entities.Annotation.from_json(_json=response.json(),
                                                               annotations=self,
                                                               dataset=self._dataset,
                                                               item=self._item)
                        exist_annotations.append(result)
                    to_delete.append(ann)
        if len(to_delete) > 0:
            annotations_batch = [[a for a in annotations if a not in to_delete] for annotations in annotations_batch]

        return annotations_batch

    def _upload_single_batch(self, annotation_batch):
        try:
            suc, response = self._client_api.gen_request(req_type='post',
                                                         path='/items/{}/annotations'.format(self.item.id),
                                                         json_req=annotation_batch)
            if suc:
                return_annotations = response.json()
                if not isinstance(return_annotations, list):
                    return_annotations = [return_annotations]
            else:
                raise exceptions.PlatformException(response)

            status = True
            result = return_annotations
        except Exception:
            status = False
            result = traceback.format_exc()

        return status, result

    def _upload_annotations_batches(self, annotation_batches):
        if len(annotation_batches) == 1:
            # no need for threads
            status, result = self._upload_single_batch(annotation_batch=annotation_batches[0])
            if status is False:
                logger.error(result)
                logger.error('Annotation/s uploaded with errors')
                # TODO need to raise errors?
            uploaded_annotations = result
        else:
            # threading
            pool = self._client_api.thread_pools(pool_name='annotation.upload')
            jobs = [None for _ in range(len(annotation_batches))]
            for i_ann, annotations_batch in enumerate(annotation_batches):
                jobs[i_ann] = pool.submit(self._upload_single_batch,
                                          annotation_batch=annotations_batch)
            # get all results
            results = [j.result() for j in jobs]
            uploaded_annotations = [ann for ann_list in results for ann in ann_list[1] if ann_list[0] is True]
            out_errors = [r[1] for r in results if r[0] is False]
            if len(out_errors) != 0:
                logger.error(out_errors)
                logger.error('Annotation/s uploaded with errors')
                # TODO need to raise errors?
        logger.info('Annotation/s uploaded successfully. num: {}'.format(len(uploaded_annotations)))
        return uploaded_annotations

    async def _async_upload_annotations(self, annotations, merge=False):
        """
        Async function to run from the uploader. will use asyncio to not break the async
        :param annotations: list of all annotations
        :param merge: bool - merge the new binary annotations with the existing annotations
        :return:
        """
        async with self._client_api.event_loop.semaphore('annotations.upload'):
            annotation_batch = self._create_batches_for_upload(annotations=annotations, merge=merge)
            output_annotations = list()
            for annotations_list in annotation_batch:
                success, response = await self._client_api.gen_async_request(req_type='post',
                                                                             path='/items/{}/annotations'
                                                                             .format(self.item.id),
                                                                             json_req=annotations_list)
                if success:
                    return_annotations = response.json()
                    if not isinstance(return_annotations, list):
                        return_annotations = [return_annotations]
                    output_annotations.extend(return_annotations)
                else:
                    if len(output_annotations) > 0:
                        logger.warning("Only {} annotations from {} annotations have been uploaded".
                                       format(len(output_annotations), len(annotations)))
                    raise exceptions.PlatformException(response)

            result = entities.AnnotationCollection.from_json(_json=output_annotations, item=self.item)
            return result

    @_api_reference.add(path='/items/{itemId}/annotations', method='post')
    def upload(self, annotations, merge=False) -> entities.AnnotationCollection:
        """
        Upload a new annotation/annotations. You must first create the annotation using the annotation *builder* method.

        **Prerequisites**: Any user can upload annotations.

        :param List[dtlpy.entities.annotation.Annotation] or dtlpy.entities.annotation.Annotation annotations: list or
        single annotation of type Annotation
        :param bool merge: optional - merge the new binary annotations with the existing annotations
        :return: list of annotation objects
        :rtype: entities.AnnotationCollection

        **Example**:

        .. code-block:: python

            annotations = item.annotations.upload(annotations='builder')
        """
        # make list if not list
        if isinstance(annotations, entities.AnnotationCollection):
            # get the annotation from a collection
            annotations = annotations.annotations
        elif isinstance(annotations, str) and os.path.isfile(annotations):
            # load annotation filepath and get list of annotations
            with open(annotations, 'r', encoding="utf8") as f:
                annotations = json.load(f)
            annotations = annotations.get('annotations', [])
            # annotations = entities.AnnotationCollection.from_json_file(filepath=annotations).annotations
        elif isinstance(annotations, entities.Annotation) or isinstance(annotations, dict):
            # convert the single Annotation to a list
            annotations = [annotations]
        elif isinstance(annotations, list):
            pass
        else:
            exceptions.PlatformException(error='400',
                                         message='Unknown annotation format. type: {}'.format(type(annotations)))
        if len(annotations) == 0:
            logger.warning('Annotation upload receives 0 annotations. Not doing anything')
            out_annotations = list()
        else:
            annotation_batches = self._create_batches_for_upload(annotations=annotations, merge=merge)
            out_annotations = self._upload_annotations_batches(annotation_batches=annotation_batches)
        out_annotations = entities.AnnotationCollection.from_json(_json=out_annotations,
                                                                  item=self.item)
        return out_annotations

    def update_status(self,
                      annotation: entities.Annotation = None,
                      annotation_id: str = None,
                      status: entities.AnnotationStatus = entities.AnnotationStatus.ISSUE
                      ) -> entities.Annotation:
        """
        Set status on annotation.

        **Prerequisites**: You must have an item that has been annotated. You must have the role of an *owner* or
        *developer* or be assigned a task that includes that item as an *annotation manager*.

        :param dtlpy.entities.annotation.Annotation annotation: Annotation object
        :param str annotation_id: optional - annotation id to set status
        :param str status: can be AnnotationStatus.ISSUE, APPROVED, REVIEW, CLEAR
        :return: Annotation object
        :rtype: dtlpy.entities.annotation.Annotation

        **Example**:

        .. code-block:: python

            annotation = item.annotations.update_status(annotation_id='annotation_id', status=dl.AnnotationStatus.ISSUE)
        """
        if annotation is None:
            if annotation_id is None:
                raise ValueError('must input on of "annotation" or "annotation_id"')
            annotation = self.get(annotation_id=annotation_id)
        if status not in list(entities.AnnotationStatus):
            raise ValueError('status must be on of: {}'.format(', '.join(list(entities.AnnotationStatus))))
        annotation.status = status
        return annotation.update(system_metadata=True)

    def builder(self):
        """
        Create Annotation collection.

        **Prerequisites**: You must have an item to be annotated. You must have the role of an *owner* or *developer*
         or be assigned a task that includes that item as an *annotation manager* or *annotator*.

        :return: Annotation collection object
        :rtype: dtlpy.entities.annotation_collection.AnnotationCollection

        **Example**:

        .. code-block:: python

            annotation_collection = item.annotations.builder()
        """
        return entities.AnnotationCollection(item=self.item)

    ##################
    # async function #
    ##################


================================================
File: dtlpy/repositories/apps.py
================================================
import logging

from .. import entities, exceptions, miscellaneous, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Apps:

    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project
        self._commands = None

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.apps repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def commands(self) -> repositories.Commands:
        if self._commands is None:
            self._commands = repositories.Commands(client_api=self._client_api)
        return self._commands

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def get(self,
            app_name: str = None,
            app_id: str = None,
            fetch: bool = None) -> entities.App:
        """
        Get an app object.

        note: It's required to pass either app_id of app_name

        :param str app_id: optional - search by id.
        :param str app_name: optional - search by name.
        :param bool fetch: optional - fetch entity from platform, default taken from cookie

        **Example**:

        .. code-block:: python
            app = self.apps.get(app_id='app_id')
        """
        if fetch is None:
            fetch = self._client_api.fetch_entities

        if app_id is None and app_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='You must provide an identifier in inputs')
        if fetch:
            if app_name is not None:
                app = self.__get_by_name(name=app_name)
            else:
                success, response = self._client_api.gen_request(req_type='get', path="/apps/{}".format(app_id))
                if not success:
                    raise exceptions.PlatformException(response)
                app = entities.App.from_json(client_api=self._client_api,
                                             _json=response.json(),
                                             project=self._project)
        else:
            app = entities.App.from_json(
                _json={
                    'id': app_id,
                    'name': app_name
                },
                client_api=self._client_api,
                project=self._project,
                is_fetched=False
            )
        assert isinstance(app, entities.App)
        return app

    def __get_by_name(self, name) -> entities.App:
        filters = entities.Filters(field='name',
                                   values=name,
                                   resource=entities.FiltersResource.APP,
                                   use_defaults=False)
        if self._project is not None:
            filters.add(field='projectId', values=self.project.id)
        apps = self.list(filters=filters)
        if apps.items_count == 0:
            raise exceptions.PlatformException(
                error='404',
                message='app not found. Name: {}'.format(name))
        elif apps.items_count > 1:
            raise exceptions.PlatformException(
                error='400',
                message='More than one app found by the name of: {} '.format(apps))
        return apps.items[0]

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.App]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.App._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item,
                                          'project': self._project})
        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        items = miscellaneous.List([r[1] for r in results if r[0] is True])
        return items

    def _list(self, filters: entities.Filters):
        url = '/apps/query'

        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def list(self, filters: entities.Filters = None, project_id: str = None) -> entities.PagedEntities:
        """
        List the available apps in the specified project.

        :param entities.Filters filters: the filters to apply to the list
        :param str project_id: the project id to apply thew filters on.
        :return a paged entity representing the list of apps.

        ** Example **
        .. code-block:: python
            apps = dl.apps.list(project_id='id')
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.APP)
            # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.APP:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.APP. Got: {!r}'.format(filters.resource))

        # noinspection DuplicatedCode
        if project_id is None and self._project is not None:
            project_id = self._project.id

        if project_id is not None:
            filters.add(field='projectId', values=project_id)

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       project_id=project_id,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def update(self, app: entities.App = None, app_id: str = None, wait: bool = True) -> bool:
        """
        Update the current app to the new configuration

        :param entities.App app: The app to update.
        :param str app_id: The app id to update.
        :param bool wait: wait for the operation to finish.
        :return bool whether the operation ran successfully or not

        **Example**
        .. code-block:: python
            succeed = dl.apps.update(app)
        """
        if app_id is not None and app is None:
            app = self.get(app_id=app_id)
        if app is None:
            raise exceptions.PlatformException(error='400', message='You must provide app or app_id')
        success, response = self._client_api.gen_request(req_type='put',
                                                         path=f"/apps/{app.id}",
                                                         json_req=app.to_json())
        if not success:
            raise exceptions.PlatformException(response)

        app = entities.App.from_json(
            _json=response.json(),
            client_api=self._client_api,
            project=self.project
        )
        if app.metadata:
            command_id = app.metadata.get('system', {}).get('commands', {}).get('update', None)
            if wait and app.status == entities.CompositionStatus.UPDATING and command_id is not None:
                command = self.commands.get(command_id=command_id, url='api/v1/commands/faas/{}'.format(command_id))
                command.wait()
                app = self.get(app_id=app.id)

        return success

    def install(self,
                dpk: entities.Dpk,
                app_name: str = None,
                organization_id: str = None,
                custom_installation: dict = None,
                scope: entities.AppScope = None,
                wait: bool = True,
                integrations: list = None
                ) -> entities.App:
        """
        Install the specified app in the project.

        Note: You must pass either the app_id or app_name
        :param entities.App dpk: the app entity
        :param str app_name: installed app name. default is the dpk name
        :param str organization_id: the organization which you want to apply on the filter.
        :param dict custom_installation: partial installation.
        :param str scope: the scope of the app. default is project.
        :param bool wait: wait for the operation to finish.
        :param list integrations: list of integrations to install with the app.

        :return the installed app.
        :rtype entities.App

        **Example**
        .. code-block:: python
            app = dl.apps.install(dpk=dpk)
        """
        if dpk is None:
            raise exceptions.PlatformException(error='400', message='You must provide an app')

        if app_name is None:
            app_name = dpk.display_name
        if isinstance(scope, entities.AppScope):
            scope = scope.value
        app = entities.App.from_json(_json={'name': app_name,
                                            'projectId': self.project.id,
                                            'orgId': organization_id,
                                            'dpkName': dpk.name,
                                            "customInstallation": custom_installation,
                                            'dpkVersion': dpk.version,
                                            'scope': scope,
                                            'integrations': integrations
                                            },
                                     client_api=self._client_api,
                                     project=self.project)
        success, response = self._client_api.gen_request(req_type='post',
                                                         path="/apps",
                                                         json_req=app.to_json())
        if not success:
            raise exceptions.PlatformException(response)
        app = entities.App.from_json(_json=response.json(),
                                     client_api=self._client_api,
                                     project=self.project)

        if app.metadata:
            command_id = app.metadata.get('system', {}).get('commands', {}).get('install', None)
            if wait and app.status == entities.CompositionStatus.INITIALIZING and command_id is not None:
                command = self.commands.get(command_id=command_id, url='api/v1/commands/faas/{}'.format(command_id))
                command.wait()
                app = self.get(app_id=app.id)

        return app

    def uninstall(self, app_id: str = None, app_name: str = None, wait: bool = True) -> bool:
        """
        Delete an app entity.

        Note: You are required to add either app_id or app_name.

        :param str app_id: optional - the id of the app.
        :param str app_name: optional - the name of the app.
        :param bool wait: optional - wait for the operation to finish.
        :return whether we succeed uninstalling the specified app.
        :rtype bool

        **Example**
        .. code-block:: python
            # succeed = dl.apps.delete(app_id='app_id')
        """
        if app_id is None and app_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='You must provide an identifier in inputs')
        if app_name is not None:
            app = self.__get_by_name(app_name)
            app_id = app.id

        success, response = self._client_api.gen_request(req_type='delete', path='/apps/{}'.format(app_id))
        if not success:
            raise exceptions.PlatformException(response)

        try:
            app = self.get(app_id=app_id)
        except Exception as e:
            if e.status_code == '404':
                return success
            else:
                raise e
        if app.metadata:
            command_id = app.metadata.get('system', {}).get('commands', {}).get('uninstall', None)
            if wait and app.status == entities.CompositionStatus.TERMINATING and command_id is not None:
                command = self.commands.get(command_id=command_id, url='api/v1/commands/faas/{}'.format(command_id))
                command.wait()

        logger.debug(f"App deleted successfully (id: {app_id}, name: {app_name}")
        return success

    def resume(self, app: entities.App = None, app_id: str = None) -> bool:
        """
        Activate an app entity.

        Note: You are required to add either app or app_id.

        :param entities.App app: the app entity
        :param str app_id: optional - the id of the app.
        :return whether we succeed activating the specified app.
        :rtype bool

        **Example**
        .. code-block:: python
            # succeed = dl.apps.resume(app)
        """
        if app_id is not None and app is None:
            app = self.get(app_id=app_id)

            if app and app.status == entities.CompositionStatus.INSTALLED:
                raise exceptions.PlatformException(
                    error='400',
                    message='Application is already active'
                )
        if app is None:
            raise exceptions.PlatformException(error='400', message='You must provide app or app_id')

        success, response = self._client_api.gen_request(req_type='post', path='/apps/{}/activate'.format(app.id))
        if not success:
            raise exceptions.PlatformException(response)

        logger.debug(f"App resumed successfully (id: {app.id}, name: {app.name}")
        return success

    def pause(self, app: entities.App = None, app_id: str = None) -> bool:
        """
        Pausing an app entity.

        Note: You are required to add either app or app_id.

        :param entities.App app: the app entity
        :param str app_id: optional - the id of the app.
        :return whether we succeed pausing the specified app.
        :rtype bool

        **Example**
        .. code-block:: python
            # succeed = dl.apps.pause(app)
        """
        if app_id is not None and app is None:
            app = self.get(app_id=app_id)

            if app and app.status == entities.CompositionStatus.UNINSTALLED:
                raise exceptions.PlatformException(
                    error='400',
                    message='Application is already inactive'
                )
        if app is None:
            raise exceptions.PlatformException(error='400', message='You must provide app or app_id')

        success, response = self._client_api.gen_request(req_type='post', path='/apps/{}/deactivate'.format(app.id))
        if not success:
            raise exceptions.PlatformException(response)

        logger.debug(f"App paused successfully (id: {app.id}, name: {app.name}")
        return success


================================================
File: dtlpy/repositories/artifacts.py
================================================
import tempfile
import requests
import logging
import shutil
import os

from .. import entities, miscellaneous, PlatformException, exceptions, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Artifacts:
    """
    Artifacts repository
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None,
                 dataset: entities.Dataset = None,
                 project_id: str = None,
                 model: entities.Model = None,
                 package: entities.Package = None,
                 dataset_name=None):
        self._client_api = client_api
        self._project = project
        self._dataset = dataset
        self._items_repository = None
        self.dataset_name = dataset_name
        self.project_id = project_id
        self.model = model
        self.package = package

    ############
    # entities #
    ############
    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            # get dataset from project
            try:
                if self.dataset_name is None:
                    self.dataset_name = 'Binaries'
                    self._dataset = self.project.datasets._get_binaries_dataset()
                else:
                    self._dataset = self.project.datasets.get(dataset_name=self.dataset_name)
            except exceptions.NotFound:
                raise ValueError(
                    f'Missing "{self.dataset_name}" dataset in the project. Please contact support for help')

        return self._dataset

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            if self._dataset is not None:
                self._project = self._dataset.project
            elif self.project_id is not None:
                self._project = repositories.Projects(client_api=self._client_api).get(project_id=self.project_id)
            else:
                raise PlatformException(error='400',
                                        message='Artifacts doesnt have a project, at least one must not be None: dataset, project or project_id')
        return self._project

    ################
    # repositories #
    ################
    @property
    def items_repository(self):
        if self._items_repository is None:
            # load Binaries dataset
            # load items repository
            self._items_repository = self.dataset.items
            self._items_repository.set_items_entity(entities.Artifact)
        return self._items_repository

    ###########
    # methods #
    ###########
    @staticmethod
    def _build_path_header(
            package_name=None,
            package=None,
            execution_id=None,
            execution=None,
            model_name=None,
    ):
        remote_path = '/artifacts'
        if package_name is not None or package is not None:
            if package is not None:
                package_name = package.name
            remote_path += '/packages/{}'.format(package_name)
        if execution_id is not None or execution is not None:
            if execution is not None:
                execution_id = execution.id
            remote_path += '/executions/{}'.format(execution_id)
        if model_name is not None:
            remote_path += '/models/{}'.format(model_name)

        return remote_path

    def list(self,
             execution_id: str = None,
             package_name: str = None,
             model_name: str = None) -> miscellaneous.List[entities.Artifact]:
        """
        List of artifacts

        :param str execution_id: list by execution id
        :param str package_name: list by package name
        :param str model_name: list by model name
        :return: list of artifacts
        :rtype: miscellaneous.List[dtlpy.entities.artifact.Artifact]

        **Example**:

        .. code-block:: python

            project.artifacts.list(package_name='package_name')
        """
        if self.model is not None:
            model_name = self.model.name
        if self.package is not None:
            package_name = self.package.name

        filters = entities.Filters()
        remote_path = self._build_path_header(
            package_name=package_name,
            execution_id=execution_id,
            model_name=model_name
        )

        remote_path += '/*'
        filters.add(field='filename', values=remote_path)
        pages = self.items_repository.list(filters=filters)
        items = [entities.Artifact.from_json(_json=item.to_json(),
                                             client_api=self._client_api,
                                             dataset=item._dataset,
                                             project=self.project)
                 for page in pages for item in page]
        return miscellaneous.List(items)

    def get(self,
            artifact_id: str = None,
            artifact_name: str = None,
            model_name: str = None,
            execution_id: str = None,
            package_name: str = None) -> entities.ItemArtifact:
        """

        Get an artifact object by name, id or type
        If by name or type - need to input also execution/task id for the artifact folder

        :param str artifact_id: search by artifact id
        :param str artifact_name: search by artifact id
        :param str model_name: search by model name
        :param str execution_id: search by execution id
        :param str package_name: search by  package name
        :return: Artifact object
        :rtype: dtlpy.entities.artifact.Artifact

        **Example**:

        .. code-block:: python

            project.artifacts.get(artifact_id='artifact_id')
        """
        if self.model is not None:
            model_name = self.model.name
        if self.package is not None:
            package_name = self.package.name

        if artifact_id is not None:
            artifact = self.items_repository.get(item_id=artifact_id)
            # verify input artifact name is same as the given id
            if artifact_name is not None and artifact.name != artifact_name:
                logger.warning(
                    "Mismatch found in artifacts.get: artifact_name is different then artifact.name:"
                    " {!r} != {!r}".format(
                        artifact_name,
                        artifact.name))
        elif artifact_name is not None:
            artifacts = self.list(
                execution_id=execution_id,
                package_name=package_name,
                model_name=model_name
            )
            artifact = [artifact for artifact in artifacts if artifact.name == artifact_name]
            if len(artifact) == 1:
                artifact = artifact[0]
            elif len(artifact) > 1:
                raise PlatformException('404', 'More Than one Artifact found')
            else:
                raise PlatformException('404', 'Artifact not found')
        else:
            msg = 'one input must be not None: artifact_id or artifact_name'
            raise ValueError(msg)
        return artifact

    def download(
            self,
            artifact_id: str = None,
            artifact_name: str = None,
            execution_id: str = None,
            package_name: str = None,
            model_name: str = None,
            local_path: str = None,
            overwrite: bool = False,
            save_locally: bool = True
    ):
        """

        Download artifact binary.
        Get artifact by name, id or type

        :param str artifact_id: search by artifact id
        :param str artifact_name: search by artifact id
        :param str execution_id: search by execution id
        :param str package_name: search by package name
        :param str model_name: search by model name
        :param str local_path: artifact will be saved to this filepath
        :param bool overwrite: optional - default = False
        :param bool save_locally: to save the file local
        :return: file path
        :rtype: str

        **Example**:

        .. code-block:: python

            project.artifacts.download(artifact_id='artifact_id',
                                        local_path='your_path',
                                        overwrite=True,
                                        save_locally=True)
        """
        if self.model is not None:
            model_name = self.model.name
        if self.package is not None:
            package_name = self.package.name

        if artifact_id is not None:
            # download
            artifact = self.items_repository.download(items=artifact_id,
                                                      save_locally=save_locally,
                                                      local_path=local_path,
                                                      overwrite=overwrite)
        elif artifact_name is not None:
            artifact_obj: entities.ItemArtifact = self.get(artifact_id=artifact_id,
                                                           execution_id=execution_id,
                                                           package_name=package_name,
                                                           artifact_name=artifact_name)

            artifact = artifact_obj.download(save_locally=save_locally,
                                             local_path=local_path,
                                             overwrite=overwrite)

        else:
            if self.model is not None:
                artifact = list()
                for m_artifact in self.model.model_artifacts:
                    if isinstance(m_artifact, entities.ItemArtifact):
                        if not m_artifact.is_fetched:
                            m_artifact = self.items_repository.get(item_id=m_artifact.id)
                        model_remote_root = m_artifact.filename.split('/')
                        model_remote_root = '/'.join(model_remote_root[:4])
                        # remove the prefix with relpath
                        local_dst = os.path.join(local_path,
                                                 os.path.relpath(m_artifact.filename, model_remote_root))
                        if not os.path.isfile(local_dst) or overwrite:
                            # need_to_download
                            # 1. download to temp folder
                            temp_dir = tempfile.mkdtemp()
                            local_temp_file = m_artifact.download(
                                local_path=temp_dir,
                                overwrite=overwrite,
                                to_items_folder=False,
                            )
                            src = local_temp_file
                            # remove the prefix with relpath
                            dst = local_dst
                            os.makedirs(os.path.dirname(dst), exist_ok=True)
                            shutil.move(src=src, dst=dst)
                            # clean temp dir
                            if os.path.isdir(temp_dir):
                                shutil.rmtree(temp_dir)
                        artifact.append(local_path)
                    elif isinstance(m_artifact, entities.LinkArtifact):
                        # remove the prefix with relpath
                        local_dst = os.path.join(local_path, m_artifact.filename)
                        if not os.path.isfile(local_dst) or overwrite:
                            # need_to_download
                            # 1. download to temp folder
                            temp_dir = tempfile.mkdtemp()
                            response = requests.get(m_artifact.url, stream=True)
                            local_temp_file = os.path.join(temp_dir, m_artifact.filename)
                            with open(local_temp_file, "wb") as handle:
                                for data in response.iter_content(chunk_size=8192):
                                    handle.write(data)
                            src = local_temp_file
                            # remove the prefix with relpath
                            dst = local_dst
                            os.makedirs(os.path.dirname(dst), exist_ok=True)
                            shutil.move(src=src, dst=dst)
                            # clean temp dir
                            if os.path.isdir(temp_dir):
                                shutil.rmtree(temp_dir)
                        artifact.append(local_path)
                    elif isinstance(m_artifact, entities.LocalArtifact):
                        pass
                    else:
                        raise ValueError('unsupported artifact type: {}'.format(type(m_artifact)))
            else:
                # for package artifacts - download using filter on the package directory
                if all(elem is None for elem in [package_name, execution_id]):
                    raise PlatformException(error='400', message='Must input package or execution (id or entity)')
                remote_path = self._build_path_header(
                    package_name=package_name,
                    execution_id=execution_id,
                    model_name=model_name,
                )
                without_relative_path = remote_path
                remote_path += '/*'
                filters = entities.Filters()
                filters.add(field='filename', values=remote_path)
                artifact = self.items_repository.download(filters=filters,
                                                          save_locally=save_locally,
                                                          local_path=local_path,
                                                          to_items_folder=False,
                                                          overwrite=overwrite,
                                                          without_relative_path=without_relative_path)
        return artifact

    def upload(self,
               # what to upload
               filepath: str,
               # where to upload
               package_name: str = None,
               package: entities.Package = None,
               execution_id: str = None,
               execution: entities.Execution = None,
               model_name: str = None,
               # add information
               overwrite: bool = False):
        """

        Upload binary file to artifact. get by name, id or type.
        If artifact exists - overwriting binary
        Else and if create==True a new artifact will be created and uploaded

        :param str filepath: local binary file
        :param str package_name: package name that include the artifact
        :param dtlpy.entities.package.Package package: package object
        :param str execution_id: execution id that include the artifact
        :param dtlpy.entities.execution.Execution execution: execution object
        :param str model_name: model name that include the artifact
        :param bool overwrite: optional - default = False to overwrite an existing object
        :return: Artifact Object
        :rtype: dtlpy.entities.artifact.Artifact

        **Example**:

        .. code-block:: python

            project.artifacts.upload(filepath='filepath',
                                    package_name='package_name')
        """
        if self.model is not None:
            model_name = self.model.name
        if self.package is not None:
            package_name = self.package.name

        remote_path = self._build_path_header(package_name=package_name,
                                              package=package,
                                              execution=execution,
                                              execution_id=execution_id,
                                              model_name=model_name)

        if all(elem is None for elem in [package_name, package, execution_id, execution, model_name]):
            raise ValueError('Must input package or execution (id or entity)')

        artifact = self.items_repository.upload(local_path=filepath,
                                                remote_path=remote_path,
                                                overwrite=overwrite,
                                                output_entity=entities.Artifact)
        if self.model is not None:
            # list and update model
            filters = entities.Filters()
            filters.add(field='dir', values=remote_path + '*')
            pages = self.items_repository.list(filters=filters)
            model_artifacts = list()
            for item in pages.all():
                model_artifacts.append(entities.Artifact.from_json(_json=item.to_json(),
                                                                   client_api=self._client_api,
                                                                   dataset=item._dataset))
            self.model.model_artifacts = model_artifacts
            self.model.update()
        logger.debug('Artifact uploaded successfully')
        return artifact

    def delete(self,
               artifact_id=None,
               artifact_name=None,
               execution_id=None,
               model_name=None,
               package_name=None):
        """
        Delete artifacts

        :param str artifact_id: search by artifact id
        :param str artifact_name: search by artifact id
        :param str execution_id: search by execution id
        :param str model_name: search by model name
        :param str package_name: search by package name
        :return: True if success
        :rtype: bool

         **Example**:

        .. code-block:: python

            project.artifacts.delete(artifact_id='artifact_id',
                                    package_name='package_name')
        """
        if self.model is not None:
            model_name = self.model.name
        if self.package is not None:
            package_name = self.package.name

        if artifact_id is not None or artifact_name is not None:
            artifacts = [
                self.get(
                    artifact_id=artifact_id,
                    artifact_name=artifact_name,
                    model_name=model_name,
                )
            ]
        elif execution_id is not None or package_name is not None:
            artifacts = self.list(
                execution_id=execution_id,
                package_name=package_name,
                model_name=model_name,
            )
        else:
            raise PlatformException('400',
                                    'Must provide one of: artifact_id, artifact_name, execution_id, package_name')

        values = [artifact.id for artifact in artifacts]
        self.items_repository.delete(filters=entities.Filters(field='id', values=values,
                                                              operator=entities.FiltersOperations.IN))

        return True


================================================
File: dtlpy/repositories/assignments.py
================================================
import logging

from .. import exceptions, miscellaneous, entities, repositories, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Assignments:
    """
    Assignments Repository

    The Assignments class allows users to manage assignments and their properties.
    Read more about `Task Assignment <https://developers.dataloop.ai/tutorials/task_workflows/create_a_task/chapter/>`_ in our Developers documentation.
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None,
                 task: entities.Task = None,
                 dataset: entities.Dataset = None,
                 project_id=None):
        self._client_api = client_api
        self._project = project
        self._dataset = dataset
        self._task = task

        self._project_id = project_id
        if self._project_id is None and self._project is not None:
            self._project_id = self._project.id

    ############
    # entities #
    ############
    @property
    def task(self) -> entities.Task:
        if self._task is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "task". need to set an Task entity or use task.assignments repository')
        assert isinstance(self._task, entities.Task)
        return self._task

    @task.setter
    def task(self, task: entities.Task):
        if not isinstance(task, entities.Task):
            raise ValueError('Must input a valid Task entity')
        self._task = task

    @property
    def project_id(self):
        if self._project_id is not None:
            return self._project_id
        elif self._project is not None:
            return self._project.id
        else:
            return None

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.assignments repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ###########
    # methods #
    ###########
    def list(self,
             project_ids: list = None,
             status: str = None,
             assignment_name: str = None,
             assignee_id: str = None,
             pages_size: int = None,
             page_offset: int = None,
             task_id: int = None
             ) -> miscellaneous.List[entities.Assignment]:
        """
        Get Assignment list to be able to use it in your code.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param list project_ids: search assignment by given list of project ids
        :param str status: search assignment by a given task status
        :param str assignment_name: search assignment by a given assignment name
        :param str assignee_id: the user email that assignee the assignment to it
        :param int pages_size: pages size of the output generator
        :param int page_offset: page offset of the output generator
        :param str task_id: search assignment by given task id
        :return: List of Assignment objects
        :rtype: miscellaneous.List[dtlpy.entities.assignment.Assignment]

        **Example**:

        .. code-block:: python

            assignments = task.assignments.list(status='complete', assignee_id='user@dataloop.ai', pages_size=100, page_offset=0)
        """

        # url
        url = '/assignments'

        query = list()
        if project_ids is not None:
            if not isinstance(project_ids, list):
                project_ids = [project_ids]
        elif self._project_id is not None:
            project_ids = [self._project_id]
        elif self._project is not None:
            project_ids = [self._project.id]
        else:
            raise exceptions.PlatformException(error='400', message='Must provide project')

        project_ids = ','.join(project_ids)
        query.append('projects={}'.format(project_ids))

        if status is not None:
            query.append('status={}'.format(status))
        if assignment_name is not None:
            query.append('name={}'.format(assignment_name))
        if assignee_id is not None:
            query.append('annotator={}'.format(assignee_id))
        if pages_size is not None:
            query.append('pageSize={}'.format(pages_size))
        if pages_size is None:
            query.append('pageSize={}'.format(500))
        if page_offset is not None:
            query.append('pageOffset={}'.format(page_offset))

        if task_id is None and self._task is not None:
            task_id = self._task.id
        if task_id is not None:
            query.append('taskId={}'.format(task_id))

        if len(query) > 0:
            query_string = '&'.join(query)
            url = '{}?{}'.format(url, query_string)

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)
        if success:
            assignments = miscellaneous.List(
                [entities.Assignment.from_json(client_api=self._client_api,
                                               _json=_json, project=self._project, dataset=self._dataset,
                                               task=self._task)
                 for _json in response.json()['items']])
        else:
            logger.error('Platform error getting assignments')
            raise exceptions.PlatformException(response)
        return assignments

    @_api_reference.add(path='/assignments/{id}', method='get')
    def get(self,
            assignment_name: str = None,
            assignment_id: str = None):
        """
        Get Assignment object to use it in your code.

        :param str assignment_name: optional - search by name
        :param str assignment_id: optional - search by id
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment

        **Example**:

        .. code-block:: python

            assignment = task.assignments.get(assignment_id='assignment_id')
        """

        if assignment_id is not None:
            url = '/assignments/{}'.format(assignment_id)
            success, response = self._client_api.gen_request(req_type='get',
                                                             path=url)
            if not success:
                raise exceptions.PlatformException('404', 'Assignment not found')
            else:
                assignment = entities.Assignment.from_json(_json=response.json(),
                                                           client_api=self._client_api,
                                                           project=self._project,
                                                           dataset=self._dataset,
                                                           task=self._task)
                # verify input assignment name is same as the given id
                if assignment_name is not None and assignment.name != assignment_name:
                    logger.warning(
                        "Mismatch found in assignments.get: assignment_name is different then assignment.name: "
                        "{!r} != {!r}".format(
                            assignment_name,
                            assignment.name))
        elif assignment_name is not None:
            assignments = [assignment for assignment in self.list() if assignment.name == assignment_name]
            if len(assignments) == 0:
                raise exceptions.PlatformException('404', 'Assignment not found')
            elif len(assignments) > 1:
                raise exceptions.PlatformException('404',
                                                   'More than one assignment exist with the same name: {}'.format(
                                                       assignment_name))
            else:
                assignment = assignments[0]
        else:
            raise exceptions.PlatformException('400', 'Must provide either assignment name or assignment id')

        assert isinstance(assignment, entities.Assignment)
        return assignment

    @property
    def platform_url(self):
        if self.task.id is None or self.project_id is None:
            raise ValueError("must have project and task")

        return self._client_api._get_resource_url(
            "projects/{}/tasks/{}/assignments".format(self.project_id, self.task.id))

    def open_in_web(self,
                    assignment_name: str = None,
                    assignment_id: str = None,
                    assignment: str = None):
        """
        Open the assignment in the platform.

        **Prerequisites**: All users.

        :param str assignment_name: the name of the assignment
        :param str assignment_id: the Id of the assignment
        :param dtlpy.entities.assignment.Assignment assignment: assignment object

        **Example**:

        .. code-block:: python

            task.assignments.open_in_web(assignment_id='assignment_id')
        """
        if assignment_name is not None:
            assignment = self.get(assignment_name=assignment_name)
        if assignment is not None:
            assignment.open_in_web()
        elif assignment_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(assignment_id))
        else:
            self._client_api._open_in_web(url=self.platform_url)

    @_api_reference.add(path='/assignments/{id}/reassign', method='post')
    def reassign(self,
                 assignee_id: str,
                 assignment: entities.Assignment = None,
                 assignment_id: str = None,
                 task: entities.Task = None,
                 task_id: str = None,
                 wait: bool = True):
        """
        Reassign an assignment.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param str assignee_id: the email of the user that want to assign the assignment
        :param dtlpy.entities.assignment.Assignment assignment: assignment object
        :param assignment_id: the Id of the assignment
        :param dtlpy.entities.task.Task task: task object
        :param str task_id: the Id of the task that include the assignment
        :param bool wait: wait until reassign assignment finish
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment

        **Example**:

        .. code-block:: python

            assignment = task.assignments.reassign(assignee_ids='annotator1@dataloop.ai')
        """
        if assignment_id is None and assignment is None:
            raise exceptions.PlatformException('400', 'Must provide either assignment or assignment_id')
        elif assignment_id is None:
            assignment_id = assignment.id

        if task_id is None and task is None:
            raise exceptions.PlatformException('400', 'Must provide either task or task_id')
        elif task_id is None:
            task_id = task.id

        url = '/assignments/{}/reassign'.format(assignment_id)

        payload = {
            'taskId': task_id,
            'annotator': assignee_id,
            'asynced': wait
        }

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=payload)
        if success:
            command = entities.Command.from_json(_json=response.json(),
                                                 client_api=self._client_api)
            if not wait:
                return command
            command = command.wait(timeout=0)
            if 'toAssignment' not in command.spec:
                raise exceptions.PlatformException(error='400',
                                                   message="'toAssignment' key is missing in command response: {}"
                                                   .format(response))
            return self.get(assignment_id=command.spec['toAssignment'])
        else:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/assignments/{id}/redistribute', method='post')
    def redistribute(self,
                     workload: entities.Workload,
                     assignment: entities.Assignment = None,
                     assignment_id: str = None,
                     task: entities.Task = None,
                     task_id: str = None,
                     wait: bool = True):
        """
        Redistribute an assignment.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        **Example**:

        :param dtlpy.entities.assignment.Workload workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param dtlpy.entities.assignment.Assignment assignment: assignment object
        :param str assignment_id: the Id of the assignment
        :param dtlpy.entities.task.Task task: the task object that include the assignment
        :param str task_id: the Id of the task that include the assignment
        :param bool wait: wait until redistribute assignment finish
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment assignment

        .. code-block:: python

            assignment = task.assignments.redistribute(workload=dl.Workload([dl.WorkloadUnit(assignee_id="annotator1@dataloop.ai", load=50),
                                                                dl.WorkloadUnit(assignee_id="annotator2@dataloop.ai", load=50)]))
        """
        if assignment_id is None and assignment is None:
            raise exceptions.PlatformException('400', 'Must provide either assignment or assignment_id')
        elif assignment_id is None:
            assignment_id = assignment.id

        if task_id is None and task is None:
            raise exceptions.PlatformException('400', 'Must provide either task or task_id')
        elif task_id is None:
            task_id = task.id

        url = '/assignments/{}/redistribute'.format(assignment_id)

        payload = {
            'taskId': task_id,
            'workload': workload.to_json(),
            'asynced': wait
        }

        if self._task is None:
            self._task = self.get(assignment_id=assignment_id).task
        if task is None:
            task = self._task

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=payload)
        if success:
            command = entities.Command.from_json(_json=response.json(),
                                                 client_api=self._client_api)
            if not wait:
                return command
            command = command.wait(timeout=0)
            if 'workload' not in command.spec:
                raise exceptions.PlatformException(error='400',
                                                   message="workload key is missing in command response: {}"
                                                   .format(response))

            task_assignments = task.assignments.list()
            workers = list()
            for worker in workload:
                workers.append(worker.assignee_id.lower())

            redistributed_assignments = list()
            for ass in task_assignments:
                if ass.annotator in workers:
                    redistributed_assignments.append(ass)
                    workers.remove(ass.annotator)
                    if not workers:
                        break

            return miscellaneous.List(redistributed_assignments)
        else:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/assignments/{id}', method='patch')
    def update(self,
               assignment: entities.Assignment = None,
               system_metadata: bool = False
               ) -> entities.Assignment:
        """
        Update an assignment.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param dtlpy.entities.assignment.Assignment assignment assignment: assignment entity
        :param bool system_metadata: True, if you want to change metadata system
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment assignment

        **Example**:

        .. code-block:: python

            assignment = task.assignments.update(assignment='assignment_entity', system_metadata=False)
        """
        url = '/assignments/{}'.format(assignment.id)

        if system_metadata:
            url += '?system=true'

        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url,
                                                         json_req=assignment.to_json())
        if success:
            return entities.Assignment.from_json(_json=response.json(),
                                                 client_api=self._client_api, project=self._project,
                                                 dataset=self._dataset, task=self._task)
        else:
            raise exceptions.PlatformException(response)

    def create(self,
               assignee_id: str,
               task: entities.Task = None,
               filters: entities.Filters = None,
               items: list = None) -> entities.Assignment:
        """
        Create a new assignment.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param str assignee_id: the email of the user that want to assign the assignment
        :param dtlpy.entities.task.Task task: the task object that include the assignment
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param list items: list of items (item Id or objects) to insert to the assignment
        :return: Assignment object
        :rtype: dtlpy.entities.assignment.Assignment assignment

        **Example**:

        .. code-block:: python

            assignment = task.assignments.create(assignee_id='annotator1@dataloop.ai')
        """
        return self._create_in_task(assignee_id=assignee_id, task=task, filters=filters, items=items)

    def _create_in_task(self, assignee_id, task, filters=None, items=None) -> entities.Assignment:

        if task is None:
            if self._task is None:
                raise exceptions.PlatformException('400', 'Must provide task')
            task = self._task

        assignments_before = [ass.id for ass in task.assignments.list()]

        if filters is None and items is None:
            raise exceptions.PlatformException('400', 'Must provide either filters or items list')

        workload = entities.Workload.generate(assignee_ids=[assignee_id])
        task = task.add_items(filters=filters, items=items, workload=workload, limit=None)
        assignments = [ass for ass in task.assignments.list() if ass.id not in assignments_before]

        if len(assignments) < 1:
            raise exceptions.PlatformException('Error creating an assignment, '
                                               'Please use task.add_items() to perform this action')

        return assignments[0]

    def __item_operations(self, dataset: entities.Dataset,
                          op, assignment_id=None, assignment_name=None, filters=None, items=None):
        if assignment_id is None and assignment_name is None:
            raise exceptions.PlatformException('400', 'Must provide either assignment name or assignment id')
        elif assignment_id is None:
            assignment_id = self.get(assignment_name=assignment_name).id

        try:
            if filters is None and items is None:
                raise exceptions.PlatformException('400', 'Must provide either filters or items list')

            if filters is None:
                if not isinstance(items, list):
                    items = [items]
                filters = entities.Filters(field='id',
                                           values=[item.id for item in items],
                                           operator=entities.FiltersOperations.IN,
                                           use_defaults=False)

            filters._ref_assignment = True
            filters._ref_assignment_id = assignment_id
            filters._ref_op = op

            return dataset.items.update(filters=filters)
        finally:
            if filters is not None:
                filters._nullify_refs()

    def get_items(self,
                  assignment: entities.Assignment = None,
                  assignment_id=None,
                  assignment_name=None,
                  dataset=None,
                  filters=None
                  ) -> entities.PagedEntities:
        """
        Get all the items in the assignment.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param dtlpy.entities.assignment.Assignment assignment: assignment object
        :param assignment_id: the Id of the assignment
        :param str assignment_name: the name of the assignment
        :param dtlpy.entities.dataset.Dataset dataset: dataset object, the dataset that refer to the assignment
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: pages of the items
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            items = task.assignments.get_items(assignment_id='assignment_id')
        """
        if assignment is None and assignment_id is None and assignment_name is None:
            raise exceptions.PlatformException('400',
                                               'Please provide either assignment,  assignment_id or assignment_name')

        if assignment_id is None:
            if assignment is None:
                assignment = self.get(assignment_name=assignment_name)
            assignment_id = assignment.id

        if dataset is None and self._dataset is None:
            if assignment is None:
                assignment = self.get(assignment_id=assignment_id, assignment_name=assignment_name)
            if assignment.dataset_id is None:
                raise exceptions.PlatformException('400', 'Please provide a dataset entity')
            dataset = repositories.Datasets(client_api=self._client_api, project=self._project).get(
                dataset_id=assignment.dataset_id)
        elif dataset is None:
            dataset = self._dataset

        if filters is None:
            filters = entities.Filters(use_defaults=False)
        filters.add(field='metadata.system.refs.id', values=[assignment_id], operator=entities.FiltersOperations.IN)

        return dataset.items.list(filters=filters)

    def set_status(self,
                   status: str,
                   operation: str,
                   item_id: str,
                   assignment_id: str
                   ) -> bool:
        """
        Set item status within assignment.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned as *owner* of the annotation task.

        :param str status: string the describes the status
        :param str operation: the status action need 'create' or 'delete'
        :param str item_id: item id that want to set his status
        :param assignment_id: the Id of the assignment
        :return: True id success
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = task.assignments.set_status(assignment_id='assignment_id',
                                        status='complete',
                                        operation='created',
                                        item_id='item_id')
        """
        url = '/assignments/{assignment_id}/items/{item_id}/status'.format(assignment_id=assignment_id, item_id=item_id)
        payload = {
            'operation': operation,
            'status': status
        }
        success, response = self._client_api.gen_request(
            req_type='post',
            path=url,
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

        return True


================================================
File: dtlpy/repositories/bots.py
================================================
import logging
from .. import entities, miscellaneous, exceptions
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Bots:
    """
    Bots Repository

    The Bots class allows the user to manage bots and their properties.
    """

    def __init__(self, client_api: ApiClient, project: entities.Project):
        self._client_api = client_api
        self._project = project

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.bots repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ###########
    # methods #
    ###########
    def list(self) -> miscellaneous.List[entities.Bot]:
        """
        Get a project or package bots list.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :return: List of Bots objects
        :rtype: list

        **Example**:

        .. code-block:: python

            bots_list = service.bots.list()

        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/projects/{}/bots'.format(self.project.id))

        if success:
            bots_json = response.json()
            pool = self._client_api.thread_pools(pool_name='entity.create')
            jobs = [None for _ in range(len(bots_json))]
            # return triggers list
            for i_bot, bot in enumerate(bots_json):
                jobs[i_bot] = pool.submit(entities.Bot._protected_from_json,
                                          **{'project': self.project,
                                             'bots': self,
                                             'client_api': self._client_api,
                                             '_json': bot})

            # get all results
            results = [j.result() for j in jobs]
            # log errors
            _ = [logger.warning(r[1]) for r in results if r[0] is False]
            # return good jobs
            bots = miscellaneous.List([r[1] for r in results if r[0] is True])
        else:
            logger.error('Platform error getting bots')
            raise exceptions.PlatformException(response)
        return bots

    def get(self,
            bot_email: str = None,
            bot_id: str = None,
            bot_name: str = None):
        """
        Get a Bot object.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str bot_email: get bot by email
        :param str bot_id: get bot by id
        :param str bot_name: get bot by name
        :return: Bot object
        :rtype: dtlpy.entities.bot.Bot

        **Example**:

        .. code-block:: python

            service.bots.get(bot_id='bot_id')

        """
        if bot_id is None:
            if bot_name is not None:
                bots = self.list()
                bot = [bot for bot in bots if bot.name == bot_name]

            elif bot_email is not None:
                bots = self.list()
                bot = [bot for bot in bots if bot.email == bot_email]
            else:
                raise exceptions.PlatformException('400', 'Must choose by "bot_id" or "bot_name"')
            if not bot:
                # list is empty
                raise exceptions.PlatformException('404', 'Bot not found. Name: {}'.format(bot_email))
                # project = None
            elif len(bot) > 1:
                # more than one matching project
                raise exceptions.PlatformException('404', 'More than one bot with same name. Please "get" by id')
            else:
                bot_id = bot[0].id

        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/projects/{}/bots/{}'.format(self.project.id,
                                                                                            bot_id))
        if success:
            bot = entities.Bot.from_json(_json=response.json(),
                                         project=self.project,
                                         bots=self, client_api=self._client_api)
            # verify input bot name and bot email are same as the given id
            if bot_name is not None and bot.name != bot_name:
                logger.warning(
                    "Mismatch found in bots.get: bot_name is different then bot.name: "
                    "{!r} != {!r}".format(
                        bot_name,
                        bot.name))
            if bot_email is not None and bot.email != bot_email:
                logger.warning(
                    "Mismatch found in bots.get: bot_email is different then bot.email: "
                    "{!r} != {!r}".format(
                        bot_email,
                        bot.email))
        else:
            raise exceptions.PlatformException(response)

        assert isinstance(bot, entities.Bot)
        return bot

    def delete(self,
               bot_id: str = None,
               bot_email: str = None):
        """
        Delete a Bot.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        You must provide at least ONE of the following params: bot_id, bot_email

        :param str bot_id: bot id to delete
        :param str bot_email: bot email to delete
        :return: True if successful
        :rtype: bool

        **Example**:

        .. code-block:: python

            service.bots.delete(bot_id='bot_id')

        """
        if bot_id is None:
            if bot_email is None:
                raise exceptions.PlatformException(error='400',
                                                   message='must input one of bot_id or bot_email to delete')
            bot = self.get(bot_email=bot_email)
            bot_id = bot.id
        success, response = self._client_api.gen_request(req_type='delete',
                                                         path='/projects/{}/bots/{}'.format(self.project.id,
                                                                                            bot_id))
        if not success:
            raise exceptions.PlatformException(response)
        logger.info('Bot {} deleted successfully'.format(bot_id))
        return True

    def create(self,
               name: str,
               return_credentials: bool = False):
        """
        Create a new Bot.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str name: bot name
        :param str return_credentials: True will return the password when created
        :return: Bot object
        :rtype: dtlpy.entities.bot.Bot

        **Example**:

        .. code-block:: python

            service.bots.delete(name='bot', return_credentials=False)
        """
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects/{}/bots'.format(self.project.id),
                                                         json_req={'name': name,
                                                                   'returnCredentials': return_credentials})
        if success:
            bot = entities.Bot.from_json(_json=response.json(),
                                         project=self.project,
                                         bots=self, client_api=self._client_api)
        else:
            raise exceptions.PlatformException(response)
        assert isinstance(bot, entities.Bot)
        return bot


================================================
File: dtlpy/repositories/codebases.py
================================================
import hashlib
import logging
import os
import io
import random
from typing import List

from .. import entities, PlatformException, exceptions, repositories, miscellaneous
from ..services.api_client import ApiClient
from ..services.api_client import client as client_api

logger = logging.getLogger(name='dtlpy')


class Codebases:
    """
    Codebase Repository

    The Codebases class allows the user to manage codebases and their properties.
    The codebase is the code the user uploads for the user's packages to run.
    Read more about `codebase <https://developers.dataloop.ai/tutorials/faas/introduction/chapter/>`_ in our FaaS (function as a service).
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None,
                 dataset: entities.Dataset = None,
                 project_id: str = None):
        self._client_api = client_api
        self._project = project
        self._project_id = project_id
        self._dataset = dataset
        self._items_repository = None
        self.git_utils = miscellaneous.GitUtils()

    @property
    def items_repository(self) -> repositories.Items:
        if self._items_repository is None:
            if self._dataset is not None or self._project is not None:
                self._items_repository = self.dataset.items
            else:
                self._items_repository = repositories.Items(client_api=client_api)
        assert isinstance(self._items_repository, repositories.Items)
        return self._items_repository

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            # project is None - try dataset
            if self._dataset is None:
                # dataset is None - try from project id
                if self._project_id is None:
                    raise PlatformException(error='400',
                                            message='Cannot perform this action without a project')
                else:
                    self._project = repositories.Projects(client_api=self._client_api).get(project_id=self._project_id)
            else:
                # dataset is not None - take project from there
                self._project = self.dataset.project
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            # get dataset from project
            try:
                self._dataset = self.project.datasets._get_binaries_dataset()
            except exceptions.NotFound:
                raise ValueError('Missing "Binaries" dataset in the project. Please contact support for help')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @dataset.setter
    def dataset(self, dataset: entities.Dataset):
        if not isinstance(dataset, entities.Dataset):
            raise ValueError('Must input a valid Dataset entity')
        self._dataset = dataset

    @staticmethod
    def __file_hash(filepath):
        m = hashlib.md5()
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                m.update(chunk)
        return m.hexdigest()

    def list_versions(self, codebase_name: str) -> entities.PagedEntities:
        """
        List all codebase versions.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        **Example**:

        .. code-block:: python

            package.codebases.list_versions(codebase_name='codebase_name')

        :param str codebase_name: code base name
        :return: list of versions
        :rtype: list
        """
        filters = entities.Filters()
        filters.add(field='filename', values='/codebases/{}/*'.format(codebase_name))
        versions = self.items_repository.list(filters=filters)
        return versions

    def list(self) -> entities.PagedEntities:
        """
        List all codebases.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        **Example**:

        .. code-block:: python

            package.codebases.list()

        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities
        """
        filters = entities.Filters()
        filters.add(field='filename', values='/codebases/*')
        filters.add(field='type', values='dir')
        codebases = self.items_repository.list(filters=filters)
        return codebases

    def get(self,
            codebase_name: str = None,
            codebase_id: str = None,
            version: str = None):
        """
        Get a Codebase object to use in your code.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        **Example**:

        .. code-block:: python

            package.codebases.get(codebase_name='codebase_name')

        :param str codebase_name: optional - search by name
        :param str codebase_id: optional - search by id
        :param str version: codebase version. default is latest. options: "all", "latest" or ver number - "10"
        :return: Codebase object
        """
        if codebase_id is not None:
            matched_version = self.items_repository.get(item_id=codebase_id)
            # verify input codebase name is same as the given id
            if codebase_name is not None and matched_version.name != codebase_name:
                logger.warning(
                    "Mismatch found in codebases.get: codebase_name is different then codebase.name: "
                    "{!r} != {!r}".format(
                        codebase_name,
                        matched_version.name))
            codebase = entities.Codebase(type='item',
                                         client_api=self._client_api,
                                         itemId=matched_version.id,
                                         item=matched_version)
            return codebase

        if codebase_name is None:
            raise PlatformException(error='400', message='Either "codebase_name" or "codebase_id" is needed')
        if version is None:
            version = 'latest'

        if version not in ['all', 'latest']:
            try:
                matched_version = self.items_repository.get(
                    filepath='/codebases/{}/{}.zip'.format(codebase_name, version))
            except Exception:
                raise PlatformException(error='404',
                                        message='No matching version was found. version: {}'.format(version))
            codebase = entities.Codebase(type='item',
                                         client_api=self._client_api,
                                         itemId=matched_version.id,
                                         item=matched_version)
            return codebase

        # get all or latest
        versions_pages = self.list_versions(codebase_name=codebase_name)
        if versions_pages.items_count == 0:
            raise PlatformException(error='404', message='No codebase was found. name: {}'.format(codebase_name))
        else:
            if version == 'all':
                codebase = [entities.Codebase(type='item',
                                              client_api=self._client_api,
                                              itemId=mv.id,
                                              item=mv) for mv in versions_pages.all()]
            elif version == 'latest':
                max_ver = -1
                matched_version = None
                for page in versions_pages:
                    for ver in page:
                        if ver.type == 'dir':
                            continue
                        # extract version from filepath
                        ver_int = int(os.path.splitext(ver.name)[0])
                        if ver_int > max_ver:
                            max_ver = ver_int
                            matched_version = ver
                if matched_version is None:
                    raise PlatformException(error='404',
                                            message='No codebase was found. name: {}'.format(codebase_name))
                else:
                    codebase = entities.Codebase(type='item',
                                                 client_api=self._client_api,
                                                 itemId=matched_version.id,
                                                 item=matched_version)
            else:
                raise PlatformException(error='404', message='Unknown version string: {}'.format(version))

        return codebase

    @staticmethod
    def get_current_version(all_versions_pages, zip_md):
        """
        This method returns the current version of the codebase and other versions found.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        :param codebase all_versions_pages: codebase object
        :param zip_md: zipped file of codebase
        :return: current version and all versions found of codebase
        :rtype: int, int

        **Example**:

        .. code-block:: python

            package.codebases.get_current_version(all_versions_pages='codebase_entity', zip_md='path')
        """
        latest_version = 0
        same_version_found = None
        # go over all existing versions
        for v_item in all_versions_pages:
            # get latest version
            if int(os.path.splitext(v_item.item.name)[0]) > latest_version:
                latest_version = int(os.path.splitext(v_item.item.name)[0])
            # check md5 to find same codebase
            if 'md5' in v_item.item.metadata['system'] and v_item.item.metadata['system']['md5'] == zip_md:
                same_version_found = v_item
                break
        return latest_version + 1, same_version_found

    def pack(self,
             directory: str,
             name: str = None,
             extension: str = 'zip',
             description: str = '',
             ignore_directories: List[str] = None,
             ignore_max_file_size: bool = False):
        """
        Zip a local code directory and post to codebases.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        :param str directory: local directory to pack
        :param str name: codebase name
        :param str extension: the extension of the file
        :param str description: codebase description
        :param list[str] ignore_directories: directories to ignore.
        :return: Codebase object
        :rtype: dtlpy.entities.codebase.Codebase

        **Example**:

        .. code-block:: python

            package.codebases.pack(directory='path_dir', name='codebase_name')
        """
        # create/get .dataloop dir
        cwd = os.getcwd()
        dl_dir = os.path.join(cwd, '.dataloop')
        if not os.path.isdir(dl_dir):
            os.mkdir(dl_dir)

        # get codebase name
        if name is None:
            name = os.path.basename(directory)

        # create/get dist folder
        zip_filename = os.path.join(dl_dir, '{}_{}.{}'.format(name, str(random.randrange(0, 1000)), extension))

        try:
            if not os.path.isdir(directory):
                raise PlatformException(error='400', message='Not a directory: {}'.format(directory))
            directory = os.path.abspath(directory)

            # create zipfile
            miscellaneous.Zipping.zip_directory(zip_filename=zip_filename,
                                                directory=directory,
                                                ignore_directories=ignore_directories,
                                                ignore_max_file_size=ignore_max_file_size)
            zip_md = self.__file_hash(zip_filename)

            # get latest version
            same_version_found = None
            try:
                all_versions_pages = self.get(codebase_name=name, version='all')
            except exceptions.NotFound:
                all_versions_pages = None
            if all_versions_pages is None:
                # no codebase with that name - create new version
                current_version = 0
            else:
                current_version, same_version_found = self.get_current_version(all_versions_pages=all_versions_pages,
                                                                               zip_md=zip_md)

            if same_version_found is not None:
                # same md5 hash file found in version - return the matched version
                codebase = same_version_found
            else:
                # no matched version was found - create a new version
                # read from zipped file
                with open(zip_filename, 'rb') as f:
                    buffer = io.BytesIO(f.read())
                    buffer.name = '{}.{}'.format(str(current_version), extension)

                # upload item
                item = self.items_repository.upload(local_path=buffer,
                                                    remote_path='/codebases/{}'.format(name))
                if isinstance(item, list) and len(item) == 0:
                    raise PlatformException(error='400', message='Failed upload codebase, check log file for details')

                # add source code to metadata
                if 'system' not in item.metadata:
                    item.metadata['system'] = dict()
                item.metadata['system']['description'] = description
                item.metadata['system']['md5'] = zip_md

                # add git info to metadata
                if miscellaneous.GitUtils.is_git_repo(path=directory):
                    # create 'git' field in metadata
                    if 'git' not in item.metadata:
                        item.metadata['git'] = dict()

                    # add to metadata
                    item.metadata['git']['status'] = miscellaneous.GitUtils.git_status(path=directory)
                    item.metadata['git']['log'] = miscellaneous.GitUtils.git_log(path=directory)
                    item.metadata['git']['url'] = miscellaneous.GitUtils.git_url(path=directory)

                # update item
                item = self.items_repository.update(item=item, system_metadata=True)
                codebase = entities.Codebase(type='item',
                                             client_api=self._client_api,
                                             itemId=item.id,
                                             item=item)
        except Exception:
            logger.error('Error when packing:')
            raise
        finally:
            # cleanup
            if zip_filename is not None:
                if os.path.isfile(zip_filename):
                    os.remove(zip_filename)
        return codebase

    def _unpack_single(self,
                       codebase,
                       download_path: str,
                       local_path: str):
        """
        :param dtlpy.entities.codebase.Codebase codebase: codebase object
        :param str download_path:
        :param str local_path:
        """
        # downloading with specific filename
        if isinstance(codebase, entities.ItemCodebase):
            artifact_filepath = self.items_repository.download(items=codebase.item_id,
                                                               save_locally=True,
                                                               local_path=os.path.join(download_path,
                                                                                       codebase.item.name),
                                                               to_items_folder=False)
            if not os.path.isfile(artifact_filepath):
                raise PlatformException(error='404',
                                        message='error downloading codebase. see above for more information')
            miscellaneous.Zipping.unzip_directory(zip_filename=artifact_filepath,
                                                  to_directory=local_path)
            os.remove(artifact_filepath)
            logger.info('Source code was unpacked to: {}'.format(artifact_filepath))
        elif isinstance(codebase, entities.Item):
            artifact_filepath = codebase.download(save_locally=True,
                                                  local_path=os.path.join(download_path,
                                                                          codebase.name),
                                                  to_items_folder=False)
            if not os.path.isfile(artifact_filepath):
                raise PlatformException(error='404',
                                        message='error downloading codebase. see above for more information')
            miscellaneous.Zipping.unzip_directory(zip_filename=artifact_filepath,
                                                  to_directory=local_path)
            os.remove(artifact_filepath)
            logger.info('Source code was unpacked to: {}'.format(artifact_filepath))
        elif isinstance(codebase, entities.GitCodebase):
            if codebase.is_git_repo(local_path) or \
                    codebase.is_git_repo(os.path.join(local_path, codebase.git_repo_name)):
                artifact_filepath = self.pull_git(codebase=codebase, local_path=local_path)
            else:  # Clone the repo if not exist
                artifact_filepath = self.clone_git(codebase=codebase, local_path=local_path)
        else:
            raise ValueError('Not implemented: "_unpack_single" for codebase type: {!r}'.format(codebase.type))
        return artifact_filepath

    def _get_single(self, codebase: entities.GitCodebase, name: str):
        value = None
        integration_id = codebase.credentials.get(name, {}).get('id', None)
        key = codebase.credentials.get(name, {}).get('key', None)
        if integration_id is not None:
            try:
                key = self.project.integrations.get(integrations_id=integration_id).name
            except Exception:
                pass
            value = os.environ.get(key, None)

        return value

    def _get_credential(self, codebase: entities.GitCodebase):
        username = password = None

        if codebase.credentials:
            try:
                username = self._get_single(codebase=codebase, name='username')
                password = self._get_single(codebase=codebase, name='password')
            except Exception:
                logger.exception('Failed to get credentials from codebase')

        return username, password

    def clone_git(self,
                  codebase: entities.Codebase,
                  local_path: str):
        """
        Clone code base

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        :param dtlpy.entities.codebase.Codebase codebase: codebase object
        :param str local_path: local path
        :return: path where the clone will be
        :rtype: str

         **Example**:

        .. code-block:: python

            package.codebases.clone_git(codebase='codebase_entity', local_path='local_path')
        """
        if not isinstance(codebase, entities.GitCodebase):
            raise RuntimeError('only support Git Codebase')
        username, password = self._get_credential(codebase=codebase)
        response = self.git_utils.git_clone(path=local_path,
                                            git_url=codebase.git_url,
                                            tag=codebase.git_tag,
                                            username=username,
                                            password=password)
        if response:
            logger.info('Source code was cloned from {}(Git) to: {}'.format(codebase.git_url, local_path))
        else:
            raise RuntimeError('Failed cloning. See above for full log. codebase: {}'.format(codebase))
        return local_path

    def pull_git(self, codebase, local_path):
        """
        Pull (download) a codebase.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        :param dtlpy.entities.codebase.Codebase codebase: codebase object
        :param str local_path: local path
        :return: path where the Pull will be
        :rtype: str

        **Example**:

        .. code-block:: python

            package.codebases.pull_git(codebase='codebase_entity', local_path='local_path')
        """
        pull_cmd = 'git pull'
        if not codebase.is_git_repo(local_path):
            local_path = os.path.join(local_path, codebase.git_repo_name)
        response = self.git_utils.git_command(path=local_path, cmd=pull_cmd)
        if response:
            logger.info('pull successful {}(Git) to: {}'.format(codebase.git_url, os.path.dirname(local_path)))
        else:
            logger.critical("Could not pull")

        # we can test if this is not the same repo if needed...
        # FIXME need to change the order - checkout new branch and pull
        response_2 = self.git_utils.git_command(path=local_path, cmd='git checkout {}'.format(codebase.git_tag))
        return local_path

    def unpack(self,
               codebase: entities.Codebase = None,
               codebase_name: str = None,
               codebase_id: str = None,
               local_path: str = None,
               version: str = None):
        """
        Unpack codebase locally. Download source code and unzip.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.

        :param dtlpy.entities.codebase.Codebase codebase: `dl.Codebase` object
        :param str codebase_name: search by name
        :param str codebase_id: search by id
        :param str local_path: local path to save codebase
        :param str version: codebase version to unpack. default - latest
        :return: String (dirpath)
        :rtype: str

        **Example**:

        .. code-block:: python

            package.codebases.unpack(codebase='codebase_entity', local_path='local_path')
        """
        # get the codebase / multiple codebase
        if codebase is None:
            codebase = self.get(codebase_name=codebase_name,
                                codebase_id=codebase_id,
                                version=version)
        elif codebase_name is not None or codebase_id is not None:
            logger.warning("Using given codebase. Does not preforming search with name {!r} / id {!r}".
                           format(codebase_name, codebase_id))
        download_path = local_path
        if isinstance(codebase, entities.PagedEntities):
            for page in codebase:
                for item in page:
                    local_path = os.path.join(download_path, 'v.' + item.name.split('.')[0])
                    self._unpack_single(codebase=item,
                                        download_path=download_path,
                                        local_path=local_path)
            return os.path.dirname(local_path)
        elif isinstance(codebase, list):
            for item in codebase:
                local_path = os.path.join(download_path, 'v.' + item.item.name.split('.')[0])
                self._unpack_single(codebase=item,
                                    download_path=download_path,
                                    local_path=local_path)
            return os.path.dirname(local_path)
        elif isinstance(codebase, (entities.ItemCodebase, entities.Item, entities.GitCodebase)):
            artifact_filepath = self._unpack_single(codebase=codebase,
                                                    download_path=download_path,
                                                    local_path=local_path)
            if isinstance(codebase, (entities.ItemCodebase, entities.Item)):
                dir_path = os.path.dirname(artifact_filepath)  # use the directory of the artifact
            else:
                dir_path = artifact_filepath
            logger.info('Source code was unpacked to: {}'.format(dir_path))
        else:
            raise PlatformException(
                error='404',
                message='Codebase was not found! name:{name}, id:{id}'.format(name=codebase_name,
                                                                              id=codebase_id))
        return dir_path


================================================
File: dtlpy/repositories/collections.py
================================================
from venv import logger
from dtlpy import entities, exceptions, repositories
from dtlpy.entities.dataset import Dataset
from dtlpy.entities.filters import FiltersMethod
from dtlpy.services.api_client import ApiClient
from typing import List

class Collections:
    def __init__(self, 
                client_api: ApiClient,
                item: entities.Item = None, 
                dataset: entities.Dataset = None
            ):
        self._client_api = client_api
        self._dataset = dataset
        self._item = item

    def create(self, name: str) -> entities.Collection:
        """
        Creates a new collection in the dataset.

        :param name: The name of the new collection.
        :return: The created collection details.
        """
        dataset_id = self._dataset.id
        self.validate_max_collections()
        self.validate_collection_name(name)
        payload = {"name": name}
        success, response = self._client_api.gen_request(
            req_type="post", path=f"/datasets/{dataset_id}/items/collections", json_req=payload
        )
        if success:
            collection_json = self._single_collection(data=response.json(), name=name)
            return entities.Collection.from_json(client_api=self._client_api, _json=collection_json)
        else:
            raise exceptions.PlatformException(response) 

    def update(self, collection_name: str, new_name: str) -> entities.Collection:
        """
        Updates the name of an existing collection.

        :param collection_id: The ID of the collection to update.
        :param new_name: The new name for the collection.
        :return: The updated collection details.
        """
        dataset_id = self._dataset.id
        self.validate_collection_name(new_name)
        payload = {"name": new_name}
        success, response = self._client_api.gen_request(
            req_type="patch", path=f"/datasets/{dataset_id}/items/collections/{collection_name}", json_req=payload
        )
        if success:
            collection_json = self._single_collection(data=response.json(), name=new_name)
            return entities.Collection.from_json(client_api=self._client_api, _json=collection_json)
        else:
            raise exceptions.PlatformException(response)

    def delete(self, collection_name: str) -> bool:
        """
        Deletes a collection from the dataset.

        :param collection_name: The name of the collection to delete.
        """
        dataset_id = self._dataset.id
        success, response = self._client_api.gen_request(
            req_type="delete", path=f"/datasets/{dataset_id}/items/collections/{collection_name}"
        )
        if success:
            # Wait for the split operation to complete
            command = entities.Command.from_json(_json=response.json(),
                                                client_api=self._client_api)
            command.wait()
            return True
        else:
            raise exceptions.PlatformException(response)

    def clone(self, collection_name: str) -> dict:
        """
        Clones an existing collection, creating a new one with a unique name.

        :param collection_name: The name of the collection to clone.
        :return: The cloned collection details as a dictionary.
        """
        self.validate_max_collections()
        collections = self.list_all_collections()
        original_collection = next((c for c in collections if c["name"] == collection_name), None)

        if not original_collection:
            raise ValueError(f"Collection with name '{collection_name}' not found.")

        source_name = original_collection["name"]
        num = 0
        clone_name = ""
        while True:
            num += 1
            clone_name = f"{source_name}-clone-{num}"
            if not any(c["name"] == clone_name for c in collections):  # Use c["name"] for comparison
                break

        # Create the cloned collection
        cloned_collection = self.create(name=clone_name)
        self.assign(dataset_id=self._dataset.id, collections=[cloned_collection.name], collection_key=original_collection['key'])
        return cloned_collection


    def list_all_collections(self) -> entities.Collection:
        """
        Retrieves all collections in the dataset.

        :return: A list of collections in the dataset.
        """
        dataset_id = self._dataset.id
        success, response = self._client_api.gen_request(
            req_type="GET", path=f"/datasets/{dataset_id}/items/collections"
        )
        if success:
            data = response.json()
            return self._list_collections(data)
        else:
            raise exceptions.PlatformException(response)
        
    def validate_collection_name(self, name: str):
        """
        Validate that the collection name is unique.

        :param name: The name of the collection to validate.
        :raises ValueError: If a collection with the same name already exists.
        """
        collections = self.list_all_collections()
        if any(c["name"] == name for c in collections): 
            raise ValueError(f"A collection with the name '{name}' already exists.")

    def validate_max_collections(self) -> None:
        """
        Validates that the dataset has not exceeded the maximum allowed collections.

        :raises ValueError: If the dataset has 10 or more collections.
        """
        collections = self.list_all_collections()
        if len(collections) >= 10:
            raise ValueError("The dataset already has the maximum number of collections (10).")
        
    def list_unassigned_items(self) -> list:
        """
        List unassigned items in a dataset (items where all collection fields are false).

        :return: List of unassigned item IDs
        :rtype: list
        """
        filters = entities.Filters(method=FiltersMethod.AND)  # Use AND method for all conditions
        collection_fields = [
            "collections0",
            "collections1",
            "collections2",
            "collections3",
            "collections4",
            "collections5",
            "collections6",
            "collections7",
            "collections8",
            "collections9",
        ]

        # Add each field to the filter with a value of False
        for field in collection_fields:
            filters.add(field=field, values=False, method=FiltersMethod.AND)

        missing_ids = []
        pages = self._dataset.items.list(filters=filters)
        for page in pages:
            for item in page:
                # Items that pass filters mean all collections are false
                missing_ids.append(item.id)

        return missing_ids

    def assign(
        self, 
        dataset_id: str, 
        collections: List[str],
        item_id: str = None, 
        collection_key: str = None
    ) -> bool:
        """
        Assign an item to a collection. Creates the collection if it does not exist.

        :param dataset_id: ID of the dataset.
        :param collections: List of the collections to assign the item to.
        :param item_id: (Optional) ID of the item to assign. If not provided, all items in the dataset will be updated.
        :param collection_key: (Optional) Key for the bulk assignment. If not provided, no specific metadata will be updated.
        :return: True if the assignment was successful, otherwise raises an exception.
        """
        # Build the query structure
        if collection_key:
            query = {
                "filter": {
                    f"metadata.system.collections.{collection_key}": True
                }
            }
        elif item_id:
            query = {
                "id": {"$eq": item_id}
            }
        else:
            raise ValueError("Either collection_key or item_id must be provided.")

        # Create the payload
        payload = {
            "query": query,
            "collections": collections,
        }

        # Make the API request to assign the item
        success, response = self._client_api.gen_request(
            req_type="post",
            path=f"/datasets/{dataset_id}/items/collections/bulk-add",
            json_req=payload,
        )

        if success:
            # Wait for the operation to complete
            command = entities.Command.from_json(_json=response.json(), client_api=self._client_api)
            command.wait()
            return True
        else:
            raise exceptions.PlatformException(f"Failed to assign item to collections: {response}")

        
    def unassign(self, dataset_id: str, item_id: str, collections: List[str]) -> bool:
        """
        Unassign an item from a collection.
        :param item_id: ID of the item.
        :param collections: List of collection names to unassign.
        """
        payload = {
            "query": {"id": {"$eq": item_id}},
            "collections": collections,
        }
        success, response = self._client_api.gen_request(
            req_type="post",
            path=f"/datasets/{dataset_id}/items/collections/bulk-remove",
            json_req=payload,
        )
        if success:
            # Wait for the split operation to complete
            command = entities.Command.from_json(_json=response.json(),
                                                client_api=self._client_api)
            command.wait()
            return True
        else:
            raise exceptions.PlatformException(response)

    def _single_collection(sef, data: dict, name: str):
        """
        Retrieves the key-value pair from the dictionary where the collection's name matches the given name.

        :param data: A dictionary containing collection data in the format:
                    { "metadata.system.collections.c0": {"name": "Justice League"}, ... }
        :param name: The name of the collection to find.
        :return: The key-value pair where the name matches, or None if not found.
        """
        for key, value in data.items():
            if value.get("name") == name:
                return {key: value}
        return None
    
    def _list_collections(self, data: dict):
        """
        Create a list of Collection entities from the dataset JSON.

        :param data: The flat JSON containing collection data in the format:
                    { "metadata.system.collections.c0": {"name": "Justice League"}, ... }
        :return: A list of Collection entities.
        """
        collections = []
        for full_key, value in data.items():
            if "metadata.system.collections" in full_key:
                # Strip the prefix
                key = full_key.replace("metadata.system.collections.", "")
                collection_name = value.get("name")
                collections.append({"key": key, "name": collection_name})
        return collections
    
    def get_name_by_key(self, key: str) -> str:
        """
        Get the name of a collection by its key.

        :param key: The key of the collection (e.g., 'c0', 'c1').
        :return: The name of the collection if it exists; otherwise, an empty string.
        """
        # Assuming collections is a list of dictionaries
        collections = self.list_all_collections()
        for collection in collections:
            if collection.get("key") == key:
                return collection.get("name", "")
        return ""

================================================
File: dtlpy/repositories/commands.py
================================================
import numpy as np
import logging
import time
import tqdm
import sys

from .. import exceptions, entities, miscellaneous
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')

MAX_SLEEP_TIME = 30


class Commands:
    """
    Service Commands repository
    """

    def __init__(self, client_api: ApiClient):
        self._client_api = client_api

    ############
    # entities #
    ############

    def list(self):
        """
        List of commands

        :return: list of commands
        """
        url = '/commands'

        # request
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)
        if not success:
            raise exceptions.PlatformException(response)

        commands = miscellaneous.List([entities.Command.from_json(_json=_json,
                                                                  client_api=self._client_api) for _json in
                                       response.json()])
        return commands

    def get(self,
            command_id: str = None,
            url: str = None
            ) -> entities.Command:
        """
        Get Service command object

        :param str command_id:
        :param str url: command url
        :return: Command object
        """
        if url is None:
            url_path = "/commands/{}".format(command_id)
        else:
            url_path = url.split('api/v1')[1]

        success, response = self._client_api.gen_request(req_type="get",
                                                         path=url_path)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Command.from_json(client_api=self._client_api,
                                          _json=response.json())

    def wait(self, command_id, timeout=0, step=None, url=None, backoff_factor=1):
        """
        Wait for command to finish

        backoff_factor: A backoff factor to apply between attempts after the second try
        {backoff factor} * (2 ** ({number of total retries} - 1))
        seconds. If the backoff_factor is 1, then :func:`.sleep` will sleep
        for [0s, 2s, 4s, ...] between retries. It will never be longer
        than 30 sec

        :param str command_id: Command id to wait to
        :param int timeout: int, seconds to wait until TimeoutError is raised. if 0 - wait until done
        :param int step: int, seconds between polling
        :param str url: url to the command
        :param float backoff_factor: A backoff factor to apply between attempts after the second try
        :return: Command  object
        """

        elapsed = 0
        start = time.time()
        if timeout is None or timeout <= 0:
            timeout = np.inf

        command = None
        pbar = tqdm.tqdm(total=100,
                         disable=self._client_api.verbose.disable_progress_bar_command_progress,
                         file=sys.stdout,
                         desc='Command Progress')
        num_tries = 1
        while elapsed < timeout:
            command = self.get(command_id=command_id, url=url)
            if command.type == 'ExportDatasetAsJson':
                self._client_api.callbacks.run_on_event(event=self._client_api.callbacks.CallbackEvent.DATASET_EXPORT,
                                                        context=command.spec,
                                                        progress=command.progress)

            pbar.update(command.progress - pbar.n)
            if not command.in_progress():
                break
            elapsed = time.time() - start
            sleep_time = np.min([timeout - elapsed, backoff_factor * (2 ** num_tries), MAX_SLEEP_TIME])
            num_tries += 1
            logger.debug("Command {!r} is running for {:.2f}[s] and now Going to sleep {:.2f}[s]".format(command.id,
                                                                                                         elapsed,
                                                                                                         sleep_time))
            time.sleep(sleep_time)
        pbar.close()
        if command is None:
            raise ValueError('Nothing to wait for')
        if elapsed >= timeout:
            raise TimeoutError("command wait() got timeout. id: {!r}, status: {}, progress {}%".format(
                command.id, command.status, command.progress))
        if command.status != entities.CommandsStatus.SUCCESS:
            raise exceptions.PlatformException(error='424',
                                               message="Command {!r} {}: '{}'".format(command.id,
                                                                                      command.status,
                                                                                      command.error))
        return command

    def abort(self, command_id: str):
        """
        Abort Command

        :param str command_id: command id
        :return:
        """
        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/commands/{}/abort'.format(command_id))

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)
        else:
            return entities.Command.from_json(_json=response.json(),
                                              client_api=self._client_api)


================================================
File: dtlpy/repositories/compositions.py
================================================
import logging
from .. import entities, repositories, exceptions, miscellaneous, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Compositions:
    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            try:
                self._project = repositories.Projects(client_api=self._client_api).get()
            except exceptions.NotFound:
                raise exceptions.PlatformException(
                    error='2001',
                    message='Missing "project". need to set a Project entity or use project.pipelines repository')
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def get(self,
            composition_id=None,
            fetch=None
            ) -> entities.Pipeline:

        if fetch is None:
            fetch = self._client_api.fetch_entities

        if composition_id is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        elif fetch:
            if composition_id is not None:
                success, response = self._client_api.gen_request(
                    req_type="get",
                    path="/compositions/{}".format(composition_id))
                if not success:
                    raise exceptions.PlatformException(response)
                composition = response.json()
            else:
                raise exceptions.PlatformException(
                    error='400',
                    message='No checked-out pipeline was found, must checkout or provide an identifier in inputs')
        else:
            composition = {'id': composition_id}

        return composition


================================================
File: dtlpy/repositories/computes.py
================================================
import base64
import datetime
import json

from ..services.api_client import ApiClient
from .. import exceptions, entities, repositories
from typing import List, Optional, Dict
from ..entities import ComputeCluster, ComputeContext, ComputeType, Project
from ..entities.integration import IntegrationType


class Computes:

    def __init__(self, client_api: ApiClient):
        self._client_api = client_api
        self._base_url = '/compute'
        self._commands = None
        self._projects = None
        self._organizations = None

    @property
    def commands(self) -> repositories.Commands:
        if self._commands is None:
            self._commands = repositories.Commands(client_api=self._client_api)
        return self._commands

    @property
    def projects(self):
        if self._projects is None:
            self._projects = repositories.Projects(client_api=self._client_api)
        return self._projects

    @property
    def organizations(self):
        if self._organizations is None:
            self._organizations = repositories.Organizations(client_api=self._client_api)
        return self._organizations

    def create(
            self,
            name: str,
            context: entities.ComputeContext,
            shared_contexts: Optional[List[entities.ComputeContext]],
            cluster: entities.ComputeCluster,
            type: entities.ComputeType = entities.ComputeType.KUBERNETES,
            is_global: Optional[bool] = False,
            features: Optional[Dict] = None,
            wait=True,
            status: entities.ComputeStatus = None
    ):
        """
        Create a new compute

        :param name: Compute name
        :param context: Compute context
        :param shared_contexts: Shared contexts
        :param cluster: Compute cluster
        :param type: Compute type
        :param is_global: Is global
        :param features: Features
        :param wait: Wait for compute creation
        :param status: Compute status
        :return: Compute
        """

        payload = {
            'name': name,
            'context': context.to_json(),
            'type': type.value,
            'global': is_global,
            'features': features,
            'shared_contexts': [sc.to_json() for sc in shared_contexts],
            'cluster': cluster.to_json(),
            'status': status
        }

        # request
        success, response = self._client_api.gen_request(
            req_type='post',
            path=self._base_url,
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

        compute = entities.Compute.from_json(
            _json=response.json(),
            client_api=self._client_api
        )

        if wait:
            command_id = compute.metadata.get('system', {}).get('commands', {}).get('create', None)
            if command_id is not None:
                command = self.commands.get(command_id=command_id, url='api/v1/commands/faas/{}'.format(command_id))
                command.wait()
                compute = self.get(compute_id=compute.id)

        return compute

    def get(self, compute_id: str):
        """
        Get a compute

        :param compute_id: Compute ID
        :return: Compute
        """

        # request
        success, response = self._client_api.gen_request(
            req_type='get',
            path=self._base_url + '/{}'.format(compute_id)
        )

        if not success:
            raise exceptions.PlatformException(response)

        compute = entities.Compute.from_json(
            _json=response.json(),
            client_api=self._client_api
        )

        return compute

    def update(self, compute: entities.Compute):
        """
        Update a compute

        :param compute: Compute
        :return: Compute
        """

        # request
        success, response = self._client_api.gen_request(
            req_type='patch',
            path=self._base_url + '/{}'.format(compute.id),
            json_req=compute.to_json()
        )

        if not success:
            raise exceptions.PlatformException(response)

        compute = entities.Compute.from_json(
            _json=response.json(),
            client_api=self._client_api
        )

        return compute

    def delete(self, compute_id: str):
        """
        Delete a compute

        :param compute_id: compute ID
        """

        # request
        success, response = self._client_api.gen_request(
            req_type='delete',
            path=self._base_url + '/{}'.format(compute_id)
        )

        if not success:
            raise exceptions.PlatformException(response)

        return True

    @staticmethod
    def read_file(file_path):
        try:
            with open(file_path, 'r') as file:
                content = file.read()
            return content
        except FileNotFoundError:
            print(f"The file at {file_path} was not found.")
        except IOError:
            print(f"An error occurred while reading the file at {file_path}.")

    def decode_and_parse_input(self, file_path):
        """Decode a base64 encoded string from file a and parse it as JSON."""
        decoded_bytes = base64.b64decode(self.read_file(file_path))
        return json.loads(decoded_bytes)

    @staticmethod
    def create_integration(org, name, auth_data):
        """Create a new key-value integration within the specified project."""
        return org.integrations.create(
            integrations_type=IntegrationType.KEY_VALUE,
            name=name,
            options={
                'key': name,
                'value': json.dumps(auth_data)
            }
        )

    def setup_compute_cluster(self, config, integration, org_id, project=None):
        """Set up a compute cluster using the provided configuration and integration."""
        cluster = ComputeCluster.from_setup_json(config, integration)
        project_id = None
        if project is not None:
            project_id = project.id
        compute = self.create(
            config['config']['name'],
            ComputeContext([], org_id, project_id),
            [],
            cluster,
            ComputeType.KUBERNETES,
            status=config['config'].get('status', None))
        return compute

    def create_from_config_file(self, config_file_path, org_id, project_name: Optional[str] = None):
        config = self.decode_and_parse_input(config_file_path)
        project = None
        if project_name is not None:
            project = self.projects.get(project_name=project_name)
        org = self.organizations.get(organization_id=org_id)
        integration_name = ('cluster_integration_test_' + datetime.datetime.now().isoformat().split('.')[0]
                            .replace(':', '_'))
        integration = self.create_integration(org, integration_name, config['authentication'])
        compute = self.setup_compute_cluster(config, integration, org_id, project)
        return compute


class ServiceDrivers:

    def __init__(self, client_api: ApiClient):
        self._client_api = client_api
        self._base_url = '/serviceDrivers'

    def create(
            self,
            name: str,
            compute_id: str,
            context: entities.ComputeContext
    ):
        """
        Create a new service driver

        :param name: Service driver name
        :param compute_id: Compute ID
        :param context: Compute context
        :return: Service driver

        """

        payload = {
            'name': name,
            'computeId': compute_id,
            'context': context.to_json()
        }

        # request
        success, response = self._client_api.gen_request(
            req_type='post',
            path=self._base_url,
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

        service_driver = entities.ServiceDriver.from_json(
            _json=response.json(),
            client_api=self._client_api
        )

        return service_driver

    def get(self, service_driver_id: str):
        """
        Get a service driver

        :param service_driver_id: Service driver ID
        :return: Service driver
        """

        # request
        success, response = self._client_api.gen_request(
            req_type='get',
            path=self._base_url + '/{}'.format(service_driver_id)
        )

        if not success:
            raise exceptions.PlatformException(response)

        service_driver = entities.ServiceDriver.from_json(
            _json=response.json(),
            client_api=self._client_api
        )

        return service_driver

    def delete(self, service_driver_id: str):
        """
        Delete a service driver

        :param service_driver_id: Service driver ID
        """

        # request
        success, response = self._client_api.gen_request(
            req_type='delete',
            path=self._base_url + '/{}'.format(service_driver_id)
        )

        if not success:
            raise exceptions.PlatformException(response)

        return True

    def set_default(self, service_driver_id: str, org_id: str, update_existing_services=False):
        """
        Set a service driver as default

        :param service_driver_id: Compute name
        :param org_id: Organization ID
        :param update_existing_services: Update existing services

        :return: Service driver
        """

        # request
        success, response = self._client_api.gen_request(
            req_type='post',
            path=self._base_url + '/default',
            json_req={
                'organizationId': org_id,
                'updateExistingServices': update_existing_services,
                'driverName': service_driver_id
            }
        )

        if not success:
            raise exceptions.PlatformException(response)

        service_driver = entities.ServiceDriver.from_json(
            _json=response.json(),
            client_api=self._client_api
        )

        return service_driver


================================================
File: dtlpy/repositories/downloader.py
================================================
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
from PIL import Image
import numpy as np
import traceback
import warnings
import requests
import logging
import shutil
import json
import tqdm
import sys
import os
import io

from .. import entities, repositories, miscellaneous, PlatformException, exceptions
from ..services import Reporter

logger = logging.getLogger(name='dtlpy')

NUM_TRIES = 3  # try to download 3 time before fail on item


class Downloader:
    def __init__(self, items_repository):
        self.items_repository = items_repository

    def download(self,
                 # filter options
                 filters: entities.Filters = None,
                 items=None,
                 # download options
                 local_path=None,
                 file_types=None,
                 save_locally=True,
                 to_array=False,
                 overwrite=False,
                 annotation_filters: entities.Filters = None,
                 annotation_options: entities.ViewAnnotationOptions = None,
                 to_items_folder=True,
                 thickness=1,
                 with_text=False,
                 without_relative_path=None,
                 avoid_unnecessary_annotation_download=False,
                 include_annotations_in_output=True,
                 export_png_files=False,
                 filter_output_annotations=False,
                 alpha=1,
                 export_version=entities.ExportVersion.V1
                 ):
        """
        Download dataset by filters.
        Filtering the dataset for items and save them local
        Optional - also download annotation, mask, instance and image mask of the item

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param items: download Item entity or item_id (or a list of item)
        :param local_path: local folder or filename to save to.
        :param file_types: a list of file type to download. e.g ['video/webm', 'video/mp4', 'image/jpeg', 'image/png']
        :param save_locally: bool. save to disk or return a buffer
        :param to_array: returns Ndarray when True and local_path = False
        :param overwrite: optional - default = False
        :param annotation_options: download annotations options. options: list(dl.ViewAnnotationOptions)
        :param annotation_filters: Filters entity to filter annotations for download
        :param to_items_folder: Create 'items' folder and download items to it
        :param with_text: optional - add text to annotations, default = False
        :param thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param without_relative_path: bool - download items without the relative path from platform
        :param avoid_unnecessary_annotation_download: DEPRECATED only items and annotations in filters are downloaded
        :param include_annotations_in_output: default - False , if export should contain annotations
        :param export_png_files: default - True, if semantic annotations should be exported as png files
        :param filter_output_annotations: default - False, given an export by filter - determine if to filter out annotations
        :param alpha: opacity value [0 1], default 1
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :return: Output (list)
        """

        ###################
        # Default options #
        ###################
        # annotation options
        if annotation_options is None:
            annotation_options = list()
        elif not isinstance(annotation_options, list):
            annotation_options = [annotation_options]
        for ann_option in annotation_options:
            if not isinstance(ann_option, entities.ViewAnnotationOptions):
                if ann_option not in list(entities.ViewAnnotationOptions):
                    raise PlatformException(
                        error='400',
                        message='Unknown annotation download option: {}, please choose from: {}'.format(
                            ann_option, list(entities.ViewAnnotationOptions)))
        #####################
        # items to download #
        #####################
        if items is not None:
            # convert input to a list
            if not isinstance(items, list):
                items = [items]
            # get items by id
            if isinstance(items[0], str):
                items = [self.items_repository.get(item_id=item_id) for item_id in items]
            elif isinstance(items[0], entities.Item):
                pass
            else:
                raise PlatformException(
                    error="400",
                    message='Unknown items type to download. Expecting str or Item entities. Got "{}" instead'.format(
                        type(items[0])
                    )
                )
            # create filters to download annotations
            filters = entities.Filters(field='id',
                                       values=[item.id for item in items],
                                       operator=entities.FiltersOperations.IN)
            filters._user_query = 'false'

            # convert to list of list (like pages and page)
            items_to_download = [items]
            num_items = len(items)
        else:
            # filters
            if filters is None:
                filters = entities.Filters()
                filters._user_query = 'false'
            # file types
            if file_types is not None:
                filters.add(field='metadata.system.mimetype', values=file_types, operator=entities.FiltersOperations.IN)
            if annotation_filters is not None:
                for annotation_filter_and in annotation_filters.and_filter_list:
                    filters.add_join(field=annotation_filter_and.field,
                                     values=annotation_filter_and.values,
                                     operator=annotation_filter_and.operator,
                                     method=entities.FiltersMethod.AND)
                for annotation_filter_or in annotation_filters.or_filter_list:
                    filters.add_join(field=annotation_filter_or.field,
                                     values=annotation_filter_or.values,
                                     operator=annotation_filter_or.operator,
                                     method=entities.FiltersMethod.OR)
            else:
                annotation_filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
                filters._user_query = 'false'

            items_to_download = self.items_repository.list(filters=filters)
            num_items = items_to_download.items_count

        if num_items == 0:
            logger.warning('No items found! Nothing was downloaded')
            return list()

        ##############
        # local path #
        ##############
        is_folder = False
        if local_path is None:
            # create default local path
            local_path = self.__default_local_path()

        if os.path.isdir(local_path):
            logger.info('Local folder already exists:{}. merge/overwrite according to "overwrite option"'.format(
                local_path))
            is_folder = True
        else:
            # check if filename
            _, ext = os.path.splitext(local_path)
            if num_items > 1:
                is_folder = True
            else:
                item_to_download = items_to_download[0][0]
                file_name = item_to_download.name
                _, ext_download = os.path.splitext(file_name)
                if ext_download != ext:
                    is_folder = True
            if is_folder and save_locally:
                path_to_create = local_path
                if local_path.endswith('*'):
                    path_to_create = os.path.dirname(local_path)
                logger.info("Creating new directory for download: {}".format(path_to_create))
                os.makedirs(path_to_create, exist_ok=True)

        ####################
        # annotations json #
        ####################
        # download annotations' json files in a new thread
        # items will start downloading and if json not exists yet - will download for each file
        if num_items > 1 and annotation_options:
            # a new folder named 'json' will be created under the "local_path"
            logger.info("Downloading annotations formats: {}".format(annotation_options))
            self.download_annotations(**{
                "dataset": self.items_repository.dataset,
                "filters": filters,
                "annotation_filters": annotation_filters,
                "local_path": local_path,
                'overwrite': overwrite,
                'include_annotations_in_output': include_annotations_in_output,
                'export_png_files': export_png_files,
                'filter_output_annotations': filter_output_annotations,
                'export_version': export_version
            })
        ###############
        # downloading #
        ###############
        # create result lists
        client_api = self.items_repository._client_api

        reporter = Reporter(num_workers=num_items,
                            resource=Reporter.ITEMS_DOWNLOAD,
                            print_error_logs=client_api.verbose.print_error_logs,
                            client_api=client_api)
        jobs = [None for _ in range(num_items)]
        # pool
        pool = client_api.thread_pools(pool_name='item.download')
        # download
        pbar = tqdm.tqdm(total=num_items, disable=client_api.verbose.disable_progress_bar_download_dataset, file=sys.stdout,
                         desc='Download Items')
        try:
            i_item = 0
            for page in items_to_download:
                for item in page:
                    if item.type == "dir":
                        continue
                    if save_locally:
                        # get local file path
                        item_local_path, item_local_filepath = self.__get_local_filepath(
                            local_path=local_path,
                            without_relative_path=without_relative_path,
                            item=item,
                            to_items_folder=to_items_folder,
                            is_folder=is_folder)

                        if os.path.isfile(item_local_filepath) and not overwrite:
                            logger.debug("File Exists: {}".format(item_local_filepath))
                            reporter.set_index(ref=item.id, status='exist', output=item_local_filepath, success=True)
                            pbar.update()
                            if annotation_options and item.annotated:
                                # download annotations only
                                jobs[i_item] = pool.submit(
                                    self._download_img_annotations,
                                    **{
                                        "item": item,
                                        "img_filepath": item_local_filepath,
                                        "overwrite": overwrite,
                                        "annotation_options": annotation_options,
                                        "annotation_filters": annotation_filters,
                                        "local_path": item_local_path,
                                        "thickness": thickness,
                                        "alpha": alpha,
                                        "with_text": with_text,
                                        "export_version": export_version,
                                    },
                                )
                            i_item += 1
                            continue
                    else:
                        item_local_path = None
                        item_local_filepath = None

                    # download single item
                    jobs[i_item] = pool.submit(
                        self.__thread_download_wrapper,
                        **{
                            "i_item": i_item,
                            "item": item,
                            "item_local_path": item_local_path,
                            "item_local_filepath": item_local_filepath,
                            "save_locally": save_locally,
                            "to_array": to_array,
                            "annotation_options": annotation_options,
                            "annotation_filters": annotation_filters,
                            "reporter": reporter,
                            "pbar": pbar,
                            "overwrite": overwrite,
                            "thickness": thickness,
                            "alpha": alpha,
                            "with_text": with_text,
                            "export_version": export_version
                        },
                    )
                    i_item += 1
        except Exception:
            logger.exception('Error downloading:')
        finally:
            _ = [j.result() for j in jobs if j is not None]
            pbar.close()
        # reporting
        n_download = reporter.status_count(status='download')
        n_exist = reporter.status_count(status='exist')
        n_error = reporter.status_count(status='error')
        logger.info("Number of files downloaded:{}".format(n_download))
        logger.info("Number of files exists: {}".format(n_exist))
        logger.info("Total number of files: {}".format(n_download + n_exist))

        # log error
        if n_error > 0:
            log_filepath = reporter.generate_log_files()
            if log_filepath is not None:
                logger.warning("Errors in {} files. See {} for full log".format(n_error, log_filepath))
        if int(n_download) <= 1 and int(n_exist) <= 1:
            try:
                return next(reporter.output)
            except StopIteration:
                return None
        return reporter.output

    def __thread_download_wrapper(self, i_item,
                                  # item params
                                  item, item_local_path, item_local_filepath,
                                  save_locally, to_array, overwrite,
                                  # annotations params
                                  annotation_options, annotation_filters, with_text, thickness,
                                  # threading params
                                  reporter, pbar, alpha, export_version):

        download = None
        err = None
        trace = None
        for i_try in range(NUM_TRIES):
            try:
                logger.debug("Download item: {path}. Try {i}/{n}. Starting..".format(path=item.filename,
                                                                                     i=i_try + 1,
                                                                                     n=NUM_TRIES))
                download = self.__thread_download(item=item,
                                                  save_locally=save_locally,
                                                  to_array=to_array,
                                                  local_path=item_local_path,
                                                  local_filepath=item_local_filepath,
                                                  annotation_options=annotation_options,
                                                  annotation_filters=annotation_filters,
                                                  overwrite=overwrite,
                                                  thickness=thickness,
                                                  alpha=alpha,
                                                  with_text=with_text,
                                                  export_version=export_version)
                logger.debug("Download item: {path}. Try {i}/{n}. Success. Item id: {id}".format(path=item.filename,
                                                                                                 i=i_try + 1,
                                                                                                 n=NUM_TRIES,
                                                                                                 id=item.id))
                if download is not None:
                    break
            except Exception as e:
                logger.debug("Download item: {path}. Try {i}/{n}. Fail.".format(path=item.filename,
                                                                                i=i_try + 1,
                                                                                n=NUM_TRIES))
                err = e
                trace = traceback.format_exc()
        pbar.update()
        if download is None:
            if err is None:
                err = self.items_repository._client_api.platform_exception
            reporter.set_index(status="error", ref=item.id, success=False,
                               error="{}\n{}".format(err, trace))
        else:
            reporter.set_index(ref=item.id, status="download", output=download, success=True)

    @staticmethod
    def download_annotations(dataset: entities.Dataset,
                             local_path: str,
                             filters: entities.Filters = None,
                             annotation_filters: entities.Filters = None,
                             overwrite=False,
                             include_annotations_in_output=True,
                             export_png_files=False,
                             filter_output_annotations=False,
                             export_version=entities.ExportVersion.V1
                             ):
        """
        Download annotations json for entire dataset

        :param dataset: Dataset entity
        :param local_path:
        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters items
        :param annotation_filters: dl.Filters entity to filters items' annotations
        :param overwrite: optional - overwrite annotations if exist, default = false
        :param include_annotations_in_output: default - True , if export should contain annotations
        :param export_png_files: default - if True, semantic annotations should be exported as png files
        :param filter_output_annotations: default - False, given an export by filter - determine if to filter out annotations
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :return:
        """
        local_path = os.path.join(local_path, "json")
        zip_filepath = None
        # only if json folder does not exist or exist and overwrite
        if not os.path.isdir(os.path.join(local_path, 'json')) or overwrite:
            # create local path to download and save to
            if not os.path.isdir(local_path):
                os.makedirs(local_path)

        try:
            payload = dict()
            if filters is not None:
                payload['itemsQuery'] = filters.prepare()
            payload['annotations'] = {
                "include": include_annotations_in_output,
                "convertSemantic": export_png_files
            }
            payload['exportVersion'] = export_version
            if annotation_filters is not None:
                payload['annotationsQuery'] = annotation_filters.prepare()
                payload['annotations']['filter'] = filter_output_annotations

            success, response = dataset._client_api.gen_request(req_type='post',
                                                                path='/datasets/{}/export'.format(dataset.id),
                                                                json_req=payload,
                                                                headers={'user_query': filters._user_query})
            if not success:
                raise exceptions.PlatformException(response)
            command = entities.Command.from_json(_json=response.json(),
                                                 client_api=dataset._client_api)
            command = command.wait(timeout=0)
            if 'outputItemId' not in command.spec:
                raise exceptions.PlatformException(
                    error='400',
                    message="outputItemId key is missing in command response: {}".format(response))
            item_id = command.spec['outputItemId']
            annotation_zip_item = repositories.Items(client_api=dataset._client_api).get(item_id=item_id)
            zip_filepath = annotation_zip_item.download(local_path=local_path, export_version=export_version)
            # unzipping annotations to directory
            if isinstance(zip_filepath, list) or not os.path.isfile(zip_filepath):
                raise exceptions.PlatformException(
                    error='404',
                    message='error downloading annotation zip file. see above for more information. item id: {!r}'.format(
                        annotation_zip_item.id))
            try:
                miscellaneous.Zipping.unzip_directory(zip_filename=zip_filepath,
                                                      to_directory=local_path)
            except Exception as e:
                logger.warning("Failed to extract zip file error: {}".format(e))

        finally:
            # cleanup
            if isinstance(zip_filepath, str) and os.path.isfile(zip_filepath):
                os.remove(zip_filepath)

    @staticmethod
    def _download_img_annotations(item: entities.Item,
                                  img_filepath,
                                  local_path,
                                  overwrite,
                                  annotation_options,
                                  annotation_filters,
                                  thickness=1,
                                  with_text=False,
                                  alpha=1,
                                  export_version=entities.ExportVersion.V1
                                  ):

        # check if local_path is a file name
        _, ext = os.path.splitext(local_path)
        if ext:
            # take the dir of the file for the annotations save
            local_path = os.path.dirname(local_path)

        # fix local path
        if local_path.endswith("/items") or local_path.endswith("\\items"):
            local_path = os.path.dirname(local_path)

        annotation_rel_path = item.filename[1:]
        if img_filepath is not None:
            dir_name = os.path.dirname(annotation_rel_path)
            base_name = os.path.basename(img_filepath)
            annotation_rel_path = os.path.join(dir_name, base_name)

        # find annotations json
        annotations_json_filepath = os.path.join(local_path, "json", annotation_rel_path)
        if export_version == entities.ExportVersion.V1:
            name, _ = os.path.splitext(annotations_json_filepath)
        else:
            name = annotations_json_filepath
        annotations_json_filepath = name + ".json"

        if os.path.isfile(annotations_json_filepath) and annotation_filters is None:
            # if exists take from json file
            with open(annotations_json_filepath, "r", encoding="utf8") as f:
                data = json.load(f)
            if "annotations" in data:
                data = data["annotations"]
            annotations = entities.AnnotationCollection.from_json(_json=data, item=item)
            # no need to use the filters here because the annotations were already downloaded with annotation_filters
        else:
            # if json file doesnt exist get the annotations from platform
            annotations = item.annotations.list(filters=annotation_filters)

        # get image shape
        is_url_item = item.metadata. \
                          get('system', dict()). \
                          get('shebang', dict()). \
                          get('linkInfo', dict()). \
                          get('type', None) == 'url'

        if item is not None:
            orientation = item.system.get('exif', {}).get('Orientation', 0)
        else:
            orientation = 0
        if item.width is not None and item.height is not None:
            if orientation in [5, 6, 7, 8]:
                img_shape = (item.width, item.height)
            else:
                img_shape = (item.height, item.width)
        elif ('image' in item.mimetype and img_filepath is not None) or \
                (is_url_item and img_filepath is not None):
            img_shape = Image.open(img_filepath).size[::-1]
        else:
            img_shape = (0, 0)

        # download all annotation options
        for option in annotation_options:
            # get path and create dirs
            annotation_filepath = os.path.join(local_path, option, annotation_rel_path)
            if not os.path.isdir(os.path.dirname(annotation_filepath)):
                os.makedirs(os.path.dirname(annotation_filepath), exist_ok=True)

            if export_version == entities.ExportVersion.V1:
                temp_path, ext = os.path.splitext(annotation_filepath)
            else:
                temp_path = annotation_filepath

            if option == entities.ViewAnnotationOptions.JSON:
                if not os.path.isfile(annotations_json_filepath):
                    annotations.download(
                        filepath=annotations_json_filepath,
                        annotation_format=option,
                        height=img_shape[0],
                        width=img_shape[1],
                    )
            elif option in [entities.ViewAnnotationOptions.MASK,
                            entities.ViewAnnotationOptions.INSTANCE,
                            entities.ViewAnnotationOptions.ANNOTATION_ON_IMAGE,
                            entities.ViewAnnotationOptions.OBJECT_ID,
                            entities.ViewAnnotationOptions.VTT]:
                if option == entities.ViewAnnotationOptions.VTT:
                    annotation_filepath = temp_path + ".vtt"
                else:
                    if 'video' in item.mimetype:
                        annotation_filepath = temp_path + ".mp4"
                    else:
                        annotation_filepath = temp_path + ".png"
                if not os.path.isfile(annotation_filepath) or overwrite:
                    # if not exists OR (exists AND overwrite)
                    if not os.path.exists(os.path.dirname(annotation_filepath)):
                        # create folder if not exists
                        os.makedirs(os.path.dirname(annotation_filepath), exist_ok=True)
                    if option == entities.ViewAnnotationOptions.ANNOTATION_ON_IMAGE and img_filepath is None:
                        raise PlatformException(
                            error="1002",
                            message="Missing image for annotation option dl.ViewAnnotationOptions.ANNOTATION_ON_IMAGE")
                    annotations.download(
                        filepath=annotation_filepath,
                        img_filepath=img_filepath,
                        annotation_format=option,
                        height=img_shape[0],
                        width=img_shape[1],
                        thickness=thickness,
                        alpha=alpha,
                        with_text=with_text,
                        orientation=orientation
                    )
            else:
                raise PlatformException(error="400", message="Unknown annotation option: {}".format(option))

    @staticmethod
    def __get_local_filepath(local_path, item, to_items_folder, without_relative_path=None, is_folder=False):
        # create paths
        _, ext = os.path.splitext(local_path)
        if ext and not is_folder:
            # local_path is a filename
            local_filepath = local_path
            local_path = os.path.dirname(local_filepath)
        else:
            # if directory - get item's filename
            if to_items_folder:
                local_path = os.path.join(local_path, "items")
            elif is_folder:
                local_path = os.path.join(local_path, "")
            if without_relative_path is not None:
                local_filepath = os.path.join(local_path, item.name)
            else:
                local_filepath = os.path.join(local_path, item.filename[1:])
        return local_path, local_filepath

    @staticmethod
    def __get_link_source(item):
        assert isinstance(item, entities.Item)
        if not item.is_fetched:
            return item, '', False

        if not item.filename.endswith('.json') or \
                item.metadata.get('system', {}).get('shebang', {}).get('dltype', '') != 'link':
            return item, '', False

        # recursively get next id link item
        while item.filename.endswith('.json') and \
                item.metadata.get('system', {}).get('shebang', {}).get('dltype', '') == 'link' and \
                item.metadata.get('system', {}).get('shebang', {}).get('linkInfo', {}).get('type', '') == 'id':
            item = item.dataset.items.get(item_id=item.metadata['system']['shebang']['linkInfo']['ref'])

        # check if link
        if item.filename.endswith('.json') and \
                item.metadata.get('system', {}).get('shebang', {}).get('dltype', '') == 'link' and \
                item.metadata.get('system', {}).get('shebang', {}).get('linkInfo', {}).get('type', '') == 'url':
            url = item.metadata['system']['shebang']['linkInfo']['ref']
            return item, url, True
        else:
            return item, '', False

    def __file_validation(self, item, downloaded_file):
        res = False
        resume = True
        if isinstance(downloaded_file, io.BytesIO):
            file_size = downloaded_file.getbuffer().nbytes
        else:
            file_size = os.stat(downloaded_file).st_size
        expected_size = item.metadata['system']['size']
        size_diff = file_size - expected_size
        if size_diff == 0:
            res = True
        if size_diff > 0:
            resume = False
        return res, file_size, resume

    def __thread_download(self,
                          item,
                          save_locally,
                          local_path,
                          to_array,
                          local_filepath,
                          overwrite,
                          annotation_options,
                          annotation_filters,
                          chunk_size=8192,
                          thickness=1,
                          with_text=False,
                          alpha=1,
                          export_version=entities.ExportVersion.V1
                          ):
        """
        Get a single item's binary data
        Calling this method will returns the item body itself , an image for example with the proper mimetype.

        :param item: Item entity to download
        :param save_locally: bool. save to file or return buffer
        :param local_path: item local folder to save to.
        :param to_array: returns Ndarray when True and local_path = False
        :param local_filepath: item local filepath
        :param overwrite: overwrite the file is existing
        :param annotation_options: download annotations options: list(dl.ViewAnnotationOptions)
        :param annotation_filters: Filters entity to filter item's annotation
        :param chunk_size: size of chunks to download - optional. default = 8192
        :param thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param with_text: optional - add text to annotations, default = False
        :param alpha: opacity value [0 1], default 1
        :param ExportVersion export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :return:
        """
        # check if need to download image binary from platform
        need_to_download = True
        if save_locally and os.path.isfile(local_filepath):
            need_to_download = overwrite

        item, url, is_url = self.__get_link_source(item=item)

        # save as byte stream
        data = io.BytesIO()
        if need_to_download:
            chunk_resume = {0: 0}
            start_point = 0
            download_done = False
            while chunk_resume.get(start_point, '') != 3 and not download_done:
                if not is_url:
                    headers = {'x-dl-sanitize': '0', 'Range': 'bytes={}-'.format(start_point)}
                    result, response = self.items_repository._client_api.gen_request(req_type="get",
                                                                                     headers=headers,
                                                                                     path="/items/{}/stream".format(
                                                                                         item.id),
                                                                                     stream=True,
                                                                                     dataset_id=item.dataset_id)
                    if not result:
                        if os.path.isfile(local_filepath + '.download'):
                            os.remove(local_filepath + '.download')
                        raise PlatformException(response)
                else:
                    _, ext = os.path.splitext(item.metadata['system']['shebang']['linkInfo']['ref'].split('?')[0])
                    local_filepath += ext
                    response = self.get_url_stream(url=url)

                if save_locally:
                    # save to file
                    if not os.path.exists(os.path.dirname(local_filepath)):
                        # create folder if not exists
                        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)

                    # decide if create progress bar for item
                    total_length = response.headers.get("content-length")
                    one_file_pbar = None
                    try:
                        one_file_progress_bar = total_length is not None and int(
                            total_length) > 10e6  # size larger than 10 MB
                        if one_file_progress_bar:
                            one_file_pbar = tqdm.tqdm(total=int(total_length),
                                                      unit='B',
                                                      unit_scale=True,
                                                      unit_divisor=1024,
                                                      position=1,
                                                      file=sys.stdout,
                                                      disable=self.items_repository._client_api.verbose.disable_progress_bar_download_item,
                                                      desc='Download Item')
                    except Exception as err:
                        one_file_progress_bar = False
                        logger.debug('Cant decide downloaded file length, bar will not be presented: {}'.format(err))

                    # start download
                    if self.items_repository._client_api.sdk_cache.use_cache and \
                            self.items_repository._client_api.cache is not None:
                        response_output = os.path.normpath(response.content)
                        if isinstance(response_output, bytes):
                            response_output = response_output.decode('utf-8')[1:-1]

                        if os.path.isfile(os.path.normpath(response_output)):
                            if response_output != local_filepath:
                                source_path = os.path.normpath(response_output)
                                shutil.copyfile(source_path, local_filepath)
                    else:
                        try:
                            temp_file_path = local_filepath + '.download'
                            with open(temp_file_path, "ab") as f:
                                try:
                                    for chunk in response.iter_content(chunk_size=chunk_size):
                                        if chunk:  # filter out keep-alive new chunks
                                            f.write(chunk)
                                            if one_file_progress_bar:
                                                one_file_pbar.update(len(chunk))
                                except Exception as err:
                                    pass
                            file_validation, start_point, chunk_resume = self.__get_next_chunk(item=item,
                                                                                              download_progress=temp_file_path,
                                                                                              chunk_resume=chunk_resume)
                            if file_validation:
                                shutil.move(temp_file_path, local_filepath)
                                download_done = True
                            else:
                                if not is_url:
                                    continue
                                else:
                                    raise PlatformException(
                                        error="400",
                                        message='Downloaded file is corrupted. Please try again. If the issue repeats please contact support.')
                        except Exception as err:
                            if os.path.isfile(temp_file_path):
                                os.remove(temp_file_path)
                            raise err
                    if one_file_progress_bar:
                        one_file_pbar.close()
                    # save to output variable
                    data = local_filepath
                    # if image - can download annotation mask
                    if item.annotated and annotation_options:
                        self._download_img_annotations(item=item,
                                                       img_filepath=local_filepath,
                                                       annotation_options=annotation_options,
                                                       annotation_filters=annotation_filters,
                                                       local_path=local_path,
                                                       overwrite=overwrite,
                                                       thickness=thickness,
                                                       alpha=alpha,
                                                       with_text=with_text,
                                                       export_version=export_version
                                                       )
                else:
                    if self.items_repository._client_api.sdk_cache.use_cache and \
                            self.items_repository._client_api.cache is not None:
                        response_output = os.path.normpath(response.content)
                        if isinstance(response_output, bytes):
                            response_output = response_output.decode('utf-8')[1:-1]

                        if os.path.isfile(response_output):
                            source_file = response_output
                            with open(source_file, 'wb') as f:
                                data = f.read()
                    else:
                        try:
                            for chunk in response.iter_content(chunk_size=chunk_size):
                                if chunk:  # filter out keep-alive new chunks
                                    data.write(chunk)
                            file_validation, start_point, chunk_resume = self.__get_next_chunk(item=item,
                                                                                              download_progress=data,
                                                                                              chunk_resume=chunk_resume)
                            if file_validation:
                                download_done = True
                            else:
                                continue
                        except Exception as err:
                            raise err
                    # go back to the beginning of the stream
                    data.seek(0)
                    data.name = item.name
                    if not save_locally and to_array:
                        if 'image' not in item.mimetype:
                            raise PlatformException(
                                error="400",
                                message='Download element type numpy.ndarray support for image only. '
                                        'Item Id: {} is {} type'.format(item.id, item.mimetype))

                        data = np.array(Image.open(data))
        else:
            data = local_filepath
        return data

    def __get_next_chunk(self, item, download_progress, chunk_resume):
        size_validation, file_size, resume = self.__file_validation(item=item,
                                                                    downloaded_file=download_progress)
        start_point = file_size
        if not size_validation:
            if chunk_resume.get(start_point, None) is None:
                chunk_resume = {start_point: 1}
            else:
                chunk_resume[start_point] += 1
            if chunk_resume[start_point] == 3 or not resume:
                raise PlatformException(
                    error=500,
                    message='The downloaded file is corrupted. Please try again. If the issue repeats please contact support.')
        return size_validation, start_point, chunk_resume

    def __default_local_path(self):

        # create default local path
        if self.items_repository._dataset is None:
            local_path = os.path.join(
                self.items_repository._client_api.sdk_cache.cache_path_bin,
                "items",
            )
        else:
            if self.items_repository.dataset._project is None:
                # by dataset name
                local_path = os.path.join(
                    self.items_repository._client_api.sdk_cache.cache_path_bin,
                    "datasets",
                    "{}_{}".format(self.items_repository.dataset.name, self.items_repository.dataset.id),
                )
            else:
                # by dataset and project name
                local_path = os.path.join(
                    self.items_repository._client_api.sdk_cache.cache_path_bin,
                    "projects",
                    self.items_repository.dataset.project.name,
                    "datasets",
                    self.items_repository.dataset.name,
                )
        logger.info("Downloading to: {}".format(local_path))
        return local_path

    @staticmethod
    def get_url_stream(url):
        """
        :param url:
        """
        # This will download the binaries from the URL user provided
        prepared_request = requests.Request(method='GET', url=url).prepare()
        with requests.Session() as s:
            retry = Retry(
                total=3,
                read=3,
                connect=3,
                backoff_factor=1,
            )
            adapter = HTTPAdapter(max_retries=retry)
            s.mount('http://', adapter)
            s.mount('https://', adapter)
            response = s.send(request=prepared_request, stream=True)

        return response


================================================
File: dtlpy/repositories/dpks.py
================================================
import json
import logging
import os
from typing import List, Optional
from pathlib import Path

from .. import exceptions, entities, services, miscellaneous, assets
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Dpks:
    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project

    @property
    def project(self) -> Optional[entities.Project]:
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def init(self, directory: str = None, name: str = None, description: str = None,
             attributes: dict = None, icon: str = None, scope: str = None):
        """
        Initialize a dpk project with the specified projects.

        :param str directory: the directory where to initialize the project
        :param str name: the name of the dpk.
        :param str description: the description of the dpk.
        :param str attributes: the attributes of the dpk.
        :param str icon: the icon of the dpk.
        :param str scope: the scope of the dpk.

        ** Example **
        .. code-block:: python
            dl.dpks.init(name='Hello World', description='A description of the dpk', attributes={
                "Provider": "Dataloop",
                "License": "",
                "Category": "Model",
                "Computer Vision": "Object Detection",
                "Media Type": "Image"
              },
                        icon='path_to_icon', scope='organization')
        """
        if directory is None:
            directory = os.getcwd()
        dpk = entities.Dpk.from_json(_json={'name': miscellaneous.JsonUtils.get_if_absent(name),
                                            'description': miscellaneous.JsonUtils.get_if_absent(description),
                                            'attributes': miscellaneous.JsonUtils.get_if_absent(attributes),
                                            'icon': miscellaneous.JsonUtils.get_if_absent(icon),
                                            'scope': miscellaneous.JsonUtils.get_if_absent(scope, 'organization'),
                                            'components': dict()
                                            },
                                     client_api=self._client_api)
        dataloop_filepath = os.path.join(directory, assets.paths.APP_JSON_FILENAME)
        with open(dataloop_filepath, 'w') as json_file:
            json_file.write(json.dumps(dpk.to_json(), indent=4))
        os.makedirs(os.path.join(directory, 'functions'), exist_ok=True)
        os.makedirs(os.path.join(directory, 'panels'), exist_ok=True)

    # noinspection PyMethodMayBeStatic
    def pack(self, directory: str = None, name: str = None, subpaths_to_append: List[str] = None) -> str:
        """
        :param str directory: optional - the project to pack, if not specified use the current project,
        :param str name: optional - the name of the dpk file.
        :param List[str] subpaths_to_append: optional - the files/directories to add to the dpk file.
                                            (along with functions, panels and dataloop.json)
        :return the path of the dpk file

        **Example**

        .. code-block:: python
            filepath = dl.apps.pack(directory='/my-current-project', name='project-name')
        """
        # create/get .dataloop dir
        cwd = os.getcwd()
        dl_dir = os.path.join(cwd, '.dataloop')
        if not os.path.isdir(dl_dir):
            os.mkdir(dl_dir)
        if directory is None:
            directory = cwd

        # get dpk name
        if name is None:
            name = os.path.basename(directory)

        with open(os.path.join(directory, 'dataloop.json'), 'r') as file:
            app_json = json.load(file)
        version = app_json.get('version', None)
        if version is None:
            logger.warning('No Version specified, setting to 1.0.0')
            version = '1.0.0'
        # create/get dist folder
        dpk_filename = os.path.join(dl_dir, '{}_{}.dpk'.format(name, version))

        if not os.path.isdir(directory):
            raise ValueError('Not a directory: {}'.format(directory))
        if subpaths_to_append is None:
            subpaths_to_append = []

        try:
            directory = os.path.abspath(directory)
            # create zipfile
            miscellaneous.Zipping.zip_directory(zip_filename=dpk_filename,
                                                directory=directory,
                                                ignore_directories=['artifacts'])
            return dpk_filename
        except Exception:
            logger.error('Error when packing:')
            raise

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Dpk]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.Dpk._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item,
                                          'project': self._project})
        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        items = miscellaneous.List([r[1] for r in results if r[0] is True])
        return items

    def pull(self, dpk: entities.Dpk = None, dpk_id: str = None, dpk_name: str = None, local_path=None) -> str:
        """
        Pulls the app from the platform as dpk file.

        Note: you must pass either dpk_name or dpk_id to the function.
        :param str dpk: DPK entity.
        :param str dpk_id: the name of the dpk.
        :param str dpk_name: the id of the dpk.
        :param local_path: the path where you want to install the dpk file.
        :return local path where the package pull

        **Example**
        ..code-block:: python
            path = dl.dpks.pull(dpk_name='my-app')
        """
        if dpk is None:
            dpk = self.get(dpk_id=dpk_id, dpk_name=dpk_name)
        if local_path is None:
            local_path = os.path.join(
                services.service_defaults.DATALOOP_PATH,
                "dpks",
                dpk.name,
                str(dpk.version))

        dpk.codebases.unpack(codebase=dpk.codebase, local_path=local_path)
        return local_path

    def __get_by_name(self, dpk_name: str, dpk_version: str = None):
        filters = entities.Filters(field='name',
                                   values=dpk_name,
                                   resource=entities.FiltersResource.DPK,
                                   use_defaults=False)
        dpks = self.list(filters=filters)
        # only latest version returns so should be only one
        if dpks.items_count == 0:
            raise exceptions.PlatformException(error='404',
                                               message='Dpk not found. Name: {}'.format(dpk_name))
        elif dpks.items_count > 1:
            raise exceptions.PlatformException(
                error='400',
                message='More than one dpk found by the name of: {}'.format(dpk_name))
        dpk: entities.Dpk = dpks.items[0]

        ########################
        # get specific version #
        ########################
        if dpk_version is not None and dpk.version != dpk_version:
            filters = entities.Filters(field='version',
                                       values=dpk_version,
                                       resource=entities.FiltersResource.DPK,
                                       use_defaults=False)
            dpk_v = dpk._get_revision_pages(filters=filters)
            if dpk_v.items_count == 0:
                raise exceptions.PlatformException(
                    error='404',
                    message=f'Dpk version not found. Name: {dpk_name}, version: {dpk_version}')
            elif dpk_v.items_count > 1:
                # should never be here - more than one with same name and version
                raise exceptions.PlatformException(
                    error='400',
                    message=f'More than one dpk found with name: {dpk_name}, version: {dpk_version}')
            dpk = dpk_v.items[0]
        return dpk

    def publish(
            self,
            dpk: entities.Dpk = None,
            ignore_max_file_size: bool = False,
            manifest_filepath='dataloop.json',
            local_path: str = None
    ) -> entities.Dpk:
        """
        Upload a dpk entity to the dataloop platform.

        :param entities.Dpk dpk: Optional. The DPK entity to publish. If None, a new DPK is created
                             from the manifest file.
        :param bool ignore_max_file_size: Optional. If True, the maximum file size check is ignored
                                        during the packaging of the codebase.
        :param str manifest_filepath: Optional. Path to the manifest file. Can be absolute or relative.
                                    Defaults to 'dataloop.json'
        :param str local_path: Optional. The path where the dpk files are located.

        :return the published dpk
        :rtype dl.entities.Dpk

        **Example**

        .. code-block:: python
            published_dpk = dl.dpks.publish()
        """
        manifest_path = Path(manifest_filepath).resolve()

        if dpk is None:
            if not manifest_path.exists():
                raise FileNotFoundError(f'{manifest_filepath} file must exist in order to publish a dpk')
            with open(manifest_filepath, 'r') as f:
                json_file = json.load(f)
            dpk = entities.Dpk.from_json(_json=json_file,
                                         client_api=self._client_api,
                                         project=self.project)

        if not dpk.context:
            dpk.context = {}
        if 'project' not in dpk.context:
            if not self.project:
                raise exceptions.PlatformException('400', 'project id must be provided in the context')
            dpk.context['project'] = self.project.id
        if 'org' not in dpk.context and dpk.scope == 'organization':
            if not self.project:
                raise exceptions.PlatformException('400', 'org id must be provided in the context')
            dpk.context['org'] = self.project.org['id']

        if self.project and self.project.id != dpk.context['project']:
            logger.warning("the project id that provide different from the dpk project id")

        if local_path is None:
            if manifest_filepath=='dataloop.json':
                local_path = os.getcwd()
            else:
                local_path = os.path.dirname(manifest_filepath)
        if dpk.codebase is None:
            dpk.codebase = self.project.codebases.pack(directory=local_path,
                                                       name=dpk.display_name,
                                                       extension='dpk',
                                                       ignore_directories=['artifacts'],
                                                       ignore_max_file_size=ignore_max_file_size)

        success_pack, response_pack = self._client_api.gen_request(req_type='post',
                                                                   json_req=dpk.to_json(),
                                                                   path='/app-registry')
        if not success_pack:
            raise exceptions.PlatformException(error=response_pack)

        return entities.Dpk.from_json(response_pack.json(), self._client_api, dpk.project)

    def update(self, dpk: entities.Dpk) -> entities.Dpk:
        """
        Update the dpk entity in the platform.

        Note: the update will update only attributes, displayName, Icon, description and pipeline template preview

        :param entities.Dpk dpk: the dpk entity to update.
        :return the updated dpk entity.
        :rtype entities.Dpk
        """
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path='/app-registry/{}'.format(dpk.id),
                                                         json_req=dpk.to_json())
        if not success:
            raise exceptions.PlatformException(response)
        res = response.json()
        logger.info(res.get('message'))
        return entities.Dpk.from_json(res.get('dpk'), self._client_api, self._project)

    def delete(self, dpk_id: str) -> bool:
        """
        Delete the dpk from the app store.

        Note: after removing the dpk, you cant get it again, it's advised to pull it first.

        :param str dpk_id: the id of the dpk.
        :return whether the operation ran successfully
        :rtype bool
        """
        success, response = self._client_api.gen_request(req_type='delete', path=f'/app-registry/{dpk_id}')
        if success:
            logger.info(f'Deleted dpk: {dpk_id} successfully')
        else:
            raise exceptions.PlatformException(response)
        return success

    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List the available dpks.

        :param entities.Filters filters: the filters to apply on the list
        :return a paged entity representing the list of dpks.

        ** Example **
        .. code-block:: python
            dpks = dl.dpks.list()
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.DPK)
        elif not isinstance(filters, entities.Filters):
            raise ValueError('Unknown filters type: {!r}'.format(type(filters)))
        elif filters.resource != entities.FiltersResource.DPK:
            raise ValueError('Filters resource must to be FiltersResource.DPK. Got: {!r}'.format(filters.resource))

        if self._project is not None:
            filters.add(field='context.project', values=self._project.id)

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def _list(self, filters: entities.Filters):
        url = '/app-registry/query'

        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def get_revisions(self, dpk_id: str, version: str):
        """
        Get the revision of a specific dpk.

        :param str dpk_id: the id of the dpk.
        :param str version: the version of the dpk.
        :return the entity of the dpk
        :rtype entities.Dpk

        ** Example **
        ..coed-block:: python
            dpk = dl.dpks.get_revisions(dpk_id='id', version='1.0.0')
        """
        if dpk_id is None or version is None:
            raise ValueError('You must provide both dpk_id and version')
        url = '/app-registry/{}/revisions/{}'.format(dpk_id, version)

        # request
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)
        if not success:
            raise exceptions.PlatformException(response)

        dpk = entities.Dpk.from_json(_json=response.json(),
                                     client_api=self._client_api,
                                     project=self._project,
                                     is_fetched=False)
        return dpk

    def get(self, dpk_name: str = None, dpk_version: str = None, dpk_id: str = None) -> entities.Dpk:
        """
        Get a specific dpk from the platform.

        Note: you must pass either dpk_id or dpk_name.

        :param str dpk_id: the id of the dpk to get.
        :param str dpk_name: the name of the dpk to get.
        :param str dpk_version: options - to get a specific dpk version
        :return the entity of the dpk
        :rtype entities.Dpk

        ** Example **
        ..coed-block:: python
            dpk = dl.dpks.get(dpk_name='name')
        """
        if dpk_id is None and dpk_name is None:
            raise ValueError('You must provide an identifier, either dpk_id or dpk_name')
        if dpk_id is not None:
            url = '/app-registry/{}'.format(dpk_id)

            # request
            success, response = self._client_api.gen_request(req_type='get',
                                                             path=url)
            if not success:
                raise exceptions.PlatformException(response)

            dpk = entities.Dpk.from_json(_json=response.json(),
                                         client_api=self._client_api,
                                         project=self._project,
                                         is_fetched=False)
        else:
            dpk = self.__get_by_name(dpk_name=dpk_name, dpk_version=dpk_version)

        return dpk


================================================
File: dtlpy/repositories/drivers.py
================================================
import logging

from .. import entities, miscellaneous, exceptions, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Drivers:
    """
    Drivers Repository
    
    The Drivers class allows users to manage drivers that are used to connect with external storage.
    Read more about external storage in our `documentation <https://dataloop.ai/docs/overview-1>`_ and `developers' docs <https://developers.dataloop.ai/tutorials/data_management/>`_.
    """

    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            # try get checkout
            project = self._client_api.state_io.get('project')
            if project is not None:
                self._project = entities.Project.from_json(_json=project, client_api=self._client_api)
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Cannot perform action WITHOUT Project entity in Drivers repository.'
                        ' Please checkout or set a project')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ###########
    # methods #
    ###########
    def __get_by_id(self, driver_id) -> entities.Driver:
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/drivers/{}'.format(driver_id))
        if success:
            _json = response.json()
            driver = self._getDriverClass(_json).from_json(client_api=self._client_api,
                                                           _json=_json)
        else:
            raise exceptions.PlatformException(response)
        return driver

    @_api_reference.add(path='/drivers', method='get')
    def list(self) -> miscellaneous.List[entities.Driver]:
        """
        Get the project's drivers list.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :return: List of Drivers objects
        :rtype: list

        **Example**:

        .. code-block:: python

            project.drivers.list()

        """

        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/drivers?projectId={}'.format(self.project.id))
        if not success:
            raise exceptions.PlatformException(response)
        drivers = miscellaneous.List([self._getDriverClass(_json).from_json(_json=_json,
                                                                            client_api=self._client_api) for _json in
                                      response.json()])
        return drivers

    def _getDriverClass(self, _json):
        driver_type = _json.get('type', None)
        if driver_type == entities.ExternalStorage.S3:
            driver_class = entities.S3Driver
        elif driver_type == entities.ExternalStorage.GCS:
            driver_class = entities.GcsDriver
        elif driver_type in [entities.ExternalStorage.AZUREBLOB, entities.ExternalStorage.AZURE_DATALAKE_GEN2]:
            driver_class = entities.AzureBlobDriver
        else:
            driver_class = entities.Driver
        return driver_class

    @_api_reference.add(path='/drivers/{id}', method='get')
    def get(self,
            driver_name: str = None,
            driver_id: str = None) -> entities.Driver:
        """
        Get a Driver object to use in your code.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        You must provide at least ONE of the following params: driver_name, driver_id.

        :param str driver_name: optional - search by name
        :param str driver_id: optional - search by id
        :return: Driver object
        :rtype: dtlpy.entities.driver.Driver

        **Example**:

        .. code-block:: python

            project.drivers.get(driver_id='driver_id')
        """
        if driver_id is not None:
            driver = self.__get_by_id(driver_id)
        elif driver_name is not None:
            drivers = self.list()
            driver = [driver for driver in drivers if driver.name == driver_name]
            if not driver:
                # list is empty
                raise exceptions.PlatformException(error='404',
                                                   message='Driver not found. Name: {}'.format(driver_name))
                # driver = None
            elif len(driver) > 1:
                # more than one matching driver
                raise exceptions.PlatformException(
                    error='404',
                    message='More than one driver with same name. Please "get" by id')
            else:
                driver = driver[0]
        else:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier (name or id) in inputs')
        return driver

    @_api_reference.add(path='/drivers', method='post')
    def create(self,
               name: str,
               driver_type: entities.ExternalStorage,
               integration_id: str,
               bucket_name: str,
               integration_type: entities.IntegrationType,
               project_id: str = None,
               allow_external_delete: bool = True,
               region: str = None,
               storage_class: str = "",
               path: str = ""):
        """
        Create a storage driver.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str name: the driver name
        :param ExternalStorage driver_type: dl.ExternalStorage (Enum). For all options run: list(dl.ExsternalStorage)
        :param str integration_id: the integration id
        :param str bucket_name: the external bucket name
        :param IntegrationType integration_type: dl.IntegrationType (Enum). For all options run: list(dl.IntegrationType)
        :param str project_id: project id
        :param bool allow_external_delete: true to allow deleting files from external storage when files are deleted in your Dataloop storage
        :param str region: relevant only for s3 - the bucket region
        :param str storage_class: relevant only for s3
        :param str path: Optional. By default path is the root folder. Path is case sensitive integration
        :return: driver object
        :rtype: dtlpy.entities.driver.Driver

        **Example**:

        .. code-block:: python

            project.drivers.create(name='driver_name',
                       driver_type=dl.ExternalStorage.S3,
                       integration_id='integration_id',
                       bucket_name='bucket_name',
                       project_id='project_id',
                       region='ey-west-1')
        """
        if integration_type is None:
            integration_type = driver_type
        if driver_type == entities.ExternalStorage.S3:
            bucket_payload = 'bucketName'
        elif driver_type == entities.ExternalStorage.GCS:
            bucket_payload = 'bucket'
        else:
            bucket_payload = 'containerName'
        payload = {
            "integrationId": integration_id,
            'integrationType': integration_type,
            "name": name,
            "metadata": {
                "system": {
                    "projectId": self.project.id if project_id is None else project_id
                }
            },
            "type": driver_type,
            "payload": {
                bucket_payload: bucket_name,
                "storageClass": storage_class,
                "region": region,
                "path": path
            },
            "allowExternalDelete": allow_external_delete,
            "creator": self._client_api.info().get('user_email')
        }

        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/drivers',
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        else:
            _json = response.json()
            return self._getDriverClass(_json).from_json(_json=_json, client_api=self._client_api)

    @_api_reference.add(path='/drivers/{id}', method='delete')
    def delete(self,
               driver_name: str = None,
               driver_id: str = None,
               sure: bool = False,
               really: bool = False):
        """
        Delete a driver forever!

        **Prerequisites**: You must be an *owner* or *developer* to use this method.

        **Example**:

        .. code-block:: python

            project.drivers.delete(dataset_id='dataset_id', sure=True, really=True)

        :param str driver_name: optional - search by name
        :param str driver_id: optional - search by id
        :param bool sure: Are you sure you want to delete?
        :param bool really: Really really sure?
        :return: True if success
        :rtype: bool
        """
        if sure and really:
            driver = self.get(driver_name=driver_name, driver_id=driver_id)
            success, response = self._client_api.gen_request(req_type='delete',
                                                             path='/drivers/{}'.format(driver.id))
            if not success:
                raise exceptions.PlatformException(response)
            logger.info('Driver {!r} was deleted successfully'.format(driver.name))
            return True
        else:
            raise exceptions.PlatformException(
                error='403',
                message='Cant delete driver from SDK. Please login to platform to delete')


================================================
File: dtlpy/repositories/executions.py
================================================
import threading
import logging
import time
from copy import deepcopy

import numpy as np

from .. import exceptions, entities, repositories, miscellaneous, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')
MAX_SLEEP_TIME = 30


class Executions:
    """
    Service Executions Repository

    The Executions class allows the users to manage executions (executions of services) and their properties.
    See our documentation for more information about `executions <https://developers.dataloop.ai/tutorials/faas/execution_control/chapter/>`_.
    """

    def __init__(self,
                 client_api: ApiClient,
                 service: entities.Service = None,
                 project: entities.Project = None):
        self._client_api = client_api
        self._service = service
        self._project = project

    ############
    # entities #
    ############
    @property
    def service(self) -> entities.Service:
        if self._service is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "service". need to set a Service entity or use service.executions repository')
        assert isinstance(self._service, entities.Service)
        return self._service

    @service.setter
    def service(self, service: entities.Service):
        if not isinstance(service, entities.Service):
            raise ValueError('Must input a valid Service entity')
        self._service = service

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            if self._service is not None:
                self._project = self._service._project
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use Project.executions repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def __get_from_entity(self, name, value):
        project_id = None
        try:
            from_dataset = False
            entity_obj = None
            for input_type in entities.FunctionIO.INPUT_TYPES:
                input_type = input_type.value
                entity = input_type.lower()
                param = '{}_id'.format(entity)
                if isinstance(value, dict):
                    if param in value:
                        repo = getattr(repositories, '{}s'.format(input_type))(client_api=self._client_api)
                        entity_obj = repo.get(**{param: value[param]})
                        if param in ['annotation_id', 'item_id']:
                            from_dataset = True
                            if param == 'item_id':
                                entity_obj = entity_obj.dataset
                            else:
                                entity_obj = repositories.Datasets(client_api=self._client_api).get(
                                    dataset_id=entity_obj.dataset_id)
                        elif param in ['dataset_id']:
                            from_dataset = True
                        break
                elif isinstance(value, str):
                    if entity == name:
                        repo = getattr(repositories, '{}s'.format(input_type))(client_api=self._client_api)
                        entity_obj = repo.get(**{param: value})
                        if name in ['annotation', 'item']:
                            from_dataset = True
                            if param == 'item_id':
                                entity_obj = entity_obj.dataset
                            else:
                                entity_obj = repositories.Datasets(client_api=self._client_api).get(
                                    dataset_id=entity_obj.dataset_id)
                        elif name in ['dataset']:
                            from_dataset = True
                        break

            if entity_obj is not None:
                if isinstance(entity_obj, entities.Project):
                    project_id = entity_obj.id
                elif from_dataset:
                    project_id = entity_obj.projects[0]
                else:
                    project_id = entity_obj.project_id
        except Exception:
            project_id = None

        return project_id

    def __get_project_id(self, project_id=None, payload=None):
        if project_id is None:
            inputs = payload.get('input', dict())
            try:
                for key, val in inputs.items():
                    project_id = self.__get_from_entity(name=key, value=val)
                    if project_id is not None:
                        break
            except Exception:
                project_id = None

            if project_id is None:
                # if still None - get from current repository
                if self._project is not None:
                    project_id = self._project.id
                else:
                    raise exceptions.PlatformException('400', 'Please provide project_id')

        return project_id

    ###########
    # methods #
    ###########
    @_api_reference.add(path='/executions/{serviceId}', method='post')
    def create(self,
               # executions info
               service_id: str = None,
               execution_input: list = None,
               function_name: str = None,
               # inputs info
               resource: entities.PackageInputType = None,
               item_id: str = None,
               dataset_id: str = None,
               annotation_id: str = None,
               project_id: str = None,
               # execution config
               sync: bool = False,
               stream_logs: bool = False,
               return_output: bool = False,
               # misc
               return_curl_only: bool = False,
               timeout: int = None) -> entities.Execution:
        """
        Execute a function on an existing service

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str service_id: service id to execute on
        :param List[FunctionIO] or dict execution_input: input dictionary or list of FunctionIO entities
        :param str function_name: function name to run
        :param str resource: input type.
        :param str item_id: optional - item id as input to function
        :param str dataset_id: optional - dataset id as input to function
        :param str annotation_id: optional - annotation id as input to function
        :param str project_id: resource's project
        :param bool sync: if true, wait for function to end
        :param bool stream_logs: prints logs of the new execution. only works with sync=True
        :param bool return_output: if True and sync is True - will return the output directly
        :param bool return_curl_only: return the cURL of the creation WITHOUT actually do it
        :param int timeout: int, seconds to wait until TimeoutError is raised. if <=0 - wait until done -
         by default wait take the service timeout
        :return: execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.create(function_name='function_name', item_id='item_id', project_id='project_id')
        """
        if service_id is None:
            if self._service is None:
                raise exceptions.PlatformException('400', 'Please provide service id')
            service_id = self._service.id

        if resource is None:
            if annotation_id is not None:
                resource = entities.PackageInputType.ANNOTATION
            elif item_id is not None:
                resource = entities.PackageInputType.ITEM
            elif dataset_id is not None:
                resource = entities.PackageInputType.DATASET

        # payload
        payload = dict()
        if execution_input is None:
            if resource is not None:
                inputs = {resource.lower(): {
                    'dataset_id': dataset_id}
                }
                if item_id is not None:
                    inputs[resource.lower()]['item_id'] = item_id
                if annotation_id is not None:
                    inputs[resource.lower()]['annotation_id'] = annotation_id
                payload['input'] = inputs
        else:
            if isinstance(execution_input, dict):
                payload['input'] = execution_input
            else:
                if not isinstance(execution_input, list):
                    execution_input = [execution_input]
                if len(execution_input) > 0 and isinstance(execution_input[0], entities.FunctionIO):
                    payload['input'] = dict()
                    for single_input in execution_input:
                        payload['input'].update(single_input.to_json(resource='execution'))
                else:
                    raise exceptions.PlatformException('400', 'Unknown input type')

        payload['projectId'] = self.__get_project_id(project_id=project_id, payload=payload)

        if function_name is not None:
            payload['functionName'] = function_name
        else:
            payload['functionName'] = entities.package_defaults.DEFAULT_PACKAGE_FUNCTION_NAME

        # request url
        url_path = '/executions/{service_id}'.format(service_id=service_id)

        if return_curl_only:
            curl = self._client_api.export_curl_request(req_type='post',
                                                        path=url_path,
                                                        json_req=payload)
            logger.warning(msg='Execution was NOT created. Exporting cURL only.')
            return curl
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path,
                                                         json_req=payload)
        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        execution = entities.Execution.from_json(_json=response.json(),
                                                 client_api=self._client_api,
                                                 project=self._project,
                                                 service=self._service)

        if sync and not return_output and not stream_logs:
            execution = self.wait(execution_id=execution.id, timeout=timeout)

        if sync and (stream_logs or return_output):
            thread = None
            if stream_logs:
                thread = threading.Thread(target=self.logs,
                                          kwargs={'execution_id': execution.id,
                                                  'follow': True,
                                                  'until_completed': True})
                thread.setDaemon(True)
                thread.start()
            execution = self.get(execution_id=execution.id,
                                 sync=True)
            # stream logs
            if stream_logs and thread is not None:
                thread.join()
        if sync and return_output:
            return execution.output
        return execution

    @_api_reference.add(path='/executions/{serviceId}', method='post')
    def create_batch(self,
                     service_id: str,
                     filters,
                     function_name: str = None,
                     execution_inputs: list = None,
                     wait=True
                     ):
        """
        Execute a function on an existing service

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str service_id: service id to execute on
        :param filters: Filters entity for a filtering before execute
        :param str function_name: function name to run
        :param List[FunctionIO] or dict execution_inputs: input dictionary or list of FunctionIO entities, that represent the extra inputs of the function
        :param bool wait: wait until create task finish
        :return: execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            command = service.executions.create_batch(
                        execution_inputs=dl.FunctionIO(type=dl.PackageInputType.STRING, value='test', name='string'),
                        filters=dl.Filters(field='dir', values='/test'),
                        function_name='run')
        """
        if service_id is None:
            if self._service is None:
                raise exceptions.PlatformException('400', 'Please provide service id')
            service_id = self._service.id

        if filters is None:
            raise exceptions.PlatformException('400', 'Please provide filter')

        if execution_inputs is None:
            execution_inputs = dict()

        if isinstance(execution_inputs, dict):
            extra_inputs = execution_inputs
        else:
            if not isinstance(execution_inputs, list):
                execution_inputs = [execution_inputs]
            if len(execution_inputs) > 0 and isinstance(execution_inputs[0], entities.FunctionIO):
                extra_inputs = dict()
                for single_input in execution_inputs:
                    extra_inputs.update(single_input.to_json(resource='execution'))
            else:
                raise exceptions.PlatformException('400', 'Unknown input type')

        # payload
        payload = dict()
        payload['batch'] = dict()
        payload['batch']['query'] = filters.prepare()
        payload['batch']['args'] = extra_inputs

        if function_name is not None:
            payload['functionName'] = function_name
        else:
            payload['functionName'] = entities.package_defaults.DEFAULT_PACKAGE_FUNCTION_NAME

        # request url
        url_path = '/executions/{service_id}'.format(service_id=service_id)

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path,
                                                         json_req=payload)
        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        response_json = response.json()
        command = entities.Command.from_json(_json=response_json,
                                             client_api=self._client_api)
        if wait:
            command = command.wait(timeout=0)
        return command

    @_api_reference.add(path='/executions/rerun', method='post')
    def rerun_batch(self,
                    filters,
                    service_id: str = None,
                    wait=True
                    ):
        """
        rerun a executions on an existing service

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a Filter.

        :param filters: Filters entity for a filtering before rerun
        :param str service_id: service id to rerun on
        :param bool wait: wait until create task finish
        :return: rerun command
        :rtype: dtlpy.entities.command.Command

        **Example**:

        .. code-block:: python

            command = service.executions.rerun_batch(
                        filters=dl.Filters(field='id', values=['executionId'], operator=dl.FiltersOperations.IN, resource=dl.FiltersResource.EXECUTION))
        """
        url_path = '/executions/rerun'

        if filters is None:
            raise exceptions.PlatformException('400', 'Please provide filter')

        if filters.resource != entities.FiltersResource.EXECUTION:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.EXECUTION. Got: {!r}'.format(filters.resource))

        if service_id is not None and not filters.has_field('serviceId'):
            filters = deepcopy(filters)
            filters.add(field='serviceId', values=service_id, method=entities.FiltersMethod.AND)

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path,
                                                         json_req={'query': filters.prepare()['filter']})
        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        response_json = response.json()
        command = entities.Command.from_json(_json=response_json,
                                             client_api=self._client_api)
        if wait:
            command = command.wait(timeout=0)
        return command

    def _list(self, filters: entities.Filters):
        """
        List service executions

        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters items
        :return:
        """
        url = '/query/faas'

        # request
        success, response = self._client_api.gen_request(req_type='POST',
                                                         path=url,
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    @_api_reference.add(path='/query/faas', method='post')
    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List service executions

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters items
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            service.executions.list()
        """
        # default filtersf
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.EXECUTION)
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(
                error='400',
                message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.EXECUTION:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.EXECUTION. Got: {!r}'.format(filters.resource))
        if self._project is not None:
            filters.add(field='projectId', values=self._project.id)
        if self._service is not None:
            filters.add(field='serviceId', values=self._service.id)

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Execution]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return execution list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.Execution._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item,
                                          'project': self._project,
                                          'service': self._service})

        # get results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        return miscellaneous.List([r[1] for r in results if r[0] is True])

    @_api_reference.add(path='/executions/{id}', method='get')
    def get(self,
            execution_id: str = None,
            sync: bool = False
            ) -> entities.Execution:
        """
        Get Service execution object

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str execution_id: execution id
        :param bool sync: if true, wait for the execution to finish
        :return: Service execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.get(execution_id='execution_id')
        """
        url_path = "/executions/{}".format(execution_id)
        if sync:
            return self.wait(execution_id=execution_id)

        success, response = self._client_api.gen_request(req_type="get",
                                                         path=url_path)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Execution.from_json(client_api=self._client_api,
                                            _json=response.json(),
                                            project=self._project,
                                            service=self._service)

    def logs(self,
             execution_id: str,
             follow: bool = True,
             until_completed: bool = True):
        """
        executions logs

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str execution_id: execution id
        :param bool follow: if true, keep stream future logs
        :param bool until_completed: if true, wait until completed
        :return: executions logs

        **Example**:

        .. code-block:: python

            service.executions.logs(execution_id='execution_id')
        """
        return self.service.log(execution_id=execution_id,
                                follow=follow,
                                until_completed=until_completed,
                                view=True)

    def increment(self, execution: entities.Execution):
        """
        Increment the number of attempts that an execution is allowed to attempt to run a service that is not responding.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.execution.Execution execution:
        :return: int
        :rtype: int

        **Example**:

        .. code-block:: python

            service.executions.increment(execution='execution_entity')
        """
        # request
        success, response = self._client_api.gen_request(
            req_type='post',
            path='/executions/{}/attempts'.format(execution.id)
        )

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)
        else:
            return response.json()

    @_api_reference.add(path='/executions/{executionId}/rerun', method='post')
    def rerun(self, execution: entities.Execution, sync: bool = False):
        """
        Rerun execution

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.execution.Execution execution:
        :param bool sync: wait for the execution to finish
        :return: Execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.rerun(execution='execution_entity')
        """

        url_path = "/executions/{}/rerun".format(execution.id)
        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)
        else:
            execution = entities.Execution.from_json(
                client_api=self._client_api,
                _json=response.json(),
                project=self._project,
                service=self._service
            )
            if sync:
                execution = self.wait(execution_id=execution.id)
        return execution

    def wait(self,
             execution_id: str = None,
             execution: entities.Execution = None,
             timeout: int = None,
             backoff_factor=1):
        """
        Get Service execution object.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str execution_id: execution id
        :param str execution: dl.Execution, optional. must input one of execution or execution_id
        :param int timeout: seconds to wait until TimeoutError is raised. if <=0 - wait until done - by default wait take the service timeout
        :param float backoff_factor: A backoff factor to apply between attempts after the second try
        :return: Service execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.wait(execution_id='execution_id')
        """
        if execution is None:
            if execution_id is None:
                raise ValueError('Must input at least one: [execution, execution_id]')
            else:
                execution = self.get(execution_id=execution_id)
        elapsed = 0
        start = int(time.time())
        if timeout is None or timeout <= 0:
            timeout = np.inf

        num_tries = 1
        while elapsed < timeout:
            execution = self.get(execution_id=execution.id)
            if not execution.in_progress():
                break
            elapsed = time.time() - start
            if elapsed >= timeout:
                raise TimeoutError(
                    f"execution wait() got timeout. id: {execution.id!r}, status: {execution.latest_status}")
            sleep_time = np.min([timeout - elapsed, backoff_factor * (2 ** num_tries), MAX_SLEEP_TIME])
            num_tries += 1
            logger.debug("Execution {!r} is running for {:.2f}[s] and now Going to sleep {:.2f}[s]".format(execution.id,
                                                                                                           elapsed,
                                                                                                           sleep_time))
            time.sleep(sleep_time)

        return execution

    @_api_reference.add(path='/executions/{id}/terminate', method='post')
    def terminate(self, execution: entities.Execution):
        """
        Terminate Execution

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.execution.Execution execution:
        :return: execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.terminate(execution='execution_entity')
        """
        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/executions/{}/terminate'.format(execution.id))

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)
        else:
            return entities.Execution.from_json(_json=response.json(),
                                                service=self._service,
                                                project=self._project,
                                                client_api=self._client_api)

    @_api_reference.add(path='/executions/{id}', method='patch')
    def update(self, execution: entities.Execution) -> entities.Execution:
        """
        Update execution changes to platform

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.execution.Execution execution: execution entity
        :return: Service execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.update(execution='execution_entity')
        """
        # payload
        payload = execution.to_json()

        # request
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path='/executions/{}'.format(execution.id),
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        if self._project is not None:
            project = self._project
        else:
            project = execution._project

        # return
        if self._service is not None:
            service = self._service
        else:
            service = execution._service

        return entities.Execution.from_json(_json=response.json(),
                                            service=service,
                                            project=self._project,
                                            client_api=self._client_api)

    def progress_update(
            self,
            execution_id: str,
            status: entities.ExecutionStatus = None,
            percent_complete: int = None,
            message: str = None,
            output: str = None,
            service_version: str = None
    ):
        """
        Update Execution Progress.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str execution_id: execution id
        :param str status: ExecutionStatus
        :param int percent_complete: percent work done
        :param str message: message
        :param str output: the output of the execution
        :param str service_version: service version
        :return: Service execution object
        :rtype: dtlpy.entities.execution.Execution

        **Example**:

        .. code-block:: python

            service.executions.progress_update(execution_id='execution_id', status='complete', percent_complete=100)
        """
        # create payload
        payload = dict()

        if status is not None:
            payload['status'] = status
        else:
            if percent_complete is not None and isinstance(percent_complete, int):
                if percent_complete < 100:
                    payload['status'] = 'inProgress'
                else:
                    payload['status'] = 'completed'
            elif output is not None:
                payload['status'] = 'completed'
            else:
                payload['status'] = 'inProgress'

        if percent_complete is not None:
            payload['percentComplete'] = percent_complete

        if message is not None:
            payload['message'] = message

        if output is not None:
            payload['output'] = output

        if service_version is not None:
            payload['serviceVersion'] = service_version

        # request
        success, response = self._client_api.gen_request(
            req_type="post",
            path="/executions/{}/progress".format(execution_id),
            json_req=payload
        )

        # exception handling
        if success:
            return entities.Execution.from_json(_json=response.json(),
                                                client_api=self._client_api,
                                                project=self._project,
                                                service=self._service)
        else:
            raise exceptions.PlatformException(response)


================================================
File: dtlpy/repositories/feature_sets.py
================================================
import logging
from .. import exceptions, entities, miscellaneous, _api_reference, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class FeatureSets:
    """
    Feature Sets repository
    """
    URL = '/features/sets'

    def __init__(self,
                 client_api: ApiClient,
                 project_id: str = None,
                 project: entities.Project = None,
                 model_id: str = None,
                 model: entities.Model = None):
        self._project = project
        self._project_id = project_id
        self._model = model
        self._model_id = model_id
        self._client_api = client_api

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None and self._project_id is not None:
            # get from id
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self._project_id)
        if self._project is None:
            # try get checkout
            project = self._client_api.state_io.get('project')
            if project is not None:
                self._project = entities.Project.from_json(_json=project, client_api=self._client_api)
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Cannot perform action WITHOUT Project entity in FeatureSets repository.'
                        ' Please checkout or set a project')
        assert isinstance(self._project, entities.Project)
        return self._project

    @property
    def model(self) -> entities.Model:
        if self._model is None and self._model_id is not None:
            # get from id
            self._model = repositories.Models(client_api=self._client_api).get(model_id=self._model_id)
        if self._model is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Cannot perform action WITHOUT Model entity in FeatureSets repository.')
        assert isinstance(self._model, entities.Model)
        return self._model

    ###########
    # methods #
    ###########

    def _list(self, filters: entities.Filters):
        # request
        success, response = self._client_api.gen_request(req_type='POST',
                                                         path='/features/sets/query',
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @_api_reference.add(path='/features/sets/query', method='post')
    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List Feature Sets

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities
        """
        # default filters
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.FEATURE_SET)
        if self._project is not None:
            filters.context = {'projects': [self._project.id]}

        # assert type filters
        if not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))

        if filters.resource != entities.FiltersResource.FEATURE_SET:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.FEATURE_SET. Got: {!r}'.format(filters.resource))

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    @_api_reference.add(path='/features/sets/{id}', method='get')
    def get(self, feature_set_name: str = None, feature_set_id: str = None) -> entities.Feature:
        """
        Get Feature Set object

        :param str feature_set_name: name of the feature set
        :param str feature_set_id: id of the feature set
        :return: Feature object
        """
        if feature_set_id is not None:
            success, response = self._client_api.gen_request(req_type="GET",
                                                             path="{}/{}".format(self.URL, feature_set_id))
            if not success:
                raise exceptions.PlatformException(response)
            feature_set = entities.FeatureSet.from_json(client_api=self._client_api,
                                                        _json=response.json())
        elif feature_set_name is not None:
            if not isinstance(feature_set_name, str):
                raise exceptions.PlatformException(
                    error='400',
                    message='feature_set_name must be string')
            filters = entities.Filters(resource=entities.FiltersResource.FEATURE_SET)
            filters.add(field='name', values=feature_set_name)
            feature_sets = self.list(filters=filters)
            if feature_sets.items_count == 0:
                raise exceptions.PlatformException(
                    error='404',
                    message='Feature set not found. name: {!r}'.format(feature_set_name))
            elif feature_sets.items_count > 1:
                # more than one matching project
                raise exceptions.PlatformException(
                    error='404',
                    message='More than one feature_set with same name. Please "get" by id')
            else:
                feature_set = feature_sets.items[0]
        else:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs, feature_set_name or feature_set_id')
        return feature_set

    @_api_reference.add(path='/features/sets', method='post')
    def create(self, name: str,
               size: int,
               set_type: str,
               entity_type: entities.FeatureEntityType,
               project_id: str = None,
               model_id: set = None,
               org_id: str = None):
        """
        Create a new Feature Set

        :param str name: the Feature name
        :param int size: the length of a single vector in the set
        :param str set_type: string of the feature type: 2d, 3d, modelFC, TSNE,PCA,FFT
        :param entity_type: the entity that feature vector is linked to. Use the enum dl.FeatureEntityType
        :param str project_id: the ID of the project where feature set will be created
        :param str model_id: the ID of the model that creates the vectors
        :param str org_id: the ID of the org where feature set will be created
        :return: Feature Set object
        """
        if project_id is None:
            if self._project is None:
                raise ValueError('Must input a project id')
            else:
                project_id = self._project.id

        payload = {'name': name,
                   'size': size,
                   'type': set_type,
                   'project': project_id,
                   'modelId': model_id,
                   'entityType': entity_type}
        if org_id is not None:
            payload['org'] = org_id
        success, response = self._client_api.gen_request(req_type="post",
                                                         json_req=payload,
                                                         path=self.URL)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.FeatureSet.from_json(client_api=self._client_api,
                                             _json=response.json()[0])

    @_api_reference.add(path='/features/sets/{id}', method='delete')
    def delete(self, feature_set_id: str):
        """
        Delete feature vector

        :param str feature_set_id: feature set id to delete
        :return: success
        :rtype: bool
        """

        success, response = self._client_api.gen_request(req_type="delete",
                                                         path="{}/{}".format(self.URL, feature_set_id))

        # check response
        if success:
            logger.debug("Feature Set deleted successfully")
            return success
        else:
            raise exceptions.PlatformException(response)

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Item]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.FeatureSet._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item})
        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        items = miscellaneous.List([r[1] for r in results if r[0] is True])
        return items


================================================
File: dtlpy/repositories/features.py
================================================
import logging

from .. import exceptions, entities, miscellaneous, _api_reference, repositories
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Features:
    """
    Features repository
    """
    URL = '/features/vectors'

    def __init__(self, client_api: ApiClient,
                 project: entities.Project = None,
                 project_id: str = None,
                 item: entities.Item = None,
                 annotation: entities.Annotation = None,
                 feature_set: entities.FeatureSet = None,
                 dataset: entities.Dataset = None):
        if project is not None and project_id is None:
            project_id = project.id
        self._dataset = dataset
        self._project = project
        self._project_id = project_id
        self._item = item
        self._annotation = annotation
        self._feature_set = feature_set
        self._client_api = client_api

    ############
    # entities #
    ############
    @property
    def feature_set(self) -> entities.FeatureSet:
        return self._feature_set

    @property
    def project(self) -> entities.Project:
        if self._project is None and self._project_id is None and self._item is not None:
            self._project = self._item.project
            self._project_id = self._project.id
        if self._project is None and self._project_id is not None:
            # get from id
            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self._project_id)
        if self._project is None:
            # try get checkout
            project = self._client_api.state_io.get('project')
            if project is not None:
                self._project = entities.Project.from_json(_json=project, client_api=self._client_api)
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Cannot perform action WITHOUT Project entity in Features repository.'
                        ' Please checkout or set a project')
        assert isinstance(self._project, entities.Project)
        return self._project

    ###########
    # methods #
    ###########
    def _list(self, filters: entities.Filters):
        """
        Get dataset feature vectors list. This is a browsing endpoint, for any given path feature count will be returned,
        user is expected to perform another request then for every folder item to actually get the item list.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: json response
        """
        # prepare request
        success, response = self._client_api.gen_request(req_type="POST",
                                                         path="{}/query".format(self.URL),
                                                         json_req=filters.prepare(),
                                                         headers={'user_query': filters._user_query}
                                                         )
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @_api_reference.add(path='/features/vectors', method='post')
    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List of features

        :param dtlpy.entities.filters.Filters filters: Filters to query the features data
        :return: Pages object
        :rtype: dtlpy.entities.paged_entities.PagedEntities
        """
        # default filters
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.FEATURE)
            filters._user_query = 'false'
        # default sorting
        if filters.sort == dict():
            filters.sort_by(field='id')
        # assert type filters
        if not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.FEATURE:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.FEATURE. Got: {!r}'.format(filters.resource))
        if self._feature_set is not None:
            filters.add(field='featureSetId', values=self._feature_set.id)
        if self._item is not None:
            filters.add(field='entityId', values=self._item.id)
        if self._dataset is not None:
            filters.add(field='datasetId', values=self._dataset.id)
        if self._project_id is None:
            self._project_id = self.project.id
        filters.context = {"projects": [self._project_id]}

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    @_api_reference.add(path='/features/vectors/{id}', method='get')
    def get(self, feature_id: str) -> entities.Feature:
        """
        Get Feature object

        :param str feature_id: feature id
        :return: Feature object
        """

        success, response = self._client_api.gen_request(req_type="GET",
                                                         path="{}/{}".format(self.URL, feature_id))

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Feature.from_json(client_api=self._client_api,
                                          _json=response.json())

    @_api_reference.add(path='/features/vectors', method='post')
    def create(self,
               value,
               project_id: str = None,
               feature_set_id: str = None,
               entity=None,
               version: str = None,
               parent_id: str = None,
               org_id: str = None
               ):
        """
        Create a new Feature vector

        :param immutable value: actual vector - immutable (list of floats [1,2,3])
        :param str project_id: the id of the project where feature will be created
        :param str feature_set_id: ref to a featureSet this vector is a part of
        :param entity: the entity the featureVector is linked to (item, annotation, etc)
        :param str version: version of the featureSet generator
        :param str parent_id: optional: parent FeatureSet id - used when FeatureVector is a subFeature
        :param str org_id: the id of the org where featureVector will be created
        :return: Feature vector:
        """
        if project_id is None:
            if self._project is not None:
                project_id = self._project.id
            elif self._project_id is not None:
                project_id = self._project_id
            else:
                raise ValueError('Must insert a project id')

        if feature_set_id is None:
            if self._feature_set is None:
                raise ValueError(
                    'Missing feature_set_id. Must insert the variable or create from context, e.g. feature_set.features.create()')
            feature_set_id = self._feature_set.id

        payload = {'project': project_id,
                   'entityId': entity.id,
                   'value': value,
                   'featureSetId': feature_set_id,
                   'datasetId': entity.dataset.id}

        if version is not None:
            payload['version'] = version
        if parent_id is not None:
            payload['parentId'] = parent_id
        if org_id is not None:
            payload['org'] = org_id

        success, response = self._client_api.gen_request(req_type="post",
                                                         json_req=payload,
                                                         path=self.URL)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Feature.from_json(client_api=self._client_api,
                                          _json=response.json()[0])

    @_api_reference.add(path='/features/vectors/{id}', method='delete')
    def delete(self, feature_id: str):
        """
        Delete feature vector

        :param str feature_id: feature id to delete
        :return: success
        :rtype: bool
        """

        success, response = self._client_api.gen_request(req_type="delete",
                                                         path="{}/{}".format(self.URL, feature_id))

        # check response
        if success:
            logger.debug("Feature deleted successfully")
            return success
        else:
            raise exceptions.PlatformException(response)

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Item]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.Feature._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item})
        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        items = miscellaneous.List([r[1] for r in results if r[0] is True])
        return items


================================================
File: dtlpy/repositories/integrations.py
================================================
"""
Integrations Repository
"""
import base64
import json
import logging
from .. import entities, exceptions, miscellaneous, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Integrations:
    """
    Integrations Repository

    The Integrations class allows you to manage data integrations from your external storage (e.g., S3, GCS, Azure)
    into your Dataloop's Dataset storage, as well as sync data in your Dataloop's Datasets with data in your external
    storage.

    For more information on Organization Storage Integration see the `Dataloop documentation <https://dataloop.ai/docs/organization-integrations>`_  and `developers' docs <https://developers.dataloop.ai/tutorials/data_management/>`_.

    """

    def __init__(self, client_api: ApiClient, org: entities.Organization = None,
                 project: entities.Project = None):
        self._client_api = client_api
        self._org = org
        self._project = project

    @property
    def project(self) -> entities.Project:
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    @property
    def org(self) -> entities.Organization:
        if self._org is None:
            if self.project is not None:
                self._org = entities.Organization.from_json(_json=self.project.org, client_api=self._client_api)
        return self._org

    @org.setter
    def org(self, org: entities.Organization):
        if not isinstance(org, entities.Organization):
            raise ValueError('Must input a valid Organization entity')
        self._org = org

    @_api_reference.add(path='/orgs/{orgId}/integrations/{integrationId}', method='delete')
    def delete(self,
               integrations_id: str,
               sure: bool = False,
               really: bool = False) -> bool:
        """
        Delete integrations from the organization.

        **Prerequisites**: You must be an organization *owner* to delete an integration.

        :param str integrations_id: integrations id
        :param bool sure: Are you sure you want to delete?
        :param bool really: Really really sure?
        :return: success
        :rtype: bool

        **Example**:

        .. code-block:: python

            project.integrations.delete(integrations_id='integrations_id', sure=True, really=True)
        """
        if sure and really:
            if self.project is None and self.org is None:
                raise exceptions.PlatformException(
                    error='400',
                    message='Must provide an identifier in inputs')

            if self.project is not None:
                organization_id = self.project.org.get('id')
            else:
                organization_id = self.org.id

            url_path = '/orgs/{}/integrations/{}'.format(organization_id, integrations_id)
            success, response = self._client_api.gen_request(req_type='delete',
                                                             path=url_path)
            if not success:
                raise exceptions.PlatformException(response)
            else:
                return True
        else:
            raise exceptions.PlatformException(
                error='403',
                message='Cant delete integrations from SDK. Please login to platform to delete')

    @_api_reference.add(path='/orgs/{orgId}/integrations', method='post')
    def create(self,
               integrations_type: entities.IntegrationType,
               name: str,
               options: dict,
               metadata: dict = None):
        """
        Create an integration between an external storage and the organization.

        **Examples for options include**:
        s3 - {key: "", secret: ""};
        gcs - {key: "", secret: "", content: ""};
        azureblob - {key: "", secret: "", clientId: "", tenantId: ""};
        key_value - {key: "", value: ""}
        aws-sts - {key: "", secret: "", roleArns: ""}
        aws-cross - {}
        gcp-cross - {}
        gcp-workload-identity-federation - {"secret": "", "content": "{}", "clientId": ""}
        private-registry (ECR) - {"name": "", "spec": {"accessKeyId": "", "secretAccessKey": "", "account": "", "region": ""}}
        private-registry (GAR) - {"name": "", "spec": {"password": ""}} (can use generate_gar_options to generate the options)

        **Prerequisites**: You must be an *owner* in the organization.

        :param IntegrationType integrations_type: integrations type dl.IntegrationType
        :param str name: integrations name
        :param dict options: dict of storage secrets
        :param dict metadata: metadata
        :return: success
        :rtype: bool

        **Example**:

        .. code-block:: python

            project.integrations.create(integrations_type=dl.IntegrationType.S3,
                            name='S3Integration',
                            options={key: "Access key ID", secret: "Secret access key"})
        """

        if self.project is None and self.org is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must have an organization or project')

        if self.project is not None:
            organization_id = self.project.org.get('id')
        else:
            organization_id = self.org.id

        url_path = '/orgs/{}/integrations'.format(organization_id)
        payload = {"type": integrations_type.value if isinstance(integrations_type,
                                                                 entities.IntegrationType) else integrations_type,
                   'name': name, 'options': options}
        if metadata is not None:
            payload['metadata'] = metadata
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        else:
            integration = entities.Integration.from_json(_json=response.json(), client_api=self._client_api)
        if integration.metadata and isinstance(integration.metadata, list) and len(integration.metadata) > 0:
            for m in integration.metadata:
                if m['name'] == 'status':
                    integration_status = m['value']
                    logger.info('Integration status: {}'.format(integration_status))
        return integration

    @_api_reference.add(path='/orgs/{orgId}/integrations', method='patch')
    def update(self,
               new_name: str = None,
               integrations_id: str = None,
               integration: entities.Integration = None,
               new_options: dict = None):
        """
        Update the integration's name.

        **Prerequisites**: You must be an *owner* in the organization.

        :param str new_name: new name
        :param str integrations_id: integrations id
        :param Integration integration: integration object
        :param dict new_options: new value
        :return: Integration object
        :rtype: dtlpy.entities.integration.Integration

        **Examples for options include**:
        s3 - {key: "", secret: ""};
        gcs - {key: "", secret: "", content: ""};
        azureblob - {key: "", secret: "", clientId: "", tenantId: ""};
        key_value - {key: "", value: ""}
        aws-sts - {key: "", secret: "", roleArns: ""}
        aws-cross - {roleArn: ""}
        gcp-cross - {"email: "", "resourceName": ""}

        **Example**:

        .. code-block:: python

            project.integrations.update(integrations_id='integrations_id', new_options={roleArn: ""})
        """
        if self.project is None and self.org is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must have an organization or project')
        if integrations_id is None and integration is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must have an integrations_id or integration')

        if self.project is not None:
            organization_id = self.project.org.get('id')
        else:
            organization_id = self.org.id

        url_path = '/orgs/{}/integrations/'.format(organization_id)
        payload = dict(integrationId=integrations_id if integrations_id is not None else integration.id)
        if new_name is not None:
            payload['name'] = new_name
        if new_options is not None:
            if integration is None:
                integration = self.get(integrations_id=integrations_id)
            payload['credentials'] = dict(options=new_options, type=integration.type)

        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)

        return entities.Integration.from_json(_json=response.json(), client_api=self._client_api)

    @_api_reference.add(path='/orgs/{orgId}/integrations/{integrationId}', method='get')
    def get(self, integrations_id: str):
        """
        Get organization integrations. Use this method to access your integration and be able to use it in your code.

        **Prerequisites**: You must be an *owner* in the organization.

        :param str integrations_id: integrations id
        :return: Integration object
        :rtype: dtlpy.entities.integration.Integration

        **Example**:

        .. code-block:: python

            project.integrations.get(integrations_id='integrations_id')
        """
        if self.project is None and self.org is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must have an organization or project')

        if self.project is not None:
            organization_id = self.project.org.get('id')
        else:
            organization_id = self.org.id

        url_path = '/orgs/{}/integrations/{}'.format(organization_id, integrations_id)

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)
        return entities.Integration.from_json(_json=response.json(), client_api=self._client_api)

    @_api_reference.add(path='/orgs/{orgId}/integrations', method='get')
    def list(self, only_available=False):
        """
        List all the organization's integrations with external storage.

        **Prerequisites**: You must be an *owner* in the organization.

        :param bool only_available: if True list only the available integrations.
        :return: groups list
        :rtype: list

        **Example**:

        .. code-block:: python

            project.integrations.list(only_available=True)
        """
        if self.project is None and self.org is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must have an organization or project')

        if self.project is not None:
            organization_id = self.project.org.get('id')
        else:
            organization_id = self.org.id

        if only_available:
            url_path = '/orgs/{}/availableIntegrations'.format(organization_id)
        else:
            url_path = '/orgs/{}/integrations'.format(organization_id)

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)

        available_integrations = miscellaneous.List(response.json())
        return available_integrations

    def generate_gar_options(self, service_account: str, location: str) -> dict:
        """
        Generates a Google Artifact Registry JSON configuration and returns it as a base64-encoded string.

        Parameters:
            location (str): The region where the repository will be created (e.g., 'us-central1').
            service_account (str): The service_account parameter represents the Google Cloud service account credentials
                                    in the form of a JSON key file. This JSON contains the private key and other metadata
                                    required for authenticating with Google Artifact Registry. It is used to generate a Kubernetes secret
                                    that stores the credentials for pulling container images from the registry.
                                    The JSON key must include fields such as client_email, private_key, and project_id,
                                    and it is typically downloaded from the Google Cloud Console when creating the service account

        Returns:
            str: A base64-encoded string representation of the repository JSON configuration.
        """
        if not service_account:
            raise ValueError('Missing Service Account')
        if not location:
            raise ValueError('Missing Location')
        user_name = "_json_key"
        cred = f"{user_name}:{service_account}"
        auth = str(base64.b64encode(bytes(cred, 'utf-8')))[2:-1]

        encoded_pass = {
            "auths": {
                f"{location}": {
                    "username": user_name,
                    "password": service_account,
                    "auth": auth
                }
            }
        }

        return {
            "name": "_json_key",
            "spec": {
                "password": str(base64.b64encode(bytes(json.dumps(encoded_pass), 'utf-8')))[2:-1]
            }
        }


================================================
File: dtlpy/repositories/items.py
================================================
import logging

from .. import entities, exceptions, repositories, miscellaneous, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Items:
    """
    Items Repository

    The Items class allows you to manage items in your datasets.
    For information on actions related to items see https://developers.dataloop.ai/tutorials/data_management/upload_and_manage_items/chapter/
    """

    def __init__(self,
                 client_api: ApiClient,
                 datasets: repositories.Datasets = None,
                 dataset: entities.Dataset = None,
                 dataset_id=None,
                 items_entity=None,
                 project=None):
        self._client_api = client_api
        self._dataset = dataset
        self._dataset_id = dataset_id
        self._datasets = datasets
        self._project = project
        # set items entity to represent the item (Item, Codebase, Artifact etc...)
        if items_entity is None:
            self.items_entity = entities.Item
        if self._dataset_id is None and self._dataset is not None:
            self._dataset_id = self._dataset.id

    ############
    # entities #
    ############
    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            if self._dataset_id is None:
                raise exceptions.PlatformException(
                    error='400',
                    message='Cannot perform action WITHOUT Dataset entity in Items repository. Please set a dataset')
            self._dataset = self.datasets.get(dataset_id=self._dataset_id, fetch=None)
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @dataset.setter
    def dataset(self, dataset: entities.Dataset):
        if not isinstance(dataset, entities.Dataset):
            raise ValueError('Must input a valid Dataset entity')
        self._dataset = dataset

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='400',
                message='Cannot perform action WITHOUT Project entity in Items repository. Please set a project')
        assert isinstance(self._dataset, entities.Dataset)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Dataset entity')
        self._project = project

    ################
    # repositories #
    ################
    @property
    def datasets(self) -> repositories.Datasets:
        if self._datasets is None:
            self._datasets = repositories.Datasets(client_api=self._client_api)
        assert isinstance(self._datasets, repositories.Datasets)
        return self._datasets

    ###########
    # methods #
    ###########

    def set_items_entity(self, entity):
        """
        Set the item entity type to `Artifact <https://dataloop.ai/docs/auto-annotation-service?#uploading-model-weights-as-artifacts>`_, Item, or Codebase.

        :param entities.Item, entities.Artifact, entities.Codebase entity: entity type [entities.Item, entities.Artifact, entities.Codebase]
        """
        if entity in [entities.Item, entities.Artifact, entities.Codebase]:
            self.items_entity = entity
        else:
            raise exceptions.PlatformException(error="403",
                                               message="Unable to set given entity. Entity give: {}".format(entity))

    def get_all_items(self, filters: entities.Filters = None) -> [entities.Item]:
        """
        Get all items in dataset.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters items
        :return: list of all items
        :rtype: list

        **Example**:

        .. code-block:: python

            dataset.items.get_all_items()

        """
        if filters is None:
            filters = entities.Filters()
            filters._user_query = 'false'
            filters.add(field='type', values='file')
        pages = self.list(filters=filters)
        num_items = pages.items_count
        items = [None for _ in range(num_items)]
        for i_item, item in enumerate(pages.all()):
            items[i_item] = item
        items = [item for item in items if item is not None]
        return items

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Item]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(self.items_entity._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item,
                                          'dataset': self.dataset})
        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        items = miscellaneous.List([r[1] for r in results if r[0] is True])
        return items

    def _list(self, filters: entities.Filters):
        """
        Get dataset items list This is a browsing endpoint, for any given path item count will be returned,
        user is expected to perform another request then for every folder item to actually get the its item list.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: json response
        """
        # prepare request
        success, response = self._client_api.gen_request(req_type="POST",
                                                         path="/datasets/{}/query".format(self.dataset.id),
                                                         json_req=filters.prepare(),
                                                         headers={'user_query': filters._user_query})
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @_api_reference.add(path='/datasets/{id}/query', method='post')
    def list(self,
             filters: entities.Filters = None,
             page_offset: int = None,
             page_size: int = None
             ) -> entities.PagedEntities:
        """
        List items in a dataset.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param int page_offset: start page
        :param int page_size: page size
        :return: Pages object
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            dataset.items.list(page_offset=0, page_size=100)
        """
        # default filters
        if filters is None:
            filters = entities.Filters()
            filters._user_query = 'false'
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.ITEM and filters.resource != entities.FiltersResource.ANNOTATION:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.ITEM. Got: {!r}'.format(filters.resource))

        # page size
        if page_size is None:
            # take from default
            page_size = filters.page_size
        else:
            filters.page_size = page_size

        # page offset
        if page_offset is None:
            # take from default
            page_offset = filters.page
        else:
            filters.page = page_offset

        if filters.resource == entities.FiltersResource.ITEM:
            items_repository = self
        else:
            items_repository = repositories.Annotations(client_api=self._client_api,
                                                        dataset=self._dataset)

        paged = entities.PagedEntities(items_repository=items_repository,
                                       filters=filters,
                                       page_offset=page_offset,
                                       page_size=page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    @_api_reference.add(path='/items/{id}', method='get')
    def get(self,
            filepath: str = None,
            item_id: str = None,
            fetch: bool = None,
            is_dir: bool = False
            ) -> entities.Item:
        """
        Get Item object

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str filepath: optional - search by remote path
        :param str item_id: optional - search by id
        :param bool fetch: optional - fetch entity from platform, default taken from cookie
        :param bool is_dir: True if you want to get an item from dir type
        :return: Item object
        :rtype: dtlpy.entities.item.Item

        **Example**:

        .. code-block:: python

            dataset.items.get(item_id='item_id')
        """
        if fetch is None:
            fetch = self._client_api.fetch_entities

        if fetch:
            if item_id is not None:
                success, response = self._client_api.gen_request(req_type="get",
                                                                 path="/items/{}".format(item_id))
                if success:
                    item = self.items_entity.from_json(client_api=self._client_api,
                                                       _json=response.json(),
                                                       dataset=self._dataset,
                                                       project=self._project)
                    # verify input filepath is same as the given id
                    if filepath is not None and item.filename != filepath:
                        logger.warning(
                            "Mismatch found in items.get: filepath is different then item.filename: "
                            "{!r} != {!r}".format(
                                filepath,
                                item.filename))
                else:
                    raise exceptions.PlatformException(response)
            elif filepath is not None:
                filters = entities.Filters()
                filters.pop(field='hidden')
                if is_dir:
                    filters.add(field='type', values='dir')
                filters.recursive = False
                filters.add(field='filename', values=filepath)
                paged_entity = self.list(filters=filters)
                if len(paged_entity.items) == 0:
                    raise exceptions.PlatformException(error='404',
                                                       message='Item not found. filepath= "{}"'.format(filepath))
                elif len(paged_entity.items) > 1:
                    raise exceptions.PlatformException(
                        error='404',
                        message='More than one item found. Please "get" by id. filepath: "{}"'.format(filepath))
                else:
                    item = paged_entity.items[0]
            else:
                raise exceptions.PlatformException(error="400",
                                                   message='Must choose by at least one. "filename" or "item_id"')
        else:
            item = entities.Item.from_json(_json={'id': item_id,
                                                  'filename': filepath},
                                           client_api=self._client_api,
                                           dataset=self._dataset,
                                           is_fetched=False,
                                           project=self._project)
        assert isinstance(item, entities.Item)
        return item

    @_api_reference.add(path='/items/{id}/clone', method='post')
    def clone(self,
              item_id: str,
              dst_dataset_id: str,
              remote_filepath: str = None,
              metadata: dict = None,
              with_annotations: bool = True,
              with_metadata: bool = True,
              with_task_annotations_status: bool = False,
              allow_many: bool = False,
              wait: bool = True):
        """
        Clone item. Read more about cloning datatsets and items in our `documentation <https://dataloop.ai/docs/clone-merge-dataset#cloned-dataset>`_ and `SDK documentation <https://developers.dataloop.ai/tutorials/data_management/data_versioning/chapter/>`_.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str item_id: item to clone
        :param str dst_dataset_id: destination dataset id
        :param str remote_filepath: complete filepath
        :param dict metadata: new metadata to add
        :param bool with_annotations: clone annotations
        :param bool with_metadata: clone metadata
        :param bool with_task_annotations_status: clone task annotations status
        :param bool allow_many: `bool` if True, using multiple clones in single dataset is allowed, (default=False)
        :param bool wait: wait for the command to finish
        :return: Item object
        :rtype: dtlpy.entities.item.Item

        **Example**:

        .. code-block:: python

            dataset.items.clone(item_id='item_id',
                    dst_dataset_id='dist_dataset_id',
                    with_metadata=True,
                    with_task_annotations_status=False,
                    with_annotations=False)
        """
        if metadata is None:
            metadata = dict()
        payload = {"targetDatasetId": dst_dataset_id,
                   "remoteFileName": remote_filepath,
                   "metadata": metadata,
                   "cloneDatasetParams": {
                       "withItemsAnnotations": with_annotations,
                       "withMetadata": with_metadata,
                       "withTaskAnnotationsStatus": with_task_annotations_status},
                   "allowMany": allow_many
                   }
        success, response = self._client_api.gen_request(req_type="post",
                                                         path="/items/{}/clone".format(item_id),
                                                         json_req=payload)
        # check response
        if not success:
            raise exceptions.PlatformException(response)

        command = entities.Command.from_json(_json=response.json(),
                                             client_api=self._client_api)
        if not wait:
            return command
        command = command.wait()

        if 'returnedModelId' not in command.spec:
            raise exceptions.PlatformException(error='400',
                                               message="returnedModelId key is missing in command response: {}"
                                               .format(response))
        cloned_item = self.get(item_id=command.spec['returnedModelId'][0])
        return cloned_item

    @_api_reference.add(path='/items/{id}', method='delete')
    def delete(self,
               filename: str = None,
               item_id: str = None,
               filters: entities.Filters = None):
        """
        Delete item from platform.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        You must provide at least ONE of the following params: item id, filename, filters.

        :param str filename: optional - search item by remote path
        :param str item_id: optional - search item by id
        :param dtlpy.entities.filters.Filters filters: optional - delete items by filter
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            dataset.items.delete(item_id='item_id')
        """
        if item_id is not None:
            success, response = self._client_api.gen_request(req_type="delete",
                                                             path="/items/{}".format(item_id),
                                                             )
        elif filename is not None:
            if not filename.startswith("/"):
                filename = "/" + filename
            items = self.get(filepath=filename)
            if not isinstance(items, list):
                items = [items]
            if len(items) == 0:
                raise exceptions.PlatformException("404", "Item not found")
            elif len(items) > 1:
                raise exceptions.PlatformException(error="404", message="More the 1 item exist by the name provided")
            else:
                item_id = items[0].id
                success, response = self._client_api.gen_request(req_type="delete",
                                                                 path="/items/{}".format(item_id))
        elif filters is not None:
            # prepare request
            success, response = self._client_api.gen_request(req_type="POST",
                                                             path="/datasets/{}/query".format(self.dataset.id),
                                                             json_req=filters.prepare(operation='delete'))
        else:
            raise exceptions.PlatformException("400", "Must provide item id, filename or filters")

        # check response
        if success:
            logger.debug("Item/s deleted successfully")
            return success
        else:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/items/{id}', method='patch')
    def update(self,
               item: entities.Item = None,
               filters: entities.Filters = None,
               update_values=None,
               system_update_values=None,
               system_metadata: bool = False):
        """
        Update item metadata.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        You must provide at least ONE of the following params: update_values, system_update_values.

        :param dtlpy.entities.item.Item item: Item object
        :param dtlpy.entities.filters.Filters filters: optional update filtered items by given filter
        :param update_values: optional field to be updated and new values
        :param system_update_values: values in system metadata to be updated
        :param bool system_metadata: True, if you want to update the metadata system
        :return: Item object
        :rtype: dtlpy.entities.item.Item

        **Example**:

        .. code-block:: python

            dataset.items.update(item='item_entity')
        """
        ref = filters is not None and (filters._ref_task or filters._ref_assignment)

        if system_update_values and not system_metadata:
            logger.warning('system metadata will not be updated because param system_metadata is False')

        # check params
        if item is None and filters is None:
            raise exceptions.PlatformException('400', 'must provide either item or filters')

        value_to_update = update_values or system_update_values

        if item is None and not ref and not value_to_update:
            raise exceptions.PlatformException('400',
                                               'Must provide update_values or system_update_values')

        if item is not None and value_to_update:
            raise exceptions.PlatformException('400',
                                               'Cannot provide "update_values" or "system_update_values" with a specific "item" for an individual update. '
                                               'These parameters are intended only for bulk updates using filters.')

        # update item
        if item is not None:
            json_req = miscellaneous.DictDiffer.diff(origin=item._platform_dict,
                                                     modified=item.to_json())
            if not json_req:
                return item
            url_path = "/items/{}".format(item.id)
            if system_metadata:
                url_path += "?system=true"
            success, response = self._client_api.gen_request(req_type="patch",
                                                             path=url_path,
                                                             json_req=json_req)
            if success:
                logger.debug("Item was updated successfully. Item id: {}".format(item.id))
                return self.items_entity.from_json(client_api=self._client_api,
                                                   _json=response.json(),
                                                   dataset=self._dataset)
            else:
                logger.error("Error while updating item")
                raise exceptions.PlatformException(response)
        # update by filters
        else:
            # prepare request
            prepared_filter = filters.prepare(operation='update',
                                              system_update=system_update_values,
                                              system_metadata=system_metadata,
                                              update=update_values)
            success, response = self._client_api.gen_request(req_type="POST",
                                                             path="/datasets/{}/query".format(self.dataset.id),
                                                             json_req=prepared_filter)
            if not success:
                raise exceptions.PlatformException(response)
            else:
                logger.debug("Items were updated successfully.")
                return response.json()

    def download(
            self,
            filters: entities.Filters = None,
            items=None,
            # download options
            local_path: str = None,
            file_types: list = None,
            save_locally: bool = True,
            to_array: bool = False,
            annotation_options: entities.ViewAnnotationOptions = None,
            annotation_filters: entities.Filters = None,
            overwrite: bool = False,
            to_items_folder: bool = True,
            thickness: int = 1,
            with_text: bool = False,
            without_relative_path=None,
            avoid_unnecessary_annotation_download: bool = False,
            include_annotations_in_output: bool = True,
            export_png_files: bool = False,
            filter_output_annotations: bool = False,
            alpha: float = 1,
            export_version=entities.ExportVersion.V1
    ):
        """
        Download dataset items by filters.

        Filters the dataset for items and saves them locally.

        Optional -- download annotation, mask, instance, and image mask of the item.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param List[dtlpy.entities.item.Item] or dtlpy.entities.item.Item items: download Item entity or item_id (or a list of item)
        :param str local_path: local folder or filename to save to.
        :param list file_types: a list of file type to download. e.g ['video/webm', 'video/mp4', 'image/jpeg', 'image/png']
        :param bool save_locally: bool. save to disk or return a buffer
        :param bool to_array: returns Ndarray when True and local_path = False
        :param list annotation_options: download annotations options:  list(dl.ViewAnnotationOptions)
        :param dtlpy.entities.filters.Filters annotation_filters: Filters entity to filter annotations for download
        :param bool overwrite: optional - default = False
        :param bool to_items_folder: Create 'items' folder and download items to it
        :param int thickness: optional - line thickness, if -1 annotation will be filled, default =1
        :param bool with_text: optional - add text to annotations, default = False
        :param bool without_relative_path: bool - download items without the relative path from platform
        :param bool avoid_unnecessary_annotation_download: default - False
        :param bool include_annotations_in_output: default - False , if export should contain annotations
        :param bool export_png_files: default - if True, semantic annotations should be exported as png files
        :param bool filter_output_annotations: default - False, given an export by filter - determine if to filter out annotations
        :param float alpha: opacity value [0 1], default 1
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :return: generator of local_path per each downloaded item
        :rtype: generator or single item

        **Example**:

        .. code-block:: python

            dataset.items.download(local_path='local_path',
                                 annotation_options=dl.ViewAnnotationOptions,
                                 overwrite=False,
                                 thickness=1,
                                 with_text=False,
                                 alpha=1,
                                 save_locally=True
                                 )
        """
        downloader = repositories.Downloader(self)
        return downloader.download(
            filters=filters,
            items=items,
            local_path=local_path,
            file_types=file_types,
            save_locally=save_locally,
            to_array=to_array,
            annotation_options=annotation_options,
            annotation_filters=annotation_filters,
            overwrite=overwrite,
            to_items_folder=to_items_folder,
            thickness=thickness,
            alpha=alpha,
            with_text=with_text,
            without_relative_path=without_relative_path,
            avoid_unnecessary_annotation_download=avoid_unnecessary_annotation_download,
            include_annotations_in_output=include_annotations_in_output,
            export_png_files=export_png_files,
            filter_output_annotations=filter_output_annotations,
            export_version=export_version
        )

    def upload(
            self,
            # what to upload
            local_path: str,
            local_annotations_path: str = None,
            # upload options
            remote_path: str = "/",
            remote_name: str = None,
            file_types: list = None,
            overwrite: bool = False,
            item_metadata: dict = None,
            output_entity=entities.Item,
            no_output: bool = False,
            export_version: str = entities.ExportVersion.V1,
            item_description: str = None,
            raise_on_error: bool = False,
            return_as_list: bool = False
    ):
        """
        Upload local file to dataset.
        Local filesystem will remain unchanged.
        If "*" at the end of local_path (e.g. "/images/*") items will be uploaded without the head directory.

        **Prerequisites**: Any user can upload items.

        :param str local_path: list of local file, local folder, BufferIO, numpy.ndarray or url to upload
        :param str local_annotations_path: path to dataloop format annotations json files.
        :param str remote_path: remote path to save.
        :param str remote_name: remote base name to save. when upload numpy.ndarray as local path, remote_name with .jpg or .png ext is mandatory
        :param list file_types: list of file type to upload. e.g ['.jpg', '.png']. default is all
        :param dict item_metadata: metadata dict to upload to item or ExportMetadata option to export metadata from annotation file
        :param bool overwrite: optional - default = False
        :param output_entity: output type
        :param bool no_output: do not return the items after upload
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :param str item_description: add a string description to the uploaded item
        :param bool raise_on_error: raise an exception if an error occurs
        :param bool return_as_list: return a list of items instead of a generator
        
        :return: Output (generator/single item)
        :rtype: generator or single item

        **Example**:

        .. code-block:: python

            dataset.items.upload(local_path='local_path',
                                 local_annotations_path='local_annotations_path',
                                 overwrite=True,
                                 item_metadata={'Hellow': 'Word'}
                                 )
        """
        # initiate and use uploader
        uploader = repositories.Uploader(items_repository=self, output_entity=output_entity, no_output=no_output)
        return uploader.upload(
            local_path=local_path,
            local_annotations_path=local_annotations_path,
            # upload options
            remote_path=remote_path,
            remote_name=remote_name,
            file_types=file_types,
            # config
            overwrite=overwrite,
            # metadata to upload with items
            item_metadata=item_metadata,
            export_version=export_version,
            item_description=item_description,
            raise_on_error=raise_on_error,
            return_as_list=return_as_list
        )

    @property
    def platform_url(self):
        return self._client_api._get_resource_url(
            "projects/{}/datasets/{}/items".format(self.dataset.project.id, self.dataset.id))

    def open_in_web(self, filepath=None, item_id=None, item=None):
        """
        Open the item in web platform

        **Prerequisites**: You must be in the role of an *owner* or *developer* or be an *annotation manager*/*annotator* with access to that item through task.

        :param str filepath: item file path
        :param str item_id: item id
        :param dtlpy.entities.item.Item item: item entity

        **Example**:

        .. code-block:: python

            dataset.items.open_in_web(item_id='item_id')

        """
        if filepath is not None:
            item = self.get(filepath=filepath)
        if item is not None:
            item.open_in_web()
        elif item_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(item_id))
        else:
            self._client_api._open_in_web(url=self.platform_url)

    def update_status(self,
                      status: entities.ItemStatus,
                      items=None,
                      item_ids=None,
                      filters=None,
                      dataset=None,
                      clear=False):
        """
        Update item status in task

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who has been assigned a task with the item.

        You must provide at least ONE of the following params: items, item_ids, filters.

        :param str status: ItemStatus.COMPLETED, ItemStatus.APPROVED, ItemStatus.DISCARDED
        :param list items: list of items
        :param list item_ids: list of items id
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param dtlpy.entities.dataset.Dataset dataset: dataset object
        :param bool clear: to delete status

        **Example**:

        .. code-block:: python

            dataset.items.update_status(item_ids='item_id', status=dl.ItemStatus.COMPLETED)

        """
        if items is None and item_ids is None and filters is None:
            raise exceptions.PlatformException('400', 'Must provide either items, item_ids or filters')

        if self._dataset is None and dataset is None:
            raise exceptions.PlatformException('400', 'Please provide dataset')
        elif dataset is None:
            dataset = self._dataset

        if filters is not None:
            items = dataset.items.list(filters=filters)
            item_count = items.items_count
        elif items is not None:
            if isinstance(items, entities.PagedEntities):
                item_count = items.items_count
            else:
                if not isinstance(items, list):
                    items = [items]
                item_count = len(items)
                items = [items]
        else:
            if not isinstance(item_ids, list):
                item_ids = [item_ids]
            item_count = len(item_ids)
            items = [[dataset.items.get(item_id=item_id, fetch=False) for item_id in item_ids]]

        pool = self._client_api.thread_pools(pool_name='item.status_update')
        jobs = [None for _ in range(item_count)]
        # call multiprocess wrapper to run service on each item in list
        for page in items:
            for i_item, item in enumerate(page):
                jobs[i_item] = pool.submit(item.update_status,
                                           **{'status': status,
                                              'clear': clear})

        # get all results
        results = [j.result() for j in jobs]
        out_success = [r for r in results if r is True]
        out_errors = [r for r in results if r is False]
        if len(out_errors) == 0:
            logger.debug('Item/s updated successfully. {}/{}'.format(len(out_success), len(results)))
        else:
            logger.error(out_errors)
            logger.error('Item/s updated with {} errors'.format(len(out_errors)))

    def make_dir(self, directory, dataset: entities.Dataset = None) -> entities.Item:
        """
        Create a directory in a dataset.

        **Prerequisites**: All users.

        :param str directory: name of directory
        :param dtlpy.entities.dataset.Dataset dataset: dataset object
        :return: Item object
        :rtype: dtlpy.entities.item.Item

        **Example**:

        .. code-block:: python

            dataset.items.make_dir(directory='directory_name')
        """
        if self._dataset_id is None and dataset is None:
            raise exceptions.PlatformException('400', 'Please provide parameter dataset')

        payload = {
            'type': 'dir',
            'path': directory
        }
        headers = {'content-type': 'application/x-www-form-urlencoded'}
        success, response = self._client_api.gen_request(req_type="post",
                                                         headers=headers,
                                                         path="/datasets/{}/items".format(self._dataset_id),
                                                         data=payload)
        if success:
            item = self.items_entity.from_json(client_api=self._client_api,
                                               _json=response.json(),
                                               dataset=self._dataset)
        else:
            raise exceptions.PlatformException(response)

        return item

    def move_items(self,
                   destination: str,
                   filters: entities.Filters = None,
                   items=None,
                   dataset: entities.Dataset = None
                   ) -> bool:
        """
        Move items to another directory.
        If directory does not exist we will create it

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str destination: destination directory
        :param dtlpy.entities.filters.Filters filters: optional - either this or items. Query of items to move
        :param items: optional - either this or filters. A list of items to move
        :param dtlpy.entities.dataset.Dataset dataset: dataset object
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            dataset.items.move_items(destination='directory_name')
        """
        if filters is None and items is None:
            raise exceptions.PlatformException('400', 'Must provide either filters or items')

        dest_dir_filter = entities.Filters(resource=entities.FiltersResource.ITEM, field='type', values='dir')
        dest_dir_filter.recursive = False
        dest_dir_filter.add(field='filename', values=destination)
        dirs_page = self.list(filters=dest_dir_filter)

        if dirs_page.items_count == 0:
            directory = self.make_dir(directory=destination, dataset=dataset)
        elif dirs_page.items_count == 1:
            directory = dirs_page.items[0]
        else:
            raise exceptions.PlatformException('404', 'More than one directory by the name of: {}'.format(destination))

        if filters is not None:
            items = self.list(filters=filters)
        elif isinstance(items, list):
            items = [items]
        elif not isinstance(items, entities.PagedEntities):
            raise exceptions.PlatformException('400', 'items must be a list of items or a pages entity not {}'.format(
                type(items)))

        item_ids = list()
        for page in items:
            for item in page:
                item_ids.append(item.id)

        success, response = self._client_api.gen_request(req_type="put",
                                                         path="/datasets/{}/items/{}".format(self._dataset_id,
                                                                                             directory.id),
                                                         json_req=item_ids)
        if not success:
            raise exceptions.PlatformException(response)

        return success


================================================
File: dtlpy/repositories/messages.py
================================================
"""
Messages Repository
"""
import json
import logging
from urllib.parse import urlencode, quote
from .. import entities, miscellaneous, exceptions, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Messages:
    """
    Messages Repository

    The Messages class allows the user to manage Messages.
    """

    def __init__(self, client_api: ApiClient):
        self._client_api = client_api

    # @_api_reference.add(path='/inbox/message/user', method='get')
    def _list(self, context: entities.NotificationEventContext = None, checkpoint: dict = None, new_only=True) -> \
            miscellaneous.List[entities.Message]:
        """
        Get messages by context.

        :param str context: Message context
        :param dict checkpoint: checkpoint
        :param bool new_only: get only new messages
        :return: List of Messages
        :rtype: list

        **Example**:

        .. code-block:: python

            messages = dl.messages.list(context={project: id})
        """

        user = self._client_api.info()['user_email']
        if not user:
            raise exceptions.PlatformException(error='400',
                                               message='No user in JWT, please login')

        url = '/inbox/message/user'

        query_params = {
            'newOnly': new_only,
            'checkpointPagination': {
                "pageSize": 1000,
                "sort": {"key": "id", "direction": "descending"}
            }
        }

        context_extent = None

        if context:
            context_extent = "contextExtent=" + quote(json.dumps(context).replace(" ", ""), safe=":,").strip()

        if checkpoint:
            query_params['checkpointPagination']['checkpoint'] = checkpoint

        checkpoint_pagination = "checkpointPagination=" + quote(
            json.dumps(query_params['checkpointPagination']).replace(" ", ""), safe=":,").strip()

        url = "{}?{}".format(url, checkpoint_pagination)

        if context_extent:
            url = "{}&{}".format(url, context_extent)

        success, response = self._client_api.gen_request(req_type='get', path=url)

        if success:
            pool = self._client_api.thread_pools('entity.create')
            message_json = response.json()
            items = message_json.get('items', list())
            jobs = [None for _ in range(len(items))]
            for i_message, message in enumerate(items):
                jobs[i_message] = pool.submit(
                    entities.Message._protected_from_json,
                    **{'client_api': self._client_api, '_json': message}
                )

            # get all results
            results = [j.result() for j in jobs]
            # log errors
            _ = [logger.warning(r[1]) for r in results if r[0] is False]
            # return good jobs
            messages = miscellaneous.List([r[1] for r in results if r[0] is True])
        else:
            raise exceptions.PlatformException(response)
        return messages


================================================
File: dtlpy/repositories/models.py
================================================
import time
from typing import List
import logging

from .. import entities, repositories, exceptions, miscellaneous
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')

MIN_INTERVAL = 1
BACKOFF_FACTOR = 1.2
MAX_INTERVAL = 12


class Models:
    """
    Models Repository
    """

    def __init__(self,
                 client_api: ApiClient,
                 package: entities.Package = None,
                 project: entities.Project = None,
                 project_id: str = None):
        self._client_api = client_api
        self._project = project
        self._package = package
        self._project_id = project_id

        if self._project is not None:
            self._project_id = self._project.id

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            if self._project_id is not None:
                projects = repositories.Projects(client_api=self._client_api)
                self._project = projects.get(project_id=self._project_id)
        if self._project is None:
            if self._package is not None:
                if self._package._project is not None:
                    self._project = self._package._project
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.models repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    @property
    def package(self) -> entities.Package:
        if self._package is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Cannot perform action WITHOUT Package entity in {} repository.'.format(
                    self.__class__.__name__) +
                        ' Please use package.models or set a model')
        assert isinstance(self._package, entities.Package)
        return self._package

    ###########
    # methods #
    ###########
    def get(self, model_name=None, model_id=None) -> entities.Model:
        """
        Get model object
        :param model_name:
        :param model_id:
        :return: dl.Model object
        """

        if model_id is not None:
            success, response = self._client_api.gen_request(req_type="get",
                                                             path="/ml/models/{}".format(model_id))
            if not success:
                raise exceptions.PlatformException(response)
            model = entities.Model.from_json(client_api=self._client_api,
                                             _json=response.json(),
                                             project=self._project,
                                             package=self._package)
            # verify input model name is same as the given id
            if model_name is not None and model.name != model_name:
                logger.warning(
                    "Mismatch found in models.get: model_name is different then model.name:"
                    " {!r} != {!r}".format(
                        model_name,
                        model.name))
        elif model_name is not None:

            filters = entities.Filters(
                resource=entities.FiltersResource.MODEL,
                field='name',
                values=model_name
            )

            project_id = None

            if self._project is not None:
                project_id = self._project.id
            elif self._project_id is not None:
                project_id = self._project_id

            if project_id is not None:
                filters.add(field='projectId', values=project_id)

            if self._package is not None:
                filters.add(field='packageId', values=self._package.id)

            models = self.list(filters=filters)

            if models.items_count == 0:
                raise exceptions.PlatformException(
                    error='404',
                    message='Model not found. Name: {}'.format(model_name))
            elif models.items_count > 1:
                raise exceptions.PlatformException(
                    error='400',
                    message='More than one Model found by the name of: {}. Try "get" by id or "list()".'.format(
                        model_name))
            model = models.items[0]
        else:
            raise exceptions.PlatformException(
                error='400',
                message='No checked-out Model was found, must checkout or provide an identifier in inputs')

        return model

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Model]:
        jobs = [None for _ in range(len(response_items))]
        pool = self._client_api.thread_pools(pool_name='entity.create')

        # return triggers list
        for i_service, service in enumerate(response_items):
            jobs[i_service] = pool.submit(entities.Model._protected_from_json,
                                          **{'client_api': self._client_api,
                                             '_json': service,
                                             'package': self._package,
                                             'project': self._project})

        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        return miscellaneous.List([r[1] for r in results if r[0] is True])

    def _list(self, filters: entities.Filters):
        # request
        success, response = self._client_api.gen_request(req_type='POST',
                                                         path='/ml/models/query',
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List project model

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities
        """
        # default filters
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.MODEL)
        if self._project is not None:
            filters.add(field='projectId', values=self._project.id)
        if self._package is not None:
            filters.add(field='packageId', values=self._package.id)

        # assert type filters
        if not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))

        if filters.resource != entities.FiltersResource.MODEL:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.MODEL. Got: {!r}'.format(filters.resource))

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def _set_model_filter(self,
                          metadata: dict,
                          train_filter: entities.Filters = None,
                          validation_filter: entities.Filters = None):
        if metadata is None:
            metadata = {}
        if 'system' not in metadata:
            metadata['system'] = {}
        if 'subsets' not in metadata['system']:
            metadata['system']['subsets'] = {}
        if train_filter is not None:
            metadata['system']['subsets']['train'] = train_filter.prepare() if isinstance(train_filter,
                                                                                          entities.Filters) else train_filter
        if validation_filter is not None:
            metadata['system']['subsets']['validation'] = validation_filter.prepare() if isinstance(validation_filter,
                                                                                                    entities.Filters) else validation_filter
        return metadata

    @staticmethod
    def add_subset(model: entities.Model, subset_name: str, subset_filter: entities.Filters):
        """
        Adds a subset for a model, specifying a subset of the model's dataset that could be used for training or
        validation.

        :param dtlpy.entities.Model model: the model to which the subset should be added
        :param str subset_name: the name of the subset
        :param dtlpy.entities.Filters subset_filter: the filtering operation that this subset performs in the dataset.

        **Example**

        .. code-block:: python

            project.models.add_subset(model=model_entity, subset_name='train', subset_filter=dtlpy.Filters(field='dir', values='/train'))
            model_entity.metadata['system']['subsets']
                {'train': <dtlpy.entities.filters.Filters object at 0x1501dfe20>}

        """
        if 'system' not in model.metadata:
            model.metadata['system'] = dict()
        if 'subsets' not in model.metadata['system']:
            model.metadata['system']['subsets'] = dict()
        model.metadata['system']['subsets'][subset_name] = subset_filter.prepare()
        model.update(system_metadata=True)

    @staticmethod
    def delete_subset(model: entities.Model, subset_name: str):
        """
        Removes a subset from a model's metadata.

        :param dtlpy.entities.Model model: the model to which the subset should be added
        :param str subset_name: the name of the subset

        **Example**

        .. code-block:: python

            project.models.add_subset(model=model_entity, subset_name='train', subset_filter=dtlpy.Filters(field='dir', values='/train'))
            model_entity.metadata['system']['subsets']
                {'train': <dtlpy.entities.filters.Filters object at 0x1501dfe20>}
            project.models.delete_subset(model=model_entity, subset_name='train')
            model_entity.metadata['system']['subsets']
                {}

        """
        if model.metadata.get("system", dict()).get("subsets", dict()).get(subset_name) is None:
            logger.error(f"Model system metadata incomplete, could not delete subset {subset_name}.")
        else:
            _ = model.metadata['system']['subsets'].pop(subset_name)
            model.update(system_metadata=True)

    def create(
            self,
            model_name: str,
            dataset_id: str = None,
            labels: list = None,
            ontology_id: str = None,
            description: str = None,
            model_artifacts: List[entities.Artifact] = None,
            project_id=None,
            tags: List[str] = None,
            package: entities.Package = None,
            configuration: dict = None,
            status: str = None,
            scope: entities.EntityScopeLevel = entities.EntityScopeLevel.PROJECT,
            version: str = '1.0.0',
            input_type=None,
            output_type=None,
            train_filter: entities.Filters = None,
            validation_filter: entities.Filters = None,
            app: entities.App = None
    ) -> entities.Model:
        """
        Create a Model entity

        :param str model_name: name of the model
        :param str dataset_id: dataset id
        :param list labels: list of labels from ontology (must mach ontology id) can be a subset
        :param str ontology_id: ontology to connect to the model
        :param str description: description
        :param model_artifacts: optional list of dl.Artifact. Can be ItemArtifact, LocaArtifact or LinkArtifact
        :param str project_id: project that owns the model
        :param list tags: list of string tags
        :param package: optional - Package object
        :param dict configuration: optional - model configuration - dict
        :param str status: `str` of the optional values of
        :param str scope: the scope level of the model dl.EntityScopeLevel
        :param str version: version of the model
        :param str input_type: the file type the model expect as input (image, video, txt, etc)
        :param str output_type: dl.AnnotationType - the type of annotations the model produces (class, box segment, text, etc)
        :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
        :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
        :param dtlpy.entities.App app: App entity to connect the model to
        :return: Model Entity

        **Example**:

        .. code-block:: python

            project.models.create(model_name='model_name', dataset_id='dataset_id', labels=['label1', 'label2'], train_filter={filter: {$and: [{dir: "/10K short videos"}]},page: 0,pageSize: 1000,resource: "items"}})

        """

        if ontology_id is not None:
            # take labels from ontology
            ontologies = repositories.Ontologies(client_api=self._client_api)
            labels = [label.tag for label in ontologies.get(ontology_id=ontology_id).labels]

        if labels is None:
            # dont have to have labels. can use an empty list
            labels = list()

        if input_type is None:
            input_type = 'image'

        if output_type is None:
            output_type = entities.AnnotationType.CLASSIFICATION

        if package is None and self._package is None:
            raise exceptions.PlatformException('Must provide a Package or create from package.models')
        elif package is None:
            package = self._package

        # TODO need to remove the entire project id user interface - need to take it from dataset id (in BE)
        if project_id is None:
            if self._project is None:
                raise exceptions.PlatformException('Please provide project_id')
            project_id = self._project.id
        else:
            if project_id != self._project_id:
                if (isinstance(package, entities.Package) and not package.is_global) or \
                        (isinstance(package, entities.Dpk) and not package.scope != 'public'):
                    logger.warning(
                        "Note! you are specified project_id {!r} which is different from repository context: {!r}".format(
                            project_id, self._project_id))

        if model_artifacts is None:
            model_artifacts = []

        if not isinstance(model_artifacts, list):
            raise ValueError('`model_artifacts` must be a list of dl.Artifact entities')

        # create payload for request
        payload = {
            'packageId': package.id,
            'name': model_name,
            'projectId': project_id,
            'datasetId': dataset_id,
            'labels': labels,
            'artifacts': [artifact.to_json(as_artifact=True) for artifact in model_artifacts],
            'scope': scope,
            'version': version,
            'inputType': input_type,
            'outputType': output_type,
        }

        if app is not None:
            if not isinstance(package, entities.Dpk):
                raise ValueError('package must be a Dpk entity')
            if app.dpk_name != package.name or app.dpk_version != package.version:
                raise ValueError('App and package must be the same')
            component_name = None
            compute_config = None
            for model in package.components.models:
                if model['name'] == model_name:
                    component_name = model['name']
                    compute_config = model.get('computeConfigs', None)
                    break
            if component_name is None:
                raise ValueError('Model name not found in package')
            payload['app'] = {
                "id": app.id,
                "componentName": component_name,
                "dpkName": package.name,
                "dpkVersion": package.version
            }
            if compute_config is not None:
                payload['app']['computeConfig'] = compute_config

        if configuration is not None:
            payload['configuration'] = configuration

        if tags is not None:
            payload['tags'] = tags

        if description is not None:
            payload['description'] = description

        if status is not None:
            payload['status'] = status

        if train_filter or validation_filter:
            metadata = self._set_model_filter(metadata={},
                                              train_filter=train_filter,
                                              validation_filter=validation_filter)
            payload['metadata'] = metadata

        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/ml/models',
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        model = entities.Model.from_json(_json=response.json(),
                                         client_api=self._client_api,
                                         project=self._project,
                                         package=package)

        return model

    def clone(self,
              from_model: entities.Model,
              model_name: str,
              dataset: entities.Dataset = None,
              configuration: dict = None,
              status=None,
              scope=None,
              project_id: str = None,
              labels: list = None,
              description: str = None,
              tags: list = None,
              train_filter: entities.Filters = None,
              validation_filter: entities.Filters = None,
              wait=True,
              ) -> entities.Model:
        """
        Clones and creates a new model out of existing one

        :param from_model: existing model to clone from
        :param str model_name: `str` new model name
        :param str dataset: dataset object for the cloned model
        :param dict configuration: `dict` (optional) if passed replaces the current configuration
        :param str status: `str` (optional) set the new status
        :param str scope: `str` (optional) set the new scope. default is "project"
        :param str project_id: `str` specify the project id to create the new model on (if other than the source model)
        :param list labels:  `list` of `str` - label of the model
        :param str description: `str` description of the new model
        :param list tags:  `list` of `str` - label of the model
        :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
        :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
        :param bool wait: `bool` wait for model to be ready
        :return: dl.Model which is a clone version of the existing model
        """
        from_json = {"name": model_name,
                     "packageId": from_model.package_id,
                     "configuration": from_model.configuration,
                     "outputType": from_model.output_type,
                     "inputType": from_model.input_type}
        if project_id is None:
            if dataset is not None:
                # take dataset project
                project_id = dataset.project.id
            else:
                # take model's project
                project_id = self.project.id
        from_json['projectId'] = project_id
        if dataset is not None:
            if labels is None:
                labels = list(dataset.labels_flat_dict.keys())
            from_json['datasetId'] = dataset.id
        if labels is not None:
            from_json['labels'] = labels
            # if there are new labels - pop the mapping from the original
            _ = from_json['configuration'].pop('id_to_label_map', None)
            _ = from_json['configuration'].pop('label_to_id_map', None)
        if configuration is not None:
            from_json['configuration'].update(configuration)
        if description is not None:
            from_json['description'] = description
        if tags is not None:
            from_json['tags'] = tags
        if scope is not None:
            from_json['scope'] = scope
        if status is not None:
            from_json['status'] = status

        metadata = self._set_model_filter(metadata={},
                                          train_filter=train_filter if train_filter is not None else from_model.metadata.get(
                                              'system', {}).get('subsets', {}).get('train', None),
                                          validation_filter=validation_filter if validation_filter is not None else from_model.metadata.get(
                                              'system', {}).get('subsets', {}).get('validation', None))
        if metadata:
            from_json['metadata'] = metadata
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/ml/models/{}/clone'.format(from_model.id),
                                                         json_req=from_json)
        if not success:
            raise exceptions.PlatformException(response)
        new_model = entities.Model.from_json(_json=response.json(),
                                             client_api=self._client_api,
                                             project=self._project,
                                             package=from_model._package)
        if wait:
            new_model = self.wait_for_model_ready(model=new_model)
        return new_model

    def wait_for_model_ready(self, model: entities.Model):
        """
        Wait for model to be ready

        :param model: Model entity
        """
        sleep_time = MIN_INTERVAL
        while model.status == entities.ModelStatus.CLONING:
            model = self.get(model_id=model.id)
            time.sleep(sleep_time)
            sleep_time = min(sleep_time * BACKOFF_FACTOR, MAX_INTERVAL)
            time.sleep(sleep_time)
        return model

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/models".format(self.project.id))

    def open_in_web(self, model=None, model_id=None):
        """
        Open the model in web platform

        :param model: model entity
        :param str model_id: model id
        """
        if model is not None:
            model.open_in_web()
        elif model_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(model_id) + '/main')
        else:
            self._client_api._open_in_web(url=self.platform_url)

    def delete(self, model: entities.Model = None, model_name=None, model_id=None):
        """
        Delete Model object

        :param model: Model entity to delete
        :param str model_name: delete by model name
        :param str model_id: delete by model id
        :return: True
        :rtype: bool
        """
        # get id and name
        if model_id is None:
            if model is not None:
                model_id = model.id
            elif model_name is not None:
                model = self.get(model_name=model_name)
                model_id = model.id
            else:
                raise exceptions.PlatformException(error='400',
                                                   message='Must input at least one parameter to models.delete')

        # request
        success, response = self._client_api.gen_request(
            req_type="delete",
            path="/ml/models/{}".format(model_id)
        )

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return results
        return True

    def update(self,
               model: entities.Model,
               system_metadata: bool = False) -> entities.Model:
        """
        Update Model changes to platform

        :param model: Model entity
        :param bool system_metadata: True, if you want to change metadata system
        :return: Model entity
        """
        # payload
        payload = model.to_json()

        # url
        url_path = '/ml/models/{}'.format(model.id)
        if system_metadata:
            url_path += '?system=true'

        # request
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Model.from_json(_json=response.json(),
                                        client_api=self._client_api,
                                        project=self._project,
                                        package=model._package)

    def train(self, model_id: str, service_config=None):
        """
        Train the model in the cloud. This will create a service and will run the adapter's train function as an execution

        :param model_id: id of the model to train
        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
        :return:
        """
        payload = dict()
        if service_config is not None:
            payload['serviceConfig'] = service_config
        success, response = self._client_api.gen_request(req_type="post",
                                                         path=f"/ml/models/{model_id}/train",
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        return entities.Execution.from_json(_json=response.json(),
                                            client_api=self._client_api,
                                            project=self._project)

    def evaluate(self, model_id: str, dataset_id: str, filters: entities.Filters = None, service_config=None):
        """
        Evaluate Model, provide data to evaluate the model on You can also provide specific config for the deployed service

        :param str model_id: Model id to predict
        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
        :param str dataset_id: ID of the dataset to evaluate
        :param entities.Filters filters: dl.Filter entity to run the predictions on
        :return:
        """

        payload = {'input': {'datasetId': dataset_id}}
        if service_config is not None:
            payload['config'] = {'serviceConfig': service_config}
        if filters is None:
            filters = entities.Filters()
        if filters is not None:
            payload['input']['datasetQuery'] = filters.prepare()
        success, response = self._client_api.gen_request(req_type="post",
                                                         path=f"/ml/models/{model_id}/evaluate",
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        return entities.Execution.from_json(_json=response.json(),
                                            client_api=self._client_api,
                                            project=self._project)

    def predict(self, model, item_ids, dataset_id=None):
        """
        Run model prediction with items

        :param model: dl.Model entity to run the prediction.
        :param item_ids: a list of item id to run the prediction.
        :param dataset_id: a dataset id to run the prediction.
        :return:
        """
        if len(model.metadata['system'].get('deploy', {}).get('services', [])) == 0:
            # no services for model
            raise ValueError("Model doesnt have any associated services. Need to deploy before predicting")
        if item_ids is None and dataset_id is None:
            raise ValueError("Need to provide either item_ids or dataset_id")
        payload_input = {}
        if item_ids is not None:
            payload_input['itemIds'] = item_ids
        if dataset_id is not None:
            payload_input['datasetId'] = dataset_id
        payload = {'input': payload_input,
                   'config': {'serviceId': model.metadata['system']['deploy']['services'][0]}}

        success, response = self._client_api.gen_request(req_type="post",
                                                         path=f"/ml/models/{model.id}/predict",
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        return entities.Execution.from_json(_json=response.json(),
                                            client_api=self._client_api,
                                            project=self._project)

    def embed(self, model, item_ids=None, dataset_id=None):
        """
        Run model embed with items

        :param model: dl.Model entity to run the prediction.
        :param item_ids: a list of item id to run the embed.
        :param dataset_id: a dataset id to run the embed.
        :return: Execution
        :rtype: dtlpy.entities.execution.Execution
        """
        if len(model.metadata['system'].get('deploy', {}).get('services', [])) == 0:
            # no services for model
            raise ValueError("Model doesnt have any associated services. Need to deploy before predicting")
        if item_ids is None and dataset_id is None:
            raise ValueError("Need to provide either item_ids or dataset_id")
        payload_input = {}
        if item_ids is not None:
            payload_input['itemIds'] = item_ids
        if dataset_id is not None:
            payload_input['datasetId'] = dataset_id
        payload = {'input': payload_input,
                   'config': {'serviceId': model.metadata['system']['deploy']['services'][0]}}

        success, response = self._client_api.gen_request(req_type="post",
                                                         path=f"/ml/models/{model.id}/embed",
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        return entities.Execution.from_json(_json=response.json(),
                                            client_api=self._client_api,
                                            project=self._project)

    def embed_datasets(self, model, dataset_ids, attach_trigger=False):
        """
        Run model embed with datasets

        :param model: dl.Model entity to run the prediction.
        :param dataset_ids: a list of dataset id to run the embed.
        :param attach_trigger: bool, if True will activate the trigger
        :return:
        """
        if len(model.metadata['system'].get('deploy', {}).get('services', [])) == 0:
            # no services for model
            raise ValueError("Model doesnt have any associated services. Need to deploy before predicting")
        if dataset_ids is None:
            raise ValueError("Need to provide either dataset_id")
        payload = {'datasetIds': dataset_ids,
                   'config': {'serviceId': model.metadata['system']['deploy']['services'][0]},
                   'attachTrigger': attach_trigger
                   }

        success, response = self._client_api.gen_request(req_type="post",
                                                         path=f"/ml/models/{model.id}/embed/datasets",
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        command = entities.Command.from_json(_json=response.json(),
                                             client_api=self._client_api)
        command = command.wait()
        return command

    def deploy(self, model_id: str, service_config=None) -> entities.Service:
        """
        Deploy a trained model. This will create a service that will execute predictions

        :param model_id: id of the model to deploy
        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
        :return: dl.Service: the deployed service
        """
        payload = dict()
        if service_config is not None:
            payload['serviceConfig'] = service_config
        success, response = self._client_api.gen_request(req_type="post",
                                                         path=f"/ml/models/{model_id}/deploy",
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)

        return entities.Service.from_json(_json=response.json(),
                                          client_api=self._client_api,
                                          project=self._project,
                                          package=self._package)


class Metrics:
    def __init__(self, client_api, model=None, model_id=None):
        self._client_api = client_api
        self._model_id = model_id
        self._model = model

    @property
    def model(self):
        return self._model

    def create(self, samples, dataset_id) -> bool:
        """
        Add Samples for model analytics and metrics

        :param samples: list of dl.PlotSample - must contain: model_id, figure, legend, x, y
        :param model_id: model id to save samples on
        :param dataset_id:
        :return: bool: True if success
        """
        if not isinstance(samples, list):
            samples = [samples]

        payload = list()
        for sample in samples:
            _json = sample.to_json()
            _json['modelId'] = self.model.id
            _json['datasetId'] = dataset_id
            payload.append(_json)
        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/ml/metrics/publish',
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return True

    def _list(self, filters: entities.Filters):
        # request
        success, response = self._client_api.gen_request(req_type='POST',
                                                         path='/ml/metrics/query',
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Model]:
        jobs = [None for _ in range(len(response_items))]
        pool = self._client_api.thread_pools(pool_name='entity.create')

        # return triggers list
        for i_service, sample in enumerate(response_items):
            jobs[i_service] = pool.submit(entities.PlotSample,
                                          **{'x': sample.get('data', dict()).get('x', None),
                                             'y': sample.get('data', dict()).get('y', None),
                                             'legend': sample.get('legend', ''),
                                             'figure': sample.get('figure', '')})

        # get all results
        results = [j.result() for j in jobs]
        # return good jobs
        return miscellaneous.List(results)

    def list(self, filters=None) -> entities.PagedEntities:
        """
        List Samples for model analytics and metrics

        :param filters: dl.Filter query entity
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.METRICS)
        if not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.METRICS:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.METRICS. Got: {!r}'.format(filters.resource))
        if self._model is not None:
            filters.add(field='modelId', values=self._model.id)
        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged


================================================
File: dtlpy/repositories/nodes.py
================================================
from .. import entities, exceptions
from ..services.api_client import ApiClient


class Nodes(list):

    def __init__(self, client_api: ApiClient, pipeline: entities.Pipeline = None, nodes=None):
        if nodes is None:
            nodes = []
        self._client_api = client_api
        self._pipeline = pipeline
        for node in nodes:
            self.add(node)

    def add(self, node: entities.PipelineNode) -> entities.PipelineNode:
        """
        Add a node to the nodes list

        :param PipelineNode node: node to add
        :return: node that add
        """
        if node not in self:
            self.append(node)
            node._pipeline = self._pipeline
            if not self._pipeline.start_nodes:
                self._pipeline.set_start_node(node=node)
        return node

    def get(self, node_name: str = None, node_id: str = None) -> entities.PipelineNode:
        """
        Get a node from the nodes list by name

        :param str node_name: the node name
        :param str node_id: the node id
        :return: the result node
        """
        if node_id is None and node_name is None:
            raise exceptions.PlatformException('400',
                                               'Must provide node_id or node_name')
        for node in self:
            if node_name == node.name or node_id == node.node_id:
                return node
        return None

    def remove(self, node_name) -> bool:
        """
        Remove a node from the nodes list by name

        :param str node_name: the node name
        :return: True if success
        """
        if not isinstance(node_name, str):
            raise ValueError('node name must be string')

        # get the wanted node
        node = self.get(node_name=node_name)
        if node:
            copy_conn = self._pipeline.connections.copy()
            # remove all node connections
            for conn in copy_conn:
                if node.node_id == conn.source.node_id or node.node_id == conn.target.node_id:
                    self._pipeline.connections.remove(conn)
            # remove the node
            for node_index in range(len(self)):
                if self[node_index].node_id == node.node_id:
                    self.pop(node_index)
                    # remove the node from the start_nodes if it exists
                    # if the node is the root node set the first node as the start node
                    for n in self._pipeline.start_nodes:
                        if n['nodeId'] == node.node_id:
                            # check if still have nodes in the pipeline nodes list
                            if self:
                                if n['type'] == 'root':
                                    self._pipeline.set_start_node(self[0])
                                else:
                                    self._pipeline.start_nodes.remove(n)
                            else:
                                self._pipeline.start_nodes = []
                    return True
        return False


================================================
File: dtlpy/repositories/ontologies.py
================================================
import logging
import traceback

from .. import entities, miscellaneous, exceptions, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Ontologies:
    """
    Ontologies Repository

    The Ontologies class allows users to manage ontologies and their properties.
    Read more about ontology in our `developers docs <https://developers.dataloop.ai/tutorials/recipe_and_ontology/ontology/chapter/>`_.
    """

    def __init__(self, client_api: ApiClient,
                 recipe: entities.Recipe = None,
                 project: entities.Project = None,
                 dataset: entities.Dataset = None):
        self._client_api = client_api
        self._recipe = recipe
        self._project = project
        self._dataset = dataset

    ############
    # entities #
    ############
    @property
    def recipe(self) -> entities.Recipe:
        if self._recipe is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "recipe". need to set a Recipe entity or use ontology.recipes repository')
        assert isinstance(self._recipe, entities.Recipe)
        return self._recipe

    @recipe.setter
    def recipe(self, recipe: entities.Recipe):
        if not isinstance(recipe, entities.Recipe):
            raise ValueError('Must input a valid Recipe entity')
        self._recipe = recipe

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.ontologies repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "dataset". need to set a Dataset entity or use dataset.ontologies repository')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @dataset.setter
    def dataset(self, dataset: entities.Dataset):
        if not isinstance(dataset, entities.Dataset):
            raise ValueError('Must input a valid Dataset entity')
        self._dataset = dataset

    def __get_project_ids(self, project_ids):
        if project_ids is not None:
            return project_ids if isinstance(project_ids, list) else [project_ids]
        elif self._recipe is not None:
            return self._recipe.project_ids
        elif self._project is not None:
            return [self._project.id]
        elif self._dataset is not None:
            return self._dataset.projects
        else:
            return project_ids

    ###########
    # methods #
    ###########
    @_api_reference.add(path='/ontologies', method='post')
    def create(self,
               labels,
               title=None,
               project_ids=None,
               attributes=None
               ) -> entities.Ontology:
        """
        Create a new ontology.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param labels: recipe tags
        :param str title: ontology title, name
        :param list project_ids: recipe project/s
        :param list attributes: recipe attributes
        :return: Ontology object
        :rtype: dtlpy.entities.ontology.Ontology

        **Example**:

        .. code-block:: python

            recipe.ontologies.create(labels='labels_entity',
                                  title='new_ontology',
                                  project_ids='project_ids')
        """
        project_ids = self.__get_project_ids(project_ids=project_ids)
        if attributes is None:
            attributes = list()
        elif not isinstance(project_ids, list):
            project_ids = [project_ids]
        # convert to platform label format (root)
        labels = self.labels_to_roots(labels)
        payload = {"roots": labels,
                   "projectIds": project_ids,
                   "attributes": attributes}
        if title is not None:
            payload['title'] = title
        success, response = self._client_api.gen_request(req_type="post",
                                                         path="/ontologies",
                                                         json_req=payload)
        if success:
            logger.info("Ontology was created successfully")
            ontology = entities.Ontology.from_json(_json=response.json(),
                                                   client_api=self._client_api,
                                                   recipe=self._recipe)
            if self._recipe:
                self._recipe.ontology_ids.append(ontology.id)
                self._recipe.update()
        else:
            logger.error("Failed to create Ontology")
            raise exceptions.PlatformException(response)
        return ontology

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Ontology]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_package, package in enumerate(response_items):
            jobs[i_package] = pool.submit(entities.Ontology._protected_from_json,
                                          **{'client_api': self._client_api,
                                             '_json': package,
                                             'project': self._project,
                                             'dataset': self._dataset,
                                             'recipe': self._recipe})

        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        ontologies = miscellaneous.List([r[1] for r in results if r[0] is True])
        return ontologies

    def _list(self, filters: entities.Filters):
        url = '/ontologies?pageOffset={}&pageSize={}'.format(filters.page, filters.page_size)
        project_ids = None
        ids = None
        for single_filter in filters.and_filter_list:
            if single_filter.field == 'projects':
                project_ids = single_filter.values
                break
        for single_filter in filters.and_filter_list:
            if single_filter.field == 'ids':
                ids = single_filter.values
                break

        if project_ids:
            url = '{}&projects={}'.format(url, ','.join(project_ids))
        if ids:
            url = '{}&ids={}'.format(url, ','.join(ids))

        # request
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def __list(self, filters: entities.Filters) -> entities.PagedEntities:
        """
        List project ontologies.
        
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return:
        """
        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def list(self, project_ids=None) -> miscellaneous.List[entities.Ontology]:
        """
        List ontologies for recipe

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param project_ids:
        :return: list of all the ontologies

        **Example**:

        .. code-block:: python

            recipe.ontologies.list(project_ids='project_ids')
        """
        if self._recipe is not None:
            ontologies = [ontology_id for ontology_id in self.recipe.ontology_ids]

            pool = self._client_api.thread_pools(pool_name='entity.create')
            jobs = [None for _ in range(len(ontologies))]
            for i_ontology, ontology_id in enumerate(ontologies):
                jobs[i_ontology] = pool.submit(self._protected_get, **{'ontology_id': ontology_id})

            # get all results
            results = [j.result() for j in jobs]
            # log errors
            _ = [logger.warning(r[1]) for r in results if r[0] is False]
            # return good jobs
            return miscellaneous.List([r[1] for r in results if r[0] is True])
        else:
            filters = entities.Filters(resource=entities.FiltersResource.ONTOLOGY)
            project_ids = self.__get_project_ids(project_ids=project_ids)
            if project_ids:
                filters.add(field='projects', values=self.__get_project_ids(project_ids=project_ids))
            if self._dataset:
                filters.add(field='ids', values=self._dataset.ontology_ids)
            ontologies = list()
            pages = self.__list(filters=filters)
            for page in pages:
                ontologies += page
            return miscellaneous.List(ontologies)

    def _protected_get(self, ontology_id):
        """
        Same as get but with try-except to catch if error
        :param ontology_id:
        :return:
        """
        try:
            ontology = self.get(ontology_id=ontology_id)
            status = True
        except Exception:
            ontology = traceback.format_exc()
            status = False
        return status, ontology

    @_api_reference.add(path='/ontologies/{id}', method='get')
    def get(self, ontology_id: str) -> entities.Ontology:
        """
        Get Ontology object to use in your code.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str ontology_id: ontology id
        :return: Ontology object
        :rtype: dtlpy.entities.ontology.Ontology

        **Example**:

        .. code-block:: python

            recipe.ontologies.get(ontology_id='ontology_id')
        """
        success, response = self._client_api.gen_request(req_type="get",
                                                         path="/ontologies/{}".format(ontology_id))
        if success:
            ontology = entities.Ontology.from_json(_json=response.json(),
                                                   client_api=self._client_api,
                                                   recipe=self._recipe,
                                                   dataset=self._dataset,
                                                   project=self._project)
        else:
            raise exceptions.PlatformException(response)
        return ontology

    @_api_reference.add(path='/ontologies/{id}', method='delete')
    def delete(self, ontology_id):
        """
        Delete Ontology from the platform.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param ontology_id: ontology id
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            recipe.ontologies.delete(ontology_id='ontology_id')
        """
        success, response = self._client_api.gen_request(req_type="delete",
                                                         path="/ontologies/%s" % ontology_id)
        if success:
            logger.debug("Ontology was deleted successfully")
            return success
        else:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/ontologies/{id}', method='put')
    def update(self, ontology: entities.Ontology, system_metadata=False) -> entities.Ontology:
        """
        Update the Ontology metadata.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

       :param dtlpy.entities.ontology.Ontology ontology: Ontology object
       :param bool system_metadata: bool - True, if you want to change metadata system
       :return: Ontology object
       :rtype: dtlpy.entities.ontology.Ontology

       **Example**:

        .. code-block:: python

            recipe.ontologies.delete(ontology='ontology_entity')
       """
        url_path = "/ontologies/%s" % ontology.id
        if system_metadata:
            url_path += "?system=true"
        success, response = self._client_api.gen_request(req_type="put",
                                                         path=url_path,
                                                         json_req=ontology.to_json())
        if success:
            logger.debug("Ontology was updated successfully")
            # update dataset labels
            ontology = entities.Ontology.from_json(_json=response.json(),
                                                   client_api=self._client_api,
                                                   recipe=self._recipe)
            if self._recipe is not None and self._recipe._dataset is not None:
                self.recipe.dataset._labels = ontology.labels
            return ontology
        else:
            logger.error("Failed to update ontology:")
            raise exceptions.PlatformException(response)

    @staticmethod
    def labels_to_roots(labels):
        """
        Converts labels dictionary to a list of platform representation of labels.

        :param dict labels: labels dict
        :return: platform representation of labels
        """
        roots = list()
        if isinstance(labels, dict):
            for label in labels:
                root = {
                    "value": {
                        "tag": label,
                        "color": labels[label],
                        "attributes": list(),
                    },
                    "children": list(),
                }
                roots.append(root)
        elif isinstance(labels, list):
            for label in labels:
                if isinstance(label, entities.Label):
                    root = label.to_root()
                elif "value" in label:
                    root = {
                        "value": label["value"],
                        "children": label.get("children", list()),
                    }
                else:
                    root = {
                        "value": {
                            "tag": label.get("tag", None),
                            "color": label.get("color", "#FFFFFF"),
                            "attributes": label.get("attributes", list()),
                        },
                        "children": label.get("children", list()),
                    }
                roots.append(root)
        for root in roots:
            if not isinstance(root["value"]["color"], str):
                # noinspection PyStringFormat
                root["value"]["color"] = "#%02x%02x%02x" % root["value"]["color"]
        return roots

    def update_attributes(self,
                          ontology_id: str,
                          title: str,
                          key: str,
                          attribute_type: entities.AttributesTypes,
                          scope: list = None,
                          optional: bool = None,
                          values: list = None,
                          attribute_range: entities.AttributesRange = None):
        """
        ADD a new attribute or update if exist

        :param str ontology_id: ontology_id
        :param str title: attribute title
        :param str key: the key of the attribute must br unique
        :param AttributesTypes attribute_type: dl.AttributesTypes your attribute type
        :param list scope: list of the labels or * for all labels
        :param bool optional: optional attribute
        :param list values: list of the attribute values ( for checkbox and radio button)
        :param dict or AttributesRange attribute_range: dl.AttributesRange object
        :return: true in success
        :rtype: bool

        **Example**:

        .. code-block:: python

            ontology.update_attributes(key='1',
                                       title='checkbox',
                                       attribute_type=dl.AttributesTypes.CHECKBOX,
                                       values=[1,2,3])
        """
        if not title:
            raise exceptions.PlatformException(400, "title must be provided")
        url_path = '/ontologies/{ontology_id}/attributes'.format(ontology_id=ontology_id)

        multi = None
        if attribute_type == entities.AttributesTypes.CHECKBOX:
            attribute_type = 'options'
            multi = True
        elif attribute_type == entities.AttributesTypes.RADIO_BUTTON:
            attribute_type = 'options'
            multi = False

        # build attribute json
        attribute_json = {
            'title': title,
            'key': key,
            'type': attribute_type,
        }

        if optional is not None:
            attribute_json['optional'] = optional

        if multi is not None:
            attribute_json['multi'] = multi

        if values is not None:
            if not isinstance(values, list):
                values = [values]
            for val in values:
                if not isinstance(val, str):
                    raise exceptions.PlatformException(400, 'Attributes values type must be list of strings')
            attribute_json['values'] = values

        if attribute_range is not None:
            attribute_json['range'] = attribute_range.to_json()

        if scope is not None:
            if not isinstance(scope, list):
                scope = [scope]
        else:
            scope = ['*']
        attribute_json['scope'] = scope

        json_req = {
            'items': [attribute_json],
            'upsert': True
        }

        success, response = self._client_api.gen_request(req_type="PATCH",
                                                         path=url_path,
                                                         json_req=json_req)
        if not success:
            raise exceptions.PlatformException(response)
        return True

    def delete_attributes(self, ontology_id, keys: list):
        """
        Delete a bulk of attributes

        :param str ontology_id: ontology id
        :param list keys: Keys of attributes to delete
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            ontology.delete_attributes(['1'])
        """

        if not isinstance(keys, list):
            keys = [keys]
        url_path = '/ontologies/{ontology_id}/attributes'.format(ontology_id=ontology_id)
        json_req = {
            'keys': keys
        }
        success, response = self._client_api.gen_request(req_type="DELETE",
                                                         path=url_path,
                                                         json_req=json_req)
        if not success:
            raise exceptions.PlatformException(response)
        return True


================================================
File: dtlpy/repositories/organizations.py
================================================
import logging

from .. import entities, miscellaneous, exceptions, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Organizations:
    """
    Organizations Repository

    Read our `documentation <https://dataloop.ai/docs/org-setup>`_ to learn more about Organizations in the Dataloop platform.
    """

    def __init__(self, client_api: ApiClient):
        self._client_api = client_api

    def create(self, organization_json: dict) -> entities.Organization:
        """
        Create a new organization.

        **Prerequisites**: This method can only be used by a **superuser**.

        :param dict organization_json: json contain the Organization fields
        :return: Organization object
        :rtype: dtlpy.entities.organization.Organization
        """

        raise exceptions.PlatformException(error='2001', message='Method organizations.create() was removed')

    def list_groups(self, organization: entities.Organization = None,
                    organization_id: str = None,
                    organization_name: str = None):
        """
        List all organization groups (groups that were created within the organization).

        **Prerequisites**: You must be an organization *owner* to use this method.

        You must provide at least ONE of the following params: organization, organization_name, or organization_id.

        :param entities.Organization organization: Organization object
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :return: groups list
        :rtype: list

        **Example**:

        .. code-block:: python

            groups_list = dl.organizations.list_groups(organization_id='organization_id')
        """
        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        url_path = '/orgs/{}/groups'.format(organization.id)

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)

        groups = miscellaneous.List(response.json())
        return groups

    def list_integrations(self, organization: entities.Organization = None,
                          organization_id: str = None,
                          organization_name: str = None,
                          only_available=False):
        """
        List all organization integrations with external cloud storage.

        **Prerequisites**: You must be an organization *owner* to use this method.

        You must provide at least ONE of the following params: organization_id, organization_name, or organization.

        :param entities.Organization organization: Organization object
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :param bool only_available: if True list only the available integrations
        :return: integrations list
        :rtype: list

        **Example**:

        .. code-block:: python

            list_integrations = dl.organizations.list_integrations(organization='organization-entity',
                                                only_available=True)
        """
        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        if only_available:
            url_path = '/orgs/{}/availableIntegrations'.format(organization.id)
        else:
            url_path = '/orgs/{}/integrations'.format(organization.id)

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)

        available_integrations = miscellaneous.List(response.json())
        return available_integrations

    @_api_reference.add(path='/orgs/{orgId}/members', method='get')
    def list_members(self, organization: entities.Organization = None,
                     organization_id: str = None,
                     organization_name: str = None,
                     role: entities.MemberOrgRole = None):
        """
        List all organization members.

        **Prerequisites**: You must be an organization *owner* to use this method.

        You must provide at least ONE of the following params: organization_id, organization_name, or organization.

        :param entities.Organization organization: Organization object
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :param entities.MemberOrgRole role: MemberOrgRole.ADMIN, MemberOrgRole.OWNER, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :return: projects list
        :rtype: list

        **Example**:

        .. code-block:: python

            list_members = dl.organizations.list_members(organization='organization-entity',
                                        role=dl.MemberOrgRole.MEMBER)
        """
        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        url_path = '/orgs/{}/members'.format(organization.id)

        if role is not None and role not in list(entities.MemberOrgRole):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(role,
                                                                                 ', '.join(
                                                                                     list(entities.MemberOrgRole))))

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)

        members = miscellaneous.List(
            [entities.User.from_json(_json=user, client_api=self._client_api, project=None) for user in
             response.json()])

        if role is not None:
            members = [member for member in members if member.role == role]

        return members

    @_api_reference.add(path='/orgs', method='get')
    def list(self) -> miscellaneous.List[entities.Organization]:
        """
        Lists all the organizations in Dataloop.

        **Prerequisites**: You must be a **superuser** to use this method.

        :return: List of Organization objects
        :rtype: list

        **Example**:

        .. code-block:: python

            organizations = dl.organizations.list()
        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/orgs')

        if success:
            pool = self._client_api.thread_pools(pool_name='entity.create')
            organization_json = response.json()
            jobs = [None for _ in range(len(organization_json))]
            # return triggers list
            for i_organization, organization in enumerate(organization_json):
                jobs[i_organization] = pool.submit(entities.Organization._protected_from_json,
                                                   **{'client_api': self._client_api,
                                                      '_json': organization})

            # get all results
            results = [j.result() for j in jobs]
            # log errors
            _ = [logger.warning(r[1]) for r in results if r[0] is False]
            # return good jobs
            organization = miscellaneous.List([r[1] for r in results if r[0] is True])
        else:
            logger.error('Platform error getting organization')
            raise exceptions.PlatformException(response)
        return organization

    @_api_reference.add(path='/orgs/{orgId}', method='get')
    def get(self,
            organization_id: str = None,
            organization_name: str = None,
            fetch: bool = None) -> entities.Organization:
        """
        Get Organization object to be able to use it in your code.

        **Prerequisites**: You must be a **superuser** to use this method.

        You must provide at least ONE of the following params: organization_name or organization_id.

        :param str organization_id: optional - search by id
        :param str organization_name: optional - search by name
        :param fetch: optional - fetch entity from platform, default taken from cookie
        :return: Organization object
        :rtype: dtlpy.entities.organization.Organization

        **Example**:

        .. code-block:: python

            org = dl.organizations.get(organization_id='organization_id')
        """
        if organization_name is None and organization_id is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if fetch is None:
            fetch = self._client_api.fetch_entities

        if fetch:
            if organization_id is not None:
                success, response = self._client_api.gen_request(req_type='get',
                                                                 path='/orgs/{}'.format(organization_id))
                if not success:
                    raise exceptions.PlatformException(response)
                organization = entities.Organization.from_json(
                    client_api=self._client_api,
                    _json=response.json()
                )
            else:
                organizations = self.list()
                organization = [organization for organization in organizations if
                                organization.name == organization_name]
                if not organization:
                    # list is empty
                    raise exceptions.PlatformException(error='404',
                                                       message='organization not found. Name: {}'.format(
                                                           organization_name))
                    # project = None
                elif len(organization) > 1:
                    # more than one matching project
                    raise exceptions.PlatformException(
                        error='404',
                        message='More than one project with same name. Please "get" by id')
                else:
                    organization = organization[0]
        else:
            organization = entities.Organization.from_json(
                _json={'id': organization_id,
                       'name': organization_name},
                client_api=self._client_api)

        return organization

    def update(self, plan: str,
               organization: entities.Organization = None,
               organization_id: str = None,
               organization_name: str = None) -> entities.Organization:
        """
        Update an organization.

        **Prerequisites**: You must be a **superuser** to update an organization.

        You must provide at least ONE of the following params: organization, organization_name, or organization_id.

        :param str plan: OrganizationsPlans.FREEMIUM, OrganizationsPlans.PREMIUM
        :param entities.Organization organization: Organization object
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :return: organization object
        :rtype: dtlpy.entities.organization.Organization

        **Example**:

        .. code-block:: python

            org = dl.organizations.update(organization='organization-entity',
                                    plan=dl.OrganizationsPlans.FREEMIUM)
        """
        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        if plan not in list(entities.OrganizationsPlans):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(plan,
                                                                                 ', '.join(list(
                                                                                     entities.OrganizationsPlans))))
        payload = {'plan': plan}
        url_path = '/orgs/{}/plan'.format(organization.id)
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=payload)
        if success:
            return organization
        else:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/orgs/{orgId}/members', method='post')
    def add_member(self, email: str,
                   role: entities.MemberOrgRole = entities.MemberOrgRole.MEMBER,
                   organization_id: str = None,
                   organization_name: str = None,
                   organization: entities.Organization = None):
        """
        Add members to your organization. Read about members and groups `here <https://dataloop.ai/docs/org-members-groups>`_.

        **Prerequisities**: To add members to an organization, you must be an *owner* in that organization.

        You must provide at least ONE of the following params: organization, organization_name, or organization_id.

        :param str email: the member's email
        :param str role: MemberOrgRole.ADMIN, MemberOrgRole.OWNER, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :param entities.Organization organization: Organization object
        :return: True if successful or error if unsuccessful
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = dl.organizations.add_member(email='user@domain.com',
                                        organization_id='organization_id',
                                        role=dl.MemberOrgRole.MEMBER)
        """

        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        if not isinstance(email, list):
            email = [email]

        if role not in list(entities.MemberOrgRole):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(role,
                                                                                 ', '.join(
                                                                                     list(entities.MemberOrgRole))))

        url_path = '/orgs/{}/members'.format(organization.id)
        payload = {"emails": email, 'role': role}
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        else:
            return True

    @_api_reference.add(path='/orgs/{orgId}/members/{memberId}', method='delete')
    def delete_member(self, user_id: str,
                      organization_id: str = None,
                      organization_name: str = None,
                      organization: entities.Organization = None,
                      sure: bool = False,
                      really: bool = False) -> bool:
        """
        Delete member from the Organization.

        **Prerequisites**: Must be an organization *owner* to delete members.

        You must provide at least ONE of the following params: organization_id, organization_name, organization.

        :param str user_id: user id
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :param entities.Organization organization: Organization object
        :param bool sure: Are you sure you want to delete?
        :param bool really: Really really sure?
        :return: True if success and error if not
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = dl.organizations.delete_member(user_id='user_id',
                                            organization_id='organization_id',
                                            sure=True,
                                            really=True)
        """
        if sure and really:
            if organization is None and organization_id is None and organization_name is None:
                raise exceptions.PlatformException(
                    error='400',
                    message='Must provide an identifier in inputs')

            if organization is None:
                organization = self.get(organization_id=organization_id, organization_name=organization_name)

            url_path = '/orgs/{}/members/{}'.format(organization.id, user_id)
            success, response = self._client_api.gen_request(req_type='delete',
                                                             path=url_path)
            if not success:
                raise exceptions.PlatformException(response)
            else:
                return True
        else:
            raise exceptions.PlatformException(
                error='403',
                message='Cant delete member from SDK. Please login to platform to delete')

    @_api_reference.add(path='/orgs/{orgId}/members', method='patch')
    def update_member(self, email: str,
                      role: entities.MemberOrgRole = entities.MemberOrgRole.MEMBER,
                      organization_id: str = None,
                      organization_name: str = None,
                      organization: entities.Organization = None):
        """
        Update member role.

        **Prerequisites**: You must be an organization *owner* to update a member's role.

        You must provide at least ONE of the following params: organization, organization_name, or organization_id.

        :param str email: the member's email
        :param str role: MemberOrgRole.ADMIN, MemberOrgRole.OWNER, MemberOrgRole.MEMBER, MemberOrgRole.WORKER
        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :param entities.Organization organization: Organization object
        :return: json of the member fields
        :rtype: dict

        **Example**:

        .. code-block:: python

            member_json = dl.organizations.update_member(email='user@domain.com',
                                            organization_id='organization_id',
                                             role=dl.MemberOrgRole.MEMBER)
        """
        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        url_path = '/orgs/{}/members'.format(organization.id)
        payload = dict(role=role, email=email)

        if role not in list(entities.MemberOrgRole):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(role,
                                                                                 ', '.join(
                                                                                     list(entities.MemberOrgRole))))

        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    def cache_action(self,
                     organization_id: str = None,
                     organization_name: str = None,
                     organization: entities.Organization = None,
                     mode=entities.CacheAction.APPLY,
                     pod_type=entities.PodType.SMALL):
        """
        Add or remove Cache for the org

        **Prerequisites**: You must be an organization *owner*

        You must provide at least ONE of the following params: organization, organization_name, or organization_id.

        :param str organization_id: Organization id
        :param str organization_name: Organization name
        :param entities.Organization organization: Organization object
        :param str mode: dl.CacheAction.APPLY or dl.CacheAction.DESTROY
        :param entities.PodType pod_type:  dl.PodType.SMALL, dl.PodType.MEDIUM, dl.PodType.HIGH
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = dl.organizations.enable_cache(organization_id='organization_id',
                                          mode=dl.CacheAction.APPLY)
        """
        if organization is None and organization_id is None and organization_name is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')

        if organization is None:
            organization = self.get(organization_id=organization_id, organization_name=organization_name)

        return organization.cache_action(mode=mode, pod_type=pod_type)


================================================
File: dtlpy/repositories/pipeline_executions.py
================================================
import logging
import time
import numpy as np

from .. import entities, repositories, exceptions, miscellaneous, services, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')
MAX_SLEEP_TIME = 30


class PipelineExecutions:
    """
    PipelineExecutions Repository

    The PipelineExecutions class allows users to manage pipeline executions. See our documentation for more information on `pipelines <https://dataloop.ai/docs/pipelines-overview>`_.
    """

    def __init__(
            self,
            client_api: ApiClient,
            project: entities.Project = None,
            pipeline: entities.Pipeline = None
    ):
        self._client_api = client_api
        self._project = project
        self._pipeline = pipeline

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            try:
                self._project = repositories.Projects(client_api=self._client_api).get()
            except exceptions.NotFound:
                raise exceptions.PlatformException(
                    error='2001',
                    message='Missing "project". need to set a Project entity or use project.pipelines repository')
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    @property
    def pipeline(self) -> entities.Pipeline:
        assert isinstance(self._pipeline, entities.Pipeline)
        return self._pipeline

    @pipeline.setter
    def pipeline(self, pipeline: entities.Pipeline):
        if not isinstance(pipeline, entities.Pipeline):
            raise ValueError('Must input a valid pipeline entity')
        self._pipeline = pipeline

    ###########
    # methods #
    ###########
    # @_api_reference.add(path='/pipelines/{pipelineId}/executions/{executionId}', method='get')
    def get(self,
            pipeline_execution_id: str,
            pipeline_id: str = None
            ) -> entities.PipelineExecution:
        """
        Get Pipeline Execution object

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param str pipeline_execution_id: pipeline execution id
        :param str pipeline_id: pipeline id
        :return: PipelineExecution object
        :rtype: dtlpy.entities.pipeline_execution.PipelineExecution

        **Example**:

        .. code-block:: python

            pipeline_executions = pipeline.pipeline_executions.get(pipeline_id='pipeline_id')
        """

        if pipeline_id is None and self._pipeline is None:
            raise exceptions.PlatformException('400', 'Must provide param pipeline_id')
        elif pipeline_id is None:
            pipeline_id = self._pipeline.id

        success, response = self._client_api.gen_request(
            req_type="get",
            path="/pipelines/{pipeline_id}/executions/{pipeline_execution_id}".format(
                pipeline_id=pipeline_id,
                pipeline_execution_id=pipeline_execution_id
            )
        )
        if not success:
            raise exceptions.PlatformException(response)

        pipeline_execution = entities.PipelineExecution.from_json(
            client_api=self._client_api,
            _json=response.json(),
            pipeline=self._pipeline
        )

        return pipeline_execution

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.PipelineExecution]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]

        for i_pipeline_execution, pipeline_execution in enumerate(response_items):
            jobs[i_pipeline_execution] = pool.submit(
                entities.PipelineExecution._protected_from_json,
                **{
                    'client_api': self._client_api,
                    '_json': pipeline_execution,
                    'pipeline': self._pipeline
                }
            )

        # get all results
        # noinspection PyUnresolvedReferences
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        pipeline_executions = miscellaneous.List([r[1] for r in results if r[0] is True])
        return pipeline_executions

    def _list(self, filters: entities.Filters):
        url = '/pipelines/query'

        # request
        success, response = self._client_api.gen_request(
            req_type='post',
            path=url,
            json_req=filters.prepare()
        )
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @_api_reference.add(path='/pipelines/query', method='post')
    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List project pipeline executions.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            pipeline_executions = pipeline.pipeline_executions.list()
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.PIPELINE_EXECUTION)
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.PIPELINE_EXECUTION:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.PIPELINE_EXECUTION. Got: {!r}'.format(
                    filters.resource))

        project_id = None
        if self._project is not None:
            project_id = self._project.id

        # TODO - uncomment this after DAT-24496 is done and cycles have projectId
        # if self._project is not None:
        #     filters.add(field='projectId', values=self.project.id)

        if self._pipeline is not None:
            filters.add(field='pipelineId', values=self._pipeline.id)

        paged = entities.PagedEntities(
            items_repository=self,
            filters=filters,
            page_offset=filters.page,
            page_size=filters.page_size,
            project_id=project_id,
            client_api=self._client_api
        )

        paged.get_page()
        return paged

    @_api_reference.add(path='/pipelines/{pipelineId}/execute', method='post')
    def create(
            self,
            pipeline_id: str = None,
            execution_input=None,
            node_id: str = None
    ):
        """
        Execute a pipeline.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param pipeline_id: pipeline id
        :param execution_input: list of the dl.FunctionIO or dict of pipeline input - example {'item': 'item_id'}
        :param node_id: node id to start from

        :return: entities.PipelineExecution object
        :rtype: dtlpy.entities.pipeline_execution.PipelineExecution

        **Example**:

        .. code-block:: python

            pipeline_execution = pipeline.pipeline_executions.create(pipeline_id='pipeline_id', execution_input={'item': 'item_id'})
        """
        if pipeline_id is None:
            if self._pipeline is None:
                raise exceptions.PlatformException('400', 'Please provide pipeline id')
            pipeline_id = self._pipeline.id

        payload = dict()
        if execution_input is None:
            # support pipeline executions without any input
            pass
        elif isinstance(execution_input, dict):
            payload['input'] = execution_input
        else:
            if not isinstance(execution_input, list):
                execution_input = [execution_input]
            if len(execution_input) > 0 and isinstance(execution_input[0], entities.FunctionIO):
                payload['input'] = dict()
                for single_input in execution_input:
                    payload['input'].update(single_input.to_json(resource='execution'))
            else:
                raise exceptions.PlatformException('400', 'Unknown input type')

        if node_id is not None:
            payload['nodeId'] = node_id

        success, response = self._client_api.gen_request(
            path='/pipelines/{}/execute'.format(pipeline_id),
            req_type='POST',
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

        execution = entities.PipelineExecution.from_json(_json=response.json(),
                                                         client_api=self._client_api,
                                                         pipeline=self._pipeline)
        return execution

    @_api_reference.add(path='/pipelines/{pipelineId}/execute', method='post')
    def create_batch(
            self,
            pipeline_id: str,
            filters,
            execution_inputs=None,
            wait=True,
            node_id: str = None
    ):
        """
        Create batch executions of a pipeline.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param pipeline_id: pipeline id
        :param filters: Filters entity for a filtering before execute
        :param execution_inputs: list of the dl.FunctionIO or dict of pipeline input - example {'item': 'item_id'}, that represent the extra inputs of the function
        :param bool wait: wait until create task finish
        :return: entities.PipelineExecution object
        :rtype: dtlpy.entities.pipeline_execution.PipelineExecution

        **Example**:

        .. code-block:: python

            command = pipeline.pipeline_executions.create_batch(
                        execution_inputs=dl.FunctionIO(type=dl.PackageInputType.STRING, value='test', name='string'),
                        filters=dl.Filters(field='dir', values='/test'))
        """
        if pipeline_id is None:
            if self._pipeline is None:
                raise exceptions.PlatformException('400', 'Please provide pipeline id')
            pipeline_id = self._pipeline.id

        if filters is None:
            raise exceptions.PlatformException('400', 'Please provide filter')
        extra_input = dict()

        if execution_inputs is None:
            execution_inputs = {}

        if isinstance(execution_inputs, dict):
            extra_input = execution_inputs
        else:
            if not isinstance(execution_inputs, list):
                execution_inputs = [execution_inputs]
            if len(execution_inputs) > 0 and isinstance(execution_inputs[0], entities.FunctionIO):
                for single_input in execution_inputs:
                    extra_input.update(single_input.to_json(resource='execution'))
            else:
                raise exceptions.PlatformException('400', 'Unknown input type')
        payload = dict()
        payload['batch'] = dict()
        payload['batch']['query'] = filters.prepare()
        payload['batch']['args'] = extra_input

        if node_id is not None:
            payload['nodeId'] = node_id

        success, response = self._client_api.gen_request(
            path='/pipelines/{}/execute'.format(pipeline_id),
            req_type='POST',
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

        response_json = response.json()
        command = entities.Command.from_json(_json=response_json,
                                             client_api=self._client_api)
        if wait:
            command = command.wait(timeout=0)
        return command

    @_api_reference.add(path='/pipelines/{pipelineId}/executions/rerun', method='post')
    def rerun(self,
              pipeline_id: str = None,
              method: str = None,
              start_nodes_ids: list = None,
              filters: entities.Filters = None,
              wait: bool = True
              ) -> bool:
        """
        Get Pipeline Execution object

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param str pipeline_id: pipeline id
        :param str method: method to run
        :param list start_nodes_ids: list of start nodes ids
        :param filters: Filters entity for a filtering before execute
        :param bool wait: wait until rerun finish
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            pipeline.pipeline_executions.rerun(pipeline_id='pipeline_id', method=dl.CycleRerunMethod.START_FROM_BEGINNING)
        """

        if pipeline_id is None and self._pipeline is None:
            raise exceptions.PlatformException('400', 'Must provide param pipeline_id')
        elif pipeline_id is None:
            pipeline_id = self._pipeline.id

        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.PIPELINE_EXECUTION)

        success, response = self._client_api.gen_request(
            req_type="post",
            path="/pipelines/{pipeline_id}/executions/rerun".format(
                pipeline_id=pipeline_id,
            ),
            json_req={
                'pipeline': {
                    "method": method,
                    "startNodeIds": start_nodes_ids,
                },
                "batch": {
                    "query": filters.prepare()
                }
            }
        )

        if not success:
            raise exceptions.PlatformException(response)

        command = entities.Command.from_json(_json=response.json(),
                                             client_api=self._client_api)
        if not wait:
            return command
        command = command.wait(timeout=0)

        if 'cycleOptions' not in command.spec:
            raise exceptions.PlatformException(error='400',
                                               message="cycleOptions key is missing in command response: {!r}"
                                               .format(response))
        return True

    def wait(self,
             pipeline_execution_id: str = None,
             pipeline_execution: entities.PipelineExecution = None,
             timeout: int = None,
             backoff_factor=1):
        """
        Get Service execution object.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str pipeline_execution_id: pipeline execution id
        :param str pipeline_execution: dl.PipelineExecution, optional. must input one of pipeline execution or pipeline_execution_id
        :param int timeout: seconds to wait until TimeoutError is raised. if <=0 - wait until done - by default wait take the service timeout
        :param float backoff_factor: A backoff factor to apply between attempts after the second try
        :return: Service execution object
        :rtype: dtlpy.entities.pipeline_execution.PipelineExecution

        **Example**:

        .. code-block:: python

            pipeline.pipeline_executions.wait(pipeline_execution_id='pipeline_execution_id')
        """
        if pipeline_execution is None:
            if pipeline_execution_id is None:
                raise ValueError('Must input at least one: [pipeline_execution, pipeline_execution_id]')
        else:
            pipeline_execution_id = pipeline_execution.id
        elapsed = 0
        start = time.time()
        if timeout is None or timeout <= 0:
            timeout = np.inf

        num_tries = 1
        while elapsed < timeout:
            pipeline_execution = self.get(pipeline_execution_id=pipeline_execution_id)
            if not pipeline_execution.in_progress():
                break
            elapsed = time.time() - start
            if elapsed >= timeout:
                raise TimeoutError(
                    f"Pipeline execution wait() function timed out. id: {pipeline_execution.id!r}, status: {pipeline_execution.status}.")
            sleep_time = np.min([timeout - elapsed, backoff_factor * (2 ** num_tries), MAX_SLEEP_TIME])
            num_tries += 1
            logger.debug(
                f"Pipeline execution {pipeline_execution.id!r} has been running for {elapsed:.2f}[s]. Sleeping for {sleep_time:.2f}[s]")
            time.sleep(sleep_time)
        return pipeline_execution


================================================
File: dtlpy/repositories/pipelines.py
================================================
import logging
from .. import entities, repositories, exceptions, miscellaneous, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


def generate_basic_pipeline():
    return dict(name="", projectId="", nodes=[], connections=[], startNodes=[], variables=[])


class Pipelines:
    """
    Pipelines Repository

    The Pipelines class allows users to manage pipelines and their properties. See our documentation for more information on `pipelines <https://dataloop.ai/docs/pipelines-overview>`_.
    """

    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            try:
                self._project = repositories.Projects(client_api=self._client_api).get()
            except exceptions.NotFound:
                raise exceptions.PlatformException(
                    error='2001',
                    message='Missing "project". need to set a Project entity or use project.pipelines repository')
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ###########
    # methods #
    ###########
    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/pipelines".format(self.project.id))

    def open_in_web(self,
                    pipeline: entities.Pipeline = None,
                    pipeline_id: str = None,
                    pipeline_name: str = None):
        """
        Open the pipeline in web platform.

        **prerequisites**: Must be *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
        :param str pipeline_id: pipeline id
        :param str pipeline_name: pipeline name

        **Example**:

        .. code-block:: python

            project.pipelines.open_in_web(pipeline_id='pipeline_id')
        """
        if pipeline_name is not None:
            pipeline = self.get(pipeline_name=pipeline_name)
        if pipeline is not None:
            pipeline.open_in_web()
        elif pipeline_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(pipeline_id))
        else:
            self._client_api._open_in_web(url=self.platform_url)

    @_api_reference.add(path='/pipelines/{pipelineId}', method='get')
    def get(self,
            pipeline_name=None,
            pipeline_id=None,
            fetch=None
            ) -> entities.Pipeline:
        """
        Get Pipeline object to use in your code.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        You must provide at least ONE of the following params: pipeline_name, pipeline_id.

        :param str pipeline_id: pipeline id
        :param str pipeline_name: pipeline name
        :param fetch: optional - fetch entity from platform, default taken from cookie
        :return: Pipeline object
        :rtype: dtlpy.entities.pipeline.Pipeline

        **Example**:

        .. code-block:: python

            pipeline = project.pipelines.get(pipeline_id='pipeline_id')
        """
        if fetch is None:
            fetch = self._client_api.fetch_entities

        if pipeline_name is None and pipeline_id is None:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide an identifier in inputs')
        elif fetch:
            if pipeline_id is not None:
                success, response = self._client_api.gen_request(
                    req_type="get",
                    path="/pipelines/{}".format(pipeline_id))
                if not success:
                    raise exceptions.PlatformException(response)
                pipeline = entities.Pipeline.from_json(
                    client_api=self._client_api,
                    _json=response.json(),
                    project=self._project
                )
                if pipeline_name is not None and pipeline.name != pipeline_name:
                    logger.warning(
                        "Mismatch found in pipeline.get: pipeline_name is different then pipeline.name:"
                        " {!r} != {!r}".format(
                            pipeline_name,
                            pipeline.name
                        )
                    )
            elif pipeline_name is not None:
                filters = entities.Filters(
                    field='name',
                    values=pipeline_name,
                    resource=entities.FiltersResource.PIPELINE,
                    use_defaults=False
                )
                if self._project is not None:
                    filters.add(field='projectId', values=self._project.id)
                pipelines = self.list(filters=filters)
                if pipelines.items_count == 0:
                    raise exceptions.PlatformException(
                        error='404',
                        message='Pipeline not found. Name: {}'.format(pipeline_name))
                elif pipelines.items_count > 1:
                    raise exceptions.PlatformException(
                        error='400',
                        message='More than one pipelines found by the name of: {} '
                                'Please get pipeline from a project entity'.format(pipeline_name))
                pipeline = pipelines.items[0]
            else:
                raise exceptions.PlatformException(
                    error='400',
                    message='No checked-out pipeline was found, must checkout or provide an identifier in inputs')
        else:
            pipeline = entities.Pipeline.from_json(
                _json={'id': pipeline_id,
                       'name': pipeline_name},
                client_api=self._client_api,
                project=self._project,
                is_fetched=False
            )

        return pipeline

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Pipeline]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]

        for i_pipeline, pipeline in enumerate(response_items):
            jobs[i_pipeline] = pool.submit(
                entities.Pipeline._protected_from_json,
                **{
                    'client_api': self._client_api,
                    '_json': pipeline,
                    'project': self._project
                }
            )

        # get all results
        # noinspection PyUnresolvedReferences
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        pipelines = miscellaneous.List([r[1] for r in results if r[0] is True])
        return pipelines

    def _list(self, filters: entities.Filters):
        url = '/pipelines/query'

        # request
        success, response = self._client_api.gen_request(
            req_type='post',
            path=url,
            json_req=filters.prepare()
        )
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    @_api_reference.add(path='/pipelines/query', method='post')
    def list(self,
             filters: entities.Filters = None,
             project_id: str = None
             ) -> entities.PagedEntities:
        """
        List project pipelines.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param str project_id: project id
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            pipelines = project.pipelines.list()
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.PIPELINE)
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.PIPELINE:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.PIPELINE. Got: {!r}'.format(filters.resource))

        if project_id is None and self._project is not None:
            project_id = self._project.id

        if project_id is not None:
            filters.add(field='projectId', values=project_id)

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       project_id=project_id,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def _name_validation(self, name: str):
        url = '/piper-misc/naming/packages/{}'.format(name)

        # request
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)
        if not success:
            raise exceptions.PlatformException(response)

    # @_api_reference.add(path='/pipelines/{pipelineId}', method='delete')
    def delete(self,
               pipeline: entities.Pipeline = None,
               pipeline_name: str = None,
               pipeline_id: str = None):
        """
        Delete Pipeline object.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

       :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
       :param str pipeline_id: pipeline id
       :param str pipeline_name: pipeline name
       :return: True if success
       :rtype: bool

       **Example**:

        .. code-block:: python

            is_deleted = project.pipelines.delete(pipeline_id='pipeline_id')
       """
        # get id and name
        if pipeline_id is None:
            if pipeline is None:
                pipeline = self.get(pipeline_name=pipeline_name)
            pipeline_id = pipeline.id

        # request
        success, response = self._client_api.gen_request(req_type="delete",
                                                         path="/pipelines/{}".format(pipeline_id))

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return results
        return True

    @_api_reference.add(path='/pipelines/{pipelineId}/settings', method='patch')
    def update_settings(self, pipeline: entities.Pipeline, settings: entities.PipelineSettings):
        """
        Update pipeline settings.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
        :param dtlpy.entities.pipeline.PipelineSettings settings: settings entity
        :return: Pipeline object
        :rtype: dtlpy.entities.pipeline.Pipeline

        **Example**:

        .. code-block:: python

            pipeline = project.pipelines.update_settings(pipeline='pipeline_entity', settings=dl.PipelineSettings(keep_triggers_active=True))
        """
        # payload
        payload = {'settings': settings.to_json()}

        # request
        success, response = self._client_api.gen_request(
            req_type='patch',
            path='/pipelines/{}'.format(pipeline.id),
            json_req=payload
        )
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Pipeline.from_json(
            _json=response.json(),
            client_api=self._client_api,
            project=self._project
        )

    def __update_variables(self, pipeline: entities.Pipeline):
        pipeline_json = pipeline.to_json()
        variables = pipeline_json.get('variables', list())

        for var in variables:
            if var.get('reference', None) is None:
                var['reference'] = pipeline.id

        # payload
        payload = {'variables': variables}

        # request
        success, response = self._client_api.gen_request(
            req_type='patch',
            path='/pipelines/{}/variables'.format(pipeline.id),
            json_req=payload
        )
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Pipeline.from_json(
            _json=response.json(),
            client_api=self._client_api,
            project=self._project
        )

    @_api_reference.add(path='/pipelines/{pipelineId}', method='patch')
    def update(self,
               pipeline: entities.Pipeline = None
               ) -> entities.Pipeline:
        """
        Update pipeline changes to platform.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
        :return: Pipeline object
        :rtype: dtlpy.entities.pipeline.Pipeline

        **Example**:

        .. code-block:: python

            pipeline = project.pipelines.update(pipeline='pipeline_entity')
        """
        # payload
        payload = pipeline.to_json()

        # update settings
        if pipeline.settings_changed():
            new_pipeline = self.update_settings(pipeline=pipeline, settings=pipeline.settings)
            payload['settings'] = new_pipeline.to_json().get('settings', payload.get('settings'))

        # update variables
        if pipeline.variables_changed():
            new_pipeline = self.__update_variables(pipeline=pipeline)
            payload['variables'] = new_pipeline.to_json().get('variables', payload.get('variables'))

        success, response = self._client_api.gen_request(
            req_type='patch',
            path='/pipelines/{}'.format(pipeline.id),
            json_req=payload
        )

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Pipeline.from_json(
            _json=response.json(),
            client_api=self._client_api,
            project=self._project
        )

    @_api_reference.add(path='/pipelines', method='post')
    def create(self,
               name: str = None,
               project_id: str = None,
               pipeline_json: dict = None
               ) -> entities.Pipeline:
        """
        Create a new pipeline.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param str name: pipeline name
        :param str project_id: project id
        :param dict pipeline_json: json containing the pipeline fields
        :return: Pipeline object
        :rtype: dtlpy.entities.pipeline.Pipeline

        **Example**:

        .. code-block:: python

            pipeline = project.pipelines.create(name='pipeline_name')
        """
        if pipeline_json is None:
            pipeline_json = generate_basic_pipeline()

        if name is not None:
            pipeline_json['name'] = name

        if project_id is not None:
            pipeline_json['projectId'] = project_id
        else:
            if not pipeline_json.get('projectId', None):
                pipeline_json['projectId'] = self.project.id

        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/pipelines',
                                                         json_req=pipeline_json)
        if success:
            pipeline = entities.Pipeline.from_json(client_api=self._client_api,
                                                   _json=response.json(),
                                                   project=self.project)
        else:
            raise exceptions.PlatformException(response)
        assert isinstance(pipeline, entities.Pipeline)
        return pipeline

    @_api_reference.add(path='/pipelines/{pipelineId}/install', method='post')
    def install(self, pipeline: entities.Pipeline = None, resume_option: entities.PipelineResumeOption = None):
        """
        Install (start) a pipeline.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
        :param dtlpy.entities.pipeline.PipelineResumeOption resume_option: optional - resume pipeline method (what to do with existing cycles)
        :return: Composition object

        **Example**:

        .. code-block:: python

            project.pipelines.install(pipeline='pipeline_entity')
        """

        payload = {}
        if resume_option:
            payload['resumeOption'] = resume_option

        success, response = self._client_api.gen_request(
            req_type='post',
            path='/pipelines/{}/install'.format(pipeline.id),
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

        return entities.Pipeline.from_json(client_api=self._client_api,
                                           _json=response.json(),
                                           project=self.project)

    @_api_reference.add(path='/pipelines/{pipelineId}/uninstall', method='post')
    def pause(self, pipeline: entities.Pipeline = None, keep_triggers_active: bool = None):
        """
        Pause a pipeline.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
        :param bool keep_triggers_active: Do we want the triggers to stay active and collect events
        :return: Composition object

        **Example**:

        .. code-block:: python

            project.pipelines.pause(pipeline='pipeline_entity')
        """

        payload = {}
        if keep_triggers_active is not None:
            payload['keepTriggersActive'] = keep_triggers_active

        success, response = self._client_api.gen_request(
            req_type='post',
            path='/pipelines/{}/uninstall'.format(pipeline.id),
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/pipelines/{pipelineId}/reset', method='post')
    def reset(self,
              pipeline: entities.Pipeline = None,
              pipeline_id: str = None,
              pipeline_name: str = None,
              stop_if_running: bool = False):
        """
        Reset pipeline counters.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity - optional
        :param str pipeline_id: pipeline_id -  optional
        :param str pipeline_name: pipeline_name -  optional
        :param bool stop_if_running: If the pipeline is installed it will stop the pipeline and reset the counters.
        :return: bool

        **Example**:

        .. code-block:: python

            success = project.pipelines.reset(pipeline='pipeline_entity')
        """

        if pipeline_id is None:
            if pipeline is None:
                if pipeline_name is not None:
                    pipeline = self.get(pipeline_name=pipeline_name)
                else:
                    raise exceptions.PlatformException(
                        '400',
                        'Must provide one of pipeline, pipeline_id or pipeline_name'
                    )
            pipeline_id = pipeline.id

        if stop_if_running is True:
            if pipeline is None:
                pipeline = self.get(pipeline_id=pipeline_id)
            pipeline.pause()

        success, response = self._client_api.gen_request(
            req_type='post',
            path='/pipelines/{}/reset'.format(pipeline_id)
        )

        if not success:
            raise exceptions.PlatformException(response)

        return True

    @_api_reference.add(path='/pipelines/{id}/statistics', method='get')
    def stats(self, pipeline: entities.Pipeline = None, pipeline_id: str = None, pipeline_name: str = None):
        """
        Get pipeline counters.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity - optional
        :param str pipeline_id: pipeline_id -  optional
        :param str pipeline_name: pipeline_name -  optional
        :return: PipelineStats
        :rtype: dtlpy.entities.pipeline.PipelineStats

        **Example**:

        .. code-block:: python

            pipeline_stats = project.pipelines.stats(pipeline='pipeline_entity')
        """

        if pipeline_id is None:
            if pipeline is None:
                if pipeline_name is not None:
                    pipeline = self.get(pipeline_name=pipeline_name)
                else:
                    raise exceptions.PlatformException(
                        '400',
                        'Must provide one of pipeline, pipeline_id or pipeline_name'
                    )
            pipeline_id = pipeline.id

        success, response = self._client_api.gen_request(
            req_type='get',
            path='/pipelines/{}/statistics'.format(pipeline_id)
        )

        if not success:
            raise exceptions.PlatformException(response)

        return entities.PipelineStats.from_json(_json=response.json())

    def execute(self,
                pipeline: entities.Pipeline = None,
                pipeline_id: str = None,
                pipeline_name: str = None,
                execution_input=None):
        """
        Execute a pipeline and return the pipeline execution as an object.

        **prerequisites**: You must be an *owner* or *developer* to use this method.

        :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
        :param str pipeline_id: pipeline id
        :param str pipeline_name: pipeline name
        :param execution_input: list of the dl.FunctionIO or dict of pipeline input - example {'item': 'item_id'}
        :return: entities.PipelineExecution object
        :rtype: dtlpy.entities.pipeline_execution.PipelineExecution

        **Example**:

        .. code-block:: python

            pipeline_execution= project.pipelines.execute(pipeline='pipeline_entity', execution_input= {'item': 'item_id'} )
        """
        if pipeline is None:
            pipeline = self.get(pipeline_id=pipeline_id, pipeline_name=pipeline_name)
        execution = repositories.PipelineExecutions(pipeline=pipeline,
                                                    client_api=self._client_api,
                                                    project=self._project).create(pipeline_id=pipeline.id,
                                                                                  execution_input=execution_input)
        return execution


================================================
File: dtlpy/repositories/projects.py
================================================
import logging
from urllib.parse import quote
import jwt

from .. import entities, miscellaneous, exceptions, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Projects:
    """
    Projects Repository

    The Projects class allows the user to manage projects and their properties.

    For more information on Projects see the `Dataloop documentation <https://dataloop.ai/docs/project#>`_.
    """

    def __init__(self, client_api: ApiClient, org=None):
        self._client_api = client_api
        self._org = org

    def __get_from_cache(self) -> entities.Project:
        project = self._client_api.state_io.get('project')
        if project is not None:
            project = entities.Project.from_json(_json=project, client_api=self._client_api)
        return project

    def __get_by_id(self, project_id: str, log_error: bool) -> entities.Project:
        """
        :param project_id:
        """
        success, response = self._client_api.gen_request(
            req_type='get',
            path='/projects/{}'.format(project_id),
            log_error=log_error
        )

        try:
            response_json = response.json()
        except Exception:
            try:
                logger.exception('Failed to parse response content: {}'.format(response.text))
            except Exception:
                logger.exception('Failed to print response content')
            raise

        if success:
            project = entities.Project.from_json(
                client_api=self._client_api,
                _json=response_json
            )
        else:
            # raise PlatformException(response)
            # TODO because of a bug in gate wrong error is returned so for now manually raise not found
            raise exceptions.PlatformException(error="404", message="Project not found")
        return project

    def __get_by_name(self, project_name: str):
        """
        :param project_name:
        """
        project_name = quote(project_name.encode("utf-8"))
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/projects/{}/name'.format(project_name))
        if success:
            projects = [entities.Project.from_json(client_api=self._client_api,
                                                   _json=project_json) for project_json in response.json()]
        else:
            # TODO because of a bug in gate wrong error is returned so for now manually raise not found
            raise exceptions.PlatformException(error="404", message="Project not found")
        return projects

    def __get_by_identifier(self, identifier=None) -> entities.Project:
        """
        :param identifier:
        """
        projects = self.list()
        projects_by_name = [project for project in projects if identifier in project.id or identifier in project.name]
        if len(projects_by_name) == 1:
            return projects_by_name[0]
        elif len(projects_by_name) > 1:
            raise Exception('Multiple projects with this name/identifier exist')
        else:
            raise Exception("Project not found")

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects")

    def open_in_web(self,
                    project_name: str = None,
                    project_id: str = None,
                    project: entities.Project = None):
        """
        Open the project in our web platform.

        **Prerequisites**: All users can open a project in the web.

        :param str project_name: The Name of the project
        :param str project_id: The Id of the project
        :param dtlpy.entities.project.Project project: project object

        **Example**:

        .. code-block:: python

            dl.projects.open_in_web(project_id='project_id')
        """
        if project_name is not None:
            project = self.get(project_name=project_name)
        if project is not None:
            project.open_in_web()
        elif project_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(project_id))
        else:
            self._client_api._open_in_web(url=self.platform_url)

    def checkout(self,
                 identifier: str = None,
                 project_name: str = None,
                 project_id: str = None,
                 project: entities.Project = None):
        """
        Checkout (switch) to a project to work on.

        **Prerequisites**: All users can open a project in the web.

        You must provide at least ONE of the following params: project_id, project_name.

        :param str identifier: project name or partial id that you wish to switch
        :param str project_name: The Name of the project
        :param str project_id: The Id of the project
        :param dtlpy.entities.project.Project project: project object

        **Example**:

        .. code-block:: python

            dl.projects.checkout(project_id='project_id')
        """
        if project is None:
            if project_id is not None or project_name is not None:
                project = self.get(project_id=project_id, project_name=project_name)
            elif identifier is not None:
                project = self.__get_by_identifier(identifier=identifier)
            else:
                raise exceptions.PlatformException(error='400',
                                                   message='Must provide partial/full id/name to checkout')
        self._client_api.state_io.put('project', project.to_json())
        logger.info('Checked out to project {}'.format(project.name))

    def _send_mail(self, project_id: str, send_to: str, title: str, content: str) -> bool:
        if project_id:
            url = '/projects/{}/mail'.format(project_id)
        else:
            url = '/outbox'
        assert isinstance(title, str)
        assert isinstance(content, str)
        if self._client_api.token is not None:
            sender = jwt.decode(self._client_api.token, algorithms=['HS256'],
                                verify=False, options={'verify_signature': False})['email']
        else:
            raise exceptions.PlatformException('600', 'Token expired please log in')

        payload = {
            'to': send_to,
            'from': sender,
            'subject': title,
            'body': content
        }

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=payload)

        if not success:
            raise exceptions.PlatformException(response)
        return True

    @_api_reference.add(path='/projects/{projectId}/members/{userId}', method='post')
    def add_member(self, email: str, project_id: str, role: entities.MemberRole = entities.MemberRole.DEVELOPER):
        """
        Add a member to the project.

        **Prerequisites**: You must be in the role of an *owner* to add a member to a project.

        :param str email: member email
        :param str project_id: The Id of the project
        :param role: The required role for the user. Use the enum dl.MemberRole
        :return: dict that represent the user
        :rtype: dict

        **Example**:

        .. code-block:: python

            user_json = dl.projects.add_member(project_id='project_id', email='user@dataloop.ai', role=dl.MemberRole.DEVELOPER)
        """
        url_path = '/projects/{}/members/{}'.format(project_id, email)
        payload = dict(role=role)

        if role not in list(entities.MemberRole):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(role,
                                                                                 ', '.join(list(entities.MemberRole))))

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url_path,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    @_api_reference.add(path='/projects/{projectId}/members/{userId}', method='patch')
    def update_member(self, email: str, project_id: str, role: entities.MemberRole = entities.MemberRole.DEVELOPER):
        """
        Update member's information/details in the project.

        **Prerequisites**: You must be in the role of an *owner* to update a member.

        :param str email: member email
        :param str project_id: The Id of the project
        :param role: The required role for the user. Use the enum dl.MemberRole
        :return: dict that represent the user
        :rtype: dict

        **Example**:

        .. code-block:: python

            user_json = = dl.projects.update_member(project_id='project_id', email='user@dataloop.ai', role=dl.MemberRole.DEVELOPER)
        """
        url_path = '/projects/{}/members/{}'.format(project_id, email)
        payload = dict(role=role)

        if role not in list(entities.MemberRole):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(role,
                                                                                 ', '.join(list(entities.MemberRole))))

        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    @_api_reference.add(path='/projects/{projectId}/members/{userId}', method='delete')
    def remove_member(self, email: str, project_id: str):
        """
        Remove a member from the project.

        **Prerequisites**: You must be in the role of an *owner* to delete a member from a project.

        :param str email: member email
        :param str project_id: The Id of the project
        :return: dict that represents the user
        :rtype: dict

        **Example**:

        .. code-block:: python

            user_json = dl.projects.remove_member(project_id='project_id', email='user@dataloop.ai')
        """
        url_path = '/projects/{}/members/{}'.format(project_id, email)
        success, response = self._client_api.gen_request(req_type='delete',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    @_api_reference.add(path='/projects/{projectId}/members', method='get')
    def list_members(self, project: entities.Project, role: entities.MemberRole = None):
        """
        Get a list of the project members.

        **Prerequisites**: You must be in the role of an *owner* to list project members.

        :param dtlpy.entities.project.Project project: Project object
        :param role: The required role for the user. Use the enum dl.MemberRole
        :return: list of the project members
        :rtype: list

        **Example**:

        .. code-block:: python

            users_jsons_list = dl.projects.list_members(project_id='project_id', role=dl.MemberRole.DEVELOPER)
        """
        url_path = '/projects/{}/members'.format(project.id)

        if role is not None and role not in list(entities.MemberRole):
            raise ValueError('Unknown role {!r}, role must be one of: {}'.format(role,
                                                                                 ', '.join(list(entities.MemberRole))))

        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)
        if not success:
            raise exceptions.PlatformException(response)

        members = miscellaneous.List(
            [entities.User.from_json(_json=user, client_api=self._client_api, project=project) for user in
             response.json()])

        if role is not None:
            members = [member for member in members if member.role == role]

        return members

    @_api_reference.add(path='/projects', method='get')
    def list(self) -> miscellaneous.List[entities.Project]:
        """
        Get the user's project list

        **Prerequisites**: You must be a **superuser** to list all users' projects.

        :return: List of Project objects

        **Example**:

        .. code-block:: python

            projects = dl.projects.list()
        """
        if self._org is None:
            url_path = '/projects'
        else:
            url_path = '/orgs/{}/projects'.format(self._org.id)
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url_path)

        if success:
            pool = self._client_api.thread_pools(pool_name='entity.create')
            projects_json = response.json()
            jobs = [None for _ in range(len(projects_json))]
            # return triggers list
            for i_project, project in enumerate(projects_json):
                jobs[i_project] = pool.submit(entities.Project._protected_from_json,
                                              **{'client_api': self._client_api,
                                                 '_json': project})

            # get all results
            results = [j.result() for j in jobs]
            # log errors
            _ = [logger.warning(r[1]) for r in results if r[0] is False]
            # return good jobs
            projects = miscellaneous.List([r[1] for r in results if r[0] is True])
        else:
            logger.error('Platform error getting projects')
            raise exceptions.PlatformException(response)
        return projects

    @_api_reference.add(path='/projects/{projectId}', method='get')
    def get(self,
            project_name: str = None,
            project_id: str = None,
            checkout: bool = False,
            fetch: bool = None,
            log_error=True) -> entities.Project:
        """
        Get a Project object.

        **Prerequisites**: You must be in the role of an *owner* to get a project object.

        You must check out to a project or provide at least one of the following params: project_id, project_name

        :param str project_name: optional - search by name
        :param str project_id: optional - search by id
        :param bool checkout: set the project as a default project object (cookies)
        :param bool fetch: optional - fetch entity from platform (True), default taken from cookie
        :param bool log_error: optional - show the logs errors
        :return: Project object
        :rtype: dtlpy.entities.project.Project

        **Example**:

        .. code-block:: python

            project = dl.projects.get(project_id='project_id')
        """
        if fetch is None:
            fetch = self._client_api.fetch_entities

        if project_id is None and project_name is None:
            project = self.__get_from_cache()
            if project is None:
                raise exceptions.PlatformException(
                    error='400',
                    message='No checked-out Project was found. You must checkout to a project or provide an identifier in inputs')
        elif fetch:
            if project_id is not None:
                if not isinstance(project_id, str):
                    raise exceptions.PlatformException(
                        error='400',
                        message='project_id must be strings')

                project = self.__get_by_id(project_id, log_error=log_error)
                # verify input project name is same as the given id
                if project_name is not None and project.name != project_name:
                    logger.warning(
                        "Mismatch found in projects.get: project_name is different then project.name:"
                        " {!r} != {!r}".format(
                            project_name,
                            project.name))
            elif project_name is not None:
                if not isinstance(project_name, str):
                    raise exceptions.PlatformException(
                        error='400',
                        message='project_name must be strings')

                projects = self.__get_by_name(project_name)
                if len(projects) > 1:
                    # more than one matching project
                    raise exceptions.PlatformException(
                        error='404',
                        message='More than one project with same name. Please "get" by id')
                else:
                    project = projects[0]
            else:
                raise exceptions.PlatformException(
                    error='404',
                    message='No input and no checked-out found')
        else:
            project = entities.Project.from_json(_json={'id': project_id,
                                                        'name': project_name},
                                                 client_api=self._client_api,
                                                 is_fetched=False)
        assert isinstance(project, entities.Project)
        if checkout:
            self.checkout(project=project)
        if project.id not in self._client_api.platform_settings.working_projects:
            self._client_api.platform_settings.add_project(project.id)
            try:
                settings_list = project.settings.resolve(user_email=self._client_api.info()['user_email'],
                                                         project_id=project.id)
                self._client_api.platform_settings.add_bulk(settings_list)
            except:
                logger.warning("failed to add project settings")
        return project

    @_api_reference.add(path='/projects/{projectId}', method='delete')
    def delete(self,
               project_name: str = None,
               project_id: str = None,
               sure: bool = False,
               really: bool = False) -> bool:
        """
        Delete a project forever!

        **Prerequisites**: You must be in the role of an *owner* to delete a project.

        :param str project_name: optional - search by name
        :param str project_id: optional - search by id
        :param bool sure: Are you sure you want to delete?
        :param bool really: Really really sure?
        :return: True if success, error if not
        :rtype: bool

        **Example**:

        .. code-block:: python

            is_deleted = dl.projects.delete(project_id='project_id', sure=True, really=True)
        """
        if sure and really:
            if project_id is None:
                project = self.get(project_name=project_name)
                project_id = project.id
            success, response = self._client_api.gen_request(req_type='delete',
                                                             path='/projects/{}'.format(project_id))
            if not success:
                raise exceptions.PlatformException(response)
            logger.info('Project id {} deleted successfully'.format(project_id))
            return True
        else:
            raise exceptions.PlatformException(
                error='403',
                message='Cant delete project from SDK. Please login to platform to delete')

    @_api_reference.add(path='/projects/{projectId}', method='patch')
    def update(self,
               project: entities.Project,
               system_metadata: bool = False) -> entities.Project:
        """
        Update a project information (e.g., name, member roles, etc.).

        **Prerequisites**: You must be in the role of an *owner* to add a member to a project.

        :param dtlpy.entities.project.Project project: project object
        :param bool system_metadata: optional - True, if you want to change metadata system
        :return: Project object
        :rtype: dtlpy.entities.project.Project

        **Example**:

        .. code-block:: python

            project = dl.projects.delete(project='project_entity')
        """
        url_path = '/projects/{}'.format(project.id)
        if system_metadata:
            url_path += '?system=true'
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=project.to_json())
        if success:
            return project
        else:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/projects', method='post')
    def create(self,
               project_name: str,
               checkout: bool = False) -> entities.Project:
        """
        Create a new project.

        **Prerequisites**: Any user can create a project.

        :param str project_name: The Name of the project
        :param bool checkout: set the project as a default project object (cookies)
        :return: Project object
        :rtype: dtlpy.entities.project.Project

        **Example**:

        .. code-block:: python

            project = dl.projects.create(project_name='project_name')
        """
        payload = {'name': project_name}
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects',
                                                         data=payload)
        if success:
            project = entities.Project.from_json(client_api=self._client_api,
                                                 _json=response.json())
        else:
            raise exceptions.PlatformException(response)
        assert isinstance(project, entities.Project)
        if checkout:
            self.checkout(project=project)
        return project


================================================
File: dtlpy/repositories/recipes.py
================================================
import logging
import traceback
import urllib.parse

from .. import entities, miscellaneous, repositories, exceptions, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Recipes:
    """
    Recipes Repository

    The Recipes class allows you to manage recipes and their properties.
    For more information on Recipes, see our `documentation <https://dataloop.ai/docs/ontology>`_ and `developers' documentation <https://developers.dataloop.ai/tutorials/recipe_and_ontology/recipe/chapter/>`_.
    """

    def __init__(self,
                 client_api: ApiClient,
                 dataset: entities.Dataset = None,
                 project: entities.Project = None,
                 project_id: str = None):
        self._client_api = client_api
        self._dataset = dataset
        self._project = project
        self._project_id = project_id
        if project_id is None and project is not None:
            self._project_id = project.id

    ############
    # entities #
    ############
    @property
    def platform_url(self):
        if self._project_id is None:
            project_id = self.dataset.project.id
        else:
            project_id = self._project_id
        return self._client_api._get_resource_url("projects/{}/recipes".format(project_id))

    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "dataset". need to set a Dataset entity or use dataset.recipes repository')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @dataset.setter
    def dataset(self, dataset: entities.Dataset):
        if not isinstance(dataset, entities.Dataset):
            raise ValueError('Must input a valid Dataset entity')
        self._dataset = dataset

    ###########
    # methods #
    ###########
    @_api_reference.add(path='/recipes', method='post')
    def create(self,
               project_ids=None,
               ontology_ids=None,
               labels=None,
               recipe_name=None,
               attributes=None,
               annotation_instruction_file=None
               ) -> entities.Recipe:
        """
        Create a new Recipe.
        Note: If the param ontology_ids is None, an ontology will be created first.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str project_ids: project ids
        :param str or list ontology_ids: ontology ids
        :param labels: labels
        :param str recipe_name: recipe name
        :param attributes: attributes
        :param str annotation_instruction_file: file path or url of the recipe instruction
        :return: Recipe entity
        :rtype: dtlpy.entities.recipe.Recipe

        **Example**:

        .. code-block:: python

            dataset.recipes.create(recipe_name='My Recipe', labels=labels))
        """
        if labels is None:
            labels = list()
        if attributes is None:
            attributes = list()
        if project_ids is None:
            if self._dataset is not None:
                project_ids = [self._dataset.project.id]
            else:
                # get from cache
                project = self._client_api.state_io.get('project')
                if project is not None:
                    # build entity from json
                    p = entities.Project.from_json(_json=project, client_api=self._client_api)
                    project_ids = [p.id]
                else:
                    raise exceptions.PlatformException('Must provide project_ids')
        if ontology_ids is None:
            ontolgies = repositories.Ontologies(client_api=self._client_api,
                                                recipe=None)
            ontology = ontolgies.create(labels=labels,
                                        project_ids=project_ids,
                                        attributes=attributes)
            ontology_ids = [ontology.id]
        elif not isinstance(ontology_ids, list):
            ontology_ids = [ontology_ids]
        if recipe_name is None:
            recipe_name = self._dataset.name + " Default Recipe" if self._dataset is not None else "Default Recipe"
        payload = {'title': recipe_name,
                   'projectIds': project_ids,
                   'ontologyIds': ontology_ids,
                   'uiSettings': {
                       "allowObjectIdAutoAssign": True,
                       "studioV2App": True
                   }}
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/recipes',
                                                         json_req=payload)
        if success:
            recipe = entities.Recipe.from_json(client_api=self._client_api,
                                               _json=response.json(),
                                               dataset=self._dataset)
            if annotation_instruction_file:
                recipe.add_instruction(annotation_instruction_file=annotation_instruction_file)
        else:
            logger.error('Failed to create Recipe')
            raise exceptions.PlatformException(response)

        if self._dataset is not None:
            self._dataset.switch_recipe(recipe.id)
        return recipe

    def list(self, filters: entities.Filters = None) -> miscellaneous.List[entities.Recipe]:
        """
        List recipes for a dataset.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: list of all recipes
        :retype: list

        **Example**:

        .. code-block:: python

            dataset.recipes.list()
        """
        if self._dataset is not None:
            try:
                recipes = [recipe_id for recipe_id in self._dataset.metadata['system']['recipes']]
            except KeyError:
                recipes = list()

            pool = self._client_api.thread_pools(pool_name='entity.create')
            jobs = [None for _ in range(len(recipes))]
            for i_recipe, recipe_id in enumerate(recipes):
                jobs[i_recipe] = pool.submit(self._protected_get, **{'recipe_id': recipe_id})

            # get all results
            results = [j.result() for j in jobs]
            # log errors
            _ = [logger.warning(r[1]) for r in results if r[0] is False]
            # return good jobs
            recipes = miscellaneous.List([r[1] for r in results if r[0] is True])
        elif self._project_id is not None:
            if filters is None:
                filters = entities.Filters(resource=entities.FiltersResource.RECIPE)
            # assert type filters
            elif not isinstance(filters, entities.Filters):
                raise exceptions.PlatformException(error='400',
                                                   message='Unknown filters type: {!r}'.format(type(filters)))
            if filters.resource != entities.FiltersResource.RECIPE:
                raise exceptions.PlatformException(
                    error='400',
                    message='Filters resource must to be FiltersResource.RECIPE. Got: {!r}'.format(filters.resource))
            if not filters.has_field('projects'):
                filters.add(field='projects', values=[self._project_id])

            recipes = entities.PagedEntities(items_repository=self,
                                             filters=filters,
                                             page_offset=filters.page,
                                             page_size=filters.page_size,
                                             project_id=self._project_id,
                                             client_api=self._client_api)
            recipes.get_page()
        else:
            raise exceptions.PlatformException('400', 'Must have project or dataset entity in repository')

        return recipes

    def _list(self, filters: entities.Filters):
        url = filters.generate_url_query_params('/recipes')
        encoded_url = urllib.parse.quote(url, safe='/:?=&')
        # request
        success, response = self._client_api.gen_request(req_type='get', path=encoded_url)
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Recipe]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_rec, rec in enumerate(response_items):
            jobs[i_rec] = pool.submit(entities.Recipe._protected_from_json,
                                      **{'client_api': self._client_api,
                                         '_json': rec,
                                         'project': self._project,
                                         'dataset': self._dataset})

        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        recipes = miscellaneous.List([r[1] for r in results if r[0] is True])
        return recipes

    def _protected_get(self, recipe_id):
        """
        Same as get but with try-except to catch if error
        :param recipe_id:
        :return:
        """
        try:
            recipe = self.get(recipe_id=recipe_id)
            status = True
        except Exception:
            recipe = traceback.format_exc()
            status = False
        return status, recipe

    @_api_reference.add(path='/recipes/{id}', method='get')
    def get(self, recipe_id: str) -> entities.Recipe:
        """
        Get a Recipe object to use in your code.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str recipe_id: recipe id
        :return: Recipe object
        :rtype: dtlpy.entities.recipe.Recipe

        **Example**:

        .. code-block:: python

            dataset.recipes.get(recipe_id='recipe_id')
        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/recipes/%s' % recipe_id)
        if success:
            recipe = entities.Recipe.from_json(client_api=self._client_api,
                                               _json=response.json(),
                                               project=self._project,
                                               dataset=self._dataset)
        else:
            logger.error('Unable to get info from recipe. Recipe_id id: {}'.format(recipe_id))
            raise exceptions.PlatformException(response)

        return recipe

    def open_in_web(self,
                    recipe: entities.Recipe = None,
                    recipe_id: str = None):
        """
        Open the recipe in web platform.

        **Prerequisites**: All users.

        :param dtlpy.entities.recipe.Recipe recipe: recipe entity
        :param str recipe_id: recipe id

        **Example**:

        .. code-block:: python

            dataset.recipes.open_in_web(recipe_id='recipe_id')
        """
        if recipe is not None:
            recipe.open_in_web()
        elif recipe_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(recipe_id))
        else:
            self._client_api._open_in_web(url=self.platform_url)

    @_api_reference.add(path='/recipes/{id}', method='delete')
    def delete(self, recipe_id: str, force: bool = False):
        """
        Delete recipe from platform.
        
        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param str recipe_id: recipe id
        :param bool force: force delete recipe
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            dataset.recipes.delete(recipe_id='recipe_id')
        """
        path = '/recipes/{}'.format(recipe_id)
        if force:
            path += '?force=true'
        success, response = self._client_api.gen_request(req_type='delete',
                                                         path=path)
        if not success:
            raise exceptions.PlatformException(response)
        logger.info('Recipe id {} deleted successfully'.format(recipe_id))
        return True

    @_api_reference.add(path='/recipes/{id}', method='patch')
    def update(self, recipe: entities.Recipe, system_metadata=False) -> entities.Recipe:
        """
        Update recipe.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.recipe.Recipe recipe: Recipe object
        :param bool system_metadata: True, if you want to change metadata system
        :return: Recipe object
        :rtype: dtlpy.entities.recipe.Recipe

        **Example**:

        .. code-block:: python

            dataset.recipes.update(recipe='recipe_entity')
        """
        url_path = '/recipes/%s' % recipe.id
        if system_metadata:
            url_path += '?system=true'
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url_path,
                                                         json_req=recipe.to_json())
        if success:
            return entities.Recipe.from_json(client_api=self._client_api, _json=response.json(), dataset=self._dataset)
        else:
            logger.error('Error while updating item:')
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/recipes/{id}/clone', method='post')
    def clone(self,
              recipe: entities.Recipe = None,
              recipe_id: str = None,
              shallow: bool = False):
        """
        Clone recipe.

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

       :param dtlpy.entities.recipe.Recipe recipe: Recipe object
       :param str recipe_id: Recipe id
       :param bool shallow: If True, link to existing ontology, clones all ontologies that are linked to the recipe as well
       :return: Cloned ontology object
       :rtype: dtlpy.entities.recipe.Recipe

       **Example**:

        .. code-block:: python

            dataset.recipes.clone(recipe_id='recipe_id')
       """
        if recipe is None and recipe_id is None:
            raise exceptions.PlatformException('400', 'Must provide recipe or recipe_id')
        if recipe_id is None:
            if not isinstance(recipe, entities.Recipe):
                raise exceptions.PlatformException('400', 'Recipe must me entities.Recipe type')
            else:
                recipe_id = recipe.id

        payload = {'shallow': shallow}

        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/recipes/{}/clone'.format(recipe_id),
                                                         json_req=payload)
        if success:
            recipe = entities.Recipe.from_json(client_api=self._client_api,
                                               _json=response.json())
        else:
            logger.error('Failed to clone Recipe')
            raise exceptions.PlatformException(response)

        assert isinstance(recipe, entities.Recipe)
        logger.debug('Recipe has been cloned successfully. recipe id: {}'.format(recipe.id))

        return recipe


================================================
File: dtlpy/repositories/resource_executions.py
================================================
import logging

from .. import exceptions, entities, miscellaneous
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class ResourceExecutions:
    """
    Resource Executions Repository

    The ResourceExecutions class allows the users to manage executions (executions of Resource) and their properties.
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None,
                 resource=None):
        self._client_api = client_api
        self._project = project
        self._resource = resource

    ############
    # entities #
    ############
    @property
    def resource(self):
        if self._resource is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "resource". need to set a entity or use resource.executions repository')
        assert hasattr(entities, self._resource.__class__.__name__)
        return self._resource

    @resource.setter
    def resource(self, resource):
        if not hasattr(entities, self._resource.__class__.__name__):
            raise ValueError('Must input a valid entity')
        self._resource = resource

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use Project.executions repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def _list(self, filters: entities.Filters):
        """
        List resource executions

        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters items
        :return:
        """
        url = "/executions/resource/query"

        # request
        success, response = self._client_api.gen_request(req_type='POST',
                                                         path=url,
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List resource executions

        **Prerequisites**: You must be in the role of an *owner* or *developer*.

        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters items
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            item.resource_executions.list()
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.RESOURCE_EXECUTION)
            if self._resource is not None:
                filters.add(field='resourceType', values=self._resource.__class__.__name__)
                filters.add(field='resourceId', values=self._resource.id)
            else:
                raise exceptions.PlatformException(
                    error='400',
                    message='Must have resource')
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(
                error='400',
                message='Unknown filters type: {!r}'.format(type(filters)))
        if filters.resource != entities.FiltersResource.RESOURCE_EXECUTION:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.RESOURCE_EXECUTION. '
                        'Got: {!r}'.format(filters.resource))
        if self._project is not None:
            filters.add(field='projectId', values=self._project.id)

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Execution]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return execution list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.ResourceExecution._protected_from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item,
                                          'project': self._project,
                                          'resource': self._resource})

        # get results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        return miscellaneous.List([r[1] for r in results if r[0] is True])


================================================
File: dtlpy/repositories/schema.py
================================================
from typing import List
import logging

from .. import entities, exceptions
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class UnsearchablePaths:
    """
    Unsearchable Paths

    """

    def __init__(self, client_api: ApiClient, dataset: entities.Dataset = None):
        self._client_api = client_api
        self._dataset = dataset

    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Cannot perform action WITHOUT Dataset entity in {} repository.'.format(
                    self.__class__.__name__) + ' Please use dataset.schema or set a dataset')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    def __unsearchable_paths_request(self, payload):
        """
        Set unsearchable paths in dataset schema
        """
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/datasets/{}/schema/items'.format(self.dataset.id),
                                                         json_req=
                                                         {
                                                             "unsearchablePaths": payload
                                                         })
        if not success:
            raise exceptions.PlatformException(response)

        resp = response.json()
        if isinstance(resp, dict):
            command = entities.Command.from_json(_json=resp,
                                                 client_api=self._client_api)

            try:
                command.wait()
            except Exception as e:
                logger.error('Command failed: {}'.format(e))
        else:
            logger.warning(resp)
        return success

    def add(self, paths: List[str]):
        """
        Add metadata paths to `unsearchablePaths` to exclude keys under these paths from indexing, making them unsearchable through the Dataset Browser UI and DQL queries.

        :param paths: list of paths to create
        :return: true if success, else raise exception
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = dataset.schema.unsearchable_paths.add(paths=['metadata.key1', 'metadata.key2'])
        """
        return self.__unsearchable_paths_request(payload={"add": paths})

    def remove(self, paths: List[str]):
        """
        Remove metadata paths from `unsearchablePaths` to index keys under these paths, making them searchable through the Dataset Browser UI and DQL queries.

        :param paths: list of paths to delete
        :return: true if success, else raise exception
        :rtype: bool

        **Example**:

        .. code-block:: python

            success = dataset.schema.unsearchable_paths.remove(paths=['metadata.key1', 'metadata.key2'])
        """
        return self.__unsearchable_paths_request(payload={"remove": paths})


class Schema:
    """
    Schema Repository
    """

    def __init__(self, client_api: ApiClient, dataset: entities.Dataset):
        self._client_api = client_api
        self.dataset = dataset
        self.unsearchable_paths = UnsearchablePaths(client_api=self._client_api, dataset=dataset)

    ###########
    # methods #
    ###########
    def get(self):
        """
        Get dataset schema

        :return: dataset schema
        :rtype: dict

        **Example**:

        .. code-block:: python

            json = dataset.schema.get()
        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/datasets/{}/schema'.format(self.dataset.id))
        if not success:
            raise exceptions.PlatformException(response)

        return response.json()


================================================
File: dtlpy/repositories/settings.py
================================================
import logging
from typing import Union

from .. import exceptions, entities, ApiClient

logger = logging.getLogger(name='dtlpy')
BASE_URL = '/settings'


class Settings:
    def __init__(
            self,
            client_api: ApiClient,
            project: entities.Project = None,
            dataset: entities.Dataset = None,
            org: entities.Organization = None,
            task: entities.Task = None,
            resource=None,
            resource_type=None
    ):
        self._client_api = client_api
        self._org = org
        self._project = project
        self._dataset = dataset
        self._task = task
        self._resource = resource
        self._resource_type = resource_type

    ###########
    # methods #
    ###########

    @staticmethod
    def get_constructor(res: dict):
        constructor = entities.Setting.from_json
        return constructor

    def _build_entities_from_response(self, response_items):
        settings = list()
        for _, setting in enumerate(response_items):
            settings.append(
                self.get_constructor(setting)(
                    client_api=self._client_api,
                    _json=setting,
                    project=self._project
                )
            )

        return settings

    def _build_settings(self,
                        setting_name: str = None,
                        setting_value: entities.SettingsTypes = None,
                        setting_value_type: entities.SettingsValueTypes = None,
                        setting_default_value=None
                        ):
        if self._resource is None:
            raise exceptions.PlatformException('400', 'Must have resource')
        setting = entities.Setting(
            name=setting_name,
            value=setting_value,
            value_type=setting_value_type,
            section_name=entities.SettingsSectionNames.SDK,
            default_value=setting_default_value,
            scope=entities.SettingScope(type=self._resource_type,
                                        id=self._resource.id,
                                        role=entities.Role.ALL,
                                        prevent_override=False,
                                        visible=True),
        )
        return setting

    def create(self,
               setting: entities.Setting = None,
               setting_name: str = None,
               setting_value=None,
               setting_value_type: entities.SettingsValueTypes = None,
               setting_default_value=None,
               ) -> entities.Setting:
        """
        Create a new setting

        :param Setting setting: setting entity
        :param str setting_name: the setting name
        :param setting_value: the setting value
        :param SettingsValueTypes setting_value_type: the setting type dl.SettingsValueTypes
        :param setting_default_value: the setting default value
        :return: setting entity
        """
        if sum([1 for param in [setting_name, setting_value, setting_value_type] if param is None]) > 0 \
                and setting is None:
            raise exceptions.PlatformException('400', 'Must provide setting object or'
                                                      'setting_name, setting_value and setting_value_type')

        if setting is None:
            setting = self._build_settings(setting_name=setting_name,
                                           setting_value=setting_value,
                                           setting_value_type=setting_value_type,
                                           setting_default_value=setting_default_value)

        success, response = self._client_api.gen_request(
            req_type='post',
            path='{}'.format(
                BASE_URL
            ),
            json_req=setting.to_json()
        )

        if success:
            _json = response.json()
            constructor = self.get_constructor(_json)
        else:
            raise exceptions.PlatformException(response)

        # add settings to cookies
        self._client_api.platform_settings.add(setting.name,
                                               {
                                                   setting.scope.id: setting.value
                                               }
                                               )
        return constructor(
            _json=_json,
            client_api=self._client_api,
            project=self._project,
            org=self._org
        )

    def update(self,
               setting: entities.BaseSetting = None,
               setting_id: str = None,
               setting_name: str = None,
               setting_value=None,
               setting_default_value=None,
               ) -> entities.Setting:
        """
        Update a setting

        :param Setting setting: setting entity
        :param str setting_id: the setting id
        :param str setting_name: the setting name
        :param setting_value: the setting value
        :param setting_default_value: the setting default value
        :return: setting entity
        """
        if sum([1 for param in [setting_name, setting_value] if param is None]) > 0 \
                and setting is None:
            raise exceptions.PlatformException('400', 'Must provide setting object or'
                                                      'setting_name, setting_value and setting_value_type')

        if setting is None:
            setting = self.get(setting_id=setting_id)
            setting.name = setting_name
            setting.value = setting_value
            if setting_default_value is not None:
                setting.default_value = setting_default_value

        patch = setting.to_json()
        patch.pop('id')
        patch.pop('name')
        success, response = self._client_api.gen_request(
            req_type='patch',
            path='{}/{}'.format(
                BASE_URL,
                setting.id
            ),
            json_req=patch
        )

        if success:
            _json = response.json()
            constructor = self.get_constructor(_json)
        else:
            raise exceptions.PlatformException(response)

        # add settings to cookies
        self._client_api.platform_settings.add(setting.name,
                                               {
                                                   setting.scope.id: setting.value
                                               }
                                               )

        return constructor(
            _json=_json,
            client_api=self._client_api,
            project=self._project,
            org=self._org
        )

    def delete(self, setting_id: str) -> bool:
        """
        Delete a setting

        :param str setting_id: the setting id
        :return: True if success exceptions if not
        """
        success, response = self._client_api.gen_request(
            req_type='delete',
            path='{}/{}'.format(
                BASE_URL,
                setting_id
            )
        )

        if success:
            return True
        else:
            raise exceptions.PlatformException(response)

    def get(self, setting_name: str = None, setting_id: str = None) -> entities.Setting:
        """
        Get a setting by id

        :param str setting_name: the setting name
        :param str setting_id: the setting id
        :return: setting entity
        """
        if setting_id is not None:
            success, response = self._client_api.gen_request(
                req_type='get',
                path='{}/{}'.format(
                    BASE_URL,
                    setting_id
                )
            )

            if success:
                _json = response.json()
                constructor = self.get_constructor(_json)
                setting = constructor(
                    _json=_json,
                    client_api=self._client_api,
                    project=self._project,
                    org=self._org
                )
            else:
                raise exceptions.PlatformException(response)
        elif setting_name is not None:
            if not isinstance(setting_name, str):
                raise exceptions.PlatformException(
                    error='400',
                    message='setting_name must be strings')
            settings = self.list(
                filters=entities.Filters(field='name', values=setting_name, resource=entities.FiltersResource.SETTINGS))
            if len(settings.items) > 1:
                raise exceptions.PlatformException(
                    error='404',
                    message='More than one setting with same name. Please "get" by id')
            elif len(settings.items) == 0:
                raise exceptions.PlatformException('404', 'setting not found')
            else:
                setting = settings.items[0]
        else:
            raise exceptions.PlatformException(
                error='400',
                message='Must provide at least ONE of the following params: setting_id, setting_name.')
        return setting

    def _list(self, filters: entities.Filters):
        success, response = self._client_api.gen_request(
            req_type='post',
            path='{}/query'.format(BASE_URL),
            json_req=filters.prepare()
        )
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List settings

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.SETTINGS)
            filters.sort_by(entities.FiltersOrderByDirection.ASCENDING)

        if filters.resource != entities.FiltersResource.SETTINGS:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.SETTINGS . Got: {!r}'.format(filters.resource))

        if self._project is not None:
            filters.add(field='scope.id', values=self._project.id)

        paged = entities.PagedEntities(
            items_repository=self,
            filters=filters,
            page_offset=filters.page,
            page_size=filters.page_size,
            project_id=self._project.id if self._project is not None else None,
            client_api=self._client_api
        )

        paged.get_page()

        return paged

    def resolve(
            self,
            user_email: str,
            org_id: str = None,
            project_id: str = None,
            dataset_id: str = None,
            task_id: str = None,
    ):
        """
        return all the settings that relevant to the provider params

        :param str user_email: user email
        :param str org_id: org id
        :param str project_id: project id
        :param str dataset_id: dataset id
        :param str task_id: task id
        """
        payload = {
            'userId': user_email
        }

        if self._project:
            payload['projectId'] = self._project.id
        elif project_id:
            payload['projectId'] = project_id

        if self._org:
            payload['orgId'] = self._org.id
        elif org_id:
            payload['orgId'] = org_id

        if self._dataset:
            payload['datasetId'] = self._dataset.id
        elif dataset_id:
            payload['datasetId'] = dataset_id

        if self._task:
            payload['taskId'] = self._task.id
        elif task_id:
            payload['taskId'] = task_id

        success, response = self._client_api.gen_request(
            req_type='post',
            path='{}/resolve'.format(BASE_URL),
            json_req=payload
        )

        if success:
            _json = response.json()
            return self._build_entities_from_response(response_items=_json)
        else:
            raise exceptions.PlatformException(response)


================================================
File: dtlpy/repositories/tasks.py
================================================
import datetime
import logging
import json
from typing import Union, List

from .. import exceptions, miscellaneous, entities, repositories, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')
URL_PATH = '/annotationtasks'


class Tasks:
    """
    Tasks Repository

    The Tasks class allows the user to manage tasks and their properties.
    For more information, read in our developers' documentation about `Creating Tasks <https://developers.dataloop.ai/tutorials/task_workflows/create_a_task/chapter/>`_, and `Redistributing and Reassigning Tasks <https://developers.dataloop.ai/tutorials/task_workflows/redistributing_and_reassigning_a_task/chapter/>`_.
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None,
                 dataset: entities.Dataset = None,
                 project_id: str = None):
        self._client_api = client_api
        self._project = project
        self._dataset = dataset
        self._assignments = None
        if project_id is None:
            if self._project is not None:
                project_id = self._project.id
            elif self._dataset is not None:
                if self._dataset._project is not None:
                    project_id = self._dataset._project.id
                elif isinstance(self._dataset.projects, list) and len(self._dataset.projects) > 0:
                    project_id = self._dataset.projects[0]
        self._project_id = project_id

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.tasks repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    @property
    def dataset(self) -> entities.Dataset:
        if self._dataset is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "dataset". need to set a Dataset entity or use dataset.tasks repository')
        assert isinstance(self._dataset, entities.Dataset)
        return self._dataset

    @dataset.setter
    def dataset(self, dataset: entities.Dataset):
        if not isinstance(dataset, entities.Dataset):
            raise ValueError('Must input a valid Dataset entity')
        self._dataset = dataset

    @property
    def assignments(self) -> repositories.Assignments:
        if self._assignments is None:
            self._assignments = repositories.Assignments(client_api=self._client_api, project=self._project)
        assert isinstance(self._assignments, repositories.Assignments)
        return self._assignments

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Task]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]

        for i_task, task in enumerate(response_items):
            jobs[i_task] = pool.submit(
                entities.Task._protected_from_json,
                **{
                    'client_api': self._client_api,
                    '_json': task,
                    'project': self._project,
                    'dataset': self._dataset
                }
            )

        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        tasks = miscellaneous.List([r[1] for r in results if r[0] is True])
        return tasks

    def _list(self, filters: entities.Filters):
        url = '{}/query'.format(URL_PATH)
        query = filters.prepare()
        query['context'] = dict(projectIds=[self._project_id])
        success, response = self._client_api.gen_request(
            req_type='post',
            path=url,
            json_req=filters.prepare()
        )

        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def query(self, filters=None, project_ids=None):
        """
        List all tasks by filter.

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who has been assigned the task.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param list project_ids: list of project ids of the required tasks
        :return: Paged entity - task pages generator
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            dataset.tasks.query(project_ids='project_ids')
        """
        if project_ids is None:
            if self._project_id is not None:
                project_ids = self._project_id
            else:
                raise exceptions.PlatformException('400', 'Please provide param project_ids')

        if not isinstance(project_ids, list):
            project_ids = [project_ids]

        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.TASK)
        else:
            if not isinstance(filters, entities.Filters):
                raise exceptions.PlatformException('400', 'Unknown filters type')
            if filters.resource != entities.FiltersResource.TASK:
                raise exceptions.PlatformException('400', 'Filter resource must be task')

        if filters.context is None:
            filters.context = {'projectIds': project_ids}

        if self._project_id is not None:
            filters.add(field='projectId', values=self._project_id)

        if self._dataset is not None:
            filters.add(field='datasetId', values=self._dataset.id)

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       project_id=self._project_id,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    ###########
    # methods #
    ###########
    @_api_reference.add(path='/annotationtasks/query', method='post')
    def list(
            self,
            project_ids=None,
            status=None,
            task_name=None,
            pages_size=None,
            page_offset=None,
            recipe=None,
            creator=None,
            assignments=None,
            min_date=None,
            max_date=None,
            filters: entities.Filters = None
    ) -> Union[miscellaneous.List[entities.Task], entities.PagedEntities]:
        """
        List all tasks.

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who has been assigned the task.

        :param project_ids: search tasks by given list of project ids
        :param str status: search tasks by a given task status
        :param str task_name: search tasks by a given task name
        :param int pages_size: pages size of the output generator
        :param int page_offset: page offset of the output generator
        :param dtlpy.entities.recipe.Recipe recipe: Search tasks that use a given recipe. Provide the required recipe object
        :param str creator: search tasks created by a given creator (user email)
        :param dtlpy.entities.assignment.Assignment recipe assignments: assignments object
        :param double min_date: search all tasks created AFTER a given date, use a milliseconds format. For example: 1661780622008
        :param double max_date: search all tasks created BEFORE a given date, use a milliseconds format. For example: 1661780622008
        :param dtlpy.entities.filters.Filters filters: dl.Filters entity to filters tasks using DQL
        :return: List of Task objects

        **Example**:

        .. code-block:: python

            dataset.tasks.list(project_ids='project_ids',pages_size=100, page_offset=0)
        """
        # url
        url = URL_PATH + '/query'

        if filters is None:
            filters = entities.Filters(use_defaults=False, resource=entities.FiltersResource.TASK)
        else:
            return self.query(filters=filters, project_ids=project_ids)

        if self._dataset is not None:
            filters.add(field='datasetId', values=self._dataset.id)

        if project_ids is not None:
            if not isinstance(project_ids, list):
                project_ids = [project_ids]
        elif self._project_id is not None:
            project_ids = [self._project_id]
        else:
            raise ('400', 'Must provide project')
        filters.context = {"projectIds": project_ids}

        if assignments is not None:
            if not isinstance(assignments, list):
                assignments = [assignments]
            assignments = [
                assignments_entity.id if isinstance(assignments_entity, entities.Assignment) else assignments_entity
                for assignments_entity in assignments]
            filters.add(field='assignmentIds', values=assignments, operator=entities.FiltersOperations.IN)
        if status is not None:
            filters.add(field='status', values=status)
        if task_name is not None:
            filters.add(field='name', values=task_name)
        if pages_size is not None:
            filters.page_size = pages_size
        if pages_size is None:
            filters.page_size = 500
        if page_offset is not None:
            filters.page = page_offset
        if recipe is not None:
            if not isinstance(recipe, list):
                recipe = [recipe]
            recipe = [recipe_entity.id if isinstance(recipe_entity, entities.Recipe) else recipe_entity
                      for recipe_entity in recipe]
            filters.add(field='recipeId', values=recipe, operator=entities.FiltersOperations.IN)
        if creator is not None:
            filters.add(field='creator', values=creator)
        if min_date is not None:
            filters.add(field='dueDate', values=min_date, operator=entities.FiltersOperations.GREATER_THAN)
        if max_date is not None:
            filters.add(field='dueDate', values=max_date, operator=entities.FiltersOperations.LESS_THAN)

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=filters.prepare())
        if success:
            tasks = miscellaneous.List(
                [entities.Task.from_json(client_api=self._client_api,
                                         _json=_json, project=self._project, dataset=self._dataset)
                 for _json in response.json()['items']])
        else:
            logger.error('Platform error getting annotation task')
            raise exceptions.PlatformException(response)

        return tasks

    @_api_reference.add(path='/annotationtasks/{id}', method='get')
    def get(self, task_name=None, task_id=None) -> entities.Task:
        """
        Get a Task object to use in your code.

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who has been assigned the task.

        :param str task_name: optional - search by name
        :param str task_id: optional - search by id
        :return: task object
        :rtype: dtlpy.entities.task.Task

        **Example**:

        .. code-block:: python

            dataset.tasks.get(task_id='task_id')
        """

        # url
        url = URL_PATH

        if task_id is not None:
            url = '{}/{}'.format(url, task_id)
            success, response = self._client_api.gen_request(req_type='get',
                                                             path=url)
            if not success:
                raise exceptions.PlatformException(response)
            else:
                task = entities.Task.from_json(_json=response.json(),
                                               client_api=self._client_api, project=self._project,
                                               dataset=self._dataset)
            # verify input task name is same as the given id
            if task_name is not None and task.name != task_name:
                logger.warning(
                    "Mismatch found in tasks.get: task_name is different then task.name:"
                    " {!r} != {!r}".format(
                        task_name,
                        task.name))
        elif task_name is not None:
            tasks = self.list(filters=entities.Filters(field='name',
                                                       values=task_name,
                                                       resource=entities.FiltersResource.TASK))
            if tasks.items_count == 0:
                raise exceptions.PlatformException('404', 'Annotation task not found')
            elif tasks.items_count > 1:
                raise exceptions.PlatformException('404',
                                                   'More than one Annotation task exist with the same name: {}'.format(
                                                       task_name))
            else:
                task = tasks[0][0]
        else:
            raise exceptions.PlatformException('400', 'Must provide either Annotation task name or Annotation task id')

        assert isinstance(task, entities.Task)
        return task

    @property
    def platform_url(self):
        return self._client_api._get_resource_url("projects/{}/tasks".format(self.project.id))

    def open_in_web(self,
                    task_name: str = None,
                    task_id: str = None,
                    task: entities.Task = None):
        """
        Open the task in the web platform.

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who has been assigned the task.

        :param str task_name: the name of the task
        :param str task_id: the Id of the task
        :param dtlpy.entities.task.Task task: the task object

        **Example**:

        .. code-block:: python

            dataset.tasks.open_in_web(task_id='task_id')
        """
        if task_name is not None:
            task = self.get(task_name=task_name)
        if task is not None:
            task.open_in_web()
        elif task_id is not None:
            self._client_api._open_in_web(url=self.platform_url + '/' + str(task_id))
        else:
            self._client_api._open_in_web(url=self.platform_url)

    @_api_reference.add(path='/annotationtasks/{id}', method='delete')
    def delete(self,
               task: entities.Task = None,
               task_name: str = None,
               task_id: str = None,
               wait: bool = True):
        """
        Delete the Task.

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who created that task.

        :param dtlpy.entities.task.Task task: the task object
        :param str task_name: the name of the task
        :param str task_id: the Id of the task
        :param bool wait: wait until delete task finish
        :return: True is success
        :rtype: bool

        **Example**:

        .. code-block:: python

            dataset.tasks.delete(task_id='task_id')
        """
        if task_id is None:
            if task is None:
                if task_name is None:
                    raise exceptions.PlatformException('400',
                                                       'Must provide either annotation task, '
                                                       'annotation task name or annotation task id')
                else:
                    task = self.get(task_name=task_name)
            task_id = task.id

        url = URL_PATH
        url = '{}/{}'.format(url, task_id)
        success, response = self._client_api.gen_request(req_type='delete',
                                                         path=url,
                                                         json_req={'asynced': wait})

        if not success:
            raise exceptions.PlatformException(response)
        response_json = response.json()
        command = entities.Command.from_json(_json=response_json,
                                             client_api=self._client_api)
        if not wait:
            return command
        command = command.wait(timeout=0)
        if 'deleteTaskId' not in command.spec:
            raise exceptions.PlatformException(error='400',
                                               message="deleteTaskId key is missing in command response: {}"
                                               .format(response))
        return True

    @_api_reference.add(path='/annotationtasks/{id}', method='patch')
    def update(self,
               task: entities.Task = None,
               system_metadata=False
               ) -> entities.Task:
        """
        Update a Task.

        **Prerequisites**: You must be in the role of an *owner* or *developer* or *annotation manager* who created that task.

        :param dtlpy.entities.task.Task task: the task object
        :param bool system_metadata: True, if you want to change metadata system
        :return: Task object
        :rtype: dtlpy.entities.task.Task

        **Example**:

        .. code-block:: python

            dataset.tasks.update(task='task_entity')
        """
        url = URL_PATH
        url = '{}/{}'.format(url, task.id)

        if system_metadata:
            url += '?system=true'

        success, response = self._client_api.gen_request(req_type='patch',
                                                         path=url,
                                                         json_req=task.to_json())
        if success:
            return entities.Task.from_json(_json=response.json(),
                                           client_api=self._client_api, project=self._project, dataset=self._dataset)
        else:
            raise exceptions.PlatformException(response)

    def create_qa_task(self,
                       task: entities.Task,
                       assignee_ids,
                       due_date=None,
                       filters=None,
                       items=None,
                       query=None,
                       workload=None,
                       metadata=None,
                       available_actions=None,
                       wait=True,
                       batch_size=None,
                       max_batch_workload=None,
                       allowed_assignees=None,
                       priority=entities.TaskPriority.MEDIUM
                       ) -> entities.Task:
        """
        Create a new QA Task.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        :param dtlpy.entities.task.Task task: the parent annotation task object
        :param list assignee_ids: list the QA task assignees (contributors) that should be working on the task. Provide a list of users' emails
        :param float due_date: date by which the QA task should be finished; for example, due_date=datetime.datetime(day=1, month=1, year=2029).timestamp()
        :param entities.Filters filters: dl.Filters entity to filter items for the task
        :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
        :param dict DQL query: filter items for the task
        :param List[WorkloadUnit] workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param dict metadata: metadata for the task
        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "approved" and "discard"
        :param bool wait: wait until create task finish
        :param int batch_size: Pulling batch size (items), use with pulling allocation method. Restrictions - Min 3, max 100
        :param int max_batch_workload: Max items in assignment, use with pulling allocation method. Restrictions - Min batchSize + 2, max batchSize * 2
        :param list allowed_assignees: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
        :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
        :return: task object
        :rtype: dtlpy.entities.task.Task

        **Example**:

        .. code-block:: python

            dataset.tasks.create_qa_task(task= 'task_entity',
                                        due_date = datetime.datetime(day= 1, month= 1, year= 2029).timestamp(),
                                        assignee_ids =[ 'annotator1@dataloop.ai', 'annotator2@dataloop.ai'])
        """
        source_filter = entities.filters.SingleFilter(
            field='metadata.system.refs',
            values={
                "id": task.id,
                "type": "task",
                "metadata":
                    {
                        "status":
                            {
                                "$exists": True
                            }
                    }
            },
            operator=entities.FiltersOperations.MATCH
        )

        if query is not None:
            and_list = query.get('filter', query).get('$and', None)
            if and_list is not None:
                and_list.append(source_filter.prepare())
            else:
                if 'filter' not in query:
                    query['filter'] = {}
                query['filter']['$and'] = [source_filter.prepare()]

        else:
            if filters is None and items is None:
                filters = entities.Filters()
            if filters:
                filters.and_filter_list.append(source_filter)

        return self.create(task_name='{}_qa'.format(task.name),
                           task_type='qa',
                           task_parent_id=task.id,
                           assignee_ids=assignee_ids,
                           workload=workload,
                           task_owner=task.creator,
                           project_id=task.project_id,
                           recipe_id=task.recipe_id,
                           due_date=due_date,
                           filters=filters,
                           items=items,
                           query=query,
                           metadata=metadata,
                           available_actions=available_actions,
                           wait=wait,
                           batch_size=batch_size,
                           max_batch_workload=max_batch_workload,
                           allowed_assignees=allowed_assignees,
                           priority=priority
                           )

    def _add_task_metadata_params(self, metadata, input_value, input_name):
        if input_value is not None and not isinstance(input_value, int):
            raise exceptions.PlatformException(error='400',
                                               message="{} must be a numbers".format(input_name))
        if input_value is not None:
            metadata['system'][input_name] = input_value
        return metadata

    @_api_reference.add(path='/annotationtasks', method='post')
    def create(self,
               task_name,
               due_date=None,
               assignee_ids=None,
               workload=None,
               dataset=None,
               task_owner=None,
               task_type='annotation',
               task_parent_id=None,
               project_id=None,
               recipe_id=None,
               assignments_ids=None,
               metadata=None,
               filters=None,
               items=None,
               query=None,
               available_actions=None,
               wait=True,
               check_if_exist: entities.Filters = False,
               limit=None,
               batch_size=None,
               max_batch_workload=None,
               allowed_assignees=None,
               priority=entities.TaskPriority.MEDIUM,
               consensus_task_type=None,
               consensus_percentage=None,
               consensus_assignees=None,
               scoring=True,
               enforce_video_conversion=True,
               ) -> entities.Task:
        """
        Create a new Task (Annotation or QA).

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        :param str task_name: the name of the task
        :param float due_date: date by which the task should be finished; for example, due_date=datetime.datetime(day=1, month=1, year=2029).timestamp()
        :param list assignee_ids: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
        :param List[WorkloadUnit] List[WorkloadUnit] workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param entities.Dataset dataset: dataset object, the dataset that refer to the task
        :param str task_owner: task owner. Provide user email
        :param str task_type: task type "annotation" or "qa"
        :param str task_parent_id: optional if type is qa - parent annotation task id
        :param str project_id: the Id of the project where task will be created
        :param str recipe_id: recipe id for the task
        :param list assignments_ids: assignments ids to the task
        :param dict metadata: metadata for the task
        :param entities.Filters filters: dl.Filters entity to filter items for the task
        :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
        :param dict DQL query: filter items for the task
        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "completed" and "discard"
        :param bool wait: wait until create task finish
        :param entities.Filters check_if_exist: dl.Filters check if task exist according to filter
        :param int limit: the limit items that the task can include
        :param int  batch_size: Pulling batch size (items), use with pulling allocation method. Restrictions - Min 3, max 100
        :param int max_batch_workload: max_batch_workload: Max items in assignment, use with pulling allocation method. Restrictions - Min batchSize + 2, max batchSize * 2
        :param list allowed_assignees: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
        :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
        :param entities.ConsensusTaskType consensus_task_type: consensus_task_type of the task options in entities.ConsensusTaskType
        :param int consensus_percentage: percentage of items to be copied to multiple annotators (consensus items)
        :param int consensus_assignees: the number of different annotators per item (number of copies per item)
        :param bool scoring: create a scoring app in project
        :param bool enforce_video_conversion: Enforce WEBM conversion on video items for frame-accurate annotations. WEBM Conversion will be executed as a project service and incurs compute costs. Service compute resources can be set according to planned workload.
        :return: Task object
        :rtype: dtlpy.entities.task.Task

        **Example**:

        .. code-block:: python

            dataset.tasks.create(task= 'task_entity',
                                due_date = datetime.datetime(day= 1, month= 1, year= 2029).timestamp(),
                                assignee_ids =[ 'annotator1@dataloop.ai', 'annotator2@dataloop.ai'],
                                available_actions=[dl.ItemAction("discard"), dl.ItemAction("to-check")])
        """

        if dataset is None and self._dataset is None:
            raise exceptions.PlatformException('400', 'Please provide param dataset')
        if due_date is None:
            due_date = (datetime.datetime.now() + datetime.timedelta(days=7)).timestamp()
        if query is None:
            if filters is None and items is None:
                query = entities.Filters().prepare()
            elif filters is None:
                item_list = list()
                if isinstance(items, entities.PagedEntities):
                    for page in items:
                        for item in page:
                            item_list.append(item)
                elif isinstance(items, list):
                    item_list = items
                elif isinstance(items, entities.Item):
                    item_list.append(items)
                else:
                    raise exceptions.PlatformException('400', 'Unknown items type')
                query = entities.Filters(field='id',
                                         values=[item.id for item in item_list],
                                         operator=entities.FiltersOperations.IN,
                                         use_defaults=False).prepare()
            else:
                query = filters.prepare()

        if dataset is None:
            dataset = self._dataset

        if task_owner is None:
            task_owner = self._client_api.info()['user_email']

        if task_type not in ['annotation', 'qa']:
            raise ValueError('task_type must be one of: "annotation", "qa". got: {}'.format(task_type))

        if recipe_id is None:
            recipe_id = dataset.get_recipe_ids()[0]

        if project_id is None:
            if self._project_id is not None:
                project_id = self._project_id
            else:
                raise exceptions.PlatformException('400', 'Must provide a project id')

        if workload is None and assignee_ids is not None:
            workload = entities.Workload.generate(assignee_ids=assignee_ids)

        if assignments_ids is None:
            assignments_ids = list()

        payload = {'name': task_name,
                   'query': "{}".format(json.dumps(query).replace("'", '"')),
                   'taskOwner': task_owner,
                   'spec': {'type': task_type},
                   'datasetId': dataset.id,
                   'projectId': project_id,
                   'assignmentIds': assignments_ids,
                   'recipeId': recipe_id,
                   'dueDate': due_date * 1000,
                   'asynced': wait,
                   'priority': priority
                   }

        if check_if_exist:
            if check_if_exist.resource != entities.FiltersResource.TASK:
                raise exceptions.PlatformException(
                    '407', 'Filter resource for check_if_exist param must be {}, got {}'.format(
                        entities.FiltersResource.TASK, check_if_exist.resource
                    )
                )
            payload['checkIfExist'] = {'query': check_if_exist.prepare()}

        if workload:
            payload['workload'] = workload.to_json()

        if limit:
            payload['limit'] = limit

        if available_actions is not None:
            payload['availableActions'] = [action.to_json() for action in available_actions]

        if task_parent_id is not None:
            payload['spec']['parentTaskId'] = task_parent_id

        if not enforce_video_conversion:
            payload['disableWebm'] = not enforce_video_conversion

        is_pulling = any([batch_size, max_batch_workload])
        is_consensus = any([consensus_percentage, consensus_assignees, consensus_task_type])
        if is_pulling and is_consensus:
            raise exceptions.PlatformException(error='400',
                                               message="Consensus can not work as a pulling task")
        if any([is_pulling, is_consensus]):
            if metadata is None:
                metadata = {}
            if 'system' not in metadata:
                metadata['system'] = {}
            if allowed_assignees is not None or assignee_ids is not None:
                metadata['system']['allowedAssignees'] = allowed_assignees if allowed_assignees else assignee_ids
            if consensus_task_type is not None:
                metadata['system']['consensusTaskType'] = consensus_task_type
            metadata = self._add_task_metadata_params(metadata=metadata,
                                                      input_value=batch_size,
                                                      input_name='batchSize')
            metadata = self._add_task_metadata_params(metadata=metadata,
                                                      input_value=max_batch_workload,
                                                      input_name='maxBatchWorkload')
            metadata = self._add_task_metadata_params(metadata=metadata,
                                                      input_value=consensus_percentage,
                                                      input_name='consensusPercentage')
            metadata = self._add_task_metadata_params(metadata=metadata,
                                                      input_value=consensus_assignees,
                                                      input_name='consensusAssignees')
            metadata = self._add_task_metadata_params(metadata=metadata,
                                                      input_value=scoring,
                                                      input_name='scoring')

        if metadata is not None:
            payload['metadata'] = metadata

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=URL_PATH,
                                                         json_req=payload)
        if success:

            response_json = response.json()
            if check_if_exist is not None and 'name' in response_json:
                return entities.Task.from_json(
                    _json=response.json(),
                    client_api=self._client_api,
                    project=self._project,
                    dataset=self._dataset
                )

            command = entities.Command.from_json(_json=response_json,
                                                 client_api=self._client_api)
            if not wait:
                return command
            command = command.wait(timeout=0)
            if 'createTaskPayload' not in command.spec:
                raise exceptions.PlatformException(error='400',
                                                   message="createTaskPayload key is missing in command response: {}"
                                                   .format(response))
            task = self.get(task_id=command.spec['createdTaskId'])
        else:
            raise exceptions.PlatformException(response)

        assert isinstance(task, entities.Task)
        return task

    def __item_operations(self, dataset: entities.Dataset, op, task=None, task_id=None, filters=None, items=None):

        if task is None and task_id is None:
            raise exceptions.PlatformException('400', 'Must provide either task or task id')
        elif task_id is None:
            task_id = task.id

        try:
            if filters is None and items is None:
                raise exceptions.PlatformException('400', 'Must provide either filters or items list')

            if filters is None:
                filters = entities.Filters(field='id',
                                           values=[item.id for item in items],
                                           operator=entities.FiltersOperations.IN,
                                           use_defaults=False)

            if op == 'delete':
                if task is None:
                    task = self.get(task_id=task_id)
                assignment_ids = task.assignmentIds
                filters._ref_assignment = True
                filters._ref_assignment_id = assignment_ids

            filters._ref_task = True
            filters._ref_task_id = task_id
            filters._ref_op = op
            return dataset.items.update(filters=filters)
        finally:
            if filters is not None:
                filters._nullify_refs()

    @_api_reference.add(path='/annotationtasks/{id}/addToTask', method='post')
    def add_items(self,
                  task: entities.Task = None,
                  task_id=None,
                  filters: entities.Filters = None,
                  items=None,
                  assignee_ids=None,
                  query=None,
                  workload=None,
                  limit=None,
                  wait=True) -> entities.Task:
        """
        Add items to a Task.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        :param dtlpy.entities.task.Task task: task object
        :param str task_id: the Id of the task
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param list items: list of items (item Ids or objects) to add to the task
        :param list assignee_ids: list to assignee who works in the task
        :param dict query: query to filter the items for the task
        :param list workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
        :param int limit: the limit items that task can include
        :param bool wait: wait until add items will to finish
        :return: task entity
        :rtype: dtlpy.entities.task.Task

        **Example**:

        .. code-block:: python

            dataset.tasks.add_items(task= 'task_entity',
                                items = [items])
        """
        if filters is None and items is None and query is None:
            raise exceptions.PlatformException('400', 'Must provide either filters, query or items list')

        if task is None and task_id is None:
            raise exceptions.PlatformException('400', 'Must provide either task or task_id')

        if query is None:
            if filters is None:
                if not isinstance(items, list):
                    items = [items]
                filters = entities.Filters(field='id',
                                           values=[item.id for item in items],
                                           operator=entities.FiltersOperations.IN,
                                           use_defaults=False)
            query = filters.prepare()

        if workload is None and assignee_ids is not None:
            workload = entities.Workload.generate(assignee_ids=assignee_ids)

        if task_id is None:
            task_id = task.id

        payload = {
            "query": "{}".format(json.dumps(query).replace("'", '"')),
        }

        if workload is not None:
            payload["workload"] = workload.to_json()

        if limit is not None:
            payload['limit'] = limit

        payload['asynced'] = wait

        url = '{}/{}/addToTask'.format(URL_PATH, task_id)

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=payload)

        if success:
            command = entities.Command.from_json(_json=response.json(),
                                                 client_api=self._client_api)
            if not wait:
                return command
            backoff_factor = 2
            if command.type == 'BulkAddToTaskSetting':
                backoff_factor = 8
            command = command.wait(timeout=0, backoff_factor=backoff_factor)
            if task is None:
                task = self.get(task_id=task_id)
            if 'addToTaskPayload' not in command.spec:
                raise exceptions.PlatformException(error='400',
                                                   message="addToTaskPayload key is missing in command response: {}"
                                                   .format(response))
        else:
            raise exceptions.PlatformException(response)

        assert isinstance(task, entities.Task)
        return task

    # @_api_reference.add(path='/annotationtasks/{id}/removeFromTask', method='post')
    def remove_items(self,
                     task: entities.Task = None,
                     task_id=None,
                     filters: entities.Filters = None,
                     query=None,
                     items=None,
                     wait=True):
        """
        remove items from Task.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        :param dtlpy.entities.task.Task task: task object
        :param str task_id: the Id of the task
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :param dict query: query to filter the items use it
        :param list items: list of items to add to the task
        :param bool wait: wait until remove items finish
        :return: True if success and an error if failed
        :rtype: bool

        **Examples**:

        .. code-block:: python

            dataset.tasks.remove_items(task= 'task_entity',
                                        items = [items])

        """
        if filters is None and items is None and query is None:
            raise exceptions.PlatformException('400', 'Must provide either filters, query or items list')

        if task is None and task_id is None:
            raise exceptions.PlatformException('400', 'Must provide either task or task_id')

        if query is None:
            if filters is None:
                if not isinstance(items, list):
                    items = [items]
                filters = entities.Filters(field='id',
                                           values=[item.id for item in items],
                                           operator=entities.FiltersOperations.IN,
                                           use_defaults=False)
            query = filters.prepare()

        if task_id is None:
            task_id = task.id

        payload = {"query": "{}".format(json.dumps(query).replace("'", '"')), 'asynced': wait}

        url = '{}/{}/removeFromTask'.format(URL_PATH, task_id)

        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=payload)

        if success:
            command = entities.Command.from_json(_json=response.json(),
                                                 client_api=self._client_api)
            if not wait:
                return command
            command = command.wait(timeout=0)

            if 'removeFromTaskId' not in command.spec:
                raise exceptions.PlatformException(error='400',
                                                   message="removeFromTaskId key is missing in command response: {}"
                                                   .format(response))
        else:
            raise exceptions.PlatformException(response)
        return True

    def get_items(self,
                  task_id: str = None,
                  task_name: str = None,
                  dataset: entities.Dataset = None,
                  filters: entities.Filters = None,
                  get_consensus_items: bool = False,
                  task: entities.Task = None
                  ) -> entities.PagedEntities:
        """
        Get the task items to use in your code.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        If a filters param is provided, you will receive a PagedEntity output of the task items. If no filter is provided, you will receive a list of the items.

        :param str task_id: the id of the task
        :param str task_name: the name of the task
        :param bool get_consensus_items: get the items from the consensus assignment
        :param dtlpy.entities.Task task: task object
        :param dtlpy.entities.dataset.Dataset dataset: dataset object that refer to the task
        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: list of the items or PagedEntity output of items
        :rtype: list or dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            dataset.tasks.get_items(task_id= 'task_id')
        """
        if task is None and task_id is None and task_name is None:
            raise exceptions.PlatformException('400', 'Please provide either task_id or task_name')

        if task_id is None:
            if task is None:
                task = self.get(task_name=task_name)
            task_id = task.id

        if dataset is None and self._dataset is None:
            raise exceptions.PlatformException('400', 'Please provide a dataset entity')
        if dataset is None:
            dataset = self._dataset

        if filters is None:
            filters = entities.Filters(use_defaults=False)
        filters.add(field='metadata.system.refs.id', values=[task_id], operator=entities.FiltersOperations.IN)

        if not get_consensus_items:
            if task is None:
                task = self.get(task_id=task_id)
            if task.metadata.get('system', dict()).get('consensusAssignmentId', None):
                filters.add(
                    field='metadata.system.refs.id',
                    values=task.metadata['system']['consensusAssignmentId'],
                    operator=entities.FiltersOperations.NOT_EQUAL
                )

        return dataset.items.list(filters=filters)

    def set_status(self, status: str, operation: str, task_id: str, item_ids: List[str]):
        """
        Update an item status within a task.

        **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.

        :param str status: string the describes the status
        :param str operation: the status action need 'create' or 'delete'
        :param str task_id: the Id of the task
        :param list item_ids: List[str] id items ids
        :return: True if success
        :rtype: bool

        **Example**:

        .. code-block:: python

            dataset.tasks.set_status(task_id= 'task_id', status='complete', operation='create')
        """
        url = '/assignments/items/tasks/{task_id}/status'.format(task_id=task_id)
        payload = {
            'itemIds': item_ids,
            'statusPayload': {
                'operation': operation,
                'returnLastStatus': True,
                'status': status
            }
        }

        success, response = self._client_api.gen_request(
            req_type='post',
            path=url,
            json_req=payload
        )

        if not success:
            raise exceptions.PlatformException(response)
        if response.json() is not None:
            updated_items = set(response.json().keys())
            log_msg = 'Items status was updated successfully.'
            if len(updated_items) != len(item_ids):
                failed_items = set(item_ids).difference(updated_items)
                log_msg = '{success_count} out of TOTAL items were updated. The following items failed to update: {failed_items}'.format(
                    success_count=len(updated_items), failed_items=failed_items)
            logger.info(msg=log_msg)
        return True


================================================
File: dtlpy/repositories/times_series.py
================================================
import logging
import pandas as pd
import datetime
from dtlpy import entities, miscellaneous, exceptions
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class TimesSeries:
    """
    Time series Repository
    """

    def __init__(self, client_api: ApiClient, project: entities.Project = None):
        self._client_api = client_api
        self._project = project

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.times_series repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ############
    #  methods #
    ############
    def create(self, series_name) -> entities.TimeSeries:
        """
        Create a new time series

        :param str series_name: name
        :return: TimeSeries object
        """
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects/{}/timeSeries'.format(self.project.id),
                                                         json_req={'name': series_name})
        if success:
            ts = entities.TimeSeries.from_json(_json=response.json(),
                                               project=self.project)
        else:
            raise exceptions.PlatformException(response)
        assert isinstance(ts, entities.TimeSeries)
        return ts

    def list(self) -> miscellaneous.List[entities.TimeSeries]:
        """
        List all time series for project

        :return:
        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/projects/{}/timeSeries'.format(self.project.id))
        if success:
            tss = miscellaneous.List([entities.TimeSeries.from_json(_json=_json, project=self.project)
                                      for _json in response.json()])
        else:
            raise exceptions.PlatformException(response)
        return tss

    def get(self, series_name=None, series_id=None) -> entities.TimeSeries:
        """
        Get time series entity

        :param str series_name: by name
        :param str series_id: by id
        :return:
        """
        if series_id is not None:
            # get series
            success, response = self._client_api.gen_request(req_type='get',
                                                             path='/projects/{}/timeSeries/{}'.format(self.project.id,
                                                                                                      series_id))
            if success:
                ts = entities.TimeSeries.from_json(_json=response.json(),
                                                   project=self.project)
            else:
                raise exceptions.PlatformException(response)
            # verify input service name is same as the given id
            if series_name is not None and ts.name != series_name:
                logger.warning(
                    "Mismatch found in timeSeries.get: series_name is different then timeSeries.name:"
                    " {!r} != {!r}".format(
                        series_name,
                        ts.name))
        elif series_name is not None:
            tss = self.list()
            ts = [ts for ts in tss if ts.name == series_name]
            if not ts:
                # empty list
                raise exceptions.PlatformException(error='404',
                                                   message='Time Series not found. Name: {}'.format(series_name))
            elif len(ts) > 1:
                raise exceptions.PlatformException(error='400',
                                                   message='More than one Time Series with same name.')
            else:
                ts = ts[0]
        else:
            raise exceptions.PlatformException(error='400',
                                               message='Must choose by "series_id" or "series_name"')
        assert isinstance(ts, entities.TimeSeries)
        return ts

    def delete(self, series_id=None, series=None):
        """
        Delete a Time Series

        :param str series_id: optional - search by id
        :param series: optional - TimeSeries object
        :return: True
        :rtype: bool
        """
        if series_id is not None:
            pass
        elif series is not None and isinstance(series, entities.TimeSeries):
            series_id = series.id
        else:
            msg = 'Must choose by at least one of: "series_id", "series"'
            logger.error(msg)
            raise ValueError(msg)
        success, response = self._client_api.gen_request(req_type='delete',
                                                         path='/projects/{}/timeSeries/{}'.format(self.project.id,
                                                                                                  series_id))
        if not success:
            raise exceptions.PlatformException(response)
        logger.info('Time series id {} deleted successfully'.format(series_id))
        return True

    #########
    # Table #
    #########
    def delete_samples(self, series_id, filters):
        """
        Delete samples from table

        :param str series_id: time series id
        :param dtlpy.entities.filters.Filters filters: query to delete by
        :return:
        """
        filters = self._validate_query(query=filters)
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects/{}/timeSeries/{}/remove'.format(
                                                             self.project.id,
                                                             series_id),
                                                         json_req=filters)
        if not success:
            raise exceptions.PlatformException(response)
        return True

    @staticmethod
    def _validate_query(query):
        default_start_time = 0
        default_end_time = datetime.datetime.now().timestamp() * 1000

        if query is None:
            query = dict(startTime=default_start_time, endTime=default_end_time)
        else:
            query['startTime'] = query.get('startTime', default_start_time)
            query['endTime'] = query.get('endTime', default_end_time)

        return query

    def get_samples(self, series_id, filters=None) -> pd.DataFrame:
        """
        Get Series table

        :param str series_id: TimeSeries id
        :param dtlpy.entities.filters.Filters filters: match filters to get specific data from series
        :return:
        """
        filters = self._validate_query(query=filters)
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects/{}/timeSeries/{}/query'.format(self.project.id,
                                                                                                        series_id),
                                                         json_req=filters)
        if success:
            res = response.json()['samples']
            if isinstance(res, dict):
                df = pd.DataFrame([res])
            elif isinstance(res, list):
                df = pd.DataFrame(res)
            else:
                raise ValueError('unknown return type for time series: {}'.format(type(res)))
        else:
            raise exceptions.PlatformException(response)
        return df

    def add_samples(self, series_id, data):
        """
        Add samples to series

        :param str series_id: TimeSeries id
        :param data: list or dictionary of samples
        :return:
        """
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects/{}/timeSeries/{}/add'.format(self.project.id,
                                                                                                      series_id),
                                                         json_req=data)

        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    ######################
    # Samples Operations #
    ######################
    def get_sample(self, series_id, sample_id) -> pd.DataFrame:
        """
        Get single sample from series

        :param str series_id: TimeSeries id
        :param str sample_id: id of sample line
        :return:
        """
        success, response = self._client_api.gen_request(req_type='get',
                                                         path='/projects/{}/timeSeries/{}/samples/{}'.format(
                                                             self.project.id,
                                                             series_id,
                                                             sample_id))
        if success:
            res = response.json()
            if isinstance(res, dict):
                df = pd.DataFrame([res])
            elif isinstance(res, list):
                df = pd.DataFrame(res)
            else:
                raise ValueError('unknown return type for time series: {}'.format(type(res)))
        else:
            raise exceptions.PlatformException(response)
        return df

    def update_sample(self, series_id, sample_id, data):
        """
        Add data to existing sample

        :param str series_id: time series id
        :param str sample_id: sample line id
        :param dict data: dictionary
        :return:
        """
        success, response = self._client_api.gen_request(req_type='post',
                                                         path='/projects/{}/timeSeries/{}/samples/{}'.format(
                                                             self.project.id,
                                                             series_id,
                                                             sample_id),
                                                         json_req=data)
        if not success:
            raise exceptions.PlatformException(response)

    def delete_sample(self, series_id, sample_id):
        """
        Delete single samples form time series

        :param str series_id:
        :param str sample_id:
        :return:
        """
        success, response = self._client_api.gen_request(req_type='delete',
                                                         path='/projects/{}/timeSeries/{}/samples/{}'.format(
                                                             self.project.id,
                                                             series_id,
                                                             sample_id))
        if not success:
            raise exceptions.PlatformException(response)
        return True


================================================
File: dtlpy/repositories/triggers.py
================================================
import logging

from .. import entities, miscellaneous, exceptions, repositories, _api_reference
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Triggers:
    """
    Triggers Repository

    The Triggers class allows users to manage triggers and their properties. Triggers activate services.
    See our documentation for more information on `triggers <https://developers.dataloop.ai/tutorials/faas/concept/chapter/>`_.
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None,
                 service: entities.Service = None,
                 project_id: str = None,
                 pipeline: entities.Pipeline = None):
        self._client_api = client_api
        self._project = project
        self._service = service
        self._pipeline = pipeline
        if project_id is None:
            if self._project is not None:
                project_id = self._project.id
            elif self._service is not None:
                project_id = self._service.project_id

        self._project_id = project_id

    ############
    # entities #
    ############
    @property
    def service(self) -> entities.Service:
        if self._service is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "service". need to set a Service entity or use service.triggers repository')
        assert isinstance(self._service, entities.Service)
        return self._service

    @service.setter
    def service(self, service: entities.Service):
        if not isinstance(service, entities.Service):
            raise ValueError('Must input a valid Service entity')
        self._service = service

    @property
    def pipeline(self) -> entities.Pipeline:
        if self._pipeline is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "pipeline". need to set a Pipeline entity or use pipeline.triggers repository')
        assert isinstance(self._pipeline, entities.Pipeline)
        return self._pipeline

    @pipeline.setter
    def pipeline(self, pipeline: entities.Pipeline):
        if not isinstance(pipeline, entities.Pipeline):
            raise ValueError('Must input a valid Service entity')
        self._pipeline = pipeline

    @property
    def project(self) -> entities.Project:
        if self._project is None:
            if self._service is not None:
                self._project = self._service._project
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.triggers repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    def name_validation(self, name: str):
        """
        This method validates the trigger name. If name is not valid, this method will return an error. Otherwise, it will not return anything.

        :param str name: trigger name
        """
        url = '/piper-misc/naming/triggers/{}'.format(name)

        # request
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)
        if not success:
            raise exceptions.PlatformException(response)

    @_api_reference.add(path='/triggers', method='post')
    def create(self,
               # for both trigger types
               service_id: str = None,
               trigger_type: entities.TriggerType = entities.TriggerType.EVENT,
               name: str = None,
               webhook_id=None,
               function_name=entities.package_defaults.DEFAULT_PACKAGE_FUNCTION_NAME,
               project_id=None,
               active=True,
               # for event trigger
               filters=None,
               resource: entities.TriggerResource = entities.TriggerResource.ITEM,
               actions: entities.TriggerAction = None,
               execution_mode: entities.TriggerExecutionMode = entities.TriggerExecutionMode.ONCE,
               # for cron triggers
               start_at=None,
               end_at=None,
               inputs=None,
               cron=None,
               pipeline_id=None,
               pipeline=None,
               pipeline_node_id=None,
               root_node_namespace=None,
               **kwargs) -> entities.BaseTrigger:
        """
        Create a Trigger. Can create two types: a cron trigger or an event trigger.
        Inputs are different for each type

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        Inputs for all types:

        :param str service_id: Id of services to be triggered
        :param str trigger_type: can be cron or event. use enum dl.TriggerType for the full list
        :param str name: name of the trigger
        :param str webhook_id: id for webhook to be called
        :param str  function_name: the function name to be called when triggered (must be defined in the package)
        :param str  project_id: project id where trigger will work
        :param bool active: optional - True/False, default = True, if true trigger is active

        Inputs for event trigger:
        :param dtlpy.entities.filters.Filters filters: optional - Item/Annotation metadata filters, default = none
        :param str resource: optional - Dataset/Item/Annotation/ItemStatus, default = Item
        :param str actions: optional - Created/Updated/Deleted, default = create
        :param str execution_mode: how many times trigger should be activated; default is "Once". enum dl.TriggerExecutionMode

        Inputs for cron trigger:
        :param start_at: iso format date string to start activating the cron trigger
        :param end_at: iso format date string to end the cron activation
        :param inputs: dictionary "name":"val" of inputs to the function
        :param str cron: cron spec specifying when it should run. more information: https://en.wikipedia.org/wiki/Cron
        :param str pipeline_id: Id of pipeline to be triggered
        :param pipeline: pipeline entity to be triggered
        :param str pipeline_node_id: Id of pipeline root node to be triggered
        :param root_node_namespace: namespace of pipeline root node to be triggered

        :return: Trigger entity
        :rtype: dtlpy.entities.trigger.Trigger

        **Example**:

        .. code-block:: python

            service.triggers.create(name='triggername',
                                  execution_mode=dl.TriggerExecutionMode.ONCE,
                                  resource='Item',
                                  actions='Created',
                                  function_name='run',
                                  filters={'$and': [{'hidden': False},
                                                    {'type': 'file'}]}
                                  )
        """
        scope = kwargs.get('scope', None)

        if service_id is None and webhook_id is None and pipeline_id is None and pipeline is None:
            if self._service is not None:
                service_id = self._service.id
            elif self._pipeline is not None:
                pipeline = self._pipeline
                pipeline_id = self._pipeline.id

        if pipeline is not None:
            pipeline_id = pipeline.id

        # type
        input_num = sum(input_id is not None for input_id in [service_id, webhook_id, pipeline_id])
        if input_num != 1:
            raise exceptions.PlatformException('400',
                                               'Must provide only one of service id, webhook id, pipeline id or pipeline')

        if pipeline_id is not None:
            if pipeline is None:
                pipeline = repositories.Pipelines(client_api=self._client_api).get(pipeline_id=pipeline_id)
            if pipeline_node_id is None:
                if pipeline.start_nodes:
                    for pipe_node in pipeline.start_nodes:
                        if pipe_node['type'] == 'root':
                            pipeline_node_id = pipe_node['nodeId']
            if pipeline_node_id is None:
                raise exceptions.PlatformException('400', 'Must provide pipeline node id')
            if not actions:
                actions = [entities.TriggerAction.CREATED]
            pipeline.nodes.get(node_id=pipeline_node_id).add_trigger(
                trigger_type=trigger_type,
                filters=filters,
                resource=resource,
                actions=actions,
                execution_mode=execution_mode,
                cron=cron,
            )
            logger.info("The trigger will not create until pipeline is install")
            pipeline.update()
            return True
        else:
            if name is None:
                if self._service is not None:
                    name = self._service.name
                else:
                    name = 'defaulttrigger'

            if filters is None:
                filters = dict()
            elif isinstance(filters, entities.Filters):
                filters = filters.prepare(query_only=True).get('filter', dict())

            if webhook_id is not None:
                operation = {
                    'type': 'webhook',
                    'webhookId': webhook_id
                }
            else:
                operation = {
                    'type': 'function',
                    'serviceId': service_id,
                    'functionName': function_name

                }

            if actions is not None:
                if not isinstance(actions, list):
                    actions = [actions]
            else:
                actions = [entities.TriggerAction.CREATED]

            if len(actions) == 0:
                actions = [entities.TriggerAction.CREATED]

            if trigger_type == entities.TriggerType.EVENT:
                spec = {
                    'filter': filters,
                    'resource': resource,
                    'executionMode': execution_mode,
                    'actions': actions
                }
            elif trigger_type == entities.TriggerType.CRON:
                spec = {
                    'endAt': end_at,
                    'startAt': start_at,
                    'cron': cron,
                }
            else:
                raise ValueError('Unknown trigger type: "{}". Use dl.TriggerType for known types'.format(trigger_type))

            spec['input'] = dict() if inputs is None else inputs
            spec['operation'] = operation

            # payload
            if self._project_id is None and project_id is None:
                raise exceptions.PlatformException('400', 'Please provide a project id')
            elif project_id is None:
                project_id = self._project_id

            payload = {
                'type': trigger_type,
                'active': active,
                'projectId': project_id,
                'name': name,
                'spec': spec
            }

            if scope is not None:
                logger.warning(
                    "Only superuser is allowed to define a trigger's scope. "
                    "If you are not a superuser you will not be able to perform this action")
                payload['scope'] = scope

            # request
            success, response = self._client_api.gen_request(req_type='post',
                                                             path='/triggers',
                                                             json_req=payload)

            # exception handling
            if not success:
                raise exceptions.PlatformException(response)

            # return entity
            return entities.BaseTrigger.from_json(_json=response.json(),
                                                  client_api=self._client_api,
                                                  project=self._project if self._project_id == project_id else None,
                                                  service=self._service)

    @_api_reference.add(path='/triggers/{id}', method='get')
    def get(self, trigger_id=None, trigger_name=None) -> entities.BaseTrigger:
        """
        Get Trigger object

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str trigger_id: trigger id
        :param str  trigger_name: trigger name
        :return: Trigger entity
        :rtype: dtlpy.entities.trigger.Trigger

        **Example**:

        .. code-block:: python

            service.triggers.get(trigger_id='trigger_id')
        """
        # request
        if trigger_id is not None:
            success, response = self._client_api.gen_request(
                req_type="get",
                path="/triggers/{}".format(trigger_id)
            )

            # exception handling
            if not success:
                raise exceptions.PlatformException(response)

            # return entity
            trigger = entities.BaseTrigger.from_json(client_api=self._client_api,
                                                     _json=response.json(),
                                                     project=self._project,
                                                     service=self._service)
            # verify input trigger name is same as the given id
            if trigger_name is not None and trigger.name != trigger_name:
                logger.warning(
                    "Mismatch found in triggers.get: trigger_name is different then trigger.name:"
                    " {!r} != {!r}".format(
                        trigger_name,
                        trigger.name))
        else:
            if trigger_name is None:
                raise exceptions.PlatformException('400', 'Must provide either trigger name or trigger id')
            else:
                filters = self.__generate_default_filter()
                filters.add(field='name', values=trigger_name)
                triggers = self.list(filters)
                if triggers.items_count == 0:
                    raise exceptions.PlatformException('404', 'Trigger not found')
                elif triggers.items_count == 1:
                    trigger = triggers.items[0]
                else:
                    raise exceptions.PlatformException('404',
                                                       'More than one trigger by name {} exist'.format(trigger_name))

        return trigger

    @_api_reference.add(path='/triggers/{id}', method='delete')
    def delete(self, trigger_id=None, trigger_name=None):
        """
        Delete Trigger object

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param str trigger_id: trigger id
        :param str trigger_name: trigger name
        :return: True is successful error if not
        :rtype: bool

        **Example**:

        .. code-block:: python

            service.triggers.delete(trigger_id='trigger_id')
        """
        if trigger_id is None:
            if trigger_name is None:
                raise exceptions.PlatformException('400', 'Must provide either trigger name or trigger id')
            else:
                trigger_id = self.get(trigger_name=trigger_name).id
        # request
        success, response = self._client_api.gen_request(
            req_type="delete",
            path="/triggers/{}".format(trigger_id)
        )
        # exception handling
        if not success:
            raise exceptions.PlatformException(response)
        return True

    @_api_reference.add(path='/triggers/{id}', method='patch')
    def update(self, trigger: entities.BaseTrigger) -> entities.BaseTrigger:
        """
        Update trigger

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.trigger.Trigger trigger: Trigger entity
        :return: Trigger entity
        :rtype: dtlpy.entities.trigger.Trigger

        **Example**:

        .. code-block:: python

            service.triggers.update(trigger='trigger_entity')
        """
        # payload
        payload = trigger.to_json()

        # request
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path='/triggers/{}'.format(trigger.id),
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.BaseTrigger.from_json(_json=response.json(),
                                              client_api=self._client_api,
                                              project=self._project,
                                              service=self._service)

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.BaseTrigger]:
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return triggers list
        for i_trigger, trigger in enumerate(response_items):
            jobs[i_trigger] = pool.submit(entities.BaseTrigger._protected_from_json,
                                          **{'client_api': self._client_api,
                                             '_json': trigger,
                                             'project': self._project,
                                             'service': self._service})

        # get all results
        results = [j.result() for j in jobs]
        # log errors
        _ = [logger.warning(r[1]) for r in results if r[0] is False]
        # return good jobs
        triggers = miscellaneous.List([r[1] for r in results if r[0] is True])
        return triggers

    def _list(self, filters: entities.Filters):
        """
        List project triggers
        :return:
        """
        url = '/query/faas'

        success, response = self._client_api.gen_request(req_type='POST',
                                                         path=url,
                                                         json_req=filters.prepare())
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()

    def __generate_default_filter(self):
        filters = entities.Filters(resource=entities.FiltersResource.TRIGGER)
        if self._project is not None:
            filters.add(field='projectId', values=self._project.id)
        if self._service is not None:
            filters.add(field='spec.operation.serviceId', values=self._service.id)
        if self._pipeline is not None:
            filters.add(field='spec.operation.id', values=self._pipeline.id)

        return filters

    @_api_reference.add(path='/query/faas', method='post')
    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List triggers of a project, package, or service.

        **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities

        **Example**:

        .. code-block:: python

            service.triggers.list()
        """
        if filters is None:
            filters = self.__generate_default_filter()
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))

        if filters.resource != entities.FiltersResource.TRIGGER:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.TRIGGER. Got: {!r}'.format(filters.resource))

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def resource_information(self, resource, resource_type, action='Created'):
        """
        Returns which function should run on an item (based on global triggers).

        **Prerequisites**: You must be a **superuser** to run this method.

        :param resource: 'Item' / 'Dataset' / etc
        :param resource_type: dictionary of the resource object
        :param action: 'Created' / 'Updated' / etc.

        **Example**:

        .. code-block:: python

            service.triggers.resource_information(resource='Item', resource_type=item_object, action='Created')
        """
        url = '/trigger-resource-information'

        payload = {'resource': resource_type,
                   'entity': resource.to_json(),
                   'action': action}
        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=url,
                                                         json_req=payload)
        if not success:
            raise exceptions.PlatformException(response)
        return response.json()


================================================
File: dtlpy/repositories/upload_element.py
================================================
import os
import validators
from dtlpy import PlatformException, entities


class BaseUploadElement:

    def __init__(self, all_upload_elements: dict):
        self.upload_item_element = all_upload_elements['upload_item_element']
        self.total_size = all_upload_elements['total_size']
        self.remote_name = all_upload_elements['remote_name']
        self.remote_path = all_upload_elements['remote_path']
        self.upload_annotations_element = all_upload_elements['upload_annotations_element']
        self.item_metadata = all_upload_elements['item_metadata']
        self.with_head_folder = all_upload_elements['with_head_folder']
        self.type = None
        self.buffer = None
        self.remote_filepath = None
        self.exists_in_remote = False
        self.checked_in_remote = False
        self.annotations_filepath = all_upload_elements['annotations_filepath']
        self.link_dataset_id = None
        self.root = all_upload_elements['root']
        self.filename = all_upload_elements['filename']
        self.export_version = all_upload_elements['export_version']
        self.item_description = all_upload_elements['item_description']
        self.driver_path = all_upload_elements['driver_path']


class BinaryUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        # binary element
        if not hasattr(self.upload_item_element, "name") and self.remote_name is None:
            raise PlatformException(
                error="400",
                message='upload element type was bytes. Must put attribute "name" on buffer (with file name) '
                        'when uploading buffers or providing param "remote_name"')
        if self.remote_name is None:
            self.remote_name = self.upload_item_element.name
        remote_filepath = self.remote_path + self.remote_name
        self.type = 'buffer'
        self.buffer = self.upload_item_element
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


class DirUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        filepath = os.path.join(self.root, self.filename)  # get full image filepath
        # extract item's size
        self.total_size += os.path.getsize(filepath)
        # get annotations file
        if self.upload_annotations_element is not None:
            # change path to annotations
            annotations_filepath = filepath.replace(self.upload_item_element,
                                                    self.upload_annotations_element)
            # remove image extension
            if self.export_version == entities.ExportVersion.V1:
                annotations_filepath, _ = os.path.splitext(annotations_filepath)
            # add json extension
            annotations_filepath += ".json"
        else:
            annotations_filepath = None
        if self.with_head_folder:
            remote_filepath = self.remote_path + os.path.relpath(filepath, os.path.dirname(
                self.upload_item_element))
        else:
            remote_filepath = self.remote_path + os.path.relpath(filepath, self.upload_item_element)

        self.type = 'file'
        self.buffer = filepath
        self.remote_filepath = remote_filepath.replace("\\", "/")
        self.annotations_filepath = annotations_filepath


class ExternalItemUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)
        # self.upload_item_element = self.upload_item_element.split('//')[1]
        filepath = self.upload_item_element
        # extract item's size
        self.total_size += all_upload_elements['total_size']
        # determine item's remote base name
        if self.remote_name is None:
            self.remote_name = os.path.basename(filepath)
        # get annotations file
        if self.upload_annotations_element is not None:
            # change path to annotations
            annotations_filepath = filepath.replace(self.upload_item_element, self.upload_annotations_element)
            # remove image extension
            if self.export_version == entities.ExportVersion.V1:
                annotations_filepath, _ = os.path.splitext(annotations_filepath)
            # add json extension
            annotations_filepath += ".json"
        else:
            annotations_filepath = None
        # append to list
        if self.remote_path is None:
            split_dir = os.path.dirname(filepath).split('//')
            if len(split_dir) > 1:
                split_dir = split_dir[1] + '/'
            if self.driver_path == '/':
                self.driver_path = None
            if self.driver_path and split_dir.startswith(self.driver_path):
                self.remote_path = split_dir.replace(self.driver_path, '')
            else:
                self.remote_path = split_dir
        remote_filepath = self.remote_path + self.remote_name
        self.type = 'external_file'
        self.buffer = filepath
        self.remote_filepath = remote_filepath
        self.annotations_filepath = annotations_filepath


class FileUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        filepath = self.upload_item_element
        # extract item's size
        self.total_size += os.path.getsize(filepath)
        # determine item's remote base name
        if self.remote_name is None:
            self.remote_name = os.path.basename(filepath)
        # get annotations file
        if self.upload_annotations_element is not None:
            # change path to annotations
            annotations_filepath = filepath.replace(self.upload_item_element, self.upload_annotations_element)
            # remove image extension
            if self.export_version == entities.ExportVersion.V1:
                annotations_filepath, _ = os.path.splitext(annotations_filepath)
            # add json extension
            annotations_filepath += ".json"
        else:
            annotations_filepath = None
        # append to list
        remote_filepath = self.remote_path + self.remote_name
        self.type = 'file'
        self.buffer = filepath
        self.remote_filepath = remote_filepath
        self.annotations_filepath = annotations_filepath


class ItemLinkUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        link = entities.Link(ref=self.upload_item_element.id, type='id', dataset_id=self.upload_item_element.dataset_id,
                             name='{}_link.json'.format(self.upload_item_element.name))
        if self.remote_name is None:
            self.remote_name = link.name

        remote_filepath = '{}{}'.format(self.remote_path, self.remote_name)
        self.type = 'link'
        self.buffer = link
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


class LinkUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        if self.remote_name is None:
            self.remote_name = self.upload_item_element.name

        remote_filepath = '{}{}_link.json'.format(self.remote_path, self.remote_name)
        self.type = 'link'
        self.buffer = self.upload_item_element
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


class MultiViewUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        if self.remote_name:
            self.upload_item_element.name = self.remote_name

        if not self.upload_item_element.name.endswith('.json'):
            self.upload_item_element.name = '{}.json'.format(self.upload_item_element.name)

        remote_filepath = '{}{}'.format(self.remote_path, self.upload_item_element.name)
        self.type = 'collection'
        self.buffer = self.upload_item_element.to_bytes_io()
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


class SimilarityUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        if self.remote_name:
            self.upload_item_element.name = self.remote_name
        if self.upload_item_element.name is None:
            self.upload_item_element.name = '{}_similarity.json'.format(self.upload_item_element.ref)

        if not self.upload_item_element.name.endswith('.json'):
            remote_filepath = '{}{}.json'.format(self.remote_path, self.upload_item_element.name)
        else:
            remote_filepath = '{}{}'.format(self.remote_path, self.upload_item_element.name)

        self.type = 'collection'
        self.buffer = self.upload_item_element.to_bytes_io()
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


class PromptUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        if self.remote_name:
            self.upload_item_element.name = self.remote_name
        if self.upload_item_element.name is None:
            self.upload_item_element.name = '{}_prompt.json'.format(self.upload_item_element.ref)

        if not self.upload_item_element.name.endswith('.json'):
            remote_filepath = '{}{}.json'.format(self.remote_path, self.upload_item_element.name)
        else:
            remote_filepath = '{}{}'.format(self.remote_path, self.upload_item_element.name)

        self.type = 'prompt'
        self.buffer = self.upload_item_element.to_bytes_io()
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


class UrlUploadElement(BaseUploadElement):

    def __init__(self, all_upload_elements: dict):
        super().__init__(all_upload_elements)

        # noinspection PyTypeChecker
        if self.remote_name is None:
            self.remote_name = str(self.upload_item_element.split('/')[-1]).split('?')[0]

        remote_filepath = self.remote_path + self.remote_name
        self.type = 'url'
        self.buffer = self.upload_item_element
        self.remote_filepath = remote_filepath
        self.annotations_filepath = self.upload_annotations_element


================================================
File: dtlpy/repositories/uploader.py
================================================
import sys
from collections import deque
import validators
import traceback
import tempfile
import requests
import asyncio
import logging
import pandas
import shutil
import json
import time
import tqdm
import os
import io
import numpy as np
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
from PIL import Image

from . import upload_element

from .. import PlatformException, entities, repositories, exceptions
from ..services import Reporter

logger = logging.getLogger(name='dtlpy')

NUM_TRIES = 5  # try to upload 3 time before fail on item


class Uploader:
    def __init__(self, items_repository: repositories.Items, output_entity=entities.Item, no_output=False):
        assert isinstance(items_repository, repositories.Items)
        self.items_repository = items_repository
        self.remote_url = "/datasets/{}/items".format(self.items_repository.dataset.id)
        self.__stop_create_existence_dict = False
        self.mode = 'skip'
        self.num_files = 0
        self.i_item = 0
        self.pbar = tqdm.tqdm(total=0,
                              disable=self.items_repository._client_api.verbose.disable_progress_bar_upload_items,
                              file=sys.stdout, desc='Upload Items')
        self.reporter = Reporter(num_workers=0,
                                 resource=Reporter.ITEMS_UPLOAD,
                                 print_error_logs=items_repository._client_api.verbose.print_error_logs,
                                 output_entity=output_entity,
                                 client_api=items_repository._client_api,
                                 no_output=no_output)

    def upload(
            self,
            # what to upload
            local_path,
            local_annotations_path=None,
            # upload options
            remote_path=None,
            remote_name=None,
            file_types=None,
            overwrite=False,
            item_metadata=None,
            export_version: str = entities.ExportVersion.V1,
            item_description=None,
            raise_on_error=False,
            return_as_list=False
    ):
        """
        Upload local file to dataset.
        Local filesystem will remain.
        If `*` at the end of local_path (e.g. '/images/*') items will be uploaded without head directory

        :param local_path: local file or folder to upload
        :param local_annotations_path: path to Dataloop format annotations json files.
        :param remote_path: remote path to save.
        :param remote_name: remote base name to save.
        :param file_types: list of file type to upload. e.g ['.jpg', '.png']. default is all
        :param overwrite: optional - default = False
        :param item_metadata: upload the items with the metadata dictionary
        :param str export_version:  exported items will have original extension in filename, `V1` - no original extension in filenames
        :param str item_description: add a string description to the uploaded item
        :param bool raise_on_error: raise an exception if an error occurs
        :param bool return_as_list: always return a list of items 

        :return: Output (list)
        """
        ###################
        # Default options #
        ###################
        if overwrite:
            self.mode = 'overwrite'
        if isinstance(local_path, pandas.DataFrame):
            futures = self._build_elements_from_df(local_path)
        else:
            futures = self._build_elements_from_inputs(local_path=local_path,
                                                       local_annotations_path=local_annotations_path,
                                                       # upload options
                                                       remote_path=remote_path,
                                                       remote_name=remote_name,
                                                       file_types=file_types,
                                                       item_metadata=item_metadata,
                                                       export_version=export_version,
                                                       item_description=item_description)
        num_files = len(futures)
        while futures:
            futures.popleft().result()
        logger.info("Uploading {} items..".format(num_files))
        self.pbar.close()
        # summary
        logger.info("Number of total files: {}".format(num_files))
        status_list = self.reporter.status_list
        for action in set(status_list):
            n_for_action = self.reporter.status_count(status=action)
            logger.info("Number of files {}: {}".format(action, n_for_action))

        # log error
        errors_count = self.reporter.failure_count
        if errors_count > 0:
            log_filepath = self.reporter.generate_log_files()
            if log_filepath is not None:
                logger.warning("Errors in {n_error} files. See {log_filepath} for full log".format(
                    n_error=errors_count, log_filepath=log_filepath))
            if raise_on_error is True:
                raise PlatformException(error="400",
                                        message=f"Errors in {errors_count} files. See above trace for more information")
        
        if return_as_list is True:
            # return list of items
            return list(self.reporter.output)
        if len(status_list) == 1:
            # if there is only one item, return it
            try:
                return next(self.reporter.output)
            except StopIteration:
                # if there is no items, return None
                return None
        # if there are multiple items, return the generator
        return self.reporter.output

    def _build_elements_from_inputs(self,
                                    local_path,
                                    local_annotations_path,
                                    # upload options
                                    remote_path,
                                    file_types,
                                    remote_name,
                                    item_metadata,
                                    export_version: str = entities.ExportVersion.V1,
                                    item_description=None):
        # fix remote path
        if remote_path is None:
            if isinstance(local_path, str) and local_path.startswith('external://'):
                remote_path = None
            else:
                remote_path = "/"
        if remote_path and not remote_path.startswith('/'):
            remote_path = f"/{remote_path}"
        if remote_path and not remote_path.endswith("/"):
            remote_path = f"{remote_path}/"
        if file_types is not None and not isinstance(file_types, list):
            msg = '"file_types" should be a list of file extension. e.g [".jpg", ".png"]'
            raise PlatformException(error="400", message=msg)
        if item_metadata is not None:
            if not isinstance(item_metadata, dict) and not isinstance(item_metadata, entities.ExportMetadata):
                msg = '"item_metadata" should be a metadata dictionary. Got type: {}'.format(type(item_metadata))
                raise PlatformException(error="400", message=msg)
        if item_description is not None:
            if not isinstance(item_description, str):
                msg = '"item_description" should be a string. Got type: {}'.format(type(item_description))
                raise PlatformException(error="400", message=msg)

        ##########################
        # Convert inputs to list #
        ##########################
        local_annotations_path_list = None
        remote_name_list = None
        if not isinstance(local_path, list):
            local_path_list = [local_path]
            if remote_name is not None:
                if not isinstance(remote_name, str):
                    raise PlatformException(error="400",
                                            message='remote_name must be a string, got: {}'.format(type(remote_name)))
                remote_name_list = [remote_name]
            if local_annotations_path is not None:
                if not isinstance(local_annotations_path, str):
                    raise PlatformException(error="400",
                                            message='local_annotations_path must be a string, got: {}'.format(
                                                type(local_annotations_path)))
                local_annotations_path_list = [local_annotations_path]
        else:
            local_path_list = local_path
            if remote_name is not None:
                if not isinstance(remote_name, list):
                    raise PlatformException(error="400",
                                            message='remote_name must be a list, got: {}'.format(type(remote_name)))
                if not len(remote_name) == len(local_path_list):
                    raise PlatformException(error="400",
                                            message='remote_name and local_path_list must be of same length. '
                                                    'Received: remote_name: {}, '
                                                    'local_path_list: {}'.format(len(remote_name),
                                                                                 len(local_path_list)))
                remote_name_list = remote_name
            if local_annotations_path is not None:
                if not len(local_annotations_path) == len(local_path_list):
                    raise PlatformException(error="400",
                                            message='local_annotations_path and local_path_list must be of same lenght.'
                                                    ' Received: local_annotations_path: {}, '
                                                    'local_path_list: {}'.format(len(local_annotations_path),
                                                                                 len(local_path_list)))
                local_annotations_path_list = local_annotations_path

        if local_annotations_path is None:
            local_annotations_path_list = [None] * len(local_path_list)

        if remote_name is None:
            remote_name_list = [None] * len(local_path_list)

        try:
            driver_path = self.items_repository.dataset.project.drivers.get(
                driver_id=self.items_repository.dataset.driver).path
        except Exception:
            driver_path = None

        futures = deque()
        total_size = 0
        for upload_item_element, remote_name, upload_annotations_element in zip(local_path_list,
                                                                                remote_name_list,
                                                                                local_annotations_path_list):
            if isinstance(upload_item_element, np.ndarray):
                # convert numpy.ndarray to io.BytesI
                if remote_name is None:
                    raise PlatformException(
                        error="400",
                        message='Upload element type was numpy.ndarray. providing param "remote_name" is mandatory')
                file_extension = os.path.splitext(remote_name)
                if file_extension[1].lower() in ['.jpg', '.jpeg']:
                    item_format = 'JPEG'
                elif file_extension[1].lower() == '.png':
                    item_format = 'PNG'
                else:
                    raise PlatformException(
                        error="400",
                        message='"remote_name" with  .jpg/.jpeg or .png extension are supported '
                                'when upload element of numpy.ndarray type.')

                buffer = io.BytesIO()
                Image.fromarray(upload_item_element).save(buffer, format=item_format)
                buffer.seek(0)
                buffer.name = remote_name
                upload_item_element = buffer

            all_upload_elements = {
                'upload_item_element': upload_item_element,
                'total_size': total_size,
                'remote_name': remote_name,
                'remote_path': remote_path,
                'upload_annotations_element': upload_annotations_element,
                'item_metadata': item_metadata,
                'annotations_filepath': None,
                'with_head_folder': None,
                'filename': None,
                'root': None,
                'export_version': export_version,
                'item_description': item_description,
                'driver_path': driver_path
            }
            if isinstance(upload_item_element, str):
                with_head_folder = True
                if upload_item_element.endswith('*'):
                    with_head_folder = False
                    upload_item_element = os.path.dirname(upload_item_element)
                    all_upload_elements['upload_item_element'] = upload_item_element

                if os.path.isdir(upload_item_element):
                    for root, subdirs, files in os.walk(upload_item_element):
                        for filename in files:
                            all_upload_elements['with_head_folder'] = with_head_folder
                            all_upload_elements['filename'] = filename
                            all_upload_elements['root'] = root
                            _, ext = os.path.splitext(filename)
                            if file_types is None or ext in file_types:
                                upload_elem = upload_element.DirUploadElement(all_upload_elements=all_upload_elements)
                                futures.append(self.upload_single_element(upload_elem))
                    continue

                # add single file
                elif os.path.isfile(upload_item_element):
                    upload_elem = upload_element.FileUploadElement(all_upload_elements=all_upload_elements)

                elif upload_item_element.startswith('external://'):
                    upload_elem = upload_element.ExternalItemUploadElement(all_upload_elements=all_upload_elements)

                elif self.is_url(upload_item_element):
                    upload_elem = upload_element.UrlUploadElement(all_upload_elements=all_upload_elements)

                else:
                    raise PlatformException("404", "Unknown local path: {}".format(local_path))

            elif isinstance(upload_item_element, entities.Item):
                upload_elem = upload_element.ItemLinkUploadElement(all_upload_elements=all_upload_elements)

            elif isinstance(upload_item_element, entities.Link):
                upload_elem = upload_element.LinkUploadElement(all_upload_elements=all_upload_elements)

            elif isinstance(upload_item_element, entities.PromptItem):
                upload_elem = upload_element.PromptUploadElement(all_upload_elements=all_upload_elements)

            elif isinstance(upload_item_element, entities.ItemGis):
                buffer = io.BytesIO(json.dumps(upload_item_element.to_json()).encode('utf-8'))
                buffer.name = upload_item_element.name
                all_upload_elements['upload_item_element'] = buffer
                upload_elem = upload_element.BinaryUploadElement(all_upload_elements=all_upload_elements)

            elif isinstance(upload_item_element, bytes) or \
                    isinstance(upload_item_element, io.BytesIO) or \
                    isinstance(upload_item_element, io.BufferedReader) or \
                    isinstance(upload_item_element, io.TextIOWrapper):
                upload_elem = upload_element.BinaryUploadElement(all_upload_elements=all_upload_elements)
                # get size from binaries
                try:
                    total_size += upload_item_element.__sizeof__()
                except Exception:
                    logger.warning("Cant get binaries size")

            else:
                raise PlatformException(
                    error="400",
                    message=f"Unknown element type to upload ('local_path'). received type: {type(upload_item_element)}. "
                            "known types (or list of those types): str (dir, file, url), bytes, io.BytesIO, "
                            "numpy.ndarray, io.TextIOWrapper, Dataloop.Item, Dataloop.Link")

            futures.append(self.upload_single_element(upload_elem))
        return futures

    def upload_single_element(self, elem):
        """
        upload a signal element
        :param elem: UploadElement
        """
        self.num_files += 1
        self.i_item += 1
        self.pbar.total += 1
        self.reporter.upcount_num_workers()
        future = asyncio.run_coroutine_threadsafe(
            self.__upload_single_item_wrapper(element=elem,
                                              mode=self.mode,
                                              pbar=self.pbar,
                                              reporter=self.reporter),
            loop=self.items_repository._client_api.event_loop.loop)
        return future

    def _build_elements_from_df(self, df: pandas.DataFrame):
        futures = deque()
        for index, row in df.iterrows():
            # DEFAULTS
            elem = {'local_annotations_path': None,
                    'remote_path': None,
                    'remote_name': None,
                    'file_types': None,
                    'item_metadata': None,
                    'item_description': None}
            elem.update(row)
            future = self._build_elements_from_inputs(**elem)
            # append deque using +
            futures += future
        return futures

    async def __single_external_sync(self, element):
        storage_id = element.buffer.split('//')[1]
        req_json = dict()
        req_json['filename'] = element.remote_filepath
        req_json['storageId'] = storage_id
        success, response = self.items_repository._client_api.gen_request(req_type='post',
                                                                          path='/datasets/{}/imports'.format(
                                                                              self.items_repository.dataset.id),
                                                                          json_req=[req_json])

        if success:
            items = entities.Item.from_json(client_api=self.items_repository._client_api, _json=response.json()[0],
                                            project=self.items_repository._dataset._project,
                                            dataset=self.items_repository.dataset)
        else:
            raise exceptions.PlatformException(response)
        return items, response.headers.get('x-item-op', 'na')

    async def __single_async_upload(self,
                                    filepath,
                                    remote_path,
                                    uploaded_filename,
                                    last_try,
                                    mode,
                                    item_metadata,
                                    callback,
                                    item_description
                                    ):
        """
        Upload an item to dataset

        :param filepath: local filepath of the item
        :param remote_path: remote directory of filepath to upload
        :param uploaded_filename: optional - remote filename
        :param last_try: print log error only if last try
        :param mode: 'skip'  'overwrite'
        :param item_metadata: item metadata
        :param str item_description: add a string description to the uploaded item
        :param callback:
        :return: Item object
        """

        need_close = False
        if isinstance(filepath, str):
            # upload local file
            if not os.path.isfile(filepath):
                raise PlatformException(error="404", message="Filepath doesnt exists. file: {}".format(filepath))
            if uploaded_filename is None:
                uploaded_filename = os.path.basename(filepath)
            if os.path.isfile(filepath):
                item_type = 'file'
            else:
                item_type = 'dir'
            item_size = os.stat(filepath).st_size
            to_upload = open(filepath, 'rb')
            need_close = True

        else:
            # upload from buffer
            if isinstance(filepath, bytes):
                to_upload = io.BytesIO(filepath)
            elif isinstance(filepath, io.BytesIO):
                to_upload = filepath
            elif isinstance(filepath, io.BufferedReader):
                to_upload = filepath
            elif isinstance(filepath, io.TextIOWrapper):
                to_upload = filepath
            else:
                raise PlatformException("400", "Unknown input filepath type received: {}".format(type(filepath)))

            if uploaded_filename is None:
                if hasattr(filepath, "name"):
                    uploaded_filename = filepath.name
                else:
                    raise PlatformException(error="400",
                                            message="Must have filename when uploading bytes array (uploaded_filename)")

            item_size = to_upload.seek(0, 2)
            to_upload.seek(0)
            item_type = 'file'
        try:
            response = await self.items_repository._client_api.upload_file_async(to_upload=to_upload,
                                                                                 item_type=item_type,
                                                                                 item_size=item_size,
                                                                                 item_metadata=item_metadata,
                                                                                 remote_url=self.remote_url,
                                                                                 uploaded_filename=uploaded_filename,
                                                                                 remote_path=remote_path,
                                                                                 callback=callback,
                                                                                 mode=mode,
                                                                                 item_description=item_description)
        except Exception:
            raise
        finally:
            if need_close:
                to_upload.close()

        if response.ok:
            if item_size != response.json().get('metadata', {}).get('system', {}).get('size', 0):
                self.items_repository.delete(item_id=response.json()['id'])
                raise PlatformException(500,
                                        "The uploaded file is corrupted. "
                                        "Please try again. If it happens again please contact support.")
            item = self.items_repository.items_entity.from_json(client_api=self.items_repository._client_api,
                                                                _json=response.json(),
                                                                dataset=self.items_repository.dataset)
        else:
            raise PlatformException(response)
        return item, response.headers.get('x-item-op', 'na')

    async def __upload_single_item_wrapper(self, element, pbar, reporter, mode):
        async with self.items_repository._client_api.event_loop.semaphore('items.upload', 5):
            # assert isinstance(element, UploadElement)
            item = False
            err = None
            trace = None
            saved_locally = False
            temp_dir = None
            action = 'na'
            remote_folder, remote_name = os.path.split(element.remote_filepath)

            if element.type == 'url':
                saved_locally, element.buffer, temp_dir = self.url_to_data(element.buffer)
            elif element.type == 'link':
                element.buffer = self.link(ref=element.buffer.ref, dataset_id=element.buffer.dataset_id,
                                           type=element.buffer.type, mimetype=element.buffer.mimetype)

            for i_try in range(NUM_TRIES):
                try:
                    logger.debug("Upload item: {path}. Try {i}/{n}. Starting..".format(path=remote_name,
                                                                                       i=i_try + 1,
                                                                                       n=NUM_TRIES))
                    if element.type == 'external_file':
                        item, action = await self.__single_external_sync(element)
                    else:
                        if element.annotations_filepath is not None and \
                                element.item_metadata == entities.ExportMetadata.FROM_JSON:
                            element.item_metadata = {}
                            with open(element.annotations_filepath) as ann_f:
                                item_metadata = json.load(ann_f)
                            if 'metadata' in item_metadata:
                                element.item_metadata = item_metadata['metadata']
                        item, action = await self.__single_async_upload(filepath=element.buffer,
                                                                        mode=mode,
                                                                        item_metadata=element.item_metadata,
                                                                        remote_path=remote_folder,
                                                                        uploaded_filename=remote_name,
                                                                        last_try=(i_try + 1) == NUM_TRIES,
                                                                        callback=None,
                                                                        item_description=element.item_description)
                    logger.debug("Upload item: {path}. Try {i}/{n}. Success. Item id: {id}".format(path=remote_name,
                                                                                                   i=i_try + 1,
                                                                                                   n=NUM_TRIES,
                                                                                                   id=item.id))
                    if isinstance(item, entities.Item):
                        break
                    time.sleep(0.3 * (2 ** i_try))
                except Exception as e:
                    err = e
                    trace = traceback.format_exc()
                    logger.debug("Upload item: {path}. Try {i}/{n}. Fail.\n{trace}".format(path=remote_name,
                                                                                           i=i_try + 1,
                                                                                           n=NUM_TRIES,
                                                                                           trace=trace))

                finally:
                    if saved_locally and os.path.isdir(temp_dir):
                        shutil.rmtree(temp_dir)
            if item:
                if action in ['overwrite', 'created'] and element.annotations_filepath is not None:
                    try:
                        await self.__async_upload_annotations(annotations_filepath=element.annotations_filepath,
                                                              item=item)
                    except Exception:
                        logger.exception('Error uploading annotations to item id: {}'.format(item.id))

                reporter.set_index(status=action,
                                   output=item.to_json(),
                                   success=True,
                                   ref=item.id)
                if pbar is not None:
                    pbar.update()
                    self.items_repository._client_api.callbacks.run_on_event(
                        event=self.items_repository._client_api.callbacks.CallbackEvent.ITEMS_UPLOAD,
                        context={'item_id': item.id, 'dataset_id': item.dataset_id},
                        progress=round(pbar.n / pbar.total * 100, 0))
            else:
                if isinstance(element.buffer, str):
                    ref = element.buffer
                elif hasattr(element.buffer, "name"):
                    ref = element.buffer.name
                else:
                    ref = 'Unknown'
                reporter.set_index(ref=ref, status='error',
                                   success=False,
                                   error="{}\n{}".format(err, trace))

    async def __async_upload_annotations(self, annotations_filepath, item):
        with open(annotations_filepath, 'r', encoding="utf8") as f:
            annotations = json.load(f)
        # wait for coroutines on the current event loop
        return await item.annotations._async_upload_annotations(annotations=annotations['annotations'])

    @staticmethod
    def url_to_data(url):
        chunk_size = 8192
        max_size = 30000000
        temp_dir = None

        # This will download the binaries from the URL user provided
        prepared_request = requests.Request(method='GET', url=url).prepare()
        with requests.Session() as s:
            retry = Retry(
                total=3,
                read=3,
                connect=3,
                backoff_factor=1,
            )
            adapter = HTTPAdapter(max_retries=retry)
            s.mount('http://', adapter)
            s.mount('https://', adapter)
            response = s.send(request=prepared_request, stream=True)

        total_length = response.headers.get("content-length")
        save_locally = int(total_length) > max_size

        if save_locally:
            # save to file
            temp_dir = tempfile.mkdtemp()
            temp_path = os.path.join(temp_dir, url.split('/')[-1].split('?')[0])
            with open(temp_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:  # filter out keep-alive new chunks
                        f.write(chunk)
            # save to output variable
            data = temp_path
        else:
            # save as byte stream
            data = io.BytesIO()
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:  # filter out keep-alive new chunks
                    data.write(chunk)
            # go back to the beginning of the stream
            data.seek(0)
            data.name = url.split('/')[-1]

        return save_locally, data, temp_dir

    @staticmethod
    def is_url(url):
        try:
            return validators.url(url)
        except Exception:
            return False

    @staticmethod
    def link(ref, type, mimetype=None, dataset_id=None):
        """
        :param ref:
        :param type:
        :param mimetype:
        :param dataset_id:
        """
        link_info = {'type': type,
                     'ref': ref}

        if mimetype:
            link_info['mimetype'] = mimetype

        if dataset_id is not None:
            link_info['datasetId'] = dataset_id

        _json = {'type': 'link',
                 'shebang': 'dataloop',
                 'metadata': {'dltype': 'link',
                              'linkInfo': link_info}}

        uploaded_byte_io = io.BytesIO()
        uploaded_byte_io.write(json.dumps(_json).encode())
        uploaded_byte_io.seek(0)

        return uploaded_byte_io


================================================
File: dtlpy/repositories/webhooks.py
================================================
import logging
from urllib.parse import urlencode
from .. import exceptions, entities, miscellaneous
from ..services.api_client import ApiClient

logger = logging.getLogger(name='dtlpy')


class Webhooks:
    """
    Webhooks repository
    """

    def __init__(self,
                 client_api: ApiClient,
                 project: entities.Project = None):
        self._project = project
        self._client_api = client_api
        self._url = '/webhooks'

    ############
    # entities #
    ############
    @property
    def project(self) -> entities.Project:
        if self._project is None:
            raise exceptions.PlatformException(
                error='2001',
                message='Missing "project". need to set a Project entity or use project.webhooks repository')
        assert isinstance(self._project, entities.Project)
        return self._project

    @project.setter
    def project(self, project: entities.Project):
        if not isinstance(project, entities.Project):
            raise ValueError('Must input a valid Project entity')
        self._project = project

    ###########
    # methods #
    ###########
    def create(self,
               name: str,
               project_id: str = None,
               http_method=None,
               hook_url=None,
               project: entities.Project = None) -> entities.Webhook:
        """
        Create web hook entity

        :param name:
        :param project_id:
        :param http_method:
        :param hook_url:
        :param project:
        :return:
        """
        if project is None:
            project = self._project

        if project_id is None and project is None:
            raise exceptions.PlatformException('400', 'Must provide project or project id')

        if project_id is None:
            project_id = project.id

        # payload
        payload = {
            'name': name,
            'httpMethod': http_method,
            'hookUrl': hook_url,
            'project': project_id
        }

        # request
        success, response = self._client_api.gen_request(req_type='post',
                                                         path=self._url,
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Webhook.from_json(_json=response.json(),
                                          client_api=self._client_api,
                                          project=project)

    def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
        """
        List web hooks

        :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
        :return: Paged entity
        :rtype: dtlpy.entities.paged_entities.PagedEntities
        """
        if filters is None:
            filters = entities.Filters(resource=entities.FiltersResource.WEBHOOK)
            if self._project is not None:
                filters.add(field='projectId', values=self._project.id)
        # assert type filters
        elif not isinstance(filters, entities.Filters):
            raise exceptions.PlatformException(error='400',
                                               message='Unknown filters type: {!r}'.format(type(filters)))

        if filters.resource != entities.FiltersResource.WEBHOOK:
            raise exceptions.PlatformException(
                error='400',
                message='Filters resource must to be FiltersResource.WEBHOOK. Got: {!r}'.format(filters.resource))

        paged = entities.PagedEntities(items_repository=self,
                                       filters=filters,
                                       page_offset=filters.page,
                                       page_size=filters.page_size,
                                       client_api=self._client_api)
        paged.get_page()
        return paged

    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Webhook]:
        # handle execution
        pool = self._client_api.thread_pools(pool_name='entity.create')
        jobs = [None for _ in range(len(response_items))]
        # return execution list
        for i_item, item in enumerate(response_items):
            jobs[i_item] = pool.submit(entities.Webhook.from_json,
                                       **{'client_api': self._client_api,
                                          '_json': item,
                                          'project': self._project})

        # get all results
        items = miscellaneous.List([j.result() for j in jobs])
        return items

    def _list(self, filters: entities.Filters):
        """
        List web hooks

        :return:
        """

        query_params = {
            'pageOffset': filters.page,
            'pageSize': filters.page_size
        }

        url = self._url + '?{}'.format(
            urlencode({key: val for key, val in query_params.items() if val is not None}, doseq=True))

        # request
        success, response = self._client_api.gen_request(req_type='get',
                                                         path=url)

        if not success:
            raise exceptions.PlatformException(response)

        return response.json()

    def get(self, webhook_id=None, webhook_name=None) -> entities.Webhook:
        """
        Get Web hook

        :param webhook_id:
        :param webhook_name:
        :return: Web hook execution object
        """
        if webhook_id is None and webhook_name is None:
            raise exceptions.PlatformException('400', 'Must provide webhook_id or webhook_name id')
        elif webhook_id is None:
            webhooks_pages = self.list(
                filters=entities.Filters(field='name', values=webhook_name, resource=entities.FiltersResource.WEBHOOK))
            if webhooks_pages.items_count == 0:
                raise exceptions.PlatformException('404', 'Not found: web hook: {}'.format(webhook_name))
            elif webhooks_pages.items_count > 1:
                raise exceptions.PlatformException('404',
                                                   'More than one webhooks found: web hook: {}'.format(webhook_name))
            else:
                webhook = webhooks_pages.items[0]
        else:
            success, response = self._client_api.gen_request(
                req_type="get",
                path="{}/{}".format(self._url, webhook_id)
            )

            # exception handling
            if not success:
                raise exceptions.PlatformException(response)

            # return entity
            webhook = entities.Webhook.from_json(client_api=self._client_api,
                                                 _json=response.json(),
                                                 project=self._project)
            # verify input webhook name is same as the given id
            if webhook_name is not None and webhook.name != webhook_name:
                logger.warning(
                    "Mismatch found in webhooks.get: webhook_name is different then webhook.name:"
                    " {!r} != {!r}".format(
                        webhook_name,
                        webhook.name))

        return webhook

    def delete(self, webhook_id=None, webhook_name=None):
        """
        Delete Trigger object

        :param webhook_id:
        :param webhook_name:
        :return: True
        """
        if webhook_id is None:
            if webhook_name is None:
                raise exceptions.PlatformException('400', 'Must provide either webhook name or webhook id')
            else:
                webhook_id = self.get(webhook_name=webhook_name).id

        # request
        success, response = self._client_api.gen_request(
            req_type="delete",
            path="{}/{}".format(self._url, webhook_id)
        )
        # exception handling
        if not success:
            raise exceptions.PlatformException(response)
        return True

    def update(self, webhook: entities.Webhook) -> entities.Webhook:
        """

        :param webhook: Webhook entity
        :return: Webhook entity
        """
        assert isinstance(webhook, entities.Webhook)

        # payload
        payload = webhook.to_json()

        # request
        success, response = self._client_api.gen_request(req_type='patch',
                                                         path='{}/{}'.format(self._url, webhook.id),
                                                         json_req=payload)

        # exception handling
        if not success:
            raise exceptions.PlatformException(response)

        # return entity
        return entities.Webhook.from_json(_json=response.json(),
                                          client_api=self._client_api,
                                          project=self._project)


================================================
File: dtlpy/services/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .async_utils import AsyncResponse, AsyncThreadEventLoop
from .events import Events
from .cookie import CookieIO
from .create_logger import DataloopLogger, DtlpyFilter
from .reporter import Reporter
from . import service_defaults



================================================
File: dtlpy/services/aihttp_retry.py
================================================
# from https://pypi.org/project/aiohttp-retry/ with modification to python 3.5.4
import asyncio
import logging
from aiohttp import ClientSession, ClientResponse
from typing import Any, Callable, Optional, Set, Type

# Options
_RETRY_ATTEMPTS = 3
_RETRY_START_TIMEOUT = 0.1
_RETRY_MAX_TIMEOUT = 30
_RETRY_FACTOR = 2


class _RequestContext:
    def __init__(self, request: Callable[..., Any],  # Request operation, like POST or GET
                 url: str,  # Just url
                 retry_attempts: int = _RETRY_ATTEMPTS,  # How many times we should retry
                 retry_start_timeout: float = _RETRY_START_TIMEOUT,  # Base timeout time, then it exponentially grow
                 retry_max_timeout: float = _RETRY_MAX_TIMEOUT,  # Max possible timeout between tries
                 retry_factor: float = _RETRY_FACTOR,  # How much we increase timeout each time
                 retry_for_statuses: Optional[Set[int]] = None,  # On which statuses we should retry
                 retry_exceptions: Optional[Set[Type]] = None,  # On which exceptions we should retry
                 **kwargs: Any
                 ) -> None:
        self._request = request
        self._url = url

        self._retry_attempts = retry_attempts
        self._retry_start_timeout = retry_start_timeout
        self._retry_max_timeout = retry_max_timeout
        self._retry_factor = retry_factor

        if retry_for_statuses is None:
            retry_for_statuses = set()
        self._retry_for_statuses = retry_for_statuses

        if retry_exceptions is None:
            retry_exceptions = set()
        self._retry_exceptions = retry_exceptions

        self._kwargs = kwargs

        self._current_attempt = 0
        self._response = None

    def _exponential_timeout(self) -> float:
        timeout = self._retry_start_timeout * (self._retry_factor ** (self._current_attempt - 1))
        return min(timeout, self._retry_max_timeout)

    def _check_code(self, code: int) -> bool:
        return 500 <= code <= 599 or code in self._retry_for_statuses

    async def _do_request(self) -> ClientResponse:
        try:
            self._current_attempt += 1
            response = await self._request(url=self._url, **self._kwargs)
            code = response.status
            if self._current_attempt < self._retry_attempts and self._check_code(code):
                retry_wait = self._exponential_timeout()
                await asyncio.sleep(retry_wait)
                return await self._do_request()
            self._response = response
            return response

        except Exception as e:
            retry_wait = self._exponential_timeout()
            if self._current_attempt < self._retry_attempts:
                for exc in self._retry_exceptions:
                    if isinstance(e, exc):
                        await asyncio.sleep(retry_wait)
                        return await self._do_request()

            raise e

    async def __aenter__(self) -> ClientResponse:
        return await self._do_request()

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        if self._response is not None:
            if not self._response.closed:
                self._response.close()


class RetryClient:
    def __init__(self, logger: Any = None, *args: Any, **kwargs: Any) -> None:
        self._client = ClientSession(*args, **kwargs)
        self._closed = False

        if logger is None:
            logger = logging.getLogger("aiohttp_retry")

        self._logger = logger

    def __del__(self) -> None:
        if not self._closed:
            self._logger.warning("Aiohttp retry client was not closed")

    @staticmethod
    def _request(request: Callable[..., Any], url: str, **kwargs: Any) -> _RequestContext:
        return _RequestContext(request, url, **kwargs)

    def get(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.get, url, **kwargs)

    def options(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.options, url, **kwargs)

    def head(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.head, url, **kwargs)

    def post(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.post, url, **kwargs)

    def put(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.put, url, **kwargs)

    def patch(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.patch, url, **kwargs)

    def delete(self, url: str, **kwargs: Any) -> _RequestContext:
        return self._request(self._client.delete, url, **kwargs)

    async def close(self) -> None:
        await self._client.close()
        self._closed = True

    async def __aenter__(self) -> 'RetryClient':
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        await self.close()


================================================
File: dtlpy/services/api_reference.py
================================================
class ApiReference(object):

    def __init__(self):
        self.references = {'paths': dict()}

    def add(self, path, method):
        def wrapper(f):
            if path not in self.references['paths']:
                self.references['paths'][path] = dict()
            if method not in self.references['paths'][path]:
                self.references['paths'][path][method] = dict()

            docstring = f.__doc__
            docstring = docstring.split('\n')
            in_code = False

            code_list = list()
            docs_list = list()
            for i_line in range(len(docstring)):
                if '.. code-block::' in docstring[i_line]:
                    in_code = True
                    continue
                if '**Example**' in docstring[i_line]:
                    continue
                if docstring[i_line].startswith('    ') and not docstring[i_line].startswith('        '):
                    in_code = False
                if in_code:
                    code_list.append('{}'.format(docstring[i_line].strip()))
                else:
                    docs_list.append('# {}'.format(docstring[i_line].strip()))
                # assert False
            docstring = '\n'.join(code_list + docs_list)
            self.references['paths'][path][method]['x-codeSamples'] = [{"lang": "Python",
                                                                        "source": docstring}]
            return f

        return wrapper


api_reference = ApiReference()


================================================
File: dtlpy/services/async_utils.py
================================================
import threading
import asyncio
import logging
import io

logger = logging.getLogger(name='dtlpy')


class AsyncThreadEventLoop(threading.Thread):
    """Thread class with a stop() method. The thread itself has to check
    regularly for the stopped() condition."""

    def __init__(self, loop, n, *args, **kwargs):
        super(AsyncThreadEventLoop, self).__init__(*args, **kwargs)
        self.loop = loop
        self.n = n
        self._count = 0
        self._semaphores = dict()

    def count_up(self):
        self._count += 1

    def count_down(self):
        self._count -= 1

    def run(self):
        def exception_handler(loop, context):
            logger.debug(
                "[Asyc] EventLoop: caught the following exception: {}".format(context['message']))

        self.loop.set_exception_handler(exception_handler)
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()
        logger.debug('Ended event loop with bounded semaphore to {}'.format(self.n))

    def semaphore(self, name, n=None):
        if n is None:
            n = self.n
        else:
            n = min(n, self.n)
        if name not in self._semaphores:
            self._semaphores[name] = asyncio.BoundedSemaphore(n)
        return self._semaphores[name]

    def stop(self):
        self.loop.call_soon_threadsafe(self.loop.stop)  # here


class AsyncResponse:
    def __init__(self, text, _json, async_resp):
        self.text = text
        self._json = _json
        self.async_resp = async_resp

    def json(self):
        return self._json

    @property
    def status_code(self):
        return self.async_resp.status

    @property
    def reason(self):
        return self.async_resp.reason

    @property
    def ok(self):
        return self.async_resp.status < 400

    @property
    def request(self):
        return self.async_resp.request_info

    @property
    def headers(self):
        return self.async_resp.headers


class DummyErrorResponse:
    def __init__(self, error, trace):
        self.status = 3001  # SDK Error
        self.reason = trace
        self.error = error
        self.request_info = {}
        self.headers = {}


class AsyncResponseError(AsyncResponse):
    def __init__(self, error, trace):
        async_resp = DummyErrorResponse(error=error, trace=trace)
        _json = {'error': error}
        text = error
        super().__init__(async_resp=async_resp,
                         _json=_json,
                         text=text)


class AsyncUploadStream(io.IOBase):
    def __init__(self, buffer, callback=None, name='', chunk_timeout=10, max_retries=3):
        self.buffer = buffer
        self.buffer.seek(0)
        self.callback = callback
        self._name = name
        self.chunk_timeout = chunk_timeout
        self.max_retries = max_retries

    @property
    def name(self):
        return self._name

    async def async_read(self, size):
        retries = 0
        while retries < self.max_retries:
            try:
                import sys
                if sys.version_info < (3, 9):
                    loop = asyncio.get_event_loop()
                    data = await asyncio.wait_for(loop.run_in_executor(None, self.buffer.read, size),
                                                  timeout=self.chunk_timeout)
                else:
                    data = await asyncio.wait_for(asyncio.to_thread(self.buffer.read, size), timeout=self.chunk_timeout)
                if self.callback is not None:
                    self.callback(size)
                return data
            except asyncio.TimeoutError:
                retries += 1
                logger.warning(
                    f"Chunk read timed out after {self.chunk_timeout} seconds. Retrying {retries}/{self.max_retries}...")

        raise Exception(f"Chunk read failed after {self.max_retries} retries due to timeouts")

    def read(self, size):
        return asyncio.run(self.async_read(size))


================================================
File: dtlpy/services/calls_counter.py
================================================
from .cookie import CookieIO


class CallsCounter:
    def __init__(self, filepath):
        self.io = CookieIO(filepath)
        self.state = 'off'
        self.number = 0
        self.load()

    def add(self):
        if self.state == 'on':
            self.load()
            self.number += 1
            self.save()

    def reset(self):
        self.number = 0
        self.save()

    def save(self):
        self.io.put('calls_counter', {'state': self.state,
                                      'number': self.number})

    def on(self):
        self.state = 'on'
        self.save()

    def off(self):
        self.state = 'off'
        self.save()

    def on_exit(self):
        self.save()

    def load(self):
        calls = self.io.get('calls_counter')
        if calls is None:
            # not set in cookie - set default
            calls = {'state': 'off',
                     'number': 0}
            self.io.put('calls_counter', calls)
        self.state = calls['state']
        self.number = calls['number']


================================================
File: dtlpy/services/check_sdk.py
================================================
import time
import logging
import threading
import traceback
import jwt

logger = logging.getLogger(name='dtlpy')


def check_in_thread(version, client_api):
    try:
        # check for a valid token
        if client_api.token_expired():
            # wait for user to maybe login in the next 2 minutes
            time.sleep(120)
        # check for a valid token again
        if client_api.token_expired():
            # return if cant find a valid token
            logger.debug('Cant check_sdk without a valid token.')
            return

        # try read token for email
        try:
            payload = jwt.decode(client_api.token, algorithms=['HS256'],
                                 verify=False, options={'verify_signature': False})
            user_email = payload['email']
        except Exception:
            user_email = 'na'
        logger.debug('SDK info: user: {}, version: {}'.format(user_email, version))
        return_type, resp = client_api.gen_request(req_type='POST',
                                                   path='/sdk/check',
                                                   data={'version': version,
                                                         'email': user_email},
                                                   log_error=False)
        if resp.ok:
            resp = resp.json()
            client_api.cookie_io.put(key='check_version_status',
                                     value={'level': resp['level'],
                                            'msg': resp['msg']})
        else:
            client_api.cookie_io.put(key='check_version_status',
                                     value={'level': 'debug',
                                            'msg': 'unknown'})
    except Exception:
        logger.debug(traceback.format_exc())
        logger.debug('Error in check sdk manager.')


def check(version, client_api):
    worker = threading.Thread(target=check_in_thread,
                              kwargs={'version': version,
                                      'client_api': client_api})
    worker.daemon = True
    worker.start()
    status = client_api.cookie_io.get('check_version_status')
    if status is not None:
        level = status['level']
        msg = status['msg']
        if level.lower() == 'debug':
            logger.debug(msg=msg)
        elif level.lower() == 'info':
            logger.info(msg=msg)
        elif level.lower() == 'warning':
            logger.warning(msg=msg)
        elif level.lower() == 'error':
            logger.error(msg=msg)
        else:
            logger.debug(msg='unknown')


def resolve_platform_settings_in_thread(settings, client_api):
    try:
        # check for a valid token
        if client_api.token_expired():
            # wait for user to maybe login in the next 2 minutes
            time.sleep(120)
        # check for a valid token again
        if client_api.token_expired():
            # return if cant find a valid token
            logger.debug('Cant set settings without a valid token.')
            return
        settings_list = settings.resolve(user_email=client_api.info()['user_email'])
        client_api.platform_settings.add_bulk(settings_list)

    except Exception:
        logger.debug(traceback.format_exc())
        logger.debug('Error in add settings.')


def resolve_platform_settings(client_api, settings):
    worker = threading.Thread(target=resolve_platform_settings_in_thread,
                              kwargs={'client_api': client_api, 'settings': settings})
    worker.daemon = True
    worker.start()


================================================
File: dtlpy/services/cookie.py
================================================
"""
Dataloop cookie state
"""

import os
import time
import json
import logging
import random
from .service_defaults import DATALOOP_PATH
from filelock import FileLock

logger = logging.getLogger(name='dtlpy')

NUM_TRIES = 3


class CookieIO:
    """
    Cookie interface for Dataloop parameters
    """

    def __init__(self, path, create=True, local=False):
        self.COOKIE = path
        self.local = local
        if create:
            self.create()

    @staticmethod
    def init():
        global_cookie_file = os.path.join(DATALOOP_PATH, 'cookie.json')
        return CookieIO(global_cookie_file)

    @staticmethod
    def init_local_cookie(create=False):
        local_cookie_file = os.path.join(os.getcwd(), '.dataloop', 'state.json')
        return CookieIO(local_cookie_file, create=create, local=True)

    @staticmethod
    def init_package_json_cookie(create=False):
        package_json_file = os.path.join(os.getcwd(), 'package.json')
        return CookieIO(package_json_file, create=create, local=True)

    def create(self):
        # create directory '.dataloop' if not exists
        if not os.path.isdir(os.path.dirname(self.COOKIE)):
            os.makedirs(os.path.dirname(self.COOKIE))

        if not os.path.isfile(self.COOKIE) or os.path.getsize(self.COOKIE) == 0:
            logger.debug('COOKIE.create: File: {}'.format(self.COOKIE))
            self.reset()
        try:
            with FileLock(self.COOKIE + ".lock"):
                with open(self.COOKIE, 'r') as f:
                    json.load(f)
        except ValueError:
            print('FATAL ERROR: COOKIE {!r} is corrupted. please fix or delete the file.'.format(self.COOKIE))
            raise SystemExit

    def read_json(self, create=False):
        # which cookie
        if self.local:
            self.COOKIE = os.path.join(os.getcwd(), '.dataloop', 'state.json')

        # check if file exists - and create
        if not os.path.isfile(self.COOKIE) and create:
            self.create()

        # check if file exists
        if not os.path.isfile(self.COOKIE):
            logger.debug('COOKIE.read: File does not exist: {}. Return None'.format(self.COOKIE))
            cfg = {}
        else:
            # read cookie
            cfg = {}
            for i in range(NUM_TRIES):
                try:
                    with FileLock(self.COOKIE + ".lock"):
                        with open(self.COOKIE, 'r') as fp:
                            cfg = json.load(fp)
                    break
                except Exception:
                    if i == (NUM_TRIES - 1):
                        raise
                    time.sleep(random.random())
                    continue
        return cfg

    def get(self, key):
        if key not in ['calls_counter']:
            # ignore logging for some keys
            logger.debug('COOKIE.read: key: {}'.format(key))
        cfg = self.read_json()
        if key in cfg.keys():
            value = cfg[key]
        else:
            logger.debug(msg='Key not in platform cookie file: {}. Return None'.format(key))
            value = None
        return value

    def put(self, key, value):
        if key not in ['calls_counter']:
            # ignore logging for some keys
            logger.debug('COOKIE.write: key: {}'.format(key))
        # read and write
        cfg = self.read_json(create=True)
        cfg[key] = value
        with FileLock(self.COOKIE + ".lock"):
            with open(self.COOKIE, 'w') as fp:
                json.dump(cfg, fp, indent=2)

    def reset(self):
        with FileLock(self.COOKIE + ".lock"):
            with open(self.COOKIE, 'w') as fp:
                json.dump({}, fp, indent=2)


================================================
File: dtlpy/services/create_logger.py
================================================
import datetime
import threading
import logging.handlers
import os

from .service_defaults import DATALOOP_PATH

logger = logging.getLogger(name='dtlpy')


class DataloopLogger(logging.handlers.BaseRotatingHandler):
    """
        Based on logging.handlers.RotatingFileHandler
        Create a new log file after reached maxBytes
        Delete logs older than a threshold default is week)
    """

    def __init__(self, filename, mode='a', maxBytes=0, encoding='utf-8', delay=False):
        if maxBytes > 0:
            mode = 'a'
        super().__init__(filename=filename, mode=mode, encoding=encoding, delay=delay)
        self.maxBytes = maxBytes
        DataloopLogger.clean_dataloop_cache()

    @staticmethod
    def clean_dataloop_cache(cache_path=DATALOOP_PATH, max_param=None):
        try:
            async_clean = True
            dir_list = [os.path.join(cache_path, d) for d in os.listdir(cache_path)
                        if os.path.isdir(os.path.join(cache_path, d))]
            for path in dir_list:
                if 'cache' not in path:
                    if async_clean:
                        worker = threading.Thread(target=DataloopLogger.clean_dataloop_cache_thread,
                                                  kwargs={'path': path,
                                                          'max_param': max_param})
                        worker.daemon = True
                        worker.start()
                    else:
                        DataloopLogger.clean_dataloop_cache_thread(path=path, max_param=max_param)
        except Exception as err:
            logger.exception(err)

    @staticmethod
    def get_clean_parameter_per(path):
        # (60 * 60 * 24 * 7):  # sec * min * hour * days - delete if older than a week
        # 1e6 100MB
        path_param = [{'type': 'datasets', 'max_time': 60 * 60 * 24 * 30},
                      {'type': 'items', 'max_time': 60 * 60 * 24 * 30},
                      {'type': 'logs', 'max_time': 60 * 60 * 24 * 7, 'max_size': 200 * 1e6},
                      {'type': 'projects', 'max_time': 60 * 60 * 24 * 30}]
        for param in path_param:
            if param['type'] in path:
                return param
        return {'type': 'default', 'max_time': 60 * 60 * 24 * 30}

    @staticmethod
    def clean_dataloop_cache_thread(path, total_cache_size=0, max_param=None):
        try:
            is_root = False
            if max_param is None:
                max_param = DataloopLogger.get_clean_parameter_per(path)
                is_root = True

            now = datetime.datetime.timestamp(datetime.datetime.now())
            files = [os.path.join(path, f) for f in os.listdir(path)]
            files.sort(key=lambda x: -os.path.getmtime(x))  # newer first
            for filepath in files:
                if os.path.isdir(filepath):
                    total_cache_size = DataloopLogger. \
                        clean_dataloop_cache_thread(filepath, total_cache_size=total_cache_size, max_param=max_param)
                    # Remove the dir if empty
                    if len(os.listdir(filepath)) == 0:
                        os.rmdir(filepath)
                    continue
                if 'max_time' in max_param:
                    file_time = os.path.getmtime(filepath)
                    if (now - file_time) > max_param['max_time']:
                        try:
                            os.remove(filepath)
                        except Exception as e:
                            logger.warning("Old log file can not be removed: {}".format(e))
                        continue
                if 'max_size' in max_param:
                    file_size = os.path.getsize(filepath)
                    if (total_cache_size + file_size) > max_param['max_size']:
                        try:
                            os.remove(filepath)
                        except Exception as e:
                            logger.warning("Old log file can not be removed: {}".format(e))
                        continue
                    total_cache_size += file_size
            if is_root:
                logger.debug("clean_dataloop_cache_thread for {} directory has been ended".format(path))
            return total_cache_size
        except Exception as err:
            logger.exception(err)

    @staticmethod
    def get_log_path():
        log_path = os.path.join(DATALOOP_PATH, 'logs')
        if not os.path.isdir(log_path):
            os.makedirs(log_path, exist_ok=True)
        return log_path

    @staticmethod
    def get_log_filepath():
        log_path = DataloopLogger.get_log_path()
        log_filepath = os.path.join(log_path, '{}.log'.format(datetime.datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')))
        return log_filepath

    def doRollover(self):
        """
        Do a rollover, as described in __init__().
        """
        if self.stream:
            self.stream.close()
            self.stream = None
        # clean older logs (week old)
        DataloopLogger.clean_dataloop_cache()
        # create new log
        self.baseFilename = DataloopLogger.get_log_filepath()
        if not self.delay:
            self.stream = self._open()

    def shouldRollover(self, record):
        """
        Determine if rollover should occur.

        Basically, see if the supplied record would cause the file to exceed
        the size limit we have.
        """
        if self.stream is None:  # delay was set...
            self.stream = self._open()
        if self.maxBytes > 0:  # are we rolling over?
            msg = "%s\n" % self.format(record)
            self.stream.seek(0, 2)  # due to non-posix-compliant Windows feature
            if self.stream.tell() + len(msg) >= self.maxBytes:
                return 1
        return 0


class DtlpyFilter(logging.Filter):
    def __init__(self, package_path):
        super(DtlpyFilter, self).__init__(name='dtlpy')
        self._package_path = package_path

    def filter(self, record):
        pathname = record.pathname
        try:
            relativepath = os.path.splitext(os.path.relpath(pathname, self._package_path))[0]
            relativepath = relativepath.replace(os.sep, '.')
        except Exception:
            relativepath = ''
        record.relativepath = relativepath
        return True


================================================
File: dtlpy/services/events.py
================================================
import threading
import time
import traceback
import logging
import queue
import os

logger = logging.getLogger(name='dtlpy')


class Events(threading.Thread):
    def __init__(self, client_api, *args, **kwargs):
        super(Events, self).__init__(*args, **kwargs)
        self.client_api = client_api
        self.q = queue.Queue()
        self.mapping_events_dict = {
            'project': {'method': ['create', 'delete'], 'route': '/projects'},
            'task': {'method': ['create'], 'route': '/annotationtasks'},
        }

    def track(self, event):
        try:
            return_type, resp = self.client_api.gen_request(req_type='POST',
                                                            path='/analytics/metric/pendo',
                                                            json_req=event,
                                                            log_error=False)
            if not resp.ok:
                logger.debug('failed send event to analytics: {}'.format(resp.text))
        except Exception:
            logger.debug('failed send track event: {}'.format(traceback.format_exc()))

    def run(self):
        while True:
            try:
                event = self.q.get()
                self.track([event])
                self.q.task_done()
            except Exception:
                logger.exception('failed in loop')

    def _valid_events(self, path):
        for route in self.mapping_events_dict.values():
            if path.startswith(route['route']) and 'sdk' not in path:
                return True
        return False

    def _add_info(self, event_payload, function, resp):
        if function in ['create']:
            event_source = event_payload.get('event', None)
            resp_json = resp.json()
            if event_source == 'dtlpy:project':
                event_payload['properties'].update({'project_id': resp_json['id'],
                                                    'project_name': resp_json['name']})
            if event_source == 'dtlpy:task' and function in ['create']:
                if 'createTaskPayload' in resp_json.get('spec', {}):
                    task_payload = resp_json.get('spec', {}).get('createTaskPayload', {})
                else:
                    task_payload = resp_json
                metadata = task_payload.get('metadata', {}).get('system', {})
                task_type = task_payload.get('spec', {}).get('type', {})
                allocation_method = 'Distribution'
                if 'batchSize' in metadata and 'maxBatchWorkload' in metadata and 'allowedAssignees' in metadata:
                    allocation_method = 'Pulling'
                event_payload['properties'].update({'task_type': task_type,
                                                    'allocation_method': allocation_method})

    def put(self, event, resp=None, path=None):
        send_event = True
        if path is not None and not self._valid_events(path=path):
            send_event = False

        if resp is not None and send_event:
            event_source = os.path.normpath(event.filename).split('\\')[-1][:-4]
            event_payload = {'event': 'dtlpy:' + event_source,
                             'properties': {'sdk_event': event.function + '_' + event_source}}
            if event_source in self.mapping_events_dict and \
                    event.function in self.mapping_events_dict[event_source]['method']:
                self._add_info(event_payload=event_payload, function=event.function, resp=resp)
            else:
                send_event = False
        else:
            event_payload = event
        if send_event:
            self.q.put(event_payload)


================================================
File: dtlpy/services/logins.py
================================================
from urllib.parse import urlsplit, urlunsplit
import base64
import requests
import logging
import json
import jwt
import os

logger = logging.getLogger(name='dtlpy')


def login_m2m(api_client, email, password, client_id=None, client_secret=None, force=False):
    return login_secret(api_client=api_client,
                        email=email,
                        password=password,
                        client_id=client_id,
                        client_secret=client_secret,
                        force=force)


def login_secret(api_client, email, password, client_id, client_secret=None, force=False):
    """
    Login with email and password from environment variables
    :param api_client: ApiClient instance
    :param email: user email. if already logged in with same user - login will NOT happen. see "force"
    :param password: user password
    :param client_id: DEPRECATED
    :param client_secret: DEPRECATED
    :param force: force login. in case login with same user but want to get a new JWT
    :return:
    """
    # TODO add deprecation warning to client_id
    # check if already logged in with SAME email
    if api_client.token is not None or api_client.token == '':
        try:
            payload = jwt.decode(api_client.token, algorithms=['HS256'],
                                 options={'verify_signature': False}, verify=False)
            if 'email' in payload and \
                    payload['email'] == email and \
                    not api_client.token_expired() and \
                    not force:
                return True
        except jwt.exceptions.DecodeError:
            logger.debug('{}'.format('Cant decode token. Force login is used'))

    logger.info('[Start] Login Secret')
    env_params = api_client.environments[api_client.environment]
    # need to login
    payload = {'username': email,
               'password': password,
               'type': 'user_credentials'
               }
    headers = {'content-type': 'application/json'}
    if 'gate_url' not in env_params:
        env_params['gate_url'] = gate_url_from_host(environment=api_client.environment)
        api_client.environments[api_client.environment] = env_params
    token_url = env_params['gate_url'] + "/token?default"
    resp = requests.request("POST",
                            token_url,
                            data=json.dumps(payload),
                            headers=headers,
                            verify=env_params.get('verify_ssl', True))
    if not resp.ok:
        logout(api_client=api_client)
        api_client.print_bad_response(resp)
        return False
    else:
        response_dict = resp.json()
        api_client.token = response_dict['id_token']  # this will also set the refresh_token to None
        if 'refresh_token' in response_dict:
            api_client.refresh_token = response_dict['refresh_token']

        # set new client id for refresh
        payload = jwt.decode(api_client.token, algorithms=['HS256'],
                             options={'verify_signature': False}, verify=False)
        if 'email' in payload:
            logger.info('[Done] Login Secret. User: {}'.format(payload['email']))
        else:
            logger.info('[Done] Login Secret. User: {}'.format(email))
            logger.info(payload)
    return True


def logout(api_client):
    """
    remove JWT from cookie
    """
    api_client.token = None
    api_client.refresh_token = None
    return True


def login_html():
    try:
        location = os.path.dirname(os.path.realpath(__file__))
    except NameError:
        location = './dtlpy/services'
    filename = os.path.join(location, '..', 'assets', 'lock_open.png')

    if os.path.isfile(filename):

        with open(filename, 'rb') as f:
            image = f.read()

        html = (
            "        <!doctype html>\n"
            "        <html>\n"
            "        <head>\n"
            "            <style>\n"
            "                body {{\n"
            "                    background-color: #F7F7F9 !important;\n"
            "                    display: flex;\n"
            "                    justify-content: center;\n"
            "                    align-items: center;\n"
            "                    height: 100vh;\n"
            "                    width: 100vw;\n"
            "                    margin: 0;\n"
            "                }}\n"
            "                img {{\n"
            "                    display: block;\n"
            "                    max-width: 100%;\n"
            "                    max-height: 100%;\n"
            "                    margin: auto;\n"
            "                }}\n"
            "            </style>\n"
            "        </head>\n"
            "        <body>\n"
            "            <img src='data:image/png;base64,{image}'>\n"
            "        </body>\n"
            "        </html>\n"
        ).format(image=base64.b64encode(image).decode())
    else:
        html = "<!doctype html><html><body>Logged in successfully</body></html>"

    return html.encode('utf-8')


def login(api_client, auth0_url=None, audience=None, client_id=None, login_domain=None, callback_port=None):
    import webbrowser
    from http.server import BaseHTTPRequestHandler, HTTPServer
    from urllib.parse import urlparse, parse_qs
    logger.info('Logging in to Dataloop...')

    class LocalServer:

        class Handler(BaseHTTPRequestHandler):

            tokens_obtained = False
            id_token = None
            access_token = None
            refresh_token = None

            def log_message(self, format, *args):
                return

            def do_GET(self):
                parsed_path = urlparse(self.path)
                query = parse_qs(parsed_path.query)
                self.send_response(200)
                self.send_header('Content-type', 'text/html')
                self.end_headers()
                self.wfile.write(login_html())
                self.__class__.id_token = query['id_token'][0]
                self.__class__.access_token = query['access_token'][0]
                self.__class__.refresh_token = query['refresh_token'][0]
                self.__class__.tokens_obtained = True

        def __init__(self):
            self.port = callback_port if callback_port is not None else 3001
            self.server = HTTPServer(('', self.port), self.Handler)
            self.server.timeout = 60

        def process_request(self):
            self.server.handle_request()

            if self.Handler.tokens_obtained:
                return True, {
                    "id": self.Handler.id_token,
                    "access": self.Handler.access_token,
                    "refresh": self.Handler.refresh_token
                }
            else:
                return False, {}

        def local_endpoint(self):
            return "http://localhost:{}".format(self.port)

        def close(self):
            self.server.server_close()

    server = LocalServer()

    try:
        local_ep = server.local_endpoint()
        env_params = api_client.environments[api_client.environment]
        if 'gate_url' not in env_params:
            env_params['gate_url'] = gate_url_from_host(environment=api_client.environment)
            api_client.environments[api_client.environment] = env_params
        remote_ep = env_params['gate_url']
        login_page_url = "{}/login?callback={}".format(remote_ep, local_ep)
        if login_domain is not None:
            login_page_url = "{}&domain={}".format(login_page_url, login_domain)
        logger.info("Launching interactive login via {}".format(remote_ep))
        webbrowser.open(url=login_page_url, new=2, autoraise=True)

        success, tokens = server.process_request()

        if success:
            decoded_jwt = jwt.decode(tokens['id'], verify=False,
                                     options={'verify_signature': False})

            if 'email' in decoded_jwt:
                logger.info('Logged in: {}'.format(decoded_jwt['email']))
            else:
                logger.info('Logged in: unknown user')

            api_client.token = tokens['id']
            api_client.refresh_token = tokens['refresh']

            return True
        else:
            logout(api_client=api_client)
            logger.error('Login failed: no tokens obtained')
            return False
    except Exception as err:
        logout(api_client=api_client)
        logger.exception('Login failed: error while getting token', err)
        return False
    finally:
        server.close()


def gate_url_from_host(environment):
    parsed = urlsplit(environment)
    return urlunsplit((parsed.scheme, parsed.netloc, '', '', ''))


================================================
File: dtlpy/services/reporter.py
================================================
import os
import datetime
import json
import threading
from .. import services
from .. import exceptions
import logging
from ..caches import dl_cache

logger = logging.getLogger(name='dtlpy')
CHUNK = 200000


class Reporter:
    """
     ThreadPool Report summary
    """
    ITEMS_DOWNLOAD = 'downloader'
    ITEMS_UPLOAD = 'uploader'
    CONVERTER = 'converter'

    def __init__(self,
                 num_workers,
                 resource,
                 client_api,
                 print_error_logs=False,
                 output_entity=None,
                 no_output=False):
        self._num_workers = num_workers
        self.mutex = threading.Lock()
        self.no_output = no_output
        self._client_api = client_api
        self.key = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        self.cache_mode = client_api.cache_state.enable_cache
        if self.cache_mode is None:
            self.cache_mode = True
        self.cache_chunk = client_api.cache_state.chunk_cache
        if self.cache_chunk is None:
            self.cache_chunk = CHUNK
        self._reports = None
        self._success = dict()
        self._status = dict()
        self._errors = dict()
        self._output = dict()
        self.cache_items = {'errors': self._errors,
                            'status': self._status,
                            'output': self._output}
        self._resource = resource
        self._print_error_logs = print_error_logs
        self._output_entity = output_entity

    @property
    def has_errors(self):
        """
        return True if errors has occurred False otherwise
        """
        return self.failure_count > 0

    def build_cache(self):
        if self.cache_mode:
            try:
                self._reports = {'errors': dl_cache.DiskCache('errors-' + self.key),
                                 'output': dl_cache.DiskCache('output-' + self.key),
                                 'status': dl_cache.DiskCache('status-' + self.key),
                                 }
                self._reports.get('status').add('success', 0)
                self._reports.get('status').add('failure', 0)
                self.clear_reporter()
            except:
                raise exceptions.PlatformException(
                    error='2001',
                    message='Failed to initialize cache handler. Please disable cache usage:  dl.cache_state.enable_cache = False')

    def construct_output(self, entity):
        """
        convert the json to his entity object
        """
        if self._output_entity and entity:
            return self._output_entity.from_json(client_api=self._client_api, _json=entity)
        return entity

    @property
    def output(self):
        """
        return a generator for all the outputs or the output it self if it is single
        """
        if self.no_output:
            return
        if self.cache_mode == 'diskcache':
            output = self._reports.get('output')
            for k in output.keys():
                for item in list(output.get(key=k).values()):
                    yield self.construct_output(item)

        current = [self.construct_output(output) for output in list(self._output.values()) if output is not None]
        for item in current:
            yield item

    @property
    def status_list(self):
        """
        return a list of all the status that get
        """
        output = dict()
        if self.cache_mode == 'diskcache':
            status_cache = self._reports.get('status')
            for key in status_cache.keys():
                if key not in ['failure', 'success']:
                    output.update(status_cache.get(key=key))
        output.update(self._status)
        return list(output.values())

    @property
    def num_workers(self):
        """
        number of the threads to work
        """
        return self._num_workers

    def upcount_num_workers(self):
        """
        increase the number of the threads to work
        """
        self._num_workers += 1

    @property
    def failure_count(self):
        """
        return how many actions fail
        """
        curr = len([suc for suc in (self._success.values()) if suc is False])
        if self.cache_mode == 'diskcache':
            return self._reports.get('status').get(key='failure') + curr
        else:
            return curr

    @property
    def success_count(self):
        """
        return how many actions success
        """
        curr = len([suc for suc in (self._success.values()) if suc is True])
        if self.cache_mode == 'diskcache':
            return self._reports.get('status').get(key='success') + curr
        else:
            return curr

    def status_count(self, status):
        """
        :param status: str of status to check it
        :return: how many times this status appear
        """
        status_list = self.status_list
        return list(status_list).count(status)

    def _write_to_disk(self):
        """
        the function write to the dick the outputs that get until the chunk amount
        """
        with self.mutex:
            if self._reports is None:
                self.build_cache()
            if len(self._success) > self.cache_chunk:
                status_cache = self._reports.get('status')
                num_true = sum(list(self._success.values()))
                status_cache.incr(key='failure',
                                  value=len(self._success) - num_true)
                status_cache.incr(key='success',
                                  value=num_true)
                self._success.clear()
            for name, cache_list in self.cache_items.items():
                if len(cache_list) > self.cache_chunk:
                    self._reports[name].push(cache_list)
                    cache_list.clear()

    def set_index(self, ref, error=None, status=None, success=None, output=None):
        """
        set the values that we get from the actions in the reporter
        """
        if self.mutex.locked():
            self.mutex.acquire()
            self.mutex.release()
        if error is not None:
            self._errors[ref] = error

        if status is not None:
            self._status[ref] = status

        if success is not None:
            self._success[ref] = success

        if success and output is not None:
            if not self.no_output:
                self._output[ref] = output

        if len(self._errors) > self.cache_chunk or \
                len(self._status) > self.cache_chunk or \
                len(self._output) > self.cache_chunk or \
                len(self._success) > self.cache_chunk:
            if self.cache_mode:
                self._write_to_disk()

    def generate_log_files(self):
        """
        build a log file that display the errors
        """
        if len(self._errors) > 0 and self.cache_mode == 'diskcache':
            # write from RAM to disk
            self._reports['errors'].push(self._errors)
            self._errors.clear()

        reports_dir = os.path.join(services.service_defaults.DATALOOP_PATH, 'reporters')
        if not os.path.exists(reports_dir):
            os.mkdir(reports_dir)
        log_filepath = os.path.join(reports_dir,
                                    "log_{}_{}.json".format(self._resource,
                                                            datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")))
        errors_json = dict()
        if self.cache_mode == 'diskcache':
            err_cache = self._reports['errors']
            for k in err_cache.keys():
                errors_json.update(err_cache.get(k))
        if self._errors:
            errors_json.update(self._errors)
        if self._print_error_logs:
            for key in errors_json:
                logger.warning("{}\n{}".format(key, errors_json[key]))
            return None
        else:
            with open(log_filepath, "w") as f:
                json.dump(errors_json, f, indent=2)
            return log_filepath

    def clear_reporter(self):
        """
        clear the file system from the outputs
        """
        import shutil
        date_now = datetime.datetime(year=int(self.key.split('-')[0]),
                                     month=int(self.key.split('-')[1]),
                                     day=int(self.key.split('-')[2]))
        cache_dir = os.path.dirname(self._reports['output'].cache_dir)
        cache_files_list = os.listdir(cache_dir)
        # remove all the cache files from the last day
        for filename in cache_files_list:
            try:
                date_file = datetime.datetime(year=int(filename.split('-')[1]),
                                              month=int(filename.split('-')[2]),
                                              day=int(filename.split('-')[3]))
            except:
                continue
            if date_file < date_now:
                try:
                    shutil.rmtree(cache_dir + '\\' + filename)
                except OSError:  # Windows wonkiness
                    pass


================================================
File: dtlpy/services/service_defaults.py
================================================
import os

DATALOOP_PATH = os.environ['DATALOOP_PATH'] if 'DATALOOP_PATH' in os.environ \
    else os.path.join(os.path.expanduser('~'), '.dataloop')
DEFAULT_ENVIRONMENT = 'https://gate.dataloop.ai/api/v1'
DEFAULT_ENVIRONMENTS = {
    'https://dev-gate.dataloop.ai/api/v1':
        {'alias': 'dev',
         'audience': 'https://dataloop-development.auth0.com/api/v2/',
         'client_id': 'I4Arr9ixs5RT4qIjOGtIZ30MVXzEM4w8',
         'auth0_url': 'https://dataloop-development.auth0.com',
         'gate_url': 'https://dev-gate.dataloop.ai',
         'token': None,
         'refresh_token': None,
         'verify_ssl': True,
         'url': 'https://dev-con.dataloop.ai/'},
    'https://rc-gate.dataloop.ai/api/v1':
        {'alias': 'rc',
         'audience': 'https://dataloop-development.auth0.com/api/v2/',
         'client_id': 'I4Arr9ixs5RT4qIjOGtIZ30MVXzEM4w8',
         'auth0_url': 'https://dataloop-development.auth0.com',
         'gate_url': 'https://rc-gate.dataloop.ai',
         'token': None,
         'refresh_token': None,
         'verify_ssl': True,
         'url': 'https://rc-con.dataloop.ai/'},
    'https://custom1-gate.dataloop.ai/api/v1': {
        'gate_url': 'https://custom1-gate.dataloop.ai',
        'auth0_url': 'https://dataloop-development.auth0.com',
        'verify_ssl': True,
        'client_id': 'NEED',
        'audience': 'https://dataloop-development.auth0.com/api/v2/',
        'token': '',
        'refresh_token': '',
        'alias': 'new-dev'},
    'https://gate.dataloop.ai/api/v1': {
        'alias': 'prod',
        'audience': 'https://dataloop-production.auth0.com/userinfo',
        'client_id': 'FrG0HZga1CK5UVUSJJuDkSDqItPieWGW',
        'auth0_url': 'https://dataloop-production.auth0.com',
        'gate_url': 'https://gate.dataloop.ai',
        'token': None,
        'refresh_token': None,
        'verify_ssl': True,
        'url': 'https://console.dataloop.ai/'},
    'https://localhost:8443/api/v1': {
        'alias': 'local',
        'audience': 'https://dataloop-local.auth0.com/userinfo',
        'client_id': 'ewGhbg5brMHOoL2XZLHBzhEanapBIiVO',
        'auth0_url': 'https://dataloop-local.auth0.com',
        'gate_url': 'https://localhost:8443',
        'token': None,
        'refresh_token': None,
        'verify_ssl': False},
    'https://poc-gate.dataloop.ai/api/v1': {
        'alias': 'aws',
        'audience': 'https://dataloop-aws-poc.eu.auth0.com/api/v2/',
        'client_id': 'dHHctbFPa4TFgo1hh9Ig2Fyh71N46BEM',
        'auth0_url': 'https://dataloop-aws-poc.eu.auth0.com',
        'gate_url': 'https://poc-gate.dataloop.ai',
        'token': None,
        'refresh_token': None,
        'verify_ssl': True},
    'https://host.docker.internal:8443/api/v1': {
        'alias': 'docker_windows',
        'audience': 'https://dataloop-local.auth0.com/userinfo',
        'client_id': 'ewGhbg5brMHOoL2XZLHBzhEanapBIiVO',
        'auth0_url': 'https://dataloop-local.auth0.com',
        'gate_url': 'https://172.17.0.1:8443',
        'token': None,
        'refresh_token': None,
        'verify_ssl': False},
    'https://docker.for.mac.localhost:8443/api/v1': {
        'alias': 'minikube_local_mac',
        'audience': 'https://dataloop-local.auth0.com/userinfo',
        'client_id': 'ewGhbg5brMHOoL2XZLHBzhEanapBIiVO',
        'auth0_url': 'https://dataloop-local.auth0.com',
        'gate_url': 'https://host.docker.internal:8443',
        'token': None,
        'refresh_token': None,
        'verify_ssl': False},
    'http://kong/api/v1': {
        'alias': 'minikube',
        'audience': 'https://dataloop-local.auth0.com/userinfo',
        'client_id': 'ewGhbg5brMHOoL2XZLHBzhEanapBIiVO',
        'auth0_url': 'https://dataloop-local.auth0.com',
        'gate_url': 'http://kong',
        'token': None,
        'refresh_token': None,
        'verify_ssl': False}
}


================================================
File: dtlpy/utilities/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .converter import Converter, AnnotationFormat
from .base_package_runner import BaseServiceRunner, Progress, Context
from .videos import Videos
from .dataset_generators import DatasetGenerator
from . import local_development


================================================
File: dtlpy/utilities/base_package_runner.py
================================================
import datetime
import threading

from .. import entities


class BaseServiceRunner:
    _do_reset = False
    _auto_refresh_dtlpy_token = None
    _refresh_dtlpy_token = None
    _threads_terminated = list()
    _threads_terminated_lock = threading.Lock()
    _service_entity = None

    def do_reset(self):
        self._do_reset = True

    @staticmethod
    def ping(progress):
        progress.logger.debug('received ping at: {}'.format(datetime.datetime.now().isoformat()))
        return 'pong'

    def _terminate(self, tid):
        with self._threads_terminated_lock:
            self._threads_terminated.append(tid)

    def kill_event(self):
        ident = threading.get_ident()
        if ident in self._threads_terminated:
            with self._threads_terminated_lock:
                self._threads_terminated.pop(self._threads_terminated.index(ident))
            raise InterruptedError('Execution received termination signal')

    @property
    def service_entity(self) -> entities.Service:
        assert isinstance(self._service_entity, entities.Service), "service_entity must be a dl.Service object"
        return self._service_entity

    @service_entity.setter
    def service_entity(self, value):
        assert isinstance(value, entities.Service), "service_entity must be a dl.Service object"
        self._service_entity = value


class Progress:
    """
    Follow the event progress
    """

    def update(self,
               status=None,
               progress=0,
               message=None,
               output=None,
               duration=None,
               action=None
               ):
        """
        Update the progress flow

        :param str status: the progress status to display
        :param int progress: number of finished flow
        :param str message: the progress message to display
        :param dict output: json serializable object to update the event output
        :param float duration: the event duration
        :param str action: event action
        """
        pass


class ItemStatusEvent:
    def __init__(self, _json: dict):
        if _json is None:
            _json = dict()

        self.pipeline_id = _json.get('pipelineId', None)
        self.node_id = _json.get('nodeId', None)
        self.action = _json.get('action', None)

        status = _json.get('status', dict())
        if status is None:
            status = dict()

        self.task_id = status.get('taskId', None)
        self.assignment_id = status.get('assignmentId', None)
        self.status = status.get('status', None)
        self.creator = status.get('creator', None)
        self.timestamp = status.get('timestamp', None)


class ExecutionEventContext:
    def __init__(self, _json: dict):
        if _json is None:
            _json = dict()

        self.resource = _json.get('resource', None)
        self.source = _json.get('source', None)
        self.action = _json.get('action', None)
        self.resource_id = _json.get('resourceId', None)
        self.user_id = _json.get('userId', None)
        self.dataset_id = _json.get('datasetId', None)
        self.project_id = _json.get('projectId', None)
        self.body = _json.get('body', None)
        self.item_status_event = ItemStatusEvent(_json.get('itemStatusEvent', dict()))


class Context:
    """
    Contex of the service state
    """

    def __init__(
            self,
            service: entities.Service = None,
            package: entities.Package = None,
            project: entities.Project = None,
            event_context: dict = None,
            execution_dict: dict = None,
            progress: Progress = None,
            logger=None,
            sdk=None
    ):
        """
        A Context entity use DTLPY entities for context in a service runner


        :param dict execution_dict: the current execution dict in the state
        :param dict service: the current service entity in th state
        :param dict package: the current package entity in th state
        :param dict project: the current project entity in th state
        :param dict event_context: ExecutionEventContext json display the Execution event context
        :param dl.Progress progress: Progress object for work flow
        :param logger: logger object
        :param sdk: the dtlpy package
        """
        # dtlpy
        self._logger = logger
        self._sdk = sdk
        self._progress = progress

        self.event = ExecutionEventContext(event_context)
        if execution_dict is None:
            execution_dict = dict()
        self.execution_dict = execution_dict

        # ids
        self.trigger_id = execution_dict.get('trigger_id', None)
        self.execution_id = execution_dict.get('id', None)

        # pipeline
        pipeline = execution_dict.get('pipeline', dict())
        if pipeline is None:
            pipeline = dict()
        self.pipeline_id = pipeline.get('id', None)
        self.node_id = pipeline.get('nodeId', None)
        self.pipeline_execution_id = pipeline.get('executionId', None)

        # objects
        self._service = service
        self._package = package
        self._project = project
        self._task = None
        self._assignment = None
        self._pipeline = None
        self._node = None
        self._execution = None
        self._pipeline_execution = None

    @property
    def package(self):
        assert isinstance(self._package, entities.Package), "Missing `package` in context"
        return self._package

    @property
    def project(self):
        assert isinstance(self._project, entities.Project), "Missing `project` in context"
        return self._project

    @property
    def service(self):
        assert isinstance(self._service, entities.Service), "Missing `service` in context"
        return self._service

    @property
    def item_status_creator(self):
        return self.event.item_status_event.creator

    @property
    def item_status(self):
        return self.event.item_status_event.status

    @property
    def item_status_operation(self):
        return self.event.item_status_event.action

    @property
    def execution(self) -> entities.Execution:
        if self._execution is None:
            # noinspection PyProtectedMember
            self._execution = self.sdk.Execution.from_json(
                _json=self.execution_dict,
                client_api=self.service._client_api,
                service=self.service,
                project=self.project
            )
        return self._execution

    @property
    def task_id(self) -> str:
        return self.event.item_status_event.task_id

    @property
    def task(self) -> entities.Task:
        if self._task is None and self.task_id is not None:
            try:
                self._task = self.sdk.tasks.get(task_id=self.task_id)
            except Exception:
                self.logger.exception('Failed to get task')
        return self._task

    @property
    def assignment_id(self) -> str:
        return self.event.item_status_event.assignment_id

    @property
    def assignment(self) -> entities.Assignment:
        if self._assignment is None and self.assignment_id is not None:
            self._assignment = self.sdk.assignments.get(assignment_id=self.assignment_id)
        return self._assignment

    @property
    def pipeline(self) -> entities.Pipeline:
        if self._pipeline is None and self.pipeline_id is not None:
            self._pipeline = self.sdk.pipelines.get(pipeline_id=self.pipeline_id)
        return self._pipeline

    @property
    def node(self):
        if self._node is None and self.pipeline is not None and self.node_id is not None:
            self._node = [n for n in self.pipeline.nodes if n.node_id == self.node_id][0]
        return self._node

    @property
    def pipeline_execution(self):
        if self._pipeline_execution is None and self.pipeline_execution_id is not None:
            self._pipeline_execution = self.sdk.pipeline_executions.get(
                pipeline_execution_id=self.pipeline_execution_id,
                pipeline_id=self.pipeline_id
            )
        return self._pipeline_execution

    @property
    def sdk(self):
        if self._sdk is None:
            import dtlpy
            self._sdk = dtlpy
        return self._sdk

    @property
    def logger(self):
        if self._logger is None:
            import logging
            self._logger = logging.getLogger("[AgentContext]")
        return self._logger


================================================
File: dtlpy/utilities/annotations/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .annotation_converters import DtlpyToVoc, DtlpyToYolo


================================================
File: dtlpy/utilities/annotations/annotation_converters.py
================================================
# https://github.com/AndrewCarterUK/pascal-voc-writer
import os
import shutil
import traceback
import tempfile
import logging
from PIL import Image
import json
from jinja2 import Environment, PackageLoader
from concurrent.futures import ThreadPoolExecutor
import dtlpy as dl

logger = logging.getLogger(name='dtlpy')


class BaseConverterFromPlatform:
    def __init__(self, project_name, dataset_name, output_directory, remote_path):
        self.name = ''
        self.project_name = project_name
        self.dataset_name = dataset_name
        self.remote_path = remote_path
        self.output_directory = output_directory
        self.params = None

        # for threading outputs
        self.errors = None
        self.outputs = None
        self.results = None

    def convert_single_file(self, output_directory, item, annotations, params):
        # for user to create
        pass

    def threading_wrapper(self, func, i_item,
                          # inputs for user
                          output_directory, item, annotations, params=None):
        try:
            self.outputs[i_item] = func(output_directory=output_directory,
                                        item=item,
                                        annotations=annotations,
                                        params=params)
            self.results[i_item] = True
        except Exception as err:
            logging.exception('Error running thread conversion')
            self.errors[i_item] = traceback.format_exc()
            self.results[i_item] = False

    def run(self):
        # create temp path to save dataloop annotations
        local_annotations_path = os.path.join(tempfile.gettempdir(),
                                              'dataloop_annotations_{}'.format(hash(os.times())))
        if os.path.isdir(local_annotations_path):
            raise IsADirectoryError('path already exists')

        try:
            # download annotations zip to local directory
            project = dl.projects.get(project_name=self.project_name)
            dataset = project.datasets.get(dataset_name=self.dataset_name)

            dataset.items.download_annotations(dataset_name=self.dataset_name,
                                               local_path=os.path.join(local_annotations_path, '*'))

            # get labels to ids dictionary
            if 'labels_dict' not in self.params:
                self.params['labels_dict'] = {label: i_label for i_label, label in
                                              enumerate(list(dataset.labels.keys()))}

            output_annotations_path = os.path.join(self.output_directory, 'annotations')
            # create output directories
            if not os.path.isdir(self.output_directory):
                os.makedirs(self.output_directory)
            if not os.path.isdir(output_annotations_path):
                os.makedirs(output_annotations_path)

            # save labels
            with open(os.path.join(self.output_directory, 'labels.txt'), 'w') as f:
                f.write('\n'.join(['%s:%s' % (val, key) for key, val in self.params['labels_dict'].items()]))

            # get all items (for width and height)
            filters = dl.Filters()
            filters.add(field='filename', values=self.remote_path)
            filters.add(field='type', values='file')
            pages = dataset.items.list(filters=filters)

            # init workers and results lists
            pool = ThreadPoolExecutor(max_workers=32)
            i_item = -1
            num_items = pages.items_count
            self.outputs = [None for _ in range(num_items)]
            self.results = [None for _ in range(num_items)]
            self.errors = [None for _ in range(num_items)]

            for page in pages:
                for item in page:
                    i_item += 1
                    # create input annotations json
                    in_filepath = os.path.join(local_annotations_path, item.filename[1:])
                    name, ext = os.path.splitext(in_filepath)
                    in_filepath = name + '.json'

                    # check if annotations file exists
                    if not os.path.isfile(in_filepath):
                        self.results[i_item] = False
                        self.errors[i_item] = 'file not found: %s' % in_filepath
                        continue

                    with open(in_filepath, 'r', encoding="utf8") as f:
                        data = json.load(f)

                    pool.submit(self.threading_wrapper, **{'func': self.convert_single_file,
                                                           'i_item': i_item,
                                                           # input for "func"
                                                           'output_directory': output_annotations_path,
                                                           'item': item,
                                                           'annotations': data['annotations'],
                                                           'params': self.params})
            print('Done')
            pool.shutdown()
            dummy = [logger.error(self.errors[i_job]) for i_job, suc in enumerate(self.results) if suc is False]
            return self.outputs
        except:
            raise
        finally:
            # cleanup
            if os.path.isdir(local_annotations_path):
                shutil.rmtree(local_annotations_path)


class DtlpyToVoc(BaseConverterFromPlatform):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # specific for voc
        labels = dict()
        # annotations template
        environment = Environment(loader=PackageLoader('dtlpy', 'assets'),
                                  keep_trailing_newline=True)
        annotation_template = environment.get_template('voc_annotation_template.xml')
        self.params = {'labels': labels,
                       'annotation_template': annotation_template}

    @staticmethod
    def new_annotation(path, width, height, depth=3, database='Unknown', segmented=0):
        abspath = os.path.abspath(path)
        annotation = {
            'path': abspath,
            'filename': os.path.basename(abspath),
            'folder': os.path.basename(os.path.dirname(abspath)),
            'width': width,
            'height': height,
            'depth': depth,
            'database': database,
            'segmented': segmented,
            'objects': list()
        }
        return annotation

    def convert_single_file(self, item, annotations, output_directory, params):
        # output filepath for xml
        out_filepath = os.path.join(output_directory, item.filename[1:])
        # remove ext from output filepath
        out_filepath, ext = os.path.splitext(out_filepath)
        # add xml extension
        out_filepath += '.xml'
        if not os.path.isdir(os.path.dirname(out_filepath)):
            os.makedirs(os.path.dirname(out_filepath), exist_ok=True)

        width = item.width
        height = item.height
        depth = item.metadata['system'].get('channels', 3)
        output_annotation = {
            'path': item.filename,
            'filename': os.path.basename(item.filename),
            'folder': os.path.basename(os.path.dirname(item.filename)),
            'width': width,
            'height': height,
            'depth': depth,
            'database': 'Unknown',
            'segmented': 0,
            'objects': list()
        }

        for annotation in annotations:
            if not annotation:
                continue
            if annotation['type'] != 'box':
                continue
            label = annotation['label']
            coordinates = annotation['coordinates']

            attributes = list()
            # get attributes if exists
            if 'attributes' in annotation:
                attributes = annotation['attributes']

            try:
                left = int(coordinates[0]['x'])
                top = int(coordinates[0]['y'])
                right = int(coordinates[1]['x'])
                bottom = int(coordinates[1]['y'])
            except Exception as err:
                print('coordinates', coordinates)
                continue

            output_annotation['objects'].append({'name': label,
                                                 'xmin': left,
                                                 'ymin': top,
                                                 'xmax': right,
                                                 'ymax': bottom,
                                                 'attributes': attributes,
                                                 })
        with open(out_filepath, 'w') as file:
            content = params['annotation_template'].render(**output_annotation)
            file.write(content)


class DtlpyToYolo(BaseConverterFromPlatform):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.params = {'labels': dict()}

    @staticmethod
    def convert_bb(size, box):
        dw = 1. / size[0]
        dh = 1. / size[1]
        x = (box[0] + box[1]) / 2.0
        y = (box[2] + box[3]) / 2.0
        w = box[1] - box[0]
        h = box[3] - box[2]
        x = x * dw
        w = w * dw
        y = y * dh
        h = h * dh
        return x, y, w, h

    @staticmethod
    def convert_single_file(item, annotations, output_directory, params):
        # output filepath for xml
        out_filepath = os.path.join(output_directory, item.filename[1:])
        # remove ext from filepath
        out_filepath, ext = os.path.splitext(out_filepath)
        # add txt extension
        out_filepath += '.txt'

        if not os.path.isdir(os.path.dirname(out_filepath)):
            os.makedirs(os.path.dirname(out_filepath), exist_ok=True)

        width = item.width
        height = item.height
        yolo_annotations = list()
        for annotation in annotations:
            if annotation['type'] != 'box':
                continue
            label = annotation['label']
            coordinates = annotation['coordinates']
            try:
                left = int(coordinates[0]['x'])
                top = int(coordinates[0]['y'])
                right = int(coordinates[1]['x'])
                bottom = int(coordinates[1]['y'])
            except Exception as err:
                print('coords', coordinates)
                continue
            yolo_bb = DtlpyToYolo.convert_bb((width, height), (left, right, top, bottom))
            yolo_annotations.append(
                '%d %f %f %f %f' % (params['labels_dict'][label], yolo_bb[0], yolo_bb[1], yolo_bb[2], yolo_bb[3]))

        with open(out_filepath, 'w') as f:
            f.write('\n'.join(yolo_annotations))


================================================
File: dtlpy/utilities/dataset_generators/__init__.py
================================================
from .dataset_generator import DatasetGenerator, collate_default


================================================
File: dtlpy/utilities/dataset_generators/dataset_generator.py
================================================
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from PIL import Image
import collections.abc
import numpy as np
import collections
import logging
import shutil
import json
import copy
import tqdm
import sys
import os
import re
from ... import entities

logger = logging.getLogger(name='dtlpy')


class DataItem(dict):
    def __init__(self, *args, **kwargs):
        super(DataItem, self).__init__(*args, **kwargs)

    @property
    def image_filepath(self):
        return self['image_filepath']

    @image_filepath.setter
    def image_filepath(self, val):
        self['image_filepath'] = val


class DatasetGenerator:

    def __init__(self,
                 dataset_entity: entities.Dataset,
                 annotation_type: entities.AnnotationType,
                 item_type: list = None,
                 filters: entities.Filters = None,
                 data_path=None,
                 overwrite=False,
                 id_to_label_map=None,
                 label_to_id_map=None,
                 transforms=None,
                 transforms_callback=None,
                 num_workers=0,
                 batch_size=None,
                 collate_fn=None,
                 shuffle=True,
                 seed=None,
                 to_categorical=False,
                 to_mask=False,
                 class_balancing=False,
                 # debug flags
                 return_originals=False,
                 ignore_empty=True
                 ) -> None:
        """
        Base Dataset Generator to build and iterate over images and annotations

        * Mapping Labels *
        To set a label mapping from labels to id you can use the `label_to_id_map` or `id_to_label_map`.
        NOTE: if they are not i.i.d you'll need to input both.
        In semantic, a `$default` label should be added so that the background (and all unlabeled pixels) will be
        mapped to the model's inputs

        label_to_id_map = {'cat': 1,
                           'dog': 1,
                           '$default': 0}
        id_to_label_map = {1: 'cats_and_dogs',
                           0: 'background'}

        :param dataset_entity: dl.Dataset entity
        :param annotation_type: dl.AnnotationType - type of annotation to load from the annotated dataset
        :param item_type: list of file extension to load. default: ['jpg', 'jpeg', 'png', 'bmp']
        :param filters: dl.Filters - filtering entity to filter the dataset items
        :param data_path: Path to Dataloop annotations (root to "item" and "json").
        :param overwrite:
        :param dict id_to_label_map: Optional, {id: label_string} dictionary, default taken from dataset
        :param dict label_to_id_map: Optional, {label_string: id} dictionary
        :param transforms: Optional transform to be applied on a sample. list, imgaug.Sequence or torchvision.transforms.Compose
        :param transforms_callback: Optional function to handle the callback of each batch.
        look at default_transforms_callback for more information. available: imgaug_transforms_callback, torchvision_transforms_callback
        :param num_workers: Optional - number of separate threads to load the images
        :param batch_size: (int, optional): how many samples per batch to load, if not none - items will always be a list
        :param collate_fn: Optional - merges a list of samples to form a mini-batch of Tensor(s).
        :param shuffle: Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.
        :param seed: Optional random seed for shuffling and transformations.
        :param to_categorical: convert label id to categorical format
        :param to_mask: convert annotations to an instance mask (will be true for SEGMENTATION)
        :param class_balancing: if True - performing random over-sample with class ids as the target to balance training data
        :param return_originals: bool - If True, return ALSO images and annotations before transformations (for debug)
        :param ignore_empty: bool - If True, generator will NOT collect items without annotations
        """
        self._dataset_entity = dataset_entity

        # default item types (extension for now)
        if item_type is None:
            item_type = ['jpg', 'jpeg', 'png', 'bmp']
        if not isinstance(item_type, list):
            item_type = [item_type]
        self.item_type = item_type

        # id labels mapping
        if label_to_id_map is None and id_to_label_map is None:
            # if both are None - take from dataset
            label_to_id_map = dataset_entity.instance_map
            id_to_label_map = {int(v): k for k, v in label_to_id_map.items()}
        else:
            # one or both is NOT None
            if label_to_id_map is None:
                # set label_to_id_map from the other
                label_to_id_map = {v: int(k) for k, v in id_to_label_map.items()}
            if id_to_label_map is None:
                # set id_to_label_map from the other
                id_to_label_map = {int(v): k for k, v in label_to_id_map.items()}
            # put it on the local ontology for the annotations download
            dataset_entity._get_ontology().instance_map = label_to_id_map
        self.id_to_label_map = id_to_label_map
        self.label_to_id_map = label_to_id_map

        # if annotation type is segmentation - to_mask must be True
        if annotation_type == entities.AnnotationType.SEGMENTATION:
            to_mask = True

        if data_path is None:
            data_path = os.path.join(os.path.expanduser('~'),
                                     '.dataloop',
                                     'datasets',
                                     "{}_{}".format(dataset_entity.name,
                                                    dataset_entity.id))
        download = False
        if os.path.isdir(data_path):
            if overwrite:
                logger.warning('overwrite flag is True! deleting and overwriting')
                shutil.rmtree(data_path)
                download = True
        else:
            download = True
        if download:
            annotation_options = [entities.ViewAnnotationOptions.JSON]
            if to_mask is True:
                annotation_options.append(entities.ViewAnnotationOptions.INSTANCE)
            _ = dataset_entity.items.download(filters=filters,
                                              local_path=data_path,
                                              thickness=-1,
                                              annotation_options=annotation_options)
        self.root_dir = data_path
        self._items_path = Path(self.root_dir).joinpath('items')
        self._json_path = Path(self.root_dir).joinpath('json')
        self._mask_path = Path(self.root_dir).joinpath('instance')
        self._transforms = transforms
        self._transforms_callback = transforms_callback
        if self._transforms is not None and self._transforms_callback is None:
            # use default callback
            self._transforms_callback = default_transforms_callback

        self.annotation_type = annotation_type
        self.num_workers = num_workers
        self.to_categorical = to_categorical
        self.num_classes = len(label_to_id_map)
        self.shuffle = shuffle
        self.seed = seed
        self.to_mask = to_mask
        self.batch_size = batch_size
        self.collate_fn = collate_fn
        self.class_balancing = class_balancing
        # inits
        self.data_items = list()
        # flags
        self.return_originals = return_originals
        self.ignore_empty = ignore_empty

        ####################
        # Load annotations #
        ####################
        self.load_annotations()

    @property
    def dataset_entity(self):
        assert isinstance(self._dataset_entity, entities.Dataset)
        return self._dataset_entity

    @dataset_entity.setter
    def dataset_entity(self, val):
        assert isinstance(val, entities.Dataset)
        self._dataset_entity = val

    @property
    def n_items(self):
        return len(self.data_items)

    def _load_single(self, image_filepath, pbar=None):
        try:
            is_empty = False
            item_info = DataItem()
            # add image path
            item_info.image_filepath = str(image_filepath)
            if os.stat(image_filepath).st_size < 5:
                logger.warning('IGNORING corrupted image: {!r}'.format(image_filepath))
                return None, True
            # get "platform" path
            rel_path = image_filepath.relative_to(self._items_path)
            # replace suffix to JSON
            rel_path_wo_png_ext = rel_path.with_suffix('.json')
            # create local path
            annotation_filepath = Path(self._json_path, rel_path_wo_png_ext)

            if os.path.isfile(annotation_filepath):
                with open(annotation_filepath, 'r') as f:
                    data = json.load(f)
                    if 'id' in data:
                        item_id = data.get('id')
                    elif '_id' in data:
                        item_id = data.get('_id')
                    annotations = entities.AnnotationCollection.from_json(data)
            else:
                item_id = ''
                annotations = None
            item_info.update(item_id=item_id)
            if self.annotation_type is not None:
                # add item id from json
                polygon_coordinates = list()
                box_coordinates = list()
                classes_ids = list()
                labels = list()
                if annotations is not None:
                    for annotation in annotations:
                        if 'user' in annotation.metadata and \
                                'model' in annotation.metadata['user']:
                            # and 'name' in annotation.metadata['user']['model']:
                            # Do not use prediction annotations in the data generator
                            continue
                        if annotation.type == self.annotation_type:
                            if annotation.label not in self.label_to_id_map:
                                logger.warning(
                                    'Missing label {!r} in label_to_id_map. Skipping.. Use label_to_id_map for other behaviour'.format(
                                        annotation.label))
                            else:
                                classes_ids.append(self.label_to_id_map[annotation.label])
                            labels.append(annotation.label)
                            box_coordinates.append(np.asarray([annotation.left,
                                                               annotation.top,
                                                               annotation.right,
                                                               annotation.bottom]))
                            if self.annotation_type == entities.AnnotationType.POLYGON:
                                polygon_coordinates.append(annotation.geo)
                            if annotation.type not in [entities.AnnotationType.CLASSIFICATION,
                                                       entities.AnnotationType.SEGMENTATION,
                                                       entities.AnnotationType.BOX,
                                                       entities.AnnotationType.POLYGON]:
                                raise ValueError('unsupported annotation type: {}'.format(annotation.type))
                dtype = object if self.annotation_type == entities.AnnotationType.POLYGON else None
                # reorder for output
                item_info.update({entities.AnnotationType.BOX.value: np.asarray(box_coordinates).astype(float),
                                  entities.AnnotationType.CLASSIFICATION.value: np.asarray(classes_ids),
                                  entities.AnnotationType.POLYGON.value: np.asarray(polygon_coordinates, dtype=dtype),
                                  'labels': labels})
                if len(item_info[entities.AnnotationType.CLASSIFICATION.value]) == 0:
                    logger.debug('Empty annotation (nothing matched label_to_id_map) for image filename: {}'.format(
                        image_filepath))
                    is_empty = True
            if self.to_mask:
                # get "platform" path
                rel_path = image_filepath.relative_to(self._items_path)
                # replace suffix to PNG
                rel_path_wo_png_ext = rel_path.with_suffix('.png')
                # create local path
                mask_filepath = Path(self._mask_path, rel_path_wo_png_ext)
                if not os.path.isfile(mask_filepath):
                    logger.debug('Empty annotation for image filename: {}'.format(image_filepath))
                    is_empty = True
                item_info.update({entities.AnnotationType.SEGMENTATION.value: str(mask_filepath)})
            item_info.update(annotation_filepath=str(annotation_filepath))
            return item_info, is_empty
        except Exception:
            logger.exception('failed loading item in generator! {!r}'.format(image_filepath))
            return None, True
        finally:
            if pbar is not None:
                pbar.update()

    def load_annotations(self):
        logger.info(f"Collecting items with the following extensions: {self.item_type}")
        files = list()
        for ext in self.item_type:
            # build regex to ignore extension case
            regex = '*.{}'.format(''.join(['[{}{}]'.format(letter.lower(), letter.upper()) for letter in ext]))
            files.extend(self._items_path.rglob(regex))

        pool = ThreadPoolExecutor(max_workers=32)
        jobs = list()
        pbar = tqdm.tqdm(total=len(files),
                         desc='Loading Data Generator',
                         disable=self.dataset_entity._client_api.verbose.disable_progress_bar,
                         file=sys.stdout)
        for image_filepath in files:
            jobs.append(pool.submit(self._load_single,
                                    image_filepath=image_filepath,
                                    pbar=pbar))
        outputs = [job.result() for job in jobs]
        pbar.close()

        n_items = len(outputs)
        n_empty_items = sum([1 for _, is_empty in outputs if is_empty is True])

        output_msg = 'Done loading items. Total items loaded: {}.'.format(n_items)
        if n_empty_items > 0:
            output_msg += '{action} {n_empty_items} items without annotations'.format(
                action='IGNORING' if self.ignore_empty else 'INCLUDING',
                n_empty_items=n_empty_items)

        if self.ignore_empty:
            # take ONLY non-empty
            data_items = [data_item for data_item, is_empty in outputs if is_empty is False]
        else:
            # take all
            data_items = [data_item for data_item, is_empty in outputs]

        self.data_items = data_items
        if len(self.data_items) == 0:
            logger.warning(output_msg)
        else:
            logger.info(output_msg)
        ###################
        # class balancing #
        ###################
        labels = [label for item in self.data_items for label in item.get('labels', list())]
        logger.info(f"Data Generator labels balance statistics: {collections.Counter(labels)}")
        if self.class_balancing:
            try:
                from imblearn.over_sampling import RandomOverSampler
            except Exception:
                logger.error(
                    'Class balancing is ON but missing "imbalanced-learn". run "pip install -U imbalanced-learn" and try again')
                raise
            logger.info('Class balance is on!')
            class_ids = [class_id for item in self.data_items for class_id in item['class']]
            dummy_inds = [i_item for i_item, item in enumerate(self.data_items) for _ in item['class']]
            over_sampler = RandomOverSampler(random_state=42)
            X_res, y_res = over_sampler.fit_resample(np.asarray(dummy_inds).reshape(-1, 1), np.asarray(class_ids))
            over_sampled_data_items = [self.data_items[i] for i in X_res.flatten()]
            oversampled_labels = [label for item in over_sampled_data_items for label in item['labels']]
            logger.info(f"Data Generator labels after oversampling: {collections.Counter(oversampled_labels)}")
            self.data_items = over_sampled_data_items

        if self.shuffle:
            if self.seed is None:
                self.seed = 256
            np.random.seed(self.seed)
            np.random.shuffle(self.data_items)

    def transform(self, image, target=None):
        if self._transforms is not None:
            image, target = self._transforms_callback(transforms=self._transforms,
                                                      image=image,
                                                      target=target,
                                                      annotation_type=self.annotation_type)
        return image, target

    def _to_dtlpy(self, targets, labels=None):
        annotations = entities.AnnotationCollection(item=None)
        annotations._dataset = self._dataset_entity
        if labels is None:
            labels = [None] * len(targets)
        if self.to_mask is True:
            for label, label_ind in self.label_to_id_map.items():
                target = targets == label_ind
                if np.any(target):
                    annotations.add(annotation_definition=entities.Segmentation(geo=target,
                                                                                label=label))
        elif self.annotation_type == entities.AnnotationType.BOX:
            for target, label in zip(targets, labels):
                annotations.add(annotation_definition=entities.Box(left=target[0],
                                                                   top=target[1],
                                                                   right=target[2],
                                                                   bottom=target[3],
                                                                   label=label))
        elif self.annotation_type == entities.AnnotationType.CLASSIFICATION:
            for target, label in zip(targets, labels):
                annotations.add(annotation_definition=entities.Classification(label=label))
        elif self.annotation_type == entities.AnnotationType.POLYGON:
            for target, label in zip(targets, labels):
                annotations.add(annotation_definition=entities.Polygon(label=label,
                                                                       geo=target.astype(float)))
        else:
            raise ValueError('unsupported annotation type: {}'.format(self.annotation_type))
        # set dataset for color
        for annotation in annotations:
            annotation._dataset = self._dataset_entity
        return annotations

    def visualize(self, idx=None, return_output=False, plot=True):
        if not self.__len__():
            raise ValueError('no items selected, cannot preform visualization')
        import matplotlib.pyplot as plt
        if idx is None:
            idx = np.random.randint(self.__len__())
        if self.batch_size is not None:
            raise ValueError('can visualize only of batch_size in None')
        data_item = self.__getitem__(idx)
        image = Image.fromarray(data_item.get('image'))
        labels = data_item.get('labels')
        targets = data_item.get('annotations')
        annotations = self._to_dtlpy(targets=targets, labels=labels)
        mask = Image.fromarray(annotations.show(height=image.size[1],
                                                width=image.size[0],
                                                alpha=0.8))
        image.paste(mask, (0, 0), mask)
        marked_image = np.asarray(image)
        if plot:
            plt.figure()
            plt.imshow(marked_image)
        if return_output:
            return marked_image, annotations

    def __getsingleitem__(self, idx):
        data_item = copy.deepcopy(self.data_items[idx])

        image_filename = data_item.get('image_filepath')
        image = np.asarray(Image.open(image_filename))
        data_item.update({'image': image})

        annotations = data_item.get(self.annotation_type)
        if self.to_mask is True:
            # if segmentation - read from file
            mask_filepath = data_item.get(entities.AnnotationType.SEGMENTATION)
            annotations = np.asarray(Image.open(mask_filepath).convert('L'))
        if self.to_categorical:
            onehot = np.zeros((annotations.size, self.num_classes + 1))
            onehot[np.arange(annotations.size), annotations] = 1
            annotations = onehot
        data_item.update({'annotations': annotations})

        if self.return_originals is True:
            annotations = []
            if self.annotation_type is not None:
                annotations = data_item.get('annotations')
            data_item.update({'orig_image': image.copy(),
                              'orig_annotations': annotations.copy()})

        ###########################
        # perform transformations #
        ###########################
        if self._transforms is not None:
            annotations = data_item.get('annotations')
            image, annotations = self.transform(image, annotations)
            data_item.update({'image': image,
                              'annotations': annotations})
        return data_item

    def __iter__(self):
        """Create a generator that iterate over the Sequence."""
        for item in (self[i] for i in range(len(self))):
            yield item

    def __len__(self):
        factor = self.batch_size
        if factor is None:
            factor = 1
        return int(np.ceil(self.n_items / factor))

    def __getitem__(self, idx):
        """
            Support single index or a slice.
            Uses ThreadPoolExecutor is num_workers != 0
        """
        to_return = None
        if isinstance(idx, int):
            if self.batch_size is None:
                to_return = self.__getsingleitem__(idx)
            else:
                # if batch_size is define, convert idx to batches
                idx = slice(idx * self.batch_size, min((idx + 1) * self.batch_size, len(self.data_items)))

        if isinstance(idx, slice):
            to_return = list()
            idxs = list(range(idx.start, idx.stop,
                              idx.step if idx.step else 1))
            if self.num_workers == 0:
                for dx in idxs:
                    to_return.append(self.__getsingleitem__(dx))
            else:
                with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                    for sample in executor.map(lambda i: self.__getsingleitem__(i), idxs):
                        to_return.append(sample)

        if to_return is None:
            raise TypeError('unsupported indexing: list indices must be integers or slices, not {}'.format(type(idx)))

        if self.collate_fn is not None:
            to_return = self.collate_fn(to_return)
        return to_return


np_str_obj_array_pattern = re.compile(r'[SaUO]')

default_collate_err_msg_format = (
    "default_collate: batch must contain tensors, numpy arrays, numbers, "
    "dicts or lists; found {}")


def default_transforms_callback(transforms, image, target, annotation_type):
    """
    Recursive call to perform the augmentations in "transforms"

    :param transforms:
    :param image:
    :param target:
    :param annotation_type:
    :return:
    """
    # get the type string without importing any other package
    transforms_type = type(transforms)

    ############
    # Handle compositions and lists of augmentations with a recursive call
    if transforms_type.__module__ == 'torchvision.transforms.transforms' and transforms_type.__name__ == 'Compose':
        # torchvision compose - convert to list
        image, target = default_transforms_callback(transforms.transforms, image, target, annotation_type)
        return image, target

    if transforms_type.__module__ == 'imgaug.augmenters.meta' and transforms_type.__name__ == 'Sequential':
        # imgaug sequential - convert to list
        image, target = default_transforms_callback(list(transforms), image, target, annotation_type)
        return image, target

    if isinstance(transforms, list):
        for t in transforms:
            image, target = default_transforms_callback(t, image, target, annotation_type)
        return image, target

    ##############
    # Handle single annotations
    if 'imgaug.augmenters' in transforms_type.__module__:
        # handle single imgaug augmentation
        if target is not None and annotation_type is not None:
            # works for batch but running on a single image
            if annotation_type == entities.AnnotationType.BOX:
                image, target = transforms(images=[image], bounding_boxes=[target])
                target = target[0]
            elif annotation_type == entities.AnnotationType.SEGMENTATION:
                # expending to HxWx1 for the imgaug function to work
                target = target[..., None]
                image, target = transforms(images=[image], segmentation_maps=[target])
                target = target[0][:, :, 0]
            elif annotation_type == entities.AnnotationType.POLYGON:
                image, target = transforms(images=[image], polygons=[target])
                target = target[0]
            elif annotation_type == entities.AnnotationType.CLASSIFICATION:
                image = transforms(images=[image])
            else:
                raise ValueError('unsupported annotations type for image augmentations: {}'.format(annotation_type))
            image = image[0]
        else:
            image = transforms(images=[image])
            image = image[0]
    else:
        image = transforms(image)

    return image, target


def collate_default(batch):
    r"""Puts each data field into a tensor with outer dimension batch size"""
    elem = batch[0]
    elem_type = type(elem)
    if isinstance(elem, np.ndarray):
        return np.stack(batch, axis=0)
    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' and elem_type.__name__ != 'string_':
        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':
            # array of string classes and object
            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:
                raise TypeError(default_collate_err_msg_format.format(elem.dtype))
            return batch
            # return [tf.convert_to_tensor(b) for b in batch]
        elif elem.shape == ():  # scalars
            return batch
    elif isinstance(elem, float):
        return batch
    elif isinstance(elem, int):
        return batch
    elif isinstance(elem, str) or isinstance(elem, bytes) or elem is None:
        return batch
    elif isinstance(elem, collections.abc.Mapping):
        return {key: collate_default([d[key] for d in batch]) for key in elem}
    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
        return elem_type(*(collate_default(samples) for samples in zip(*batch)))
    elif isinstance(elem, collections.abc.Sequence):
        transposed = zip(*batch)
        return transposed
    raise TypeError(default_collate_err_msg_format.format(elem_type))


def collate_torch(batch):
    r"""Puts each data field into a tensor with outer dimension batch size"""
    import torch
    elem = batch[0]
    elem_type = type(elem)
    if isinstance(elem, torch.Tensor):
        out = None
        if torch.utils.data.get_worker_info() is not None:
            # If we're in a background process, concatenate directly into a
            # shared memory tensor to avoid an extra copy
            numel = sum(x.numel() for x in batch)
            storage = elem.storage()._new_shared(numel)
            out = elem.new(storage)
        return torch.stack(batch, 0, out=out)
    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' and elem_type.__name__ != 'string_':
        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':
            # array of string classes and object
            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:
                raise TypeError(default_collate_err_msg_format.format(elem.dtype))
            try:
                return torch.stack([torch.as_tensor(b) for b in batch])
            except RuntimeError:
                return batch
        elif elem.shape == ():  # scalars
            return torch.as_tensor(batch)
    elif isinstance(elem, float):
        return torch.tensor(batch, dtype=torch.float64)
    elif isinstance(elem, int):
        return torch.tensor(batch)
    elif isinstance(elem, str) or isinstance(elem, bytes) or elem is None:
        return batch
    elif isinstance(elem, collections.abc.Mapping):
        return {key: collate_torch([d[key] for d in batch]) for key in elem}
    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
        return elem_type(*(collate_torch(samples) for samples in zip(*batch)))
    elif isinstance(elem, collections.abc.Sequence):
        transposed = zip(*batch)
        return transposed

    raise TypeError(default_collate_err_msg_format.format(elem_type))


def collate_tf(batch):
    r"""Puts each data field into a tensor with outer dimension batch size"""
    import tensorflow as tf
    elem = batch[0]
    elem_type = type(elem)
    if isinstance(elem, tf.Tensor):
        return tf.stack(batch, axis=0)
    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' and elem_type.__name__ != 'string_':
        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':
            # array of string classes and object
            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:
                raise TypeError(default_collate_err_msg_format.format(elem.dtype))
            try:
                return tf.convert_to_tensor(batch)
            except ValueError:
                # failed on orig_image because of a mismatch in the shape (not resizing all the images so cannot stack)
                return batch
                # return [tf.convert_to_tensor(b) for b in batch]
        elif elem.shape == ():  # scalars
            return tf.convert_to_tensor(batch)
    elif isinstance(elem, float):
        return tf.convert_to_tensor(batch, dtype=tf.float64)
    elif isinstance(elem, int):
        return tf.convert_to_tensor(batch)
    elif isinstance(elem, str) or isinstance(elem, bytes) or elem is None:
        return batch
    elif isinstance(elem, collections.abc.Mapping):
        return {key: collate_tf([d[key] for d in batch]) for key in elem}
    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
        return elem_type(*(collate_tf(samples) for samples in zip(*batch)))
    elif isinstance(elem, collections.abc.Sequence):
        transposed = zip(*batch)
        return transposed
    raise TypeError(default_collate_err_msg_format.format(elem_type))


================================================
File: dtlpy/utilities/dataset_generators/dataset_generator_tensorflow.py
================================================
import logging

from .dataset_generator import DatasetGenerator

logger = logging.getLogger(name='dtlpy')

try:
    import tensorflow
except (ImportError, ModuleNotFoundError):
    logger.error('Failed importing tensorflow package, cannot use DatasetGeneratorTensorflow')


class DatasetGeneratorTensorflow(DatasetGenerator, tensorflow.keras.utils.Sequence):
    def __getitem__(self, item):
        batch = super(DatasetGeneratorTensorflow, self).__getitem__(item)
        x = batch['image']
        y = batch['annotations']
        return x, y

    def __iter__(self):
        """Create a generator that iterate over the Sequence."""
        for item in (self[i] for i in range(len(self))):
            yield item


================================================
File: dtlpy/utilities/dataset_generators/dataset_generator_torch.py
================================================
import logging
from .dataset_generator import DatasetGenerator

logger = logging.getLogger(name='dtlpy')

try:
    import torch
    from torch.utils.data import Dataset
except (ImportError, ModuleNotFoundError):
    logger.error('Failed importing torch package, cannot use DatasetGeneratorTorch')


class DatasetGeneratorTorch(DatasetGenerator, Dataset):
    """

    """

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        return super(DatasetGeneratorTorch, self).__getitem__(idx)


================================================
File: dtlpy/utilities/local_development/__init__.py
================================================
from .local_session import start_session, stop_session, pause_session


================================================
File: dtlpy/utilities/local_development/local_session.py
================================================
import requests
import logging
import shutil
import time
import sys
import os

import dtlpy as dl

logger = logging.getLogger('dtlpy')
DOCKER_CONTAINER_NAME = 'dtlpy-development'


def spinning_cursor():
    while True:
        for cursor in ["(●             ●)",
                       "( ●           ● )",
                       "(  ●         ●  )",
                       "(   ●       ●   )",
                       "(    ●     ●    )",
                       "(     ●   ●     )",
                       "(      ● ●      )",
                       "(       ●       )",
                       "(      ● ●      )",
                       "(     ●   ●     )",
                       "(    ●     ●    )",
                       "(   ●       ●   )",
                       "(  ●         ●  )",
                       "( ●           ● )",
                       "(●             ●)",
                       ]:
            yield cursor


def copy_files_to_docker_container():
    from distutils.dir_util import copy_tree
    local_state_directory = os.path.dirname(dl.client_api.state_io.COOKIE)
    root_dir = os.path.dirname(local_state_directory)
    dtlpy_code_server_dir = os.path.join(dl.__path__[0], 'assets', 'code_server')
    local_code_server_dir = os.path.join(local_state_directory, 'code_server')
    copy_tree(src=dtlpy_code_server_dir, dst=local_code_server_dir)
    if not os.path.isdir(os.path.join(root_dir, '.vscode')):
        os.makedirs(os.path.join(root_dir, '.vscode'))
        shutil.copy(src=os.path.join(dl.__path__[0], 'assets', 'code_server', 'launch.json'),
                    dst=os.path.join(root_dir, '.vscode', 'launch.json'))
        shutil.copy(src=os.path.join(dl.__path__[0], 'assets', 'code_server', 'settings.json'),
                    dst=os.path.join(root_dir, '.vscode', 'settings.json'))


def get_container(client):
    try:
        container = client.containers.get(container_id=DOCKER_CONTAINER_NAME)
    except Exception:
        container = None
    return container


def start_session():
    try:
        import docker
    except (ImportError, ModuleNotFoundError):
        logger.error(
            'Import Error! Cant import "docker". Must install docker package for local development. run "pip install docker"')
        raise

    try:
        client = docker.from_env()
        client.ping()
    except Exception:
        raise RuntimeError(
            'Cannot communicate with docker daemon. Make sure everything is up and ready (docker client ping)'
        ) from None
    copy_files_to_docker_container()
    logger.info('Starting local development session')
    development = dl.client_api.state_io.get('development')
    port = development['port']
    docker_image = development['docker_image']
    hostname = f"http://localhost:{port}"
    logger.info(f'Starting docker image: {docker_image}, to {hostname}')

    need_to_create_container = True
    container = get_container(client=client)
    if container is not None:
        if not any([tag == docker_image for tag in container.image.tags]):
            # container with different image already running - delete it
            logger.info(f'Found a running image: {container.image.tags!r}, replacing to {docker_image!r}')
            container.stop()
            container.remove()
            need_to_create_container = True
        else:
            # container with SAME image already running - verify is running
            need_to_create_container = False

    if need_to_create_container:
        logger.info(f'Creating a container from docker image: {docker_image}')
        container = client.containers.run(docker_image,
                                          name=DOCKER_CONTAINER_NAME,
                                          tty=True,
                                          volumes={os.getcwd(): {'bind': "/tmp/app", 'mode': 'rw'}},
                                          ports={port: port},
                                          detach=True,
                                          command='/bin/bash')

    logger.info('Installing vscode on in the docker container')
    container = client.containers.get(container_id=DOCKER_CONTAINER_NAME)
    if container.status in "exited":
        container.start()
    elif container.status in "paused":
        container.unpause()

    resp = container.exec_run(
        cmd=f'/tmp/app/.dataloop/code_server/installation.sh {port}',
        detach=True,
        tty=True)
    spinner = spinning_cursor()
    while True:
        try:
            resp = requests.get(hostname, verify=False, timeout=0.5)
            success = resp.ok
        except Exception:
            success = False
        # write spinner
        sys.stdout.write(f'\r{next(spinner)}')
        sys.stdout.flush()
        time.sleep(0.1)
        sys.stdout.write('\b')

        # check response to break
        if success:
            logger.info(f'VScode server is up! {hostname}')
            break

    # open webbrowser to debug
    import webbrowser
    webbrowser.open(url=hostname, new=2, autoraise=True)


def pause_session():
    try:
        import docker
    except (ImportError, ModuleNotFoundError):
        logger.error(
            'Import Error! Cant import "docker". Must install docker package for local development. run "pip install docker"')
        raise

    try:
        client = docker.from_env()
        client.ping()
    except Exception:
        raise RuntimeError(
            'Cannot communicate with docker daemon. Make sure everything is up and ready (docker client ping)'
        ) from None

    container = get_container(client)
    if container is not None:
        container.pause()
    logger.info('Local development session paused')


def stop_session():
    try:
        import docker
    except (ImportError, ModuleNotFoundError):
        logger.error(
            'Import Error! Cant import "docker". Must install docker package for local development. run "pip install docker"')
        raise

    try:
        client = docker.from_env()
        client.ping()
    except Exception:
        raise RuntimeError(
            'Cannot communicate with docker daemon. Make sure everything is up and ready (docker client ping)'
        ) from None

    container = get_container(client=client)
    if container is not None:
        container.stop()
    logger.info('Local development session stopped')


================================================
File: dtlpy/utilities/reports/__init__.py
================================================
from .report import Report
from .figures import Table, ConfusionMatrix, Doughnut, FigOptions, Bar, Hbar, Line, Pie, Scatter


================================================
File: dtlpy/utilities/reports/figures.py
================================================
from typing import Optional, Union
import pandas as pd
import numpy as np

NUMBER_TYPES = (int, float, complex)


class FigOptions:
    def __init__(self, rows_per_page: int = None, comments: dict = None, x_title: str = None, y_title: str = None,
                 colors: list = None, direction: str = None):
        self.rows_per_page = rows_per_page
        self.comments = comments
        self.x_title = x_title
        self.y_title = y_title
        self.colors = colors
        self.direction = direction

        self._validate()

    def _validate(self):
        # Mapping attribute names to their expected types
        expected_types = {
            'rows_per_page': int,
            'comments': dict,
            'x_title': str,
            'y_title': str,
            'colors': list,
            'direction': str
        }

        for attr, expected_type in expected_types.items():
            value = getattr(self, attr)
            if value is not None and not isinstance(value, expected_type):
                raise ValueError(
                    f"{attr} must be of type {expected_type.__name__}")

    def to_dict(self):
        return {
            "comments": self.comments,
            "rowsPerPage": self.rows_per_page,
            "xTitle": self.x_title,
            "yTitle": self.y_title,
            "colors": self.colors,
            "direction": self.direction,
        }


class Base:
    def __init__(self,
                 title: str,
                 type: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        self.title = title
        self.type = type
        self.title_href = title_href
        self.plot_id = plot_id
        self.labels = labels
        self.data = data
        self.options = options if options is not None and options != {} else FigOptions()

        self._validate()

    def _validate(self):
        if not isinstance(self.title, str):
            raise ValueError("title must be a string")

        if self.title_href is not None and not isinstance(self.title_href, str):
            raise ValueError("title_href must be a string or None")

        if self.plot_id is not None and not isinstance(self.plot_id, str):
            raise ValueError("plot_id must be a string or None")

        if not isinstance(self.labels, list):
            raise ValueError("labels must be a list")

        if not isinstance(self.data, (list, np.ndarray)):
            raise ValueError("data must be a list or a numpy ndarray")

        if not isinstance(self.options, FigOptions):
            raise ValueError("options must be an instance of FigOptions")

        if not isinstance(self.type, str):
            raise ValueError("type must be a string")

    def to_dict(self):
        options = self.options.to_dict()
        if isinstance(self, ConfusionMatrix):
            options["confusion"] = True

        data = self.data
        if isinstance(data, np.ndarray):
            data = data.tolist()

        return {
            "type": self.type,
            "title": self.title,
            "href": self.title_href,
            "labels": self.labels,
            "id": self.plot_id,
            "data": data,
            "options": options
        }


class Table(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        super().__init__(title=title,
                         type="table",
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)
        self._validate_table_data()

    def _validate_table_data(self):
        data, labels = self.data, self.labels
        n_cols = len(labels)

        if not isinstance(data, list):
            raise ValueError(
                'arg "data" must be a list of lists. got a {!r}'.format(type(data)))

        for i_d, d in enumerate(data):
            if not isinstance(d, list):
                raise ValueError(
                    '"data" must be a list of lists. got type {!r} at index {}'.format(type(d), i_d))
            if len(d) != n_cols:
                raise ValueError(
                    '"data" rows must be the same size as "labels": {}. got size {} at index {}'.format(n_cols, len(d),
                                                                                                        i_d))

    @classmethod
    def from_df(cls, df: pd.DataFrame, **kwargs):
        labels = df.columns.to_list()
        data = df.values.tolist()
        return cls(labels=labels, data=data, **kwargs)


class Line(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        super().__init__(title=title,
                         type="line",
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)


class Scatter(Base):
    def __init__(self,
                 title: str,
                 data: Union[list, np.ndarray],
                 labels: list = None,
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        if labels is None:
            labels = list()
        super().__init__(title=title,
                         type='scatter',
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)


class Bar(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        super().__init__(title=title,
                         type="bar",
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)


class Doughnut(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        super().__init__(title=title,
                         type="doughnut",
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)


class Pie(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        super().__init__(title=title,
                         type='pie',
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)
        self._validate_pie_data()

    def _validate_pie_data(self):
        data, labels = self.data, self.labels
        if not isinstance(data, list):
            raise ValueError(
                'arg "data" must be a list. got a {!r}'.format(type(data)))

        if len(data) != len(labels):
            raise ValueError('"data" must be the same size as "labels". got size {} and {}'.format(len(data),
                                                                                                   len(labels)))
        for i_d, d in enumerate(data):
            if not isinstance(d, NUMBER_TYPES):
                raise ValueError(
                    'all "data" fields must be number. got type {!r} at index {}'.format(type(d), i_d))


class Hbar(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None):
        super().__init__(title=title,
                         type="hbar",
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)


class ConfusionMatrix(Base):
    def __init__(self,
                 title: str,
                 labels: list,
                 data: Union[list, np.ndarray],
                 title_href: Optional[str] = None,
                 plot_id: Optional[str] = None,
                 options: Optional[FigOptions] = None,
                 href_map: Optional[list] = None,
                 color_map: Optional[np.ndarray] = None):
        self.xlabel = 'Predicted'
        self.ylabel = 'Actual'
        self.color_map = color_map
        self.href_map = href_map
        super().__init__(title=title,
                         type="table",
                         labels=labels,
                         data=data,
                         title_href=title_href,
                         plot_id=plot_id,
                         options=options)
        self._validate_confusion_matrix_data()

    def _validate_confusion_matrix_data(self):
        if self.color_map is not None and not isinstance(self.color_map, np.ndarray):
            raise ValueError('arg "color_map" must be a numpy ndarray. got a {!r}'.format(
                type(self.color_map)))
        if self.href_map is not None and not isinstance(self.href_map, list):
            raise ValueError(
                'arg "href_map" must be a list. got a {!r}'.format(type(self.href_map)))

    @staticmethod
    def _rbgtohex(r, g, b):
        return "#{0:02x}{1:02x}{2:02x}".format(int(255 * r), int(255 * g), int(255 * b))

    def to_dict(self):
        options = self.options.to_dict()
        options["confusion"] = True
        data = self.data.copy()
        if isinstance(data, np.ndarray):
            data = data.tolist()
        labels = self.labels.copy()
        for i_row, row in enumerate(data):
            for i_col, val in enumerate(row):
                color = self._rbgtohex(
                    *self.color_map[i_row, i_col, :3]) if self.color_map is not None else None
                href = self.href_map[i_row][i_col] if self.href_map is not None else None
                data[i_row][i_col] = {
                    'text': '{:.2f}'.format(val),
                    'href': href,
                    'color': color}
            row.insert(0, labels[i_row])
        labels.insert(0, f'{self.ylabel}//{self.xlabel}')
        return {
            "type": self.type,
            "title": self.title,
            "href": self.title_href,
            "labels": labels,
            "data": data,
            "options": options
        }
    # def from_df(self, df: pd.DataFrame, **kwargs):
    #     data = []
    #     for i_label, g_label in enumerate(labels):
    #         line = list()
    #         line.append(g_label)
    #         for r_label in labels:
    #             if g_label == r_label:
    #                 p = high_prob()
    #             else:
    #                 p = low_prob()
    #             line.append({'text': str(p),
    #                          'href': "https://rc-con.dataloop.ai/projects/2cb9ae90-b6e8-4d15-9016-17bacc9b7bdf/datasets/607ed8107370454e4dd3b4c7/items?view=icons&dqlFilter=%7B%22filter%22%3A+%7B%22%24and%22%3A+%5B%7B%22hidden%22%3A+false%7D%2C+%7B%22type%22%3A+%22file%22%7D%2C+%7B%22dir%22%3A+%22booking%22%7D%5D%7D%2C+%22page%22%3A+0%2C+%22pageSize%22%3A+1000%2C+%22resource%22%3A+%22items%22%2C+%22join%22%3A+%7B%22on%22%3A+%7B%22resource%22%3A+%22annotations%22%2C+%22local%22%3A+%22itemId%22%2C+%22forigen%22%3A+%22id%22%7D%2C+%22filter%22%3A+%7B%22%24and%22%3A+%5B%7B%22label%22%3A+%22immigration%22%7D%5D%7D%7D%7D",
    #                          'color': rbgtohex(*colors(p)[:3])})
    #         data.append(line)


================================================
File: dtlpy/utilities/reports/report.py
================================================
import os
import json
import numpy as np
import dtlpy as dl
import tempfile


class Report:
    def __init__(self, nrows: int = 1, ncols: int = 1):
        """
        Create a report layout with defined size
        :param int nrows: number of rows in the report json
        :param int ncols: number of columns in the report json
        """
        self.figs = np.empty(shape=(nrows, ncols), dtype=object)
        self._ncols = ncols
        self._nrows = nrows
        self._irow = 0
        self._icol = 0

    def add(self, fig, irow: int, icol: int):
        """
        Add fig to to an index location in the layout

        :param fig: input figure.
        :param int irow: row location of the added fig. should be within the layout boundaries defined in the Report init
        :param int icol: column location of the added fig. should be within the layout boundaries defined in the Report init
        :return:
        """
        if 0 > irow or irow >= self._nrows:
            raise ValueError(
                'Trying to set irow with value outside layout definition. layout rows: {}'.format(self._nrows))
        if 0 > icol or icol >= self._ncols:
            raise ValueError(
                'Trying to set icol with value outside layout definition. layout cols: {}'.format(self._ncols))
        self.figs[irow, icol] = fig.to_dict()

    def prepare(self):
        """
        Covert the layout into the json item for uploading

        :return:
        """
        return {
            "shebang": "dataloop",
            "metadata": {"dltype": "report"},
            "layout": {
                "rows": [{"cols": [fig for fig in row if fig is not None]} for row in self.figs]
                # "rows": [{"cols": [report_json]}]
            }
        }

    def upload(self, dataset: dl.Dataset, remote_name: str, remote_path: str):
        """
        Upload the report item to the platform

        :param dataset: dl.Dataset to upload the item to
        :param remote_name: name of the report. should be with .json extension
        :param remote_path: remote directory to upload the item to
        :return:
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            filepath = os.path.join(tmpdir, remote_name)
            print(filepath)
            with open(filepath, 'w') as f:
                json.dump(self.prepare(), f)
            item = dataset.items.upload(local_path=filepath,
                                        remote_path=remote_path,
                                        remote_name=remote_name,
                                        overwrite=True)
        return item


================================================
File: dtlpy/utilities/videos/__init__.py
================================================
#! /usr/bin/env python3
# This file is part of DTLPY.
#
# DTLPY is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# DTLPY is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with DTLPY.  If not, see <http://www.gnu.org/licenses/>.
from .video_player import VideoPlayer
from .videos import Videos


================================================
File: dtlpy/utilities/videos/video_player.py
================================================
import logging
import json
import time
import os
import numpy as np
import dtlpy as dl

logger = logging.getLogger(name='dtlpy')

class VideoPlayer:
    """
    Video Player GUI.
    """

    def __init__(self, project_name=None, project_id=None,
                 dataset_name=None, dataset_id=None,
                 item_filepath=None, item_id=None):
        try:
            import tkinter
        except ImportError:
            logger.error(
                'Import Error! Cant import tkinter. Annotations operations will be limited. import manually and fix errors')
            raise

        # init tkinter window
        self.window = tkinter.Tk()
        self.window.title('Dataloop Player')

        self.video_source = None
        self.video_annotations = None
        self.local_annotations_filename = None
        self.labels = None
        self.project = None
        self.dataset = None
        self.item = None
        #############################
        # load video and annotation #
        #############################
        self.platform_params = {'project_name': project_name, 'project_id': project_id,
                                'dataset_name': dataset_name, 'dataset_id': dataset_id,
                                'item_filepath': item_filepath, 'item_id': item_id}
        self.from_platform()

        # load video
        self.photo = None
        self.vid = None
        self.load_video()

        # canvas to put frames on
        self.canvas = tkinter.Canvas(self.window, width=self.vid.width, height=self.vid.height)
        self.canvas.pack()
        self.canvas.bind('<Configure>', self._resize_image)

        ########################
        # create buttons panel #
        ########################
        self.window_width = self.vid.width
        self.window_height = self.vid.height
        self.buttons_window = tkinter.Toplevel(master=self.window)
        self.buttons_window.title('Controls')
        self.buttons_window.geometry()

        button_frame = tkinter.Frame(self.buttons_window)
        button_frame.grid(sticky="W", row=0, column=0)

        ###############
        # information #
        ###############
        txt_dataset_name = tkinter.Label(button_frame)
        txt_dataset_name.grid(sticky="W", row=2, column=0, columnspan=10)
        txt_dataset_name.configure(text='Dataset name: %s' % self.dataset.name)
        txt_item_name = tkinter.Label(button_frame)
        txt_item_name.grid(sticky="W", row=3, column=0, columnspan=10)
        txt_item_name.configure(text='Item name: %s' % self.item.name)
        ###############
        # Exit Button #
        ###############
        btn_exit = tkinter.Button(button_frame, text="Close", command=self.close).grid(sticky="W", row=17, column=0)
        ################
        # Play / Pause #
        ################
        self.btn_toggle_play = tkinter.Button(button_frame, text='Play ', command=self.toggle_play)
        self.btn_toggle_play.grid(sticky="W", row=4, column=0)
        ###################
        # Next prev frame #
        ###################
        btn_next_frame = tkinter.Button(button_frame, text="Next frame", command=self.next_frame).grid(sticky="W",
                                                                                                       row=4,
                                                                                                       column=2)
        btn_prev_frame = tkinter.Button(button_frame, text="Prev frame", command=self.prev_frame).grid(sticky="W",
                                                                                                       row=4,
                                                                                                       column=3)
        self.btn_toggle_frame_num = tkinter.Button(button_frame,
                                                   text="Hide frame number",
                                                   command=self.toggle_show_frame_number)
        self.btn_toggle_frame_num.grid(sticky="W", row=5, column=0, columnspan=10)
        self.btn_toggle_annotations = tkinter.Button(button_frame,
                                                     text="Hide annotations",
                                                     command=self.toggle_show_annotations)
        self.btn_toggle_annotations.grid(sticky="W", row=6, column=0, columnspan=10)
        self.btn_toggle_label = tkinter.Button(button_frame,
                                               text="Hide label",
                                               command=self.toggle_show_label)
        self.btn_toggle_label.grid(sticky="W", row=7, column=0, columnspan=10)
        #################
        # Export button #
        #################
        tkinter.Button(button_frame, text="Export video", command=self.export) \
            .grid(sticky="W", row=16, column=2, columnspan=2)
        # ################
        # # Apply button #
        # ################
        # btn_apply_offset_fix = tkinter.Button(button_frame, text="Apply offset fix", command=self.apply_offset_fix) \
        #     .grid(sticky="W", row=16, column=0, columnspan=2)
        btn_reset = tkinter.Button(button_frame, text="Reset", command=self.reset).grid(sticky="W", row=4, column=4)
        # #####################
        # # Annotation Offset #
        # #####################
        # tkinter.Label(button_frame, text="Annotation offset (in frames):") \
        #     .grid(sticky="W", row=13, column=0,
        #           columnspan=10)
        # self.annotations_offset_entry = tkinter.Entry(button_frame)
        # self.annotations_offset_entry.bind("<Return>", self.set_annotation_offset)
        # self.annotations_offset_entry.grid(sticky="W", row=14, column=0, columnspan=10)
        # self.annotation_offset_text = tkinter.Label(button_frame)
        # self.annotation_offset_text.grid(sticky="W", row=15, column=0, columnspan=10)
        ##############
        # Set Frames #
        ##############
        tkinter.Label(button_frame, text="Set frame (in frames):") \
            .grid(sticky="W", row=10, column=0, columnspan=10)
        self.current_frame_entry = tkinter.Entry(button_frame)
        self.current_frame_entry.bind("<Return>", self.set_current_frame)
        self.current_frame_entry.grid(sticky="W", row=11, column=0, columnspan=10)
        self.current_frame_text = tkinter.Label(button_frame)
        self.current_frame_text.grid(sticky="W", row=12, column=0, columnspan=10)
        #########################
        # Timestamp information #
        #########################
        self.frame_timestamp_text = tkinter.Label(button_frame)
        self.frame_timestamp_text.grid(sticky="W", row=8, column=0, columnspan=10)
        self.annotations_timestamp_text = tkinter.Label(button_frame)
        self.annotations_timestamp_text.grid(sticky="W", row=9, column=0, columnspan=10)

        ##############
        # Parameters #
        ##############
        self.delay = None
        self.annotations_timestamp = None
        # self.annotations_offset = None
        self.annotations_timestamp = None
        self.show_frame_num = True
        self.show_annotations = True
        self.show_label = True
        self.playing = False
        self.init_all_params()
        self.show_frame(0)

        ###############
        # Start video #
        ###############
        self.delay = int(1000 * 1 / self.vid.fps)
        self.update()
        button_frame.lift()
        self.window.mainloop()

    def _resize_image(self, event):
        """
        Resize frame to match window size
        :param event:
        :return:
        """
        self.window_width = event.width
        self.window_height = event.height

    def reset(self):
        """
        Reset video and annotation
        :return:
        """
        self.from_platform()
        self.load_video()
        self.init_all_params()
        self.show_frame(0)

    def export(self):
        """
        Create an annotated video saved to file
        :return:
        """

        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        try:
            from tkinter import ttk
        except ImportError:
            logger.error(
                'Import Error! Cant import ttk from tkinter. Annotations operations will be limited. import manually and fix errors')
            raise

        # start progress bar
        p, ext = os.path.splitext(self.video_source)
        output_filename = p + '_out.mp4'

        # read input video
        reader = cv2.VideoCapture(self.video_source)
        width = int(reader.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(reader.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = reader.get(cv2.CAP_PROP_FPS)
        encoding = int(reader.get(cv2.CAP_PROP_FOURCC))
        n_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))
        writer = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))

        # create popup
        popup = tkinter.Toplevel()
        tkinter.Label(popup, text='Exporting to\n%s' % output_filename).grid(row=0, column=0)
        progress_var = tkinter.DoubleVar()
        progress_bar = ttk.Progressbar(popup, variable=progress_var, maximum=n_frames)
        progress_bar.grid(row=1, column=0)  # .pack(fill=tk.X, expand=1, side=tk.BOTTOM)
        popup.pack_slaves()

        i_frame = 0
        while reader.isOpened():
            popup.update()
            ret, frame = reader.read()
            if not ret:
                break
            # mark on frame
            annotations = [frame_annotation.get_annotation_by_frame(frame=i_frame)
                           for frame_annotation in self.video_annotations]

            for annotation in annotations:
                if annotation is None:
                    continue
                frame = annotation.show(image=frame,
                                        color=self.get_class_color(annotation.label))
                if self.show_label:
                    text = '%s-%s' % (annotation.label, ','.join(annotation.attributes))
                    frame = cv2.putText(frame,
                                        text=text,
                                        org=tuple([int(np.round(annotation.left)), int(np.round(annotation.top))]),
                                        color=(255, 0, 0),
                                        fontFace=cv2.FONT_HERSHEY_DUPLEX,
                                        fontScale=1,
                                        thickness=2)
            # write
            writer.write(frame)
            i_frame += 1
            progress_var.set(i_frame)
        reader.release()
        writer.release()
        popup.destroy()

    def load_video(self):
        """
        Load VideoCapture instance
        :return:
        """
        self.vid = VideoCapture(self.video_source)

    def init_all_params(self):
        """
        Reset all initial variables
        :return:
        """
        self.annotations_timestamp = 0
        # self.annotations_offset = 0
        # self.annotation_offset_text.configure(text='Current: %d' % self.annotations_offset)
        self.annotations_timestamp_text.configure(text='Annotation timestamp:\n %d' % self.annotations_timestamp)
        self.annotations_timestamp_text.grid(sticky="W", row=9, column=0, columnspan=10)
        # set text frames
        # self.annotations_offset_entry.delete(0, 'end')
        # self.annotations_offset_entry.insert(0, str(self.annotations_offset))
        self.current_frame_entry.delete(0, 'end')
        self.current_frame_entry.insert(0, str(self.vid.frame_number))

    def from_local(self):
        """
        Load local video and annotation
        :return:
        """
        if self.local_annotations_filename is not None:
            with open(self.local_annotations_filename, 'r') as f:
                data = json.load(f)
        self.video_annotations = dl.AnnotationCollection.from_json(data['annotations'])

    def from_platform(self):
        """
        Load video and annotations from platform
        :return:
        """
        project_name = self.platform_params['project_name']
        project_id = self.platform_params['project_id']
        dataset_name = self.platform_params['dataset_name']
        dataset_id = self.platform_params['dataset_id']
        item_filepath = self.platform_params['item_filepath']
        item_id = self.platform_params['item_id']

        # load remote item
        if dataset_id is None:
            self.project = dl.projects.get(project_name=project_name, project_id=project_id)
            if self.project is None:
                raise ValueError('Project doesnt exists. name: %s, id: %s' % (project_name, project_id))
            self.dataset = self.project.datasets.get(dataset_name=dataset_name, dataset_id=dataset_id)
        else:
            self.dataset = dl.datasets.get(dataset_id=dataset_id)
        if self.dataset is None:
            raise ValueError('Dataset doesnt exists. name: %s, id: %s' % (dataset_name, dataset_id))
        self.item = self.dataset.items.get(filepath=item_filepath, item_id=item_id)
        if self.item is None:
            raise ValueError('Item doesnt exists. name: %s, id: %s' % (item_filepath, item_id))
        self.labels = {label.tag: label.rgb for label in self.dataset.labels}
        _, ext = os.path.splitext(self.item.filename[1:])
        video_filename = os.path.join(self.dataset.__get_local_path__(), self.item.filename[1:])
        if not os.path.isdir(os.path.dirname(video_filename)):
            os.makedirs(os.path.dirname(video_filename))
        if not os.path.isfile(video_filename):
            self.item.download(local_path=os.path.dirname(video_filename), to_items_folder=False)
        self.video_source = video_filename
        self.video_annotations = self.item.annotations.list()

    def close(self):
        """
        Terminate window and application
        :return:
        """
        self.window.destroy()
        self.buttons_window.destroy()

    #
    # def set_annotation_offset(self, entry):
    #     """
    #     Set annotations offset to video start time
    #     :param entry:
    #     :return:
    #     """
    #     self.annotations_offset = int(self.annotations_offset_entry.get())
    #     self.annotation_offset_text.configure(text='Current: %d' % self.annotations_offset)

    def set_current_frame(self, entry):
        """
        Go to specific frame
        :param entry:
        :return:
        """
        input_frame = int(self.current_frame_entry.get())
        self.show_frame(frame_number=input_frame)

    def toggle_show_frame_number(self):
        """
        Show/hide frame number on frames
        :return:
        """
        if self.show_frame_num:
            self.show_frame_num = False
            self.btn_toggle_frame_num.config(text='Show frame num')
        else:
            self.show_frame_num = True
            self.btn_toggle_frame_num.config(text='Hide frame num')

    def toggle_show_annotations(self):
        """
        Show/hide annotations from frame
        :return:
        """
        if self.show_annotations:
            self.show_annotations = False
            self.btn_toggle_annotations.config(text='Show annotations')
        else:
            self.show_annotations = True
            self.btn_toggle_annotations.config(text='Hide annotations')

    def toggle_show_label(self):
        """
        Show/hide label per annotations
        :return:
        """
        if self.show_label:
            self.show_label = False
            self.btn_toggle_label.config(text='Show label')
        else:
            self.show_label = True
            self.btn_toggle_label.config(text='Hide label')

    def get_class_color(self, label):
        """
        Color of label
        :param label:
        :return:
        """
        if label not in self.labels:
            print('[WARNING] label not in dataset labels: %s' % label)
            return (255, 0, 0)
        color = self.labels[label]
        if isinstance(color, str):
            if color.startswith('rgb'):
                color = tuple(eval(color.lstrip('rgb')))
            elif color.startswith('#'):
                color = tuple(int(color.lstrip('#')[i:i + 2], 16) for i in (0, 2, 4))
            else:
                print('[WARNING] Unknown color scheme: %s' % color)
                color = (255, 0, 0)
        return color

    def get_annotations(self, frame):
        """
        Get all annotations of frame
        :param frame:
        :return:
        """
        # self.annotations_timestamp = (self.vid.frame_number + self.annotations_offset) / self.vid.fps
        self.annotations_timestamp = self.vid.frame_number / self.vid.fps
        frame = self.video_annotations.get_frame(frame_num=self.vid.frame_number).show(image=frame,
                                                                                       height=frame.shape[0],
                                                                                       width=frame.shape[1],
                                                                                       with_text=self.show_label)
        return frame

    def toggle_play(self):
        """
        Toggle play pause
        :return:
        """
        if self.playing:
            # need to pause
            self.playing = False
            self.btn_toggle_play.config(text='Play ')
        else:
            # need to play
            self.playing = True
            self.btn_toggle_play.config(text='Pause')

    def next_frame(self):
        """
        Get next frame
        :return:
        """
        self.show_frame()

    def prev_frame(self):
        """
        Get previous frame
        :return:
        """
        self.show_frame(self.vid.frame_number - 1)

    # def apply_offset_fix(self):
    #     """
    #     Apply offset to platform
    #     :return:
    #     """
    #     # delete annotations from platform
    #     for annotation in self.video_annotations.annotations:
    #         self.item.annotations.delete(annotations_id=annotation.id)
    #
    #     # apply annotations fix
    #     new_annotations = list()
    #     for annotation in self.video_annotations.annotations:
    #         new_annotation = annotation.copy()
    #         for i in range(len(new_annotation['metadata']['snapshots_'])):
    #             new_annotation['metadata']['snapshots_'][i]['startTime'] -= (self.annotations_offset / self.vid.fps)
    #             new_annotation['metadata']['snapshots_'][i]['frameNum'] = int(
    #                 new_annotation['metadata']['snapshots_'][i]['startTime'] * self.vid.fps)
    #         new_annotations.append(json.dumps(new_annotation))
    #
    #     # upload annotations
    #     self.item.annotation.upload(annotation=[new_annotations])

    def show_frame(self, frame_number=None):
        """
        Get a frame from the video source
        :param frame_number:
        :return:
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise

        try:
            import tkinter
        except ImportError:
            logger.error(
                'Import Error! Cant import tkinter. Annotations operations will be limited. import manually and fix errors')
            raise

        try:
            import PIL.ImageTk
            import PIL.Image
        except ImportError:
            logger.error(
                'Import Error! Cant import PIL.ImageTk/PIL.ImageTk. Annotations operations will be limited. '
                'import manually and fix errors')
            raise
        ret, frame = self.vid.get_frame(frame_number)
        if ret:
            if self.show_annotations:
                frame = self.get_annotations(frame)
            if self.show_frame_num:
                text = '%d - %d' % (self.vid.frame_number, np.round(self.annotations_timestamp * self.vid.fps))
                frame = cv2.putText(frame,
                                    text=text,
                                    org=(100, 100),
                                    color=(0, 0, 255),
                                    fontFace=cv2.FONT_HERSHEY_DUPLEX,
                                    fontScale=2,
                                    thickness=3)

            self.photo = PIL.ImageTk.PhotoImage(
                image=PIL.Image.fromarray(frame).resize((self.window_width, self.window_height)),
            master=self.canvas)
            self.canvas.create_image(0, 0, image=self.photo, anchor=tkinter.NW)
            # set timestamp
            self.current_frame_text.configure(text='Frame number:\n%d' % self.vid.frame_number)
            self.current_frame_text.grid(sticky="W", row=12, column=0, columnspan=10)
            millis = int(1000 * self.vid.frame_number / self.vid.fps)
            seconds = (millis / 1000) % 60
            minutes = int((millis / (1000 * 60)) % 60)
            hours = int((millis / (1000 * 60 * 60)) % 24)
            self.frame_timestamp_text.configure(
                text='Frame timestamp:\n{:02d}:{:02d}:{:.3f}'.format(hours, minutes, seconds))
            self.frame_timestamp_text.grid(sticky="W", row=8, column=0, columnspan=10)

    def update(self):
        """
        Playing the movie. Get frames at the FPS and show
        :return:
        """
        tic = time.time()
        if self.playing:
            self.show_frame()
        add_delay = np.maximum(1, self.delay - int(1000 * (time.time() - tic)))
        self.window.after(ms=add_delay, func=self.update)


class VideoCapture:
    """
    Video class using cv2 to play video
    """
    def __init__(self, source=0):
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise
        # Open the video source
        self.vid = cv2.VideoCapture(source)
        if not self.vid.isOpened():
            raise ValueError("Unable to open video source", source)

        # Get video source width and height
        self.width = self.vid.get(cv2.CAP_PROP_FRAME_WIDTH)
        self.height = self.vid.get(cv2.CAP_PROP_FRAME_HEIGHT)
        self.fps = self.vid.get(cv2.CAP_PROP_FPS)
        self.frame_number = self.vid.get(cv2.CAP_PROP_POS_FRAMES)

    def get_frame(self, frame_number=None):
        """
        get the frame from video
        :param frame_number:
        :return:
        """
        try:
            import cv2
        except (ImportError, ModuleNotFoundError):
            logger.error(
                'Import Error! Cant import cv2. Annotations operations will be limited. import manually and fix errors')
            raise
        if self.vid.isOpened():
            if self.frame_number is None:
                self.frame_number = self.vid.get(cv2.CAP_PROP_POS_FRAMES)
            else:
                self.frame_number += 1
            if frame_number is not None:
                self.frame_number = frame_number
                self.vid.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = self.vid.read()

            if ret:
                # Return a boolean success flag and the current frame converted to BGR
                return ret, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            else:
                return ret, None
        else:
            return False, None

    # Release the video source when the object is destroyed
    def __del__(self):
        if self.vid.isOpened():
            self.vid.release()


================================================
File: dtlpy/utilities/videos/videos.py
================================================
import asyncio
import time
import types

import numpy as np
import os
import logging
import dtlpy as dl
import shutil

logger = logging.getLogger(name='dtlpy')


##########
# Videos #
##########
class Videos:
    def __init__(self):
        pass

    @staticmethod
    def get_info(filepath):
        try:
            import ffmpeg
        except ImportError:
            logger.error(
                'Import Error! Cant import ffmpeg. '
                'Annotations operations will be limited. import manually and fix errors')
            raise
        probe = ffmpeg.probe(filepath)
        return probe

    @staticmethod
    def get_max_object_id(item):
        max_object_id = 1
        annotations_list = item.annotations.list().annotations
        if len(annotations_list) < 1:
            return 1
        for annotation in annotations_list:
            if annotation.object_id is not None:
                current_object_id = int(annotation.object_id)
                if current_object_id > max_object_id:
                    max_object_id = current_object_id
        return max_object_id

    @staticmethod
    def video_snapshots_generator(item_id=None, item=None, frame_interval=30, image_ext="png"):
        futures = Videos._async_video_snapshots_generator(item_id=item_id,
                                                          item=item,
                                                          frame_interval=frame_interval,
                                                          image_ext=image_ext)
        loop = asyncio.new_event_loop()
        try:
            asyncio.set_event_loop(loop)
            return loop.run_until_complete(futures)
        finally:
            try:
                loop.run_until_complete(loop.shutdown_asyncgens())
            finally:
                asyncio.set_event_loop(None)
                loop.close()

    @staticmethod
    async def _async_video_snapshots_generator(item_id=None, item=None, frame_interval=30, image_ext="png"):
        """
        Create video-snapshots

        :param item_id: item id for the video
        :param item: item id for the video
        :param frame_interval: number of frames to take next snapshot
        :param image_ext: png/jpg
        :return: the uploaded items
        """
        if item_id is not None:
            item = dl.items.get(item_id=item_id)

        if item is None:
            raise ValueError('Missing input item (or item_id)')

        if not isinstance(frame_interval, int):
            raise AttributeError('frame_interval is mast to be integer')

        if "video" not in item.mimetype:
            raise AttributeError("Got {} file type but only video files are supported".format(item.mimetype))

        video_path = item.download()

        # Get the time for single frame from metadata (duration/# of frames)
        if 'system' in item.metadata and \
                'ffmpeg' in item.metadata['system'] and \
                'duration' in item.metadata['system']['ffmpeg'] and \
                'nb_frames' in item.metadata['system']['ffmpeg']:
            nb_frames = int(item.metadata["system"]["ffmpeg"]["nb_frames"])
            duration = float(item.metadata["system"]["ffmpeg"]["duration"])
            video_fps = duration / nb_frames
        else:
            try:
                import cv2
            except (ImportError, ModuleNotFoundError):
                logger.error(
                    'Import Error! Cant import cv2. '
                    'Annotations operations will be limited. import manually and fix errors')
                raise

            video = cv2.VideoCapture(video_path)
            video_fps = video.get(cv2.CAP_PROP_FPS)
            nb_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = video_fps * nb_frames

        images_path = Videos.disassemble(filepath=video_path, frame_interval=frame_interval, image_ext=image_ext)
        snapshots_items = list()
        try:
            # rename files
            images = []
            video_basename = os.path.basename(video_path)
            for f in os.listdir(images_path):
                images.append(f)
            for image in images:
                image_split_name, ext = os.path.splitext(image)
                try:
                    frame = int(image_split_name) * frame_interval
                    file_frame_name = "{}.frame.{}{}".format(video_basename, frame, ext)
                    full_path = os.path.join(images_path, file_frame_name)
                    os.rename(os.path.join(images_path, image),
                              full_path)
                except Exception as e:
                    logger.debug("Rename {} has been failed: {}".format(os.path.join(images_path, image), e))

                remote_path = os.path.join(os.path.split(item.filename)[0], "snapshots")
                remote_url = '/items/{}/snapshots'.format(item.id)
                to_upload = open(full_path, 'rb')
                try:
                    response = await item._client_api.upload_file_async(to_upload=to_upload,
                                                                        item_type='file',
                                                                        item_size=os.stat(full_path).st_size,
                                                                        remote_url=remote_url,
                                                                        uploaded_filename=file_frame_name,
                                                                        remote_path=remote_path)
                except Exception:
                    raise
                finally:
                    to_upload.close()
                if response.ok:
                    snapshots_items.append(item.from_json(response.json(), item._client_api))
                else:
                    raise dl.PlatformException(response)

            # classification tpe annotation creation for each file
            builder = item.annotations.builder()
            annotation_itemlinks = []

            max_object_id = Videos().get_max_object_id(item=item)
            for snapshot_item in snapshots_items:
                max_object_id += 1
                item_frame = snapshot_item.name.rsplit("frame.", 1)[1].split(".")[0]
                if item_frame.isnumeric():
                    item_time = int(item_frame) * video_fps
                else:
                    item_frame = item_time = 0

                snapshot_item.metadata["system"]["itemLinks"] = [{"type": "snapshotFrom",
                                                                  "itemId": item.id,
                                                                  "frame": item_frame,
                                                                  "time": item_time}]

                annotation_itemlinks.append({"type": "snapshotTo",
                                             "itemId": snapshot_item.id,
                                             "frame": item_frame,
                                             "time": item_time})

                snapshot_item.update(system_metadata=True)
                annotation_definition = dl.Classification(label="Snapshot")
                builder.add(annotation_definition=annotation_definition,
                            frame_num=int(item_frame),
                            end_frame_num=nb_frames if int(item_frame) + int(video_fps) > nb_frames else int(
                                item_frame) + int(video_fps),
                            start_time=item_time,
                            end_time=duration if item_time + 1 > duration else item_time + 1,
                            object_id=max_object_id)

            annotations = item.annotations.upload(annotations=builder)

            # update system metadata for annotations
            count = 0
            for annotation in annotations:
                annotation.metadata["system"]["itemLinks"] = [annotation_itemlinks[count]]
                count += 1

            annotations.update(system_metadata=True)
        except Exception as err:
            logger.exception(err)
        finally:
            if os.path.isdir(images_path):
                shutil.rmtree(images_path)
            return snapshots_items

    @staticmethod
    def disassemble(filepath, fps=None, frame_interval=None, loglevel='panic', image_ext='jpg'):
        """
        Disassemble video to images

        :param filepath: input video filepath
        :param fps: rate of disassemble. e.g if 1 frame per second fps is 1. if None all frames will be extracted
        :param frame_interval: take image every frame # (if exists function ignore fps)
        :param image_ext: png/jpg
        :param loglevel: ffmpeg loglevel
        :return:
        """
        try:
            import ffmpeg
        except ImportError:
            logger.error(
                'Import Error! Cant import ffmpeg. '
                'Annotations operations will be limited. import manually and fix errors')
            raise
        # get video information
        video_props = Videos.get_info(filepath)
        if 'system' in video_props and \
                'nb_frames' in video_props['system'][0]:
            nb_frames = video_props['streams'][0]['nb_frames']
        else:
            try:
                import cv2
            except (ImportError, ModuleNotFoundError):
                logger.error(
                    'Import Error! Cant import cv2. '
                    'Annotations operations will be limited. import manually and fix errors')
                raise
            nb_frames = int(cv2.VideoCapture(filepath).get(cv2.CAP_PROP_FRAME_COUNT))

        if not os.path.isfile(filepath):
            raise IOError('File doesnt exists: {}'.format(filepath))
        basename, ext = os.path.splitext(filepath)
        # create folder for the frames
        if os.path.exists(basename):
            shutil.rmtree(basename)

        os.makedirs(basename, exist_ok=True)

        if fps is None:
            try:
                fps = eval(video_props['streams'][0]['avg_frame_rate'])
            except ZeroDivisionError:
                fps = 0
        num_of_zeros = len(str(nb_frames))
        # format the output filename
        output_regex = os.path.join(basename, '%0{}d.{}'.format(num_of_zeros, image_ext))

        try:
            if frame_interval is not None:
                frame_number = 0
                select = ""
                while frame_number < nb_frames:
                    if select != "":
                        select += '+'
                    select += 'eq(n\\,{})'.format(frame_number)
                    frame_number += frame_interval
                stream = ffmpeg.input(filepath, **{'loglevel': loglevel}).output(output_regex,
                                                                                 **{'start_number': '0',
                                                                                    'vf': 'select=\'{}'.format(select),
                                                                                    'vsync': 'vfr'})
            else:
                stream = ffmpeg.input(filepath, **{'loglevel': loglevel}).output(output_regex,
                                                                                 **{'start_number': '0',
                                                                                    'r': str(fps)})

            ffmpeg.overwrite_output(stream).run()
        except Exception:
            logger.error('ffmpeg error in disassemble:')
            raise
        return basename

    @staticmethod
    def reencode(filepath, loglevel='panic'):
        """
        Re-encode video as mp4, remove start offset and set bframes to 0

        :param filepath: input video file
        :param loglevel: ffmpeg loglevel
        :return:
        """
        try:
            import ffmpeg
        except ImportError:
            logger.error(
                'Import Error! Cant import ffmpeg. '
                'Annotations operations will be limited. import manually and fix errors')
            raise
        if not os.path.isfile(filepath):
            raise IOError('File doesnt exists: {}'.format(filepath))
        # re encode video without b frame and as mp4
        basename, ext = os.path.splitext(filepath)
        output_filepath = os.path.join(basename, os.path.basename(filepath).replace(ext, '.mp4'))
        if not os.path.isdir(os.path.dirname(output_filepath)):
            os.makedirs(os.path.dirname(output_filepath))
        try:
            stream = ffmpeg.input(filepath, **{'loglevel': loglevel}).output(output_filepath,
                                                                             **{'x264opts': 'bframes=0',
                                                                                'f': 'mp4'})
            ffmpeg.overwrite_output(stream).run()
        except Exception as e:
            logger.exception('ffmpeg error in disassemble:')
            raise

        output_probe = Videos.get_info(output_filepath)
        start_time = eval(output_probe['streams'][0]['start_time'])
        fps = eval(output_probe['streams'][0]['avg_frame_rate'])
        has_b_frames = output_probe['streams'][0]['has_b_frames']
        start_frame = fps * start_time
        if start_time != 0:
            logger.warning('Video start_time is not 0!')
        if has_b_frames != 0:
            logger.warning('Video still has b frames!')
        return output_filepath

    @staticmethod
    def split_and_upload(filepath,
                         # upload parameters
                         project_name=None, project_id=None, dataset_name=None, dataset_id=None, remote_path=None,
                         # split parameters
                         split_seconds=None, split_chunks=None, split_pairs=None,
                         loglevel='panic'):
        """
        Split video to chunks and upload to platform

        :param filepath: input video file
        :param project_name:
        :param project_id:
        :param dataset_name:
        :param dataset_id:
        :param remote_path:
        :param split_seconds: split by seconds per chunk. each chunk's length will be this in seconds
        :param split_chunks: split by number of chunks.
        :param split_pairs: a list od (start, stop) segments to split in seconds . e.g [(0,400), (600,800)]
        :param loglevel: ffmpeg loglevel
        :return:
        """
        try:
            import ffmpeg
        except ImportError:
            logger.error(
                'Import Error! Cant import ffmpeg. '
                'Annotations operations will be limited. import manually and fix errors')
            raise
        # https://www.ffmpeg.org/ffmpeg-formats.html#Examples-9

        if not os.path.isfile(filepath):
            raise IOError('File doesnt exists: {}'.format(filepath))
        logger.info('Extracting video information...')
        # call to ffmpeg to get frame rate
        probe = Videos.get_info(filepath)
        fps = eval(probe['streams'][0]['avg_frame_rate'])
        n_frames = eval(probe['streams'][0]['nb_frames'])
        video_length = eval(probe['streams'][0]['duration'])
        logger.info('Video frame rate: {}[fps]'.format(fps))
        logger.info('Video number of frames: {}'.format(n_frames))
        logger.info('Video length in seconds: {}[s]'.format(video_length))

        # check split params and calc split params for ffmpeg
        if split_seconds is not None:
            # split by seconds
            split_length = split_seconds
            if split_length <= 0:
                raise ValueError('"split_length" can\'t be 0')
            split_count = int(np.ceil(video_length / split_length))
            list_frames_to_split = [fps * split_length * n for n in range(1, split_count)]
        elif split_chunks is not None:
            # split for known number of chunks
            split_count = split_chunks
            if split_chunks <= 0:
                raise ValueError('"split_chunks" size can\'t be 0')
            split_length = int(np.ceil(video_length / split_chunks))
            list_frames_to_split = [fps * split_length * n for n in range(1, split_count)]
        elif split_pairs is not None:
            if not isinstance(split_pairs, list):
                raise ValueError('"split_times" must be a list of tuples to split at.')
            if not (isinstance(split_pairs[0], list) or isinstance(split_pairs[0], tuple)):
                raise ValueError('"split_times" must be a list of tuples to split at.')
            list_frames_to_split = [fps * split_second for segment in split_pairs for split_second in segment]
            split_count = len(list_frames_to_split)
        else:
            raise ValueError('Must input one split option ("split_chunks", "split_time" or "split_pairs")')
        if split_count == 1:
            raise ValueError('Video length is less than the target split length.')
        # to integers
        list_frames_to_split = [int(i) for i in list_frames_to_split]
        # remove 0 if in the first segmetn
        if list_frames_to_split[0] == 0:
            list_frames_to_split.pop(0)
        # add last frames if not exists
        if list_frames_to_split[-1] != n_frames:
            list_frames_to_split = list_frames_to_split + [n_frames]
        logger.info('Splitting to %d chunks' % split_count)

        basename, ext = os.path.splitext(filepath)
        output_regex = os.path.join(basename, '%%03d.mp4')
        # create folder
        if not os.path.exists(basename):
            os.makedirs(basename, exist_ok=True)
        # run ffmpeg
        try:
            stream = ffmpeg.input(filepath, **{'loglevel': loglevel}).output(output_regex,
                                                                             **{'x264opts': 'bframes=0',
                                                                                'f': 'segment',
                                                                                'reset_timestamps': '1',
                                                                                'map': '0',
                                                                                'segment_frames': ','.join(
                                                                                    [str(i) for i in
                                                                                     list_frames_to_split])
                                                                                })
            ffmpeg.overwrite_output(stream).run(capture_stdout=True)
        except Exception:
            logger.exception('ffmpeg error in disassemble:')
            raise

        # split_cmd = 'ffmpeg -y -i "%s" -b 0 -f mp4 -reset_timestamps 1 -map 0 -f segment -segment_frames %s "%s"' % (
        #     filepath, ','.join([str(int(i)) for i in list_frames_to_split]), output_regex)
        # logger.info('About to run: %s' % split_cmd)
        # subprocess.check_call(shlex.split(split_cmd), universal_newlines=True)

        # rename
        list_frames_to_split = [0] + list_frames_to_split
        filenames = list()
        for n in range(split_count):
            old_filename = output_regex.replace('%03d', '%03d' % n)
            new_filename = output_regex.replace('%03d', '%s__%s' %
                                                (time.strftime('%H_%M_%S', time.gmtime(list_frames_to_split[n] / fps)),
                                                 time.strftime('%H_%M_%S',
                                                               time.gmtime(list_frames_to_split[n + 1] / fps))))
            filenames.append(new_filename)
            # rename to informative name
            if os.path.isfile(new_filename):
                logger.warning('File already exists. Overwriting!: {}'.format(new_filename))
                os.remove(new_filename)
            os.rename(old_filename, new_filename)
            # check if in pairs, if not - delete
            if split_pairs is not None:
                start_frames = [pair[0] for pair in split_pairs]
                end_frames = [pair[1] for pair in split_pairs]
                if (list_frames_to_split[n] // fps) in start_frames and (
                        list_frames_to_split[n + 1] // fps) in end_frames:
                    # keep video
                    pass
                else:
                    os.remove(new_filename)
        Videos.upload_to_platform(project_name=project_name,
                                  project_id=project_id,
                                  dataset_name=dataset_name,
                                  dataset_id=dataset_id,
                                  remote_path=remote_path,
                                  local_path=basename)

    @staticmethod
    def upload_to_platform(project_name=None, project_id=None, dataset_name=None, dataset_id=None,
                           local_path=None, remote_path=None):

        import dtlpy as dlp
        if project_id is not None or project_name is not None:
            project = dlp.projects.get(project_name=project_name, project_id=project_id)
            dataset = project.get(dataset_name=dataset_name, dataset_id=dataset_id)
            dataset.items.upload(dataset_name=dataset_name,
                                 dataset_id=dataset_id,
                                 local_path=local_path,
                                 remote_path=remote_path,
                                 file_types=['.mp4'])
        else:
            dataset = dlp.datasets.get(dataset_name=dataset_name, dataset_id=dataset_id)
            dataset.items.upload(local_path=local_path,
                                 remote_path=remote_path,
                                 file_types=['.mp4'])


================================================
File: tests/debug.py
================================================
from behave.__main__ import main as behave_main
import logging
import os
logging.basicConfig(level='DEBUG')
feature_filename = 'test_models_create.feature'

os.environ['AVOID_TESTRAIL'] = 'true'
behave_main(['tests/features', '-i', feature_filename, '--stop', '--no-capture'])


================================================
File: tests/debug_config
================================================
module name: behave
parameters: file.feature --stop
env vars: ALLOW_RUN_TESTS_LOCALLY_WITH_USER=true;AVOID_TESTRAIL=true;PYTHONUNBUFFERED=1
working directory: /dtlpy

================================================
File: tests/env_from_git_branch.py
================================================
import subprocess
import os


def get_env_from_git_branch():
    env_name = os.environ.get('DLP_ENV_NAME', None)
    if env_name is None:
        p = subprocess.Popen(['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                             stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
        output, err = p.communicate()
        branch_name = str(output, 'utf-8').strip()
        if branch_name == 'rc':
            env_name = 'rc'
        elif branch_name == 'development':
            env_name = 'dev'
        elif branch_name == 'master':
            env_name = 'prod'
        else:
            env_name = 'rc'
            print('unknown git branch. default is "rc"')
        os.environ["DLP_ENV_NAME"] = env_name
        
    base_env = env_name.split("-")[0]
    print('Running on dataloop environment: {!r}'.format(base_env))
    return env_name, base_env


================================================
File: tests/requirements.txt
================================================
behave>=1.2.6
opencv-python>=4.2.0.32
behave-testrail-reporter>=0.5.1
filelock>=3.0.12
pycocotools
redis>=3.5
diskcache>=5.4
xmltodict>=0.13.0
python-dotenv>=0.21.1
pandas>=0.24.2
aiohttp>=3.8

================================================
File: tests/test_examples.py
================================================
import dtlpy as dl
import time
import jwt
import random
import logging
import os
from dtlpy import examples

try:
    # for local import
    from tests.env_from_git_branch import get_env_from_git_branch
except ImportError:
    # for remote import
    from env_from_git_branch import get_env_from_git_branch

try:
    # for local import
    from tests.env_from_git_branch import get_env_from_git_branch
except ImportError:
    # for remote import
    from env_from_git_branch import get_env_from_git_branch

logging.basicConfig(level='DEBUG')
_, base_env = get_env_from_git_branch()
dl.setenv(base_env)
# check token
payload = jwt.decode(dl.token(), algorithms=['HS256'], verify=False,
                     options={'verify_signature': False})
if payload['email'] not in ['oa-test-4@dataloop.ai', 'oa-test-1@dataloop.ai', 'oa-test-2@dataloop.ai',
                            'oa-test-3@dataloop.ai']:
    assert False, 'Cannot run test on user: "{}". only test users'.format(payload['email'])

TEST_DIR = os.path.dirname(os.path.realpath(__file__))
ASSETS_PATH = os.path.join(TEST_DIR, 'assets')
image_path = os.path.join(ASSETS_PATH, '0000000162.jpg')
annotations_path = os.path.join(ASSETS_PATH, 'annotations_new.json')

project = dl.projects.create('to-delete-test-project_examples_tester_{}'.format(random.randrange(1000, 100000)))
dataset = project.datasets.create('dataset_examples_tester_{}'.format(random.randrange(1000, 100000)))
item = dataset.items.upload(local_path=image_path, local_annotations_path=annotations_path)

# add labels
examples.add_labels.main(project_name=project.name, dataset_name=dataset.name)

# add metadata to item
examples.add_metadata_to_item.main(project_name=project.name, dataset_name=dataset.name, item_path=image_path)

# annotation convert to voc
second_project = dl.projects.create('to-delete-test-project_examples_tester_{}'.format(random.randrange(1000, 100000)))
second_dataset = second_project.datasets.create('dataset_examples_tester_{}'.format(random.randrange(1000, 100000)))
time.sleep(1)
second_item = second_dataset.items.upload(local_path=image_path)
examples.copy_annotations.main(first_project_name=project.name,
                               second_project_name=second_project.name,
                               first_dataset_name=dataset.name,
                               second_dataset_name=second_dataset.name,
                               first_remote_filepath=item.filename,
                               second_remote_filepath=second_item.filename)

# copy folder
examples.copy_folder.main(first_project_name=project.name,
                          second_project_name=second_project.name,
                          first_dataset_name=dataset.name,
                          second_dataset_name=second_dataset.name)

# show item and mask
examples.show_item_and_mask.main(project_name=project.name,
                                 dataset_name=dataset.name,
                                 item_remote_path=item.filename)

# show item and mask
examples.upload_items_with_modalities.main(project_name=project.name,
                                           dataset_name=dataset.name)
project.delete(True, True)
second_project.delete(True, True)


================================================
File: tests/test_login.py
================================================
import dtlpy as dl

try:
    # for local import
    from tests.env_from_git_branch import get_env_from_git_branch
except ImportError:
    # for remote import
    from env_from_git_branch import get_env_from_git_branch

import sys
import os
from datetime import datetime


def update_env_cookie_file(env_name, base_env):
    if base_env in ['prod', 'rc', 'dev']:
        return

    if base_env not in [env_dict['alias'] for env_url, env_dict in dl.client_api.environments.items()]:
        dl.add_environment(
            environment='https://{}-gate.dataloop.ai/api/v1'.format(base_env),
            verify_ssl=True,
            alias='{}'.format(base_env),
            gate_url="https://{}-gate.dataloop.ai".format(base_env),
            url="https://{}.dataloop.ai/".format(env_name)
        )

    assert base_env in [env_dict['alias'] for env_url, env_dict in
                        dl.client_api.environments.items()], "Failed to add_environment: {}".format(env_name)


def test_login():
    env_name, base_env = get_env_from_git_branch()
    # Check if needed to add new env to cookie file
    update_env_cookie_file(env_name, base_env)
    dl.setenv(base_env)
    if env_name == 'prod':
        username = os.environ["TEST_USER_PROD"]
        password = os.environ["TEST_PASSWORD_PROD"]
    else:
        username = os.environ["TEST_USERNAME"]
        password = os.environ["TEST_PASSWORD"]

    dl.login_m2m(
        email=username,
        password=password,
    )

    if dl.token_expired():
        print('Token Expired')
        sys.exit(1)
    else:
        print('Success')


def test_login_api_key():
    env_name, base_env = get_env_from_git_branch()
    # Check if needed to add new env to cookie file
    update_env_cookie_file(env_name, base_env)
    dl.setenv(base_env)
    dl.client_api.generate_api_key(description=f'Key generated by SDK automation {datetime.now()}', login=True)
    print('Success with API key')


if __name__ == "__main__":
    test_login()
    # If you want to test login with API key, set API_KEY to 'true'
    if os.environ.get("API_KEY", None) == 'true':
        # Update the login token to use API key
        test_login_api_key()
    sys.exit(0)




================================================
File: tests/test_runner.py
================================================
import subprocess
import threading
import traceback
import json
import time
from concurrent.futures import ThreadPoolExecutor, wait

import jwt
import os
import dtlpy as dl
import numpy as np
from multiprocessing.pool import ThreadPool
from tqdm import tqdm
import sys
from filelock import FileLock
import random
import string

TIMEOUT = 1.8 * 60 * 60


class TestState(threading.Thread):
    def __init__(self):
        super(TestState, self).__init__()
        self._stop_event = threading.Event()
        self.state = dict()
        self.lock = threading.Lock()

    def run(self):
        while not self._stop_event.is_set():
            self.lock.acquire()
            state = self.state.copy()
            self.lock.release()
            message = 'Currently running tests:\n'
            try:
                for test in state.values():
                    message += '{} - {:.2f} seconds\n'.format(test['name'], time.time() - test['start'])
                print(message)
            except Exception:
                print('Failed to print state\n{}'.format(traceback.format_exc()))
            time.sleep(30)

    def start_test(self, test_name: str):
        self.lock.acquire()
        self.state[test_name] = {
            'start': time.time(),
            'name': test_name,
        }
        self.lock.release()

    def end_test(self, test_name: str):
        self.lock.acquire()
        self.state.pop(test_name, None)
        self.lock.release()

    def stop(self):
        self._stop_event.set()


try:
    # for local import
    from tests.env_from_git_branch import get_env_from_git_branch
except ImportError:
    # for remote import
    from env_from_git_branch import get_env_from_git_branch

TEST_DIR = os.path.dirname(os.path.realpath(__file__))
REPORT_DIR = os.path.join(TEST_DIR, 'reports')
NUM_TRIES = 2


def clean_feature_log_file(log_filepath):
    if os.path.isfile(log_filepath):
        os.remove(log_filepath)
    directory, file = os.path.split(log_filepath)
    if os.path.isfile(os.path.join(directory, 'pass_' + file)):
        os.remove(os.path.join(directory, 'pass_' + file))
    if os.path.isfile(os.path.join(directory, 'fail_' + file)):
        os.remove(os.path.join(directory, 'fail_' + file))


def delete_single_project(i_project: dl.Project, i_pbar):
    try:
        for dataset in i_project.datasets.list():
            try:
                if dataset.readonly:
                    dataset.set_readonly(False)
            except Exception:
                pass
        for pipeline in i_project.pipelines.list().items:
            try:
                pipeline.delete()
            except Exception:
                pass
        for service in i_project.services.list().items:
            try:
                service.delete()
            except Exception as e:
                if 'Service cannot be deleted as long as it has running/pending pipeline' in str(e):
                    services = service.executions.list()
                    for page in services:
                        for s in page:
                            try:
                                s.terminate()
                            except Exception as e:
                                pass
                pass
        for model in i_project.models.list().items:
            try:
                model.delete()
            except Exception:
                pass
        for app in i_project.apps.list().items:
            try:
                app.uninstall()
            except Exception:
                pass
        for dpkg in i_project.dpks.list(filters=dl.Filters(field='creator', values=dl.info()['user_email'],
                                                           resource=dl.FiltersResource.DPK)).items:
            try:
                dpkg.delete()
            except Exception:
                apps = dl.apps.list(
                    filters=dl.Filters(use_defaults=False, resource=dl.FiltersResource.APP,
                                       field="dpkName",
                                       values=dpkg.name))
                for page in apps:
                    for app in page:
                        try:
                            app.uninstall()
                        except Exception as e:
                            pass
                try:
                    dpkg.delete()
                except:
                    pass
        i_project.delete(True, True)
    except Exception:
        print('Failed to delete project: {}'.format(i_project.name))
    i_pbar.update(1)


def delete_projects():
    start_phrase = 'to-delete-test-'
    projects = [
        p for p in dl.projects.list() if p.creator.startswith('oa-test-') and p.name.startswith(start_phrase)
    ]

    projects_pbar = tqdm(total=len(projects), desc='Deleting projects')

    projects_pool = ThreadPool(processes=32)
    for p in projects:
        projects_pool.apply_async(delete_single_project, args=(p, projects_pbar))

    projects_pool.close()
    projects_pool.join()


def update_feature_report(temp_report_filepath, w_feature_filename, REPORT_DIR):
    with open(temp_report_filepath, 'r') as json_file:
        try:
            data = json.load(json_file)
        except json.JSONDecodeError:
            print("The JSON file is empty or not a valid JSON.")
            data = []
    # Need to check the feature folder
    feature_folder = w_feature_filename.split('/')[-2]
    feature_report_path = os.path.join(REPORT_DIR,
                                       f'{check_feature_folder(feature_folder, w_feature_filename).lower()}-report.json')
    with FileLock(feature_report_path + ".lock"):
        if os.path.exists(feature_report_path):
            with open(feature_report_path, 'r+') as file:
                file_content = file.read()
                if file_content:
                    temp = json.loads(file_content)
                    if isinstance(temp, list):
                        if len(data) > 0:
                            temp.append(data[0])
                            # Move the cursor to the beginning of the file
                            file.seek(0)
                            # Write the updated list back to the file
                            file.write(json.dumps(temp, indent=4))
                            # Truncate the file to the current size to remove any leftover content
                            file.truncate()
                    else:
                        raise Exception("Expected list in the report file")
                else:
                    file.write(json.dumps(data, indent=4))

        else:
            with open(feature_report_path, 'w') as file:
                file.write(json.dumps(data, indent=4))


def test_feature_file(w_feature_filename, i_pbar):
    timeout = 10 * 60
    longer_timeout = 16 * 60

    longer_timeout_features = [
        'pipeline_active_learning.feature',
        'test_service_debug_runtime.feature',
        'test_models_clone_1.feature',
        'pipeline_rerun_cycles_2.feature',
        'test_models_context.feature',
        'test_models_flow.feature',
        'execution_monitoring_terminate.feature',
        'execution_monitoring_timeout.feature',
        'pipeline_context.feature',
    ]

    log_path = os.path.join(TEST_DIR, 'logs')
    temp_report_filepath = os.path.join(REPORT_DIR, f'temp_report_{generate_random_string(8)}.json')
    if not os.path.isdir(log_path):
        os.makedirs(log_path, exist_ok=True)
    log_filepath = None
    w_i_try = -1
    tic = time.time()
    stderr = ''
    try:
        test_state.start_test(w_feature_filename)
        for w_i_try in range(NUM_TRIES):
            print('Attempt {} for feature file: {}'.format(w_i_try + 1, w_feature_filename))
            log_filepath = os.path.join(log_path,
                                        os.path.basename(w_feature_filename) + '_try_{}.log'.format(w_i_try + 1))
            clean_feature_log_file(log_filepath)
            cmds = ['behave', features_path,
                    '-i', w_feature_filename.split("/")[-1],
                    '--stop',
                    '-o', log_filepath,
                    '--format=pretty',
                    '--logging-level=DEBUG',
                    '--summary',
                    '--no-capture',
                    '--format=json',
                    '-o', temp_report_filepath,
                    '--format=pretty',
                    ]

            # need to run a new process to avoid collisions
            p = subprocess.Popen(cmds, stderr=subprocess.PIPE)
            _, stderr = p.communicate(
                timeout=longer_timeout if os.path.basename(w_feature_filename) in longer_timeout_features else timeout
            )

            if p.returncode == 0:
                break
        toc = time.time() - tic
        if log_filepath is None:
            results[w_feature_filename] = {'status': False,
                                           'log_file': '',
                                           'try': w_i_try,
                                           'avgTime': '{:.2f}[s]'.format(toc / (1 + w_i_try))}
        else:
            directory, file = os.path.split(log_filepath)
            if p.returncode == 0:
                # passes
                new_log_filepath = os.path.join(directory, 'pass_' + file)
                if os.path.isfile(log_filepath):
                    os.rename(log_filepath, new_log_filepath)
                results[w_feature_filename] = {'status': True,
                                               'log_file': new_log_filepath,
                                               'try': w_i_try,
                                               'avgTime': '{:.2f}[s]'.format(toc / (1 + w_i_try))}
            else:
                # failed
                new_log_filepath = os.path.join(directory, 'fail_' + file)
                if os.path.isfile(log_filepath):
                    os.rename(log_filepath, new_log_filepath)
                results[w_feature_filename] = {'status': False,
                                               'log_file': new_log_filepath,
                                               'try': w_i_try,
                                               'avgTime': '{:.2f}[s]'.format(toc / (1 + w_i_try))}
                print('**** Failed feature file: {} after {} seconds and {} retries.'.format(
                    w_feature_filename, toc, w_i_try + 1
                ))
                print('**** stderr: {}'.format(stderr))

            update_feature_report(temp_report_filepath, w_feature_filename, REPORT_DIR)


    except subprocess.TimeoutExpired:
        results[w_feature_filename] = {'status': False,
                                       'log_file': log_filepath,
                                       'try': w_i_try,
                                       'avgTime': '{:.2f}[s]'.format(-1),
                                       'timeout': True}

        cmds_dry_run = ['behave', features_path,
                        '-i', w_feature_filename.split("/")[-1],
                        '--format=json',
                        '-o', temp_report_filepath,
                        '--format=pretty',
                        '--dry-run'
                        ]

        p_1 = subprocess.Popen(cmds_dry_run, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
        stdout, stderr = p_1.communicate(
            timeout=longer_timeout if os.path.basename(w_feature_filename) in longer_timeout_features else timeout)
        if p_1.returncode != 0:
            raise Exception(f"Command failed with return code {p.returncode}\nstdout: {stdout}\nstderr: {stderr}")

        update_feature_report(temp_report_filepath, w_feature_filename, REPORT_DIR)  # update report with the dry run

    except Exception:
        print(traceback.format_exc())
        results[w_feature_filename] = {'status': False,
                                       'log_file': log_filepath,
                                       'try': w_i_try,
                                       'avgTime': '{:.2f}[s]'.format(-1)}
    i_pbar.update(1)
    test_state.end_test(w_feature_filename)


def check_feature_folder(feature_folder, feature_name=None):
    if feature_folder in ["assignments_repo", "tasks_repo"]:
        return "Ramsay"
    elif feature_folder in ["pipeline_entity", "pipeline_resume", "packages_entity", "packages_flow", "packages_repo",
                            "services_entity", "services_repo", "execution_monitoring", "executions_repo",
                            "triggers_repo", "compute", "service_driver_repo"]:
        return "Piper"
    elif feature_folder in ["bot_entity", "bots_repo", "integrations_repo", "project_entity", "projects_repo"]:
        return "Hodor"
    elif feature_folder in ["app_entity", "dpk_tests", "solution", "app_integrations"]:
        return "Apps"
    elif feature_folder in ["command_entity", "dataset_entity", "datasets_repo", "drivers_repo", "ontologies_repo",
                            "ontology_entity", "recipe_entity", "recipes_repo", "artifacts_repo", "filters_entity",
                            "item_entity", "items_repo", "ann_text_object", "annotation_collection", "annotation_entity"
        , "annotations_repo", "features_vector_entity", "item_collections"]:
        return "Rubiks"
    elif feature_folder in ["checkout_testing", "cli_testing", "code_base_entity", "code_bases_repo", "converter",
                            "documentation_tests", "platform_urls", "webm_converter", "cache", "xray", "ml_subsets"]:
        return "SDK"
    elif feature_folder in ["settings"]:
        return "Woz"
    elif feature_folder in ["models_repo"]:
        return "Roberto"
    elif feature_folder in ["notifications"]:
        return "Hedwig"
    elif feature_folder in ["billing_repo"]:
        return "Billing"
    else:
        assert False, f"Feature folder '{feature_folder}' not in the services list - Please add it to correct condition.\n{feature_name}"


def generate_random_string(length):
    """Generate a random string of numbers and letters."""
    # Define the character set: numbers and letters (uppercase and lowercase)
    characters = string.ascii_letters + string.digits
    # Generate a random string from the character set
    random_string = ''.join(random.choice(characters) for _ in range(length))
    return random_string


def create_project_for_alerts(contributors, project_name):
    project = dl.projects.create(project_name=project_name)
    for con in contributors:
        try:
            project.add_member(email=con, role='developer')
        except Exception:
            pass
    return project


def send_alert():
    recipients = [
        'aharon@dataloop.ai',
        'or@dataloop.ai'
    ]

    project_name = 'DataloopAlerts'

    try:
        project = dl.projects.get(project_name=project_name)
    except Exception:
        project = create_project_for_alerts(contributors=recipients, project_name=project_name)

    title = 'FAILED - PRODUCTION - SDK TESTS'
    msg = 'SDK tests in production failed'

    for rec in recipients:
        try:
            dl.projects._send_mail(project_id=project.id, send_to=rec, title=title, content=msg)
        except Exception:
            print('Failed to send mail to: {}'.format(rec))


def report_to_xray(test_env: str = 'RC'):
    services_list = ['Ramsay', 'Piper', 'Hodor', 'Apps', 'Rubiks', 'SDK', 'Woz', 'Roberto', 'Hedwig', 'Billing']

    """
    Check if all services has a report folder and remove from the list if not
    """
    test_env = test_env.upper()
    services_to_remove = []
    for service in services_list:
        if not os.path.exists(os.path.join(REPORT_DIR, f'{service.lower()}-report.json')):
            print(f"### !!! Missing report for service: {service} !!! ###")
            services_to_remove.append(service)

    for service in services_to_remove:
        services_list.remove(service)
    """
    Loop on all services and try to report to Xray using shell script
    """

    shell_script_path = os.path.join(os.getcwd(), 'xrayreporter.sh')
    subprocess.run(['chmod', '+x', shell_script_path])
    for service in services_list:
        print(f"### Reporting for service: {service} ###")
        report_path = os.path.join(REPORT_DIR, f'{service.lower()}-report.json')
        try:
            # Make the shell script executable (optional, do this only if it's not already executable)
            result = subprocess.run([shell_script_path, test_env, service, report_path], capture_output=True, text=True)
            print(result.stdout)
        except subprocess.CalledProcessError as e:
            print(f"### Failed to report for service: {service} ###")
            print(f"### Error: {e} ###")


if __name__ == '__main__':
    print('########################################')
    print('# Running test from directory: {}'.format(TEST_DIR))
    print('########################################')

    # reset SDK api calls
    dl.client_api.calls_counter.reset()
    dl.client_api.calls_counter.on()

    # zero out api call file
    call_counters_path = os.path.join(TEST_DIR, 'assets', 'api_calls.json')
    with open(call_counters_path, 'r') as f:
        api_calls = json.load(f)

    for api_call in api_calls:
        api_calls[api_call] = 0

    with open(call_counters_path, 'w') as f:
        api_calls = json.dump(api_calls, f, indent=2)

    # set timer and environment
    start_time = time.time()
    # set env to dev
    _, base_env = get_env_from_git_branch()
    dl.setenv(base_env)
    print('Environment is: {}'.format(base_env))

    print('########################################')
    print('# Deleting projects')
    print('########################################')
    delete_projects()

    # set log level
    dl.verbose.logging_level = "warning"
    dl.verbose.print_all_responses = True

    # check token
    payload = jwt.decode(dl.token(), algorithms=['HS256'], verify=False,
                         options={'verify_signature': False})
    if payload['email'] not in ['oa-test-1@dataloop.ai',
                                'oa-test-4@dataloop.ai',
                                'oa-test-2@dataloop.ai',
                                'oa-test-3@dataloop.ai']:
        assert False, 'Cannot run test on user: "{}". only test users'.format(payload['email'])

    # run tests
    max_workers = 6
    if base_env == 'rc':
        max_workers = 10
    pool = ThreadPoolExecutor(max_workers=max_workers)
    features_path = os.path.join(TEST_DIR, 'features')
    if not os.path.exists(REPORT_DIR):
        os.makedirs(REPORT_DIR)
        assert os.path.exists(REPORT_DIR), f'Failed to create reports directory: {REPORT_DIR}'
        print(f"Created reports directory: {REPORT_DIR}")

    print(f"Index driver is {os.environ.get('INDEX_DRIVER_VAR', None)}")

    results = dict()
    features_to_run = set()
    for path, subdirs, files in os.walk(features_path):
        if "billing_repo" not in path:
            for filename in files:
                striped, ext = os.path.splitext(filename)
                if ext in ['.feature']:
                    features_to_run.add(os.path.join(path, filename))

    pbar = tqdm(total=len(features_to_run), desc="Features progress")

    test_state = TestState()
    test_state.start()

    _futures = []
    for feature_filename in features_to_run:
        results[feature_filename] = dict()
        time.sleep(.1)
        future = pool.submit(test_feature_file, **{'w_feature_filename': feature_filename, 'i_pbar': pbar})
        _futures.append(future)

    # Wait for all futures to complete or timeout
    done, not_done = wait(_futures, timeout=TIMEOUT)

    if not_done:
        print("Timeout reached. Not all tasks completed within 2 hours.")
        for future in not_done:
            future.cancel()

    pbar.close()
    pool.shutdown(wait=False)

    test_state.stop()

    # stop timer
    end_time = time.time()

    # get call count
    call_counters_path = os.path.join(TEST_DIR, 'assets', 'api_calls.json')
    with open(call_counters_path, 'r') as f:
        api_calls = json.load(f)

    # print results
    api_calls = sum(api_calls.values())

    # Summary
    all_results = [result.get('status', False) for result in results.values()]
    passed = all(all_results)

    if not passed:
        if sys.argv.__len__() > 1 and sys.argv[1] == 'scheduled':
            send_alert()
        print('-------------- Logs --------------')
        for feature, result in results.items():
            if not result:
                continue
            status = result['status']
            log_filename = result['log_file']
            i_try = result['try']
            if status is False:
                try:
                    with open(log_filename, 'r') as output:
                        print(output.read())
                except:
                    continue
    print('-------------- Summary --------------')
    print('Current loop api calls: ', str(api_calls))
    print('Tests took: {:.2f}[s]'.format(end_time - start_time))
    if passed:
        print(
            'All scenarios passed! {}/{}:'.format(np.sum([1 for res in all_results if res is True]), len(all_results)))
    else:
        print('Failed {}/{}:'.format(np.sum([1 for res in all_results if res is False]), len(all_results)))

    tests_results = results.items()

    # print timeout first
    for feature, result in tests_results:
        if result and not result.get('status', None) and 'timeout' in result:
            status = 'timeout'
            log_filename = result['log_file']
            i_try = result['try']
            avg_time = result['avgTime']
            res_msg = '{}\t in try: {}\tavg time: {}\tfeature: {}'.format(status, i_try, avg_time,
                                                                          os.path.basename(feature))
            print(res_msg)

    # print failed first
    for feature, result in tests_results:
        if result and not result.get('status', None) and 'timeout' not in result:
            status = 'failed'
            log_filename = result['log_file']
            i_try = result['try']
            avg_time = result['avgTime']
            res_msg = '{}\t in try: {}\tavg time: {}\tfeature: {}'.format(status, i_try, avg_time,
                                                                          os.path.basename(feature))
            print(res_msg)

    # print unrun tests
    for feature, result in tests_results:
        if not result:
            status = 'unrun'
            res_msg = '{}\t in try: {}\tavg time: {}\tfeature: {}'.format(status, 0, 0, os.path.basename(feature))
            print(res_msg)

    # print passes
    for feature, result in tests_results:
        if result and result.get('status'):
            status = 'passed'
            log_filename = result['log_file']
            i_try = result['try']
            avg_time = result['avgTime']
            res_msg = '{}\t in try: {}\tavg time: {}\tfeature: {}'.format(status, i_try, avg_time,
                                                                          os.path.basename(feature))
            print(res_msg)

    delete_projects()

    report_to_xray(test_env=base_env)

    # return success/failure
    if passed:
        # all True - passed
        sys.exit(0)
    else:
        sys.exit(1)


================================================
File: tests/features/environment.py
================================================
import time

from behave import fixture, use_fixture
import os
import json
import logging
from filelock import FileLock
from dotenv import load_dotenv
import subprocess

from behave.reporter.summary import SummaryReporter
from behave.formatter.base import StreamOpener
import sys

import dtlpy as dl
import shutil

try:
    # for local import
    from tests.env_from_git_branch import get_env_from_git_branch
except ImportError:
    # for remote import
    from ..env_from_git_branch import get_env_from_git_branch


def before_all(context):
    load_dotenv('.test.env')
    # Get index driver from env var
    context.index_driver_var = os.environ.get("INDEX_DRIVER_VAR", None)


@fixture
def after_feature(context, feature):
    print_feature_filename(context, feature)

    if hasattr(feature, 'bot'):
        try:
            feature.bot.delete()
        except Exception:
            logging.exception('Failed to delete bot')

    if hasattr(feature, 'apps'):
        for app in context.feature.apps:
            try:
                app.uninstall()
            except Exception:
                logging.exception('Failed to uninstall app')

    if hasattr(feature, 'dpks'):
        for dpk in context.feature.dpks:
            try:
                dpk.delete()
            except Exception:
                try:
                    apps = dl.apps.list(
                        filters=dl.Filters(use_defaults=False, resource=dl.FiltersResource.APP,
                                           field="dpkName",
                                           values=dpk.name))
                    for page in apps:
                        for app in page:
                            app.uninstall()
                    models = dl.models.list(
                        filters=dl.Filters(use_defaults=False, resource=dl.FiltersResource.MODEL,
                                           field="app.dpkName",
                                           values=dpk.name))
                    for page in models:
                        for model in page:
                            model.delete()
                    dpk.delete()
                except:
                    logging.exception('Failed to delete dpk')

    if hasattr(feature, 'dataloop_feature_integration'):
        all_deleted = True
        time.sleep(7)  # Wait for drivers to delete
        for integration_id in feature.to_delete_integrations_ids:
            try:
                feature.dataloop_feature_project.integrations.delete(integrations_id=integration_id, sure=True,
                                                                     really=True)
            except feature.dataloop_feature_dl.exceptions.NotFound:
                pass
            except:
                all_deleted = False
                logging.exception('Failed deleting integration: {}'.format(integration_id))
        assert all_deleted

    if hasattr(feature, 'dataloop_feature_project'):
        try:
            if 'frozen_dataset' in feature.tags:
                fix_project_with_frozen_datasets(project=feature.dataloop_feature_project)
            feature.dataloop_feature_project.delete(True, True)
        except Exception:
            logging.exception('Failed to delete project')

    if hasattr(context.feature, 'dataloop_feature_org'):
        try:
            username = os.environ["TEST_SU_USERNAME"]
            password = os.environ["TEST_SU_PASSWORD"]
            login = dl.login_m2m(
                email=username,
                password=password
            )
            assert login, "TEST FAILED: User login failed"
            context.dl = dl
            success, response = dl.client_api.gen_request(req_type='delete',
                                                          path=f'/orgs/{feature.dataloop_feature_org.id}')
            if not success:
                raise dl.exceptions.PlatformException(response)
            logging.info(f'Organization id {feature.dataloop_feature_org.id} deleted successfully')
            username = os.environ["TEST_USERNAME"]
            password = os.environ["TEST_PASSWORD"]
            login = dl.login_m2m(
                email=username,
                password=password
            )
            assert login, "TEST FAILED: User login failed"
            context.dl = dl
            return True
        except Exception:
            logging.exception('Failed to delete organization')

    # update api call json
    if hasattr(feature, 'dataloop_feature_dl'):
        try:
            api_calls_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], 'api_calls.json')
            with open(api_calls_path, 'r') as f:
                api_calls = json.load(f)
            if context.feature.name in api_calls:
                api_calls[context.feature.name] += feature.dataloop_feature_dl.client_api.calls_counter.number
            else:
                api_calls[context.feature.name] = feature.dataloop_feature_dl.client_api.calls_counter.number
            # lock the file for multi processes needs
            with FileLock("api_calls.json.lock"):
                with open(api_calls_path, 'w') as f:
                    json.dump(api_calls, f)
        except Exception:
            logging.exception('Failed to update api calls')

    if hasattr(feature, 'dataloop_feature_compute'):
        try:
            compute = context.feature.dataloop_feature_compute
            dl.computes.delete(compute_id=compute.id)
        except Exception:
            logging.exception('Failed to delete compute')


@fixture
def before_scenario(context, scenario):
    context.scenario.return_to_user = False


@fixture
def after_scenario(context, scenario):
    if context.scenario.return_to_user == True:
        username = os.environ["TEST_USERNAME"]
        password = os.environ["TEST_PASSWORD"]
        login = dl.login_m2m(
            email=username,
            password=password,
        )
        assert login, "TEST FAILED: User login failed"
        print("----------Changed to a Regular user----------")
        context.scenario.return_to_user = False
        context.dl = dl


def get_step_key(step):
    return '{}: line {}. {}'.format(step.location.filename, step.location.line, step.name)


@fixture
def before_step(context, step):
    key = get_step_key(step)
    setattr(context, key, time.time())


@fixture
def after_step(context, step):
    key = get_step_key(step)
    start_time = getattr(context, key, None)
    total_time = time.time() - start_time
    if total_time > 3:
        print("######## {}\nStep Duration: {}".format(key, total_time))
    delattr(context, key)


@fixture
def before_feature(context, feature):
    if 'rc_only' in context.tags and 'rc' not in os.environ.get("DLP_ENV_NAME"):
        feature.skip("Marked with @rc_only")
        return
    if 'skip_test' in context.tags:
        feature.skip("Marked with @skip_test")
        return


def fix_project_with_frozen_datasets(project):
    datasets = project.datasets.list()
    for dataset in datasets:
        if dataset.readonly:
            dataset.set_readonly(False)


@fixture
def before_tag(context, tag):
    if "skip_test" in tag:
        """
        For example: @skip_test_DAT-99999
        """
        dat = tag.split("_")[-1] if "DAT" in tag else ""
        if hasattr(context, "scenario"):
            context.scenario.skip(f"Test mark as SKIPPED, Should be merged after {dat}")
    if 'rc_only' in context.tags and 'rc' not in os.environ.get("DLP_ENV_NAME"):
        if hasattr(context, "scenario"):
            context.scenario.skip(f"Test mark as SKIPPED, Should be run only on RC")


@fixture
def after_tag(context, tag):
    if tag == 'services.delete':
        try:
            use_fixture(delete_services, context)
        except Exception:
            logging.exception('Failed to delete service')
    elif tag == 'packages.delete':
        try:
            use_fixture(delete_packages, context)
        except Exception:
            logging.exception('Failed to delete package')
    elif tag == 'pipelines.delete':
        try:
            use_fixture(delete_pipeline, context)
        except Exception:
            logging.exception('Failed to delete package')
    elif tag == 'feature_set.delete':
        try:
            use_fixture(delete_feature_set, context)
        except Exception:
            logging.exception('Failed to delete feature set')
    elif tag == 'feature.delete':
        try:
            use_fixture(delete_feature, context)
        except Exception:
            logging.exception('Failed to delete feature set')
    elif tag == 'bot.create':
        try:
            use_fixture(delete_bots, context)
        except Exception:
            logging.exception('Failed to delete bots')
    elif tag == 'second_project.delete':
        try:
            use_fixture(delete_second_project, context)
        except Exception:
            logging.exception('Failed to delete second project')
    elif tag == 'converter.platform_dataset.delete':
        try:
            use_fixture(delete_converter_dataset, context)
        except Exception:
            logging.exception('Failed to delete converter dataset')
    elif tag == 'datasets.delete':
        try:
            use_fixture(datasets_delete, context)
        except Exception:
            logging.exception('Failed to delete dataset')
    elif tag == 'drivers.delete':
        try:
            use_fixture(drivers_delete, context)
        except Exception:
            logging.exception('Failed to delete driver')
    elif tag == 'models.delete':
        try:
            use_fixture(models_delete, context)
        except Exception:
            logging.exception('Failed to delete model')
    elif tag == 'setenv.reset':
        try:
            use_fixture(reset_setenv, context)
        except Exception:
            logging.exception('Failed to reset env')
    elif tag == 'restore_json_file':
        try:
            use_fixture(restore_json_file, context)
        except Exception:
            logging.exception('Failed to restore json file')
    elif tag == 'compute_serviceDriver.delete':
        try:
            use_fixture(delete_compute_servicedriver, context)
        except Exception:
            logging.exception('Failed to delete service')
    elif tag == 'frozen_dataset':
        pass
    elif 'testrail-C' in tag:
        pass
    elif tag == 'wip':
        pass
    elif any(i_tag in tag for i_tag in ['DAT-', 'qa-', 'rc_only', 'skip_test']):
        pass
    else:
        raise ValueError('unknown tag: {}'.format(tag))


@fixture
def delete_second_project(context):
    if hasattr(context, 'second_project'):
        context.second_project.delete(True, True)


@fixture
def delete_bots(context):
    if not hasattr(context, 'to_delete_projects_ids'):
        return

    all_deleted = True
    while context.to_delete_projects_ids:
        project_id = context.to_delete_projects_ids.pop(0)
        try:
            project = context.dl.projects.get(project_id=project_id)
            for bot in project.bots.list():
                try:
                    bot.delete()
                except:
                    logging.exception('Failed deleting bots: ')
                    all_deleted = False
                    pass
        except context.dl.exceptions.NotFound:
            pass
        except:
            logging.exception('Failed deleting bots: ')
    assert all_deleted


@fixture
def delete_packages(context):
    if not hasattr(context, 'to_delete_packages_ids'):
        return

    all_deleted = True
    while context.to_delete_packages_ids:
        package_id = context.to_delete_packages_ids.pop(0)
        try:
            context.dl.packages.delete(package_id=package_id)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting package: ')
    assert all_deleted


@fixture
def delete_feature_set(context):
    if not hasattr(context, 'to_delete_feature_set_ids'):
        return

    all_deleted = True
    while context.to_delete_feature_set_ids:
        feature_set = context.to_delete_feature_set_ids.pop(0)
        try:
            context.dl.feature_sets.delete(feature_set_id=feature_set)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting feature_set: ')
    assert all_deleted


@fixture
def delete_feature(context):
    if not hasattr(context, 'to_delete_feature_ids'):
        return

    all_deleted = True
    while context.to_delete_feature_ids:
        feature = context.to_delete_feature_ids.pop(0)
        try:
            context.dl.feature.delete(feature_id=feature)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting feature: ')
    assert all_deleted


@fixture
def delete_pipeline(context):
    if not hasattr(context, 'to_delete_pipelines_ids'):
        return

    all_deleted = True
    while context.to_delete_pipelines_ids:
        pipeline_id = context.to_delete_pipelines_ids.pop(0)
        try:
            filters = context.dl.Filters(resource=context.dl.FiltersResource.EXECUTION, field='latestStatus.status',
                                         values=['created', 'in-progress'], operator='in')
            filters.add(field='pipeline.id', values=pipeline_id)
            executions = context.dl.executions.list(filters=filters)
            for execution in executions.items:
                execution.terminate()
            context.dl.pipelines.delete(pipeline_id=pipeline_id)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting pipeline: ')
    assert all_deleted


@fixture
def delete_converter_dataset(context):
    if hasattr(context, 'platform_dataset'):
        context.platform_dataset.delete(True, True)


@fixture
def delete_services(context):
    if not hasattr(context, 'to_delete_services_ids'):
        return

    all_deleted = True
    while context.to_delete_services_ids:
        service_id = context.to_delete_services_ids.pop(0)
        try:
            context.dl.services.delete(service_id=service_id)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting service: ')
    assert all_deleted


@fixture
def drivers_delete(context):
    if not hasattr(context, 'to_delete_drivers_ids'):
        return

    all_deleted = True
    time.sleep(25)  # Wait for datasets to delete
    for driver_id in context.to_delete_drivers_ids:
        try:
            context.project.drivers.delete(driver_id=driver_id, sure=True, really=True)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting driver: {}'.format(driver_id))
    assert all_deleted


@fixture
def datasets_delete(context):
    if not hasattr(context, 'to_delete_datasets_ids'):
        return

    all_deleted = True
    for dataset_id in context.to_delete_datasets_ids:
        try:
            context.project.datasets.delete(dataset_id=dataset_id, sure=True, really=True)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting dataset: {}'.format(dataset_id))
    assert all_deleted


@fixture
def reset_setenv(context):
    _, base_env = get_env_from_git_branch()
    cmds = ["dlp", "api", "setenv", "-e", "{}".format(base_env)]
    p = subprocess.Popen(cmds, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    context.out, context.err = p.communicate()
    # save return code
    context.return_code = p.returncode
    assert context.return_code == 0, "AFTER TEST FAILED: {}".format(context.err)


def print_feature_filename(context, feature):
    s_r = SummaryReporter(context.config)
    stream = getattr(sys, s_r.output_stream_name, sys.stderr)
    p_stream = StreamOpener.ensure_stream_with_encoder(stream)
    p_stream.write(f"Feature Finished : {feature.filename.split('/')[-1]}\n")
    p_stream.write(f"Status: {str(feature.status).split('.')[-1]} - Duration: {feature.duration:.2f} seconds\n")


@fixture
def models_delete(context):
    all_deleted = True
    if hasattr(context, 'to_delete_model_ids'):
        for model_id in context.to_delete_model_ids:
            try:
                context.project.models.delete(model_id=model_id)
            except context.dl.exceptions.NotFound:
                pass
            except:
                all_deleted = False
                logging.exception('Failed deleting model: {}'.format(model_id))

    for model in context.project.models.list().all():
        try:
            model.delete()
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting model: {}'.format(model.id))
    assert all_deleted


def delete_compute_servicedriver(context):
    if not hasattr(context, 'to_delete_computes_ids') and not hasattr(context, 'to_delete_service_drivers_ids'):
        return

    all_deleted = True
    for service_driver_id in context.to_delete_service_drivers_ids:
        try:
            context.dl.service_drivers.delete(service_driver_id=service_driver_id)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting serviceDriver: {}'.format(service_driver_id))
    assert all_deleted

    all_deleted = True
    for compute_id in context.to_delete_computes_ids:
        try:
            context.dl.computes.delete(compute_id=compute_id)
        except context.dl.exceptions.NotFound:
            pass
        except:
            all_deleted = False
            logging.exception('Failed deleting compute: {}'.format(compute_id))
    assert all_deleted


def restore_json_file(context):
    if not hasattr(context.feature, 'dataloop_feature_project'):
        return
    if not hasattr(context, 'backup_path') or not hasattr(context, 'original_path'):
        assert False, 'Please make sure to set the original_path and backup_path in the context'
        # Restore the file from the backup
    if os.path.exists(context.backup_path):
        shutil.copy(context.backup_path, context.original_path)
        os.remove(context.backup_path)  # Clean up the backup
    else:
        raise FileNotFoundError(f"Backup file not found for {context.original_path}")


================================================
File: tests/features/ann_text_object/test_annotation_text.feature
================================================
Feature: Annotations collection testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_collection"
        And I create a dataset with a random name


    @testrail-C4523048
    @DAT-46451
    Scenario: Annotation a text item
        When I upload a file in path "ann_text_object/tx.txt"
        Then Item exist in host
        When Item is annotated with annotations in file: "ann_text_object/text.json"
        Then Item annotations in host equal annotations in file "ann_text_object/text.json"
        And Object "Annotations" to_json() equals to Platform json.



================================================
File: tests/features/annotation_collection/test_annotation_collection.feature
================================================
Feature: Annotations collection testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_collection"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
              | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |

    @testrail-C4523040
    @DAT-46436
   Scenario: Update - image
       Given Classes in file: "classes_new.json" are uploaded to test Dataset
       And Item in path "assets_split/annotation_collection/0000000162.jpg" is uploaded to "Dataset"
       And Item is annotated with annotations in file: "annotations_new.json"
       And I get item annotation collection
       And I change all image annotations label to "ball"
       When I update annotation collection
       Then Image annotations in host have label "ball"

    @testrail-C4523040
    @DAT-46436
    Scenario: Update - video
        Given Labels in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "video_annotations.json"
        And I get item annotation collection
        And I change all annotations label to "ball"
        When I update annotation collection
        Then Annotations in host have label "ball"

    @testrail-C4523040
    @DAT-46436
    Scenario: Delete
        Given Classes in file: "classes_new.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_collection/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotation_collection/annotations_new.json"
        And I get item annotation collection
        When I delete annotation collection
        Then Item in host has no annotations

    @testrail-C4523040
    @DAT-46436
    Scenario: Upload - image
        Given Classes in file: "assets_split/annotation_collection/classes_new.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_collection/0000000162.jpg" is uploaded to "Dataset"
        And I create item annotation collection
        And I add a few annotations to image
        When I upload annotation collection
        Then Annotations in host equal annotations uploaded

    @testrail-C4523040
    @DAT-46436
    Scenario: Upload - video
        Given Labels in file: "assets_split/annotation_collection/video_classes.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_collection/sample_video.mp4" is uploaded to "Dataset"
        And I create item annotation collection
        And I add a few annotations to video
        And I add a few frames to annotations
        When I upload annotation collection
        Then Annotations in host equal annotations uploaded

    @testrail-C4523040
    @DAT-46436
   Scenario: Upload annotation from_json - with UTF-8 character
       Given Classes in file: "annotation_collection/classes.json" are uploaded to test Dataset
       And Item in path "assets_split/annotation_collection/0000000162.jpg" is uploaded to "Dataset"
       And I get AnnotationCollection from json "annotation_collection/annotations_new.json"
       When I upload annotation collection
       Then Annotations in host equal annotations uploaded



================================================
File: tests/features/annotation_entity/test_annotation_add.feature
================================================
Feature: Annotation Entity Add annotation

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_add"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |

    @testrail-C4523041
    @DAT-46437
    Scenario: Image - using add annotation method
        Given Classes in file: "classes_new.json" are uploaded to test Dataset
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        When I add annotation to item using add annotation method
        And I upload annotation created
        Then Item in host has annotation added

    @testrail-C4523041
    @DAT-46437
    Scenario: Video - using add annotation method
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I add annotation to item using add annotation method
        And I add some frames to annotation
        And I upload annotation created
        Then Item in host has annotation added

    @testrail-C4523041
    @DAT-46437
    Scenario: Audio - using add annotation method
        Given Classes in file: "classes_new.json" are uploaded to test Dataset
        And Item in path "simple_audio.mp3" is uploaded to "Dataset"
        When I add annotation to audio using add annotation method
        Then audio in host has annotation added

    @DAT-56077
    Scenario: annotation color is set to default
        Given Item in path "0000000162.jpg" is uploaded to "Dataset"
        And I have a segmentation annotation
        And Classes in file: "classes_new.json" are uploaded to test Dataset
        Then annotation color is set to recipe color

================================================
File: tests/features/annotation_entity/test_annotation_attributes.feature
================================================
Feature: Ontology Entity attributes testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "annotation_attributes"
    And I create a dataset with a random name
    And Dataset has ontology
    And Classes in file: "classes_new.json" are uploaded to test Dataset

  @DAT-69516
  Scenario: Add attributes to annotations
    When I add "yes_no" attribute to ontology
      | key=4 | title=test4 | scope=all |
    Then I validate attribute "yes_no" added to ontology
    Given Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I add annotation with attrs "4" "true" to item using add annotation method
    And I upload annotation created
    Then Item in host has annotation added
    Then I validate annotation has attribute "4" with value "true"

  @DAT-71099
  Scenario: Update attributes in annotations
    When I add "yes_no" attribute to ontology
      | key=4 | title=test4 | scope=all |
    Then I validate attribute "yes_no" added to ontology
    Given Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I add annotation with attrs "4" "true" to item using add annotation method
    And I upload annotation created
    And I update annotation attributes with params
      | key | value |
      | 4   | false |
    Then Item in host has annotation added
    Then I validate annotation has attribute "4" with value "false"

  @DAT-71100
  Scenario: Remove attributes from annotations
    When I add "yes_no" attribute to ontology
      | key=4 | title=test4 | scope=all |
    Then I validate attribute "yes_no" added to ontology
    Given Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I add annotation with attrs "4" "true" to item using add annotation method
    And I upload annotation created
    And I update annotation attributes to empty dict
    Then Item in host has annotation added
    Then I validate annotation has no attributes


  @DAT-85316
  Scenario: Add attributes to annotations
    When I add "yes_no" attribute to ontology
      | key=4 | title=test4 | scope=all |
    Then I validate attribute "yes_no" added to ontology
    Given Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I add annotation with attrs "4" "true" to item using add annotation method
    And I upload annotation created
    And I add annotation to item using add annotation method
    And I add annotation to item using add annotation method with empty dict attrs
    Then I validate annotation has all expected attributes

================================================
File: tests/features/annotation_entity/test_annotation_description.feature
================================================
Feature: Annotation description

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And   I create a project by the name of "annotations_description_project"
        And   I create a dataset by the name of "annotations_description_dataset" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |


    @testrail-C4532735
    @DAT-46438
    Scenario: Add and delete annotation description to uploaded item - image item
        Given I upload an item in the path "0000000162.jpg" to the dataset
        And   I upload annotation in the path "0162_annotations.json" to the item
        When  I add description "Annotation description" to the annotation
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


================================================
File: tests/features/annotation_entity/test_annotation_description_audio_item.feature
================================================
Feature: Annotation description with builder - audio item

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And   I create a project by the name of "annotations_description_project"
        And   I create a dataset by the name of "annotations_description_dataset" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532807
    @DAT-46439
    Scenario: Upload classification annotation with description
        Given I upload an item of type "mp3 audio" to the dataset
        And   I upload "subtitle" annotation with description "Annotation description" to the audio item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


================================================
File: tests/features/annotation_entity/test_annotation_description_image_item.feature
================================================
Feature: Annotation description with builder - image item

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And   I create a project by the name of "annotations_description_project"
        And   I create a dataset by the name of "annotations_description_dataset" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532805
    @DAT-46440
    Scenario: Upload classification annotation with description
        Given I upload an item of type "bmp image" to the dataset
        Given I upload "classification" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload point annotation with description
        Given I upload an item of type "jfif image" to the dataset
        And   I upload "point" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload box annotation with description
        Given I upload an item of type "jpeg image" to the dataset
        And   I upload "box" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload rotated box annotation with description
        Given I upload an item of type "jpg image" to the dataset
        And   I upload "rotated box" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload rotated cube annotation with description
        Given I upload an item of type "png image" to the dataset
        And   I upload "cube" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload semantic segmentation annotation with description
        Given I upload an item of type "tif image" to the dataset
        And   I upload "semantic segmentation" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload polygon annotation with description
        Given I upload an item of type "tiff image" to the dataset
        And   I upload "polygon" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload polyline annotation with description
        Given I upload an item of type "webp image" to the dataset
        And   I upload "polyline" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload ellipse annotation with description
        Given I upload an item of type "bmp image" to the dataset
        And   I upload "ellipse" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532805
    @DAT-46440
    Scenario: Upload note annotation with description
        Given I upload an item of type "jfif image" to the dataset
        And   I upload "note" annotation with description "Annotation description" to the image item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


================================================
File: tests/features/annotation_entity/test_annotation_description_text_item.feature
================================================
Feature: Annotation description with builder - text item

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And   I create a project by the name of "annotations_description_project"
        And   I create a dataset by the name of "annotations_description_dataset" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532808
    @DAT-46441
    Scenario: Upload classification annotation with description
        Given I upload an item of type "txt text" to the dataset
        And   I upload "classification" annotation with description "Annotation description" to the text item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532808
    @DAT-46441
    Scenario: Upload point annotation with description
        Given I upload an item of type "txt text" to the dataset
        And   I upload "text mark" annotation with description "Annotation description" to the text item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


================================================
File: tests/features/annotation_entity/test_annotation_description_video_item.feature
================================================
Feature: Annotation description with builder - video item

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And   I create a project by the name of "annotations_description_project"
        And   I create a dataset by the name of "annotations_description_dataset" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532806
    @DAT-46442
    Scenario: Upload classification annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "classification" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload point annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "point" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload box annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "box" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload rotated box annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "rotated box" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload rotated cube annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "cube" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload polygon annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "polygon" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload polyline annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "polyline" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


    @testrail-C4532806
    @DAT-46442
    Scenario: Upload note annotation with description
        Given I upload an item of type "webm video" to the dataset
        And   I upload "note" annotation with description "Annotation description" to the video item
        Then  I validate annotation.description has "Annotation description" value
        When  I remove description from the annotation
        Then  I validate annotation.description has "None" value


================================================
File: tests/features/annotation_entity/test_annotation_draw.feature
================================================
Feature: Annotation Entity Draw annotation

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_draw"
        And I create a dataset with a random name

    @testrail-C4523042
    @DAT-46443
    Scenario: Draw - mask
        Given Classes in file: "classes_new.json" are uploaded to test Dataset
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |
        When Item is annotated with annotations in file: "annotations_new.json"
        And I draw to image in "0000000162.jpg" all annotations with param "mask"
        Then Annotations drawn equal numpy in "draw_image_mask_should_be.npy"

    #TODO - not implemented yet
    # Scenario: Draw - instance
    #     Given Classes in file: "classes_new.json" are uploaded to test Dataset
    #     And Dataset ontology has attributes "attr1" and "attr2"
    #     And Item in path "0000000162.jpg" is uploaded to "Dataset"
    #     When Item is annotated with annotations in file: "annotations_new.json"
    #     And I draw to image in "0000000162.jpg" all annotations with param "instance"
    #     Then Annotations drawn equal numpy in "draw_image_instance_should_be.npy"




================================================
File: tests/features/annotation_entity/test_annotation_json_to_object.feature
================================================
Feature: Annotation Entity Json to Object testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_json_to_object"
        And I create a dataset with a random name

    @testrail-C4523043
    @DAT-46444
    Scenario: Image 
        Given Classes in file: "assets_split/ann_json_to_object/classes_new.json" are uploaded to test Dataset
        And Item in path "assets_split/ann_json_to_object/0000000162.jpg" is uploaded to "Dataset"
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |
        When Item is annotated with annotations in file: "assets_split/ann_json_to_object/annotations_new.json"
        Then Item annotations in host equal annotations in file "assets_split/ann_json_to_object/annotations_new.json"
        And Object "Annotations" to_json() equals to Platform json.

    @testrail-C4523043
    @DAT-46444
    Scenario: Video
        Given Classes in file: "assets_split/ann_json_to_object/video_classes.json" are uploaded to test Dataset
        And Item in path "assets_split/ann_json_to_object/sample_video.mp4" is uploaded to "Dataset"
        When Item is annotated with annotations in file: "assets_split/ann_json_to_object/video_annotations.json"
        Then Item annotations in host equal annotations in file "assets_split/ann_json_to_object/video_annotations.json"
        And Object "Annotations" to_json() equals to Platform json.


================================================
File: tests/features/annotation_entity/test_annotation_repo_methods.feature
================================================
Feature: Annotation Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_repo_methods"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |

    @testrail-C4523044
    @DAT-46445
    Scenario: Annotation delete
        Given Labels in file: "assets_split/annotation_repo/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_repo/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotation_repo/0162_annotations.json"
        And There is annotation x
        When I delete entity annotation x
        Then Annotation x does not exist in item

    @testrail-C4523044
    @DAT-46445
    Scenario: Updateing annotations entity
        Given Labels in file: "assets_split/annotation_repo/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_repo/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotation_repo/0162_annotations.json"
        And There is annotation x
        And I change annotation x label to "new_label"
        When I update annotation entity
        And I get annotation x from host
        Then Annotation x in host has label "new_label"

    @testrail-C4523044
    @DAT-46445
    Scenario: Annotation download
        Given Labels in file: "assets_split/annotation_repo/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_repo/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotation_repo/0162_annotations.json"
        And There are no files in folder "downloaded_annotations"
        And There is annotation x
        When I download Entity annotation x with "mask" to "downloaded_annotations/mask.png"
        And I download Entity annotation x with "instance" to "downloaded_annotations/instance.png"
        Then Item annotation "mask" has been downloaded to "downloaded_annotations"
        And Item annotation "instance" has been downloaded to "downloaded_annotations"

    @testrail-C4523044
    @DAT-46445
    Scenario: Uploading annotations - Image
        Given Labels in file: "assets_split/annotation_repo/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_repo/0000000162.jpg" is uploaded to "Dataset"
        And I create an annotation
        When I upload annotation entity to item
        Then Item in host has annotation entity created

    @testrail-C4523044
    @DAT-46445
    Scenario: Uploading annotations - Video
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        Given I create a video annotation
        When I upload video annotation entity to item
        Then Item in host has video annotation entity created

    @DAT-44231
    Scenario: Upload items batch with random annotations
        Given Labels in file: "assets_split/annotation_repo/labels.json" are uploaded to test Dataset
        When I upload item batch from "upload_batch/to_upload"
        And I upload random x annotations
        Then analytic should say I have x annotations

    @DAT-45327
    Scenario: same frames - Video
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I create video annotation by frames
        And I add same frames to video annotation
        Then Item in host has video annotation entity created
        And annotation do not have snapshots

    @DAT-77996
    Scenario: UpdatedBy annotation validation
        Given Labels in file: "assets_split/annotation_repo/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_repo/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotation_repo/0162_annotations.json"
        And There is annotation x
        And I change annotation x label to "new_label"
        When I update annotation entity
        And I create filters
        And I add "annotations" filter with "updatedBy" and "dataset.updated_by"
        Then I verify that I have annotations


================================================
File: tests/features/annotation_entity/test_create_video_annotation_from_blank.feature
================================================
Feature: Annotation Entity create video annotation from blank

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_create_video_annotation_from_blank"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |

    @testrail-C4523045
    @DAT-46446
    Scenario: Video - using add annotation method
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I create a blank annotation to item
        And I add frames to annotation
        And I upload annotation created
        Then Item in host has video annotation added

    @testrail-C4523045
    @DAT-46446
    Scenario: Video - Add annotation to video with fix frame false
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I create a blank annotation to item
        And I create a false fixed annotation in video
        And I upload annotation created
        Then Video has annotation without snapshots


    @testrail-C4523045
    @DAT-46446
    Scenario: Video - get annotation from dl
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I create a blank annotation to item
        And I add frames to annotation
        And I upload annotation created
        And I get annotation using dl
        Then I validate annotation have frames

    @testrail-C4523045
    @DAT-46446
    Scenario: Video - change attrs for frames
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I add class annotation to item using add annotation method
        And I set frame "3" annotation attributes
        And I upload annotation created
        Then I validity "3" has the updated attributes


================================================
File: tests/features/annotation_entity/test_segmentation_to_box.feature
================================================
Feature: Annotation Segmentation to box

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_add"
        And I create a dataset with a random name
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        And Classes in file: "classes_new.json" are uploaded to test Dataset

    @testrail-C4523046
    @DAT-46447
    Scenario: Segmentation to box
        Given I have a segmentation annotation
        When I execute to_box function on segmentation annotation
        Then Box will be generate

    @testrail-C4523046
    @DAT-46447
    Scenario: Box from Segmentation
        Given I have a segmentation annotation
        When I create Box annotation with  from_segmentation function with mask
        Then Box will be generate

    @testrail-C4523046
    @DAT-46447
    Scenario: Multi segmentation to boxes
        Given I have a multi segmentation annotations
        When I execute to_box function on segmentation annotation
        Then Boxes will be generate


    @testrail-C4523046
    @DAT-46447
    Scenario: Box from Segmentation
        Given I have a multi segmentation annotations
        When I create Box annotation with  from_segmentation function with mask
        Then Boxes will be generate



================================================
File: tests/features/annotation_entity/test_segmentation_to_polygon.feature
================================================
Feature: Annotation Segmentation to polygon

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And   I create a project by the name of "segmentation_to_polygon_project"

  @testrail-C4533901
  @DAT-46448
  Scenario: Segmentation to polygon
    Given I create a dataset with a random name
    And   Item in path "0000000162.jpg" is uploaded to "Dataset"
    And   Classes in file: "classes_new.json" are uploaded to test Dataset
    And   I have a segmentation annotation
    When  I call Polygon.from_segmentation() using "1" nax_instances
    Then  The polygon will match to the json file "segmentation_to_polygon/0000000162_single.json"

  @testrail-C4533901
  @DAT-46448
  Scenario: Multi segmentation to polygons
    Given I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"
    And   Classes in file: "classes_new.json" are uploaded to test Dataset
    And   I have a multi segmentation annotations
    When  I call Polygon.from_segmentation() using "2" nax_instances
    Then  The polygon will match to the json file "segmentation_to_polygon/0000000162_multi.json"



================================================
File: tests/features/annotation_entity/test_upload_annotations.feature
================================================
Feature: Upload annotation testing

    Background: Initiate Platform Interface
      Given Platform Interface is initialized as dlp and Environment is set according to git branch
      And I create a project by the name of "upload_annotations"
      And I create a dataset with a random name
      When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
      When I add "free_text" attribute to ontology
          | key=2 | title=attr2 | scope=all |

    @testrail-C4523047
    @DAT-46449
    Scenario: Upload image annotations from file
          Given Classes in file: "assets_split/annotations_upload/classes_new.json" are uploaded to test Dataset
          And Item in path "assets_split/annotations_upload/0000000162.jpg" is uploaded to "Dataset"
          When Item is annotated with annotations in file: "assets_split/annotations_upload/annotations_new.json"
          Then Item annotations in host equal annotations in file "assets_split/annotations_upload/annotations_new.json"

    @testrail-C4523047
    @DAT-46449
    Scenario: Upload video annotations from file
          Given Classes in file: "assets_split/annotations_upload/video_classes.json" are uploaded to test Dataset
          And Item in path "assets_split/annotations_upload/sample_video.mp4" is uploaded to "Dataset"
          When Item is annotated with annotations in file: "assets_split/annotations_upload/video_annotations.json"
          Then Item video annotations in host equal annotations in file "assets_split/annotations_upload/video_annotations.json"

    @testrail-C4523047
    @DAT-46449
    Scenario: Upload audio annotations from file
          Given Classes in file: "classes_new.json" are uploaded to test Dataset
          And Item in path "simple_audio.mp3" is uploaded to "Dataset"
          When Item is annotated with annotations in file: "audio_annotations.json"
          Then audio in host has annotation added





================================================
File: tests/features/annotation_entity/test_video_annotation_updated.feature
================================================
Feature: Annotation Entity update video annotation start time

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_video_annotation_updated"
        And I create a dataset with a random name


    @testrail-C4532496
    @DAT-46450
    Scenario: Video - Update start time should update in snapshot
        Given Classes in file: "video_classes.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        When I create a blank annotation to item
        And I add frames to annotation
        And I upload annotation created
        And I update annotation start time to "1"
        Then I validate snapshot has the correct start frame


    @testrail-C4532496
    @DAT-46450
    Scenario: Audio - Update time should update annotation
        Given Classes in file: "classes_new.json" are uploaded to test Dataset
        And Item in path "simple_audio.mp3" is uploaded to "Dataset"
        When I add annotation to audio using add annotation method
        And I update annotation start time "0" end time "27"
        Then I validate audio has the correct start and end time


================================================
File: tests/features/annotations_repo/test_annotations_adding_multiple_frames.feature
================================================
Feature: Annotations adding multiple frames

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_annotations_format_json"
    And I create a dataset by the name of "Dataset_annotations_format_json" in the project
    And I upload an item of type "webm video" to the dataset
    When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

  @testrail-C4533707
  @DAT-46419
  Scenario: Annotations with the same object_id wont get overwritten
    When   I upload "5" annotation with the same object id
    Then   I check that I got "5" keyframes

  @DAT-45661
  Scenario: frame attribute update
    When   I upload "5" annotation with the same object id
    When I update the "11" frame attribute of the annotation
    Then   I the only frame "11" attribute is updated


================================================
File: tests/features/annotations_repo/test_annotations_compare_snapshots_with_platform.feature
================================================
Feature: Annotaions repository Get service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "annotations_get"
        And I create a dataset with a random name

    @testrail-C4525771
    @DAT-46420
    Scenario: Get an existing annotation by id
        Given Classes in file: "assets_split/annotations_upload/video_classes.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_upload/sample_video.mp4" is uploaded to "Dataset"
        When Item is annotated with annotations in file: "test_snapshots/snapshots_video_annotations.json"
        And I get the only annotation
        Then Annotation snapshots equal to platform snapshots



================================================
File: tests/features/annotations_repo/test_annotations_context.feature
================================================
Feature: Annotation repository Context testing

    Background: Initiate Platform Interface and create a projects, datasets and Item
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "project1 project2"
        And I create datasets by the name of "dataset1 dataset2"
        And I set Dataset to Dataset 1
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        And I append item to Items
        And Classes in file: "classes_new.json" are uploaded to test Dataset
        And I have a segmentation annotation

    @testrail-C4523031
    @DAT-46421
    Scenario: Get Annotation from the Dataset it belong to
        When I get the annotation from dataset number 1
        Then Annotation dataset_id is equal to dataset 1 id
        And Annotation dataset.id is equal to dataset 1 id
        And Annotation item_id is equal to item 1 id
        And Annotation item.id is equal to item 1 id

    @testrail-C4523031
    @DAT-46421
    Scenario: Get Annotation from the Dataset it not belong to
        When I get the annotation from dataset number 2
        Then Annotation dataset_id is equal to dataset 1 id
        And Annotation dataset.id is equal to dataset 1 id
        And Annotation item_id is equal to item 1 id
        And Annotation item.id is equal to item 1 id

    @testrail-C4523031
    @DAT-46421
    Scenario: Get Annotation from the Item it belong to
        When I get the annotation from item number 1
        Then Annotation dataset_id is equal to dataset 1 id
        And Annotation dataset.id is equal to dataset 1 id
        And Annotation item_id is equal to item 1 id
        And Annotation item.id is equal to item 1 id

    @testrail-C4523031
    @DAT-46421
    Scenario: Get Annotation from the Item it not belong to
        Given I set Dataset to Dataset 2
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        And I append Item to Items
        When I get the annotation from item number 2
        Then Annotation dataset_id is equal to dataset 1 id
        And Annotation dataset.id is equal to dataset 1 id
        And Annotation item_id is equal to item 1 id
        And Annotation item.id is equal to item 1 id


================================================
File: tests/features/annotations_repo/test_annotations_delete.feature
================================================
Feature: Annotaions repository Delete service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotations_delete"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |

    @testrail-C4523032
    @DAT-46422
    Scenario: Delete annotation
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        And There is annotation x
        When I delete a annotation x
        Then Annotation x does not exist in item

    @testrail-C4523032
    @DAT-46422
    Scenario: Delete a non-existing Annotation
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        When I try to delete a non-existing annotation
        Then "NotFound" exception should be raised
        And No annotation was deleted

    @testrail-C4523032
    @DAT-46422
    Scenario: Delete Annotation using filters on Dataset level
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And Item in path "artifacts_repo/artifact_item.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And I count other Annotation except "box" using "dataset" entity
        When I delete annotation from type "box" using "dataset" entity
        Then I verify that I has the right number of annotations

    @testrail-C4523032
    @DAT-46422
    Scenario: Delete Annotation using filters on Item level
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And Item in path "artifacts_repo/artifact_item.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And I count other Annotation except "box" using "item" entity
        When I delete annotation from type "box" using "item" entity
        Then I verify that I has the right number of annotations


================================================
File: tests/features/annotations_repo/test_annotations_download.feature
================================================
Feature: Annotaions repository download service testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_annotations_download"
    And I create a dataset with a random name
    When I add "free_text" attribute to ontology
      | key=1 | title=attr1 | scope=all |
    When I add "free_text" attribute to ontology
      | key=2 | title=attr2 | scope=all |

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "mask" to "downloaded_annotations/mask.png"
    Then Item annotation "mask" has been downloaded to "downloaded_annotations"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with instance
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "instance" to "downloaded_annotations/instance.png"
    Then Item annotation "instance" has been downloaded to "downloaded_annotations"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with json
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "json" to "downloaded_annotations/json.json"
    Then Item annotation "json" has been downloaded to "downloaded_annotations"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with default
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "default" to "downloaded_annotations/json.json"
    Then Item annotation "json" has been downloaded to "downloaded_annotations"


  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with img_mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "img_mask" to "downloaded_annotations/img_mask.png"
    Then Item annotation "img_mask" has been downloaded to "downloaded_annotations"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with vtt
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "vtt" to "downloaded_annotations/vtt.vtt"
    Then Item annotation "vtt" has been downloaded to "downloaded_annotations"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with No specific ViewAnnotationOptions
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    Then I download the items annotations with ViewAnnotationOptions "None" enum to find "Unknown annotation download option"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with JSON ViewAnnotationsOptions
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    Then I download the items annotations with ViewAnnotationOptions "JSON" enum to find "downloaded_annotations/json/0000000162.json"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with VVT ViewAnnotationOptions
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    Then I download the items annotations with ViewAnnotationOptions "VTT" enum to find "downloaded_annotations/vvt/0000000162.vtt"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download item annotations with MASK ViewAnnotationOptions
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    Then I download the items annotations with ViewAnnotationOptions "MASK" enum to find "downloaded_annotations/mask/0000000162.png"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download annotation by id
    Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
    And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
    And There is annotation x
    And There are no files in folder "downloaded_annotations"
    When I get the annotation by id
    Then I download the annotation to "downloaded_annotations/annotation/annotation.png" with "mask" type
    And annotation file exist in the path "downloaded_annotations/annotation"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download annotation by id VTT
    Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
    And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
    And There is annotation x
    And There are no files in folder "downloaded_annotations"
    When I get the annotation by id
    Then I download the annotation to "downloaded_annotations/annotation/annotation.vtt" with "vtt" type
    And annotation file exist in the path "downloaded_annotations/annotation"

  @testrail-C4523033
  @DAT-46423
  Scenario: Download video annotation by id
    Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/ann_json_to_object/sample_video.mp4" is uploaded to "Dataset"
    And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
    And There is annotation x
    And There are no files in folder "downloaded_annotations"
    When I get the annotation by id
    Then I download the annotation to "downloaded_annotations/annotation/annotation.mp4" with "mask" type
    And annotation file exist in the path "downloaded_annotations/annotation"


  @testrail-C4523033
  @DAT-46423
  Scenario: Download dataset annotations with img_mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download dataset annotations with "img_mask" to "downloaded_annotations/img_mask.png"
    Then dataset "img_mask" folder has been downloaded to "downloaded_annotations"

  @DAT-47547
  Scenario: Download annotation by id - with out file type
    Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
    And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
    And There is annotation x
    And There are no files in folder "downloaded_annotations"
    When I get the annotation by id
    Then I download the annotation to "downloaded_annotations/annotation" with "mask" type
    And annotation file exist in the path "downloaded_annotations/annotation"


================================================
File: tests/features/annotations_repo/test_annotations_download_video.feature
================================================
Feature: Annotaions repository download service testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_annotations_download"
    And I create a dataset with a random name
    When I add "free_text" attribute to ontology
      | key=1 | title=attr1 | scope=all |
    When I add "free_text" attribute to ontology
      | key=2 | title=attr2 | scope=all |

  @testrail-C4532194
  @DAT-46424
  Scenario: Download item annotations with mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/video_test.mp4" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "mask" to "downloaded_annotations/mask.mp4"
    Then video Item annotation "mask" has been downloaded to "downloaded_annotations"

  @testrail-C4532194
  @DAT-46424
  Scenario: Download item annotations with instance
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/video_test.mp4" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "instance" to "downloaded_annotations/instance.mp4"
    Then video Item annotation "instance" has been downloaded to "downloaded_annotations"

  @testrail-C4532194
  @DAT-46424
  Scenario: Download item annotations with json
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/video_test.mp4" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "json" to "downloaded_annotations/json.json"
    Then video Item annotation "json" has been downloaded to "downloaded_annotations"


  @testrail-C4532194
  @DAT-46424
  Scenario: Download item annotations with img_mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/video_test.mp4" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations with "img_mask" to "downloaded_annotations/img_mask.mp4"
    Then video Item annotation "img_mask" has been downloaded to "downloaded_annotations"


================================================
File: tests/features/annotations_repo/test_annotations_draw.feature
================================================
Feature: Annotations repository Draw method testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_annotations_draw"
        And I create a dataset with a random name

    @testrail-C4523034
    @DAT-46425
    Scenario: Draw mask
        Given Classes in file: "classes_new.json" are uploaded to test Dataset
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When I add "free_text" attribute to ontology
            | key=2 | title=attr2 | scope=all |
        Given Item in path "0000000162.jpg" is uploaded to "Dataset"
        When Item is annotated with annotations in file: "annotations_new.json"
        And I draw items annotations with param "mask" to image in "0000000162.jpg"
        Then I receive annotations mask and it is equal to mask in "draw_collection_should_be.npy"

    #TODO  - not implemented
    # Scenario: Draw instance
    #     Given Classes in file: "classes_new.json" are uploaded to test Dataset
    #     And Dataset ontology has attributes "attr1" and "attr2"
    #     And Item in path "0000000162.jpg" is uploaded to "Dataset"
    #     When Item is annotated with annotations in file: "annotations_new.json"
    #     And I draw items annotations with param "instance"
    #     Then I receive annotations mask and it is equal to mask in "new_instance_should_be_draw.npy"




================================================
File: tests/features/annotations_repo/test_annotations_edit.feature
================================================
Feature: Annotaions repository update service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_annotations_edit"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4523035
    @DAT-46426
    Scenario: Updateing annotations: remove attributes
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And There are no files in folder "downloaded_annotations"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        And I remove annotations attributes
        When I update annotations
        Then  Item annotations has no attributes

    @testrail-C4523035
    @DAT-46426
    Scenario: Updateing annotations: start and end time
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "sample_video.mp4" is uploaded to "Dataset"
        And There are no files in folder "downloaded_annotations"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        And There is annotation x
        And I set start frame to "20" and end frame to "70"
        When I update annotation
        Then annotation x metadata should be changed accordingly





================================================
File: tests/features/annotations_repo/test_annotations_format_json.feature
================================================
Feature: Annotations format json

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_annotations_format_json"
        And I create a dataset by the name of "Dataset_annotations_format_json" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532882
    @DAT-46427
    Scenario: Check default download is json
        Given I upload an item of type "png image" to the dataset
        And   I upload "box" annotation to the image item
        And   I create the dir path "annotations_format_json"
        When  I call Annotation.download() using the given params
             | Parameter | Value                                    |
             | filepath  | annotations_format_json/annotations.json |
        Then  I delete content in path path "annotations_format_json"


    @testrail-C4532882
    @DAT-46427
    Scenario: Check all json download options - image item
        Given I upload an item of type "png image" to the dataset
        And   I upload "box" annotation to the image item
        And   I create the dir path "annotations_format_json1"
        And   I create the dir path "annotations_format_json2"
        And   I create the dir path "annotations_format_json3"
        Then  I delete content in path path "annotations_format_json1"
        And   I delete content in path path "annotations_format_json2"
        And   I delete content in path path "annotations_format_json3"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                        |
             | filepath           | annotations_format_json1/png_image_item.json |
             | annotation_format  | JSON                                         |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                    |
             | filepath           | annotations_format_json2 |
             | annotation_format  | JSON                     |
        Then  I compare json annotations between the files in dirs
             | Parameter           | Value                    |
             | annotation_json_dir | annotations_format_json1 |
             | item_json_dir       | annotations_format_json2 |
        Given I init Filters() using the given params
             | Parameter | Value |
             | resource  | ITEM  |
        When  I call Filters.add() using the given params
             | Parameter | Value              |
             | field     | name               |
             | values    | png_image_item.png |
        And   I call dataset.download_annotations() using the given params
             | Parameter          | Value                    |
             | local_path         | annotations_format_json3 |
             | annotation_options | JSON                     |
        Then  I compare json metadata and annotationsCount between the files in dirs
             | Parameter        | Value                         |
             | item_json_dir    | annotations_format_json2      |
             | dataset_json_dir | annotations_format_json3/json |

    @testrail-C4532882
    @DAT-46427
    Scenario: Check all json download options - video item
        Given I upload an item of type "webm video" to the dataset
        And   I upload "box" annotation to the video item
        And   I create the dir path "annotations_format_json1"
        And   I create the dir path "annotations_format_json2"
        And   I create the dir path "annotations_format_json3"
        Then  I delete content in path path "annotations_format_json1"
        And   I delete content in path path "annotations_format_json2"
        And   I delete content in path path "annotations_format_json3"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                         |
             | filepath           | annotations_format_json1/webm_video_item.json |
             | annotation_format  | JSON                                          |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                    |
             | filepath           | annotations_format_json2 |
             | annotation_format  | JSON                     |
        Then  I compare json annotations between the files in dirs
             | Parameter           | Value                    |
             | annotation_json_dir | annotations_format_json1 |
             | item_json_dir       | annotations_format_json2 |
        Given I init Filters() using the given params
             | Parameter | Value |
             | resource  | ITEM  |
        When  I call Filters.add() using the given params
             | Parameter | Value                |
             | field     | name                 |
             | values    | webm_video_item.webm |
        And   I call dataset.download_annotations() using the given params
             | Parameter          | Value                    |
             | local_path         | annotations_format_json3 |
             | annotation_options | JSON                     |
        Then  I compare json metadata and annotationsCount between the files in dirs
             | Parameter        | Value                         |
             | item_json_dir    | annotations_format_json2      |
             | dataset_json_dir | annotations_format_json3/json |

    @testrail-C4532882
    @DAT-46427
    Scenario: Check all json download options - audio item
        Given I upload an item of type "mp3 audio" to the dataset
        And   I upload "subtitle" annotation to the audio item
        And   I create the dir path "annotations_format_json1"
        And   I create the dir path "annotations_format_json2"
        And   I create the dir path "annotations_format_json3"
        Then  I delete content in path path "annotations_format_json1"
        And   I delete content in path path "annotations_format_json2"
        And   I delete content in path path "annotations_format_json3"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                        |
             | filepath           | annotations_format_json1/mp3_audio_item.json |
             | annotation_format  | JSON                                         |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                    |
             | filepath           | annotations_format_json2 |
             | annotation_format  | JSON                     |
        Then  I compare json annotations between the files in dirs
             | Parameter           | Value                    |
             | annotation_json_dir | annotations_format_json1 |
             | item_json_dir       | annotations_format_json2 |
        Given I init Filters() using the given params
             | Parameter | Value |
             | resource  | ITEM  |
        When  I call Filters.add() using the given params
             | Parameter | Value              |
             | field     | name               |
             | values    | mp3_audio_item.mp3 |
        And   I call dataset.download_annotations() using the given params
             | Parameter          | Value                    |
             | local_path         | annotations_format_json3 |
             | annotation_options | JSON                     |
        Then  I compare json metadata and annotationsCount between the files in dirs
             | Parameter        | Value                         |
             | item_json_dir    | annotations_format_json2      |
             | dataset_json_dir | annotations_format_json3/json |

    @testrail-C4532882
    @DAT-46427
    Scenario: Check all json download options - text item
        Given I upload an item of type "txt text" to the dataset
        And   I upload "text mark" annotation to the text item
        And   I create the dir path "annotations_format_json1"
        And   I create the dir path "annotations_format_json2"
        And   I create the dir path "annotations_format_json3"
        Then  I delete content in path path "annotations_format_json1"
        And   I delete content in path path "annotations_format_json2"
        And   I delete content in path path "annotations_format_json3"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                        |
             | filepath           | annotations_format_json1/txt_text_item.json |
             | annotation_format  | JSON                                         |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                    |
             | filepath           | annotations_format_json2 |
             | annotation_format  | JSON                     |
        Then  I compare json annotations between the files in dirs
             | Parameter           | Value                    |
             | annotation_json_dir | annotations_format_json1 |
             | item_json_dir       | annotations_format_json2 |
        Given I init Filters() using the given params
             | Parameter | Value |
             | resource  | ITEM  |
        When  I call Filters.add() using the given params
             | Parameter | Value             |
             | field     | name              |
             | values    | txt_text_item.txt |
        And   I call dataset.download_annotations() using the given params
             | Parameter          | Value                    |
             | local_path         | annotations_format_json3 |
             | annotation_options | JSON                     |
        Then  I compare json metadata and annotationsCount between the files in dirs
             | Parameter        | Value                         |
             | item_json_dir    | annotations_format_json2      |
             | dataset_json_dir | annotations_format_json3/json |


================================================
File: tests/features/annotations_repo/test_annotations_format_mask.feature
================================================
Feature: Annotations format mask

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_annotations_format_mask"
        And I create a dataset by the name of "Dataset_annotations_format_mask" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532883
    @DAT-46428
    Scenario: Check all mask download options - image item
        Given I upload an item of type "png image" to the dataset
        And   I upload "box" annotation to the image item
        And   I create the dir path "annotations_format_mask1"
        And   I create the dir path "annotations_format_mask2"
        Then  I delete content in path path "annotations_format_mask1"
        And   I delete content in path path "annotations_format_mask2"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                       |
             | filepath           | annotations_format_mask1/png_image_item.png |
             | annotation_format  | MASK                                        |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                    |
             | filepath           | annotations_format_mask2 |
             | annotation_format  | MASK                     |
        Then  I compare between the dirs
             | Parameter | Value                    |
             | dir1      | annotations_format_mask1 |
             | dir2      | annotations_format_mask2 |

    @testrail-C4532883
    @DAT-46428
    Scenario: Check all mask download options - video item
        Given I upload an item of type "mp4 video" to the dataset
        And   I upload "box" annotation to the video item
        And   I create the dir path "annotations_format_mask1"
        And   I create the dir path "annotations_format_mask2"
        Then  I delete content in path path "annotations_format_mask1"
        And   I delete content in path path "annotations_format_mask2"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                       |
             | filepath           | annotations_format_mask1/mp4_video_item.mp4 |
             | annotation_format  | MASK                                        |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                    |
             | filepath           | annotations_format_mask2 |
             | annotation_format  | MASK                     |
        Then  I compare between the dirs
             | Parameter | Value                    |
             | dir1      | annotations_format_mask1 |
             | dir2      | annotations_format_mask2 |



================================================
File: tests/features/annotations_repo/test_annotations_format_vtt.feature
================================================
Feature: Annotations format vtt

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_annotations_format_vtt"
        And I create a dataset by the name of "Dataset_annotations_format_vtt" in the project
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4532884
    @DAT-46429
    Scenario: Check all vtt download options
        Given I upload an item of type "mp3 audio" to the dataset
        And   I upload "subtitle" annotation to the audio item
        And   I create the dir path "annotations_format_vtt1"
        And   I create the dir path "annotations_format_vtt2"
        And   I create the dir path "annotations_format_vtt3"
        Then  I delete content in path path "annotations_format_vtt1"
        And   I delete content in path path "annotations_format_vtt2"
        And   I delete content in path path "annotations_format_vtt3"
        When  I call Annotation.download() using the given params
             | Parameter          | Value                                      |
             | filepath           | annotations_format_vtt1/mp3_audio_item.vtt |
             | annotation_format  | VTT                                        |
        And   I call Item.annotations.download() using the given params
             | Parameter          | Value                   |
             | filepath           | annotations_format_vtt2 |
             | annotation_format  | VTT                     |
        Then  I compare between the dirs
             | Parameter | Value                   |
             | dir1      | annotations_format_vtt1 |
             | dir2      | annotations_format_vtt2 |
        Given I init Filters() using the given params
             | Parameter | Value |
             | resource  | ITEM  |
        When  I call Filters.add() using the given params
             | Parameter | Value              |
             | field     | name               |
             | values    | mp3_audio_item.mp3 |
        And   I call dataset.download_annotations() using the given params
             | Parameter          | Value                   |
             | local_path         | annotations_format_vtt3 |
             | annotation_options | VTT                     |
        Then  I compare json metadata and annotationsCount between the files in dirs
             | Parameter        | Value                       |
             | item_json_dir    | annotations_format_vtt2     |
             | dataset_json_dir | annotations_format_vtt3/vtt |



================================================
File: tests/features/annotations_repo/test_annotations_get.feature
================================================
Feature: Annotaions repository Get service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "annotations_get"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4523036
    @DAT-46430
    Scenario: Get an existing annotation by id
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        And There is annotation x
        When I get the annotation by id
        Then I receive an Annotation object
        And Annotation received equals to annotation x

    @testrail-C4523036
    @DAT-46430
    Scenario: Get non-existing annotation
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        When I try to get a non-existing annotation
        Then "BadRequest" exception should be raised




================================================
File: tests/features/annotations_repo/test_annotations_gis_upload.feature
================================================
Feature: Annotations repository Upload gis testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "annotations_upload"
    And I create a dataset with a random name

  @DAT-80650
  Scenario: GIS item - Upload annotations from file
    Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
    And Item in path "gis/gis.json" is uploaded to "Dataset"
    When I upload annotations from file: "gis/gisann.json" with merge "False"
    Then Item should have annotations uploaded

  @DAT-80650
  Scenario: GIS item - Upload a single annotation
    Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
    And gis item in path "gis/gis.json" is uploaded to "Dataset"
    When I add a gis annotations to item
    Then Item should have all gis annotation types uploaded

  @DAT-80996
  Scenario: GIS item - Download item annotations should work
    Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
    And gis item in path "gis/gis.json" is uploaded to "Dataset"
    When I add a gis annotations to item
    Then Item should have all gis annotation types uploaded
    Given There are no files in folder "downloaded_annotations"
    When I download items annotations with "json" to "downloaded_annotations/json.json"
    Then Validate annotation file has "4" annotations





================================================
File: tests/features/annotations_repo/test_annotations_list.feature
================================================
Feature: Annotaions repository List service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "annotations_list"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4523037
    @DAT-46431
    Scenario: List all annotations when no annotations exists
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And There are no annotations
        When I list all annotations
        Then I receive an empty annotations list

    @testrail-C4523037
    @DAT-46431
    Scenario: List all annotations when annotations exist
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "assets_split/annotations_crud/0162_annotations.json"
        When I list all annotations
        Then I receive a list of all annotations
        And The annotations in the list equals the annotations uploaded



================================================
File: tests/features/annotations_repo/test_annotations_merge.feature
================================================
Feature: Annotations repository Upload service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "annotations_merge"
        And I create a dataset with a random name

    @DAT-78978
    Scenario: Upload annotations from file - merge
        Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
        And Item in path "ann_merage/1.png" is uploaded to "Dataset"
        When I upload annotations from file: "ann_merage/1.json" with merge "True"
        And I save binary annotation coordinates
        When I upload annotations from file: "ann_merage/2.json" with merge "True"
        Then binary annotation has been merged

    @DAT-78978
    Scenario: Upload annotations from file - new 2 binary only one upload
        Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
        And Item in path "ann_merage/1.png" is uploaded to "Dataset"
        When I upload annotations from file: "ann_merage/2.json" with merge "True"
        Then "8" annotations are upload





================================================
File: tests/features/annotations_repo/test_annotations_show.feature
================================================
Feature: Annotations repository show method testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "annotations_show"
        And I create a dataset with a random name

    @testrail-C4523038
    @DAT-46432
    Scenario: Show mask
        Given Classes in file: "assets_split/annotations_show/classes_new.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_show/0000000162.jpg" is uploaded to "Dataset"
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And I show items annotations with param "mask"
        Then I receive annotations mask and it is equal to mask in "new_mask_should_be.npy"

    @testrail-C4523038
    @DAT-46432
    Scenario: Show instance
        Given Classes in file: "assets_split/annotations_show/classes_new.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_show/0000000162.jpg" is uploaded to "Dataset"
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |
        When Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And I show items annotations with param "instance"
        Then I receive annotations mask and it is equal to mask in "new_instance_should_be.npy"

    @testrail-C4523038
    @DAT-46432
    Scenario: Show object id
        Given Classes in file: "assets_split/annotations_show/classes_new.json" are uploaded to test Dataset
        And Dataset ontology has attributes "attr1" and "attr2"
        And Item in path "assets_split/annotations_show/0000000162.jpg" is uploaded to "Dataset"
        When Item is annotated with annotations in file: "assets_split/annotations_show/annotations_new.json"
        And Every annotation has an object id
        And I show items annotations with param "object_id"
        Then I receive annotations mask and it is equal to mask in "new_object_id_should_be.npy"




================================================
File: tests/features/annotations_repo/test_annotations_thumbnail.feature
================================================
Feature: Item Annotations thumbnail

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "annotation_thumbnail"
    And I create a dataset with a random name


  @DAT-71001
  Scenario: Show annotation thumbnail - Should create thumbnail v2 on item
    When I upload item in "0000000162.jpg"
    And I annotate item
    And I Show annotation thumbnail for the item
    Then I should see a thumbnail v2 on the item

  @DAT-71001
  Scenario: Show annotation thumbnail after update - Should create new thumbnail v2 on item
    When I upload item in "0000000162.jpg"
    And I annotate item
    And I Show annotation thumbnail for the item
    And I update annotation label with new name "edited"
    And I Show annotation thumbnail for the item
    Then I should see a thumbnail v2 on the item

  @DAT-71001
  Scenario: Show annotation thumbnail after delete - Should delete the thumbnail v2 from the item
    When I upload item in "0000000162.jpg"
    And I annotate item
    And I Show annotation thumbnail for the item
    And I delete annotation
    And I Show annotation thumbnail for the item
    Then I should not see a thumbnail v2 on the item

  @DAT-71001
  Scenario: Show annotation thumbnail after delete and add - Should create new thumbnail v2 on item
    When I upload item in "0000000162.jpg"
    And I annotate item
    And I Show annotation thumbnail for the item
    And I delete annotation
    And I Show annotation thumbnail for the item
    And I annotate item
    And I Show annotation thumbnail for the item
    Then I should see a thumbnail v2 on the item



================================================
File: tests/features/annotations_repo/test_annotations_upload.feature
================================================
Feature: Annotations repository Upload service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "annotations_upload"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4523039
    @DAT-46433
    Scenario: Upload annotations from file
        Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_collection/0000000162.jpg" is uploaded to "Dataset"
        When I upload annotations from file: "assets_split/annotations_upload/0162_annotations.json" with merge "False"
        Then Item should have annotations uploaded

    @testrail-C4523039
    @DAT-46433
    Scenario: Upload a single annotation
        Given Labels in file: "assets_split/annotations_upload/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotation_collection/0000000162.jpg" is uploaded to "Dataset"
        And There is an annotation description
        When I upload annotation description to Item
        Then Item should have annotation uploaded

    # TODO - allowing me to upload illegal annotation
    # Scenario: Upload a single annotation: illegal
    #     Given There is an illegal annotation description
    #     When I try to upload annotation description to Item
    #     Then "InternalServerError" exception should be raised





================================================
File: tests/features/annotations_repo/test_note_annotation_with_messages.feature
================================================
Feature: Upload annotation note with messages

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "note_annotation_with_messages project"
        And I create a dataset by the name of "note_annotation_with_messages dataset" in the project

    @testrail-C4533541
    @DAT-46434
    Scenario: Upload note annotation with messages - image item
        Given I upload an item of type "bmp image" to the dataset
        And I upload note annotation to the item with the params
            | Parameter  | Value                    |
            | top        | 10                       |
            | left       | 10                       |
            | bottom     | 100                      |
            | right      | 100                      |
            | label      | 10                       |
            | messages   | ['message1', 'message2'] |
        Then I will see the on the note annotations the following messages
            | Parameter | Value                    |
            | messages  | ['message1', 'message2'] |

    @testrail-C4533541
    @DAT-46434
    Scenario: Upload note annotation with messages - video item
        Given I upload an item of type "webm video" to the dataset
        And I upload note annotation to the item with the params
            | Parameter      | Value                     |
            | top            | 10                        |
            | left           | 10                        |
            | bottom         | 100                       |
            | right          | 100                       |
            | label          | 10                        |
            | messages       | ['message1', 'message2']  |
            | object_id      | 1                         |
            | frame_num      | 5                         |
            | end_frame_num  | 10                        |
        Then I will see the on the note annotations the following messages
            | Parameter | Value                    |
            | messages  | ['message1', 'message2'] |



================================================
File: tests/features/annotations_repo/test_rotated_box_points.feature
================================================
Feature: Rotated box points testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        When I create two project and datasets by the name of "rotated_box_off" "rotated_box_on"
        When I upload item in "0000000162.jpg" to both datasets

    @testrail-C4532243
    @DAT-46435
    Scenario: Check rotated box geo with flag off
        Given I create 4ptBox setting to the "first" project
        And I set 4ptBox setting project setting to "False"
        When I upload rotated box annotation to item "1"
        Then The Geo will be of the "old" format

    @testrail-C4532243
    @DAT-46435
    Scenario: Check rotated box geo with flag on
        Given I create 4ptBox setting to the "second" project
        And I set 4ptBox setting project setting to "True"
        When I upload rotated box annotation to item "2"
        Then The Geo will be of the "new" format





================================================
File: tests/features/app_entity/app_update_external_entities.feature
================================================
Feature: App service update entities externally

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "app_service_update_entities"
    And I create a dataset with a random name


  @DAT-86672
  Scenario: Update app service sdk version
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I add a service to the DPK
    When I publish a dpk to the platform
    When I install the app
    When I update app service SDK version to "1.99.12"
    Then SDK version should be updated to "1.99.12"
    And App custom installation service should be updated to "1.99.12"


================================================
File: tests/features/app_entity/app_with_fs_panel.feature
================================================
Feature: Testing App with fs served panel

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "app-with-fs-panel"

  @DAT-80890
  @restore_json_file
  Scenario: Create and update app with FS panel
    Given I have an app with a filesystem panel in path "apps/filesystem_panel"
    When I fetch the panel
    Then I should find the sdk version from the computeConfig in the panel
    And no services deployed in the project
    Given I update the panel with a new sdk version
    When I fetch the panel
    Then I should find the sdk version from the computeConfig in the panel
    And no services deployed in the project
    Then I uninstall the app


  @DAT-83973
  @restore_json_file
  Scenario: Create app with FS panel - Deactivate and active the app Should work
    Given I have an app with a filesystem panel in path "apps/filesystem_panel"
    When I fetch the panel
    Then I should find the sdk version from the computeConfig in the panel
    When I pause the app
    Then The deactivation should succeed
    When I resume the app
    Then The activation should succeed
    Then I should find the sdk version from the computeConfig in the panel
    Then I uninstall the app


  @DAT-84375
  @restore_json_file
  Scenario: Create pipeline app with FS panel - Start pipeline should success
    Given I have an app with a filesystem panel in path "apps/pipeline_panel"
    When I fetch the panel
    Then I should find the sdk version from the computeConfig in the panel
    Given I create pipeline from json in path "pipelines_json/panel_node_fs.json"
    When I install pipeline in context
    Then Pipeline status is "Installed"
    Then I clean the project


  @DAT-84466
  @pipelines.delete
  @restore_json_file
  Scenario: Module without config - Start pipeline should success and service should get default configuration
    Given I have an app with a filesystem panel in path "apps/pipeline_panel" and remove key "components.modules.0.computeConfig"
    Given I create pipeline from json in path "pipelines_json/panel_node_fs.json"
    When I install pipeline in context
    Then Pipeline status is "Installed"
    When I list services in project
    Then I receive a Service list of "1" objects


================================================
File: tests/features/app_entity/test_app_auto_update.feature
================================================
Feature: Test app umbrella refs - Auto update Pipeline nodes

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "auto-update-refs"

  @DAT-72128
  @DAT-75739
  @pipelines.delete
  Scenario: Auto update app with custom node scope node - Should update the node service to latest app version
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    When I install the app
    Given I create pipeline from json in path "pipelines_json/pipeline_scope_node.json"
    And I install pipeline in context
    When I update app auto update to "True"
    And I get the pipeline service
    And I try get the "published_dpk" by id
    Then "service" has app scope
    When I try get the "published_dpk" by id
    And I set code path "update_metadata" to context
    And I pack directory by name "update_metadata"
    And I add codebase to dpk
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    And I get the pipeline service
    Then "service" has app scope
    And I pause pipeline in context
    And I install pipeline in context
    When I get the pipeline service
    Then "service" has app scope
    And I pause pipeline in context
    And I uninstall the app
    And I clean the project

  @DAT-72128
  @pipelines.delete
  Scenario: Auto update app with custom node scope node and update pipeline - Should update the node service to latest app version
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    When I install the app
    Given I create pipeline from json in path "pipelines_json/pipeline_scope_node.json"
    When I update pipeline description
    And I install pipeline in context
    When I update app auto update to "True"
    And I get the pipeline service
    And I try get the "published_dpk" by id
    Then "service" has app scope
    When I try get the "published_dpk" by id
    And I set code path "update_metadata" to context
    And I pack directory by name "update_metadata"
    And I add codebase to dpk
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    And I get the pipeline service
    Then "service" has app scope
    And I pause pipeline in context
    And I uninstall the app
    And I clean the project

  @DAT-72129
  @DAT-75739
  @pipelines.delete
  Scenario: Auto update app with cloned model nodes - Should update the nodes service to latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And i fetch the model by the name "test-model"
    And I clone a model and set status "created"
    And I update app auto update to "True"
    Given I create pipeline from json in path "pipelines_json/train_evaluate_nodes.json"
    And I install pipeline in context
    When I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    And i fetch the model by the name "clone_model"
    And I execute pipeline with input type "Model"
    And I wait "4"
    And i have a model service
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope
    And I pause pipeline in context
    And I install pipeline in context
    When I get service in index "-1"
    Then "service" has app scope
    And I pause pipeline in context
    And I uninstall the app
    And I clean the project

  @DAT-72310
  Scenario: Auto update app with cloned model deploy service - Should update deploy service to latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And I update app auto update to "True"
    And I get last model in project
    And I clone a model and set status "trained"
    And I "deploy" the model
    And I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    And I get last model in project
    And I get service in index "-1"
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope
    And I clean the project



================================================
File: tests/features/app_entity/test_app_auto_update2.feature
================================================
Feature: Test app umbrella refs - Auto update Pipeline nodes

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "auto-update-refs"

  @DAT-72549
  @DAT-72923
  @DAT-75739
  Scenario: Auto update app model service - update code and not update config
    Given I publish a model dpk from file "model_dpk/basicModelDpk.json" package "dummymodel" with status "trained"
    When I install the app without custom_installation
    And I update app auto update to "True"
    And I get last model in project
    And I "deploy" the model
    And I get the dpk by name
    And i update dpk compute config "default" runtime "runnerImage" to "jjanzic"
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    And I get last model in project
    And I get service in index "-1"
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope
    And service runnerImage is "jjanzic/docker-python3-opencv"
    And model status should be "deployed" with execution "False" that has function "None"


================================================
File: tests/features/app_entity/test_app_bot_creation.feature
================================================
Feature: Installing apps several times should not create more than one bot

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_app_bot_creation"

  @DAT-66254
  Scenario: Installing apps several times should not create more than one bot
    Given I have an app entity from "apps/app.json"
    And publish the app
    When I install the app
    Then I should get the app with the same id
    Then The project have only one bot
    When I uninstall the app
    Then The app shouldn't be in listed
    When I install the app
    Then I should get the app with the same id
    Then The project have only one bot

================================================
File: tests/features/app_entity/test_app_dpk_config.feature
================================================
Feature: Testing App custom_installation attribute

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "app-custom_installation"

  @DAT-63173
  Scenario: Install app - custom_installation should be equal to published dpk.components
    Given I fetch the dpk from 'apps/app_compare_models.json' file
    When I publish a dpk to the platform
    And I install the app
    And I wait "4"
    Then I validate app.custom_installation is equal to published.dpk components
    And I uninstall the app

  @DAT-64292
  Scenario: Install app with partial custom_installation - Should install only expected services
    Given I fetch the dpk from 'apps/app_three_services.json' file
    When I publish a dpk to the platform
    And I create a context.custom_installation var
    And I remove the last service from context.custom_installation
    And I install the app with custom_installation "True"
    And I wait "4"
    Then I validate app.custom_installation is equal to composition
    And I uninstall the app

  @DAT-64293
  Scenario: Add service to custom_installation - Should extend the dpk and install the expected services
    Given I fetch the dpk from 'apps/app_three_services.json' file
    When I publish a dpk to the platform
    And I create a context.custom_installation var
    And I add service to context.custom_installation
    And I install the app with custom_installation "True"
    And I wait "4"
    And I list services in project
    Then I receive a Service list of "4" objects
    Then I validate app.custom_installation is equal to composition
    And I uninstall the app

  @DAT-64311
  @DAT-62824
  Scenario: Update installed app.custom_installation with new service - Should update composition with new service
    Given I fetch the dpk from 'apps/app_three_services.json' file
    When I publish a dpk to the platform
    And I install the app
    And I wait "4"
    And I create a context.custom_installation var
    And I add service to context.custom_installation
    And I update an app
    And I wait "4"
    Then I validate app.custom_installation is equal to composition
    And services should be updated
    And I uninstall the app

  @DAT-64312
  Scenario: Update dpk and published new version and update app - Should update get new att from dpk to composition
    Given I fetch the dpk from 'apps/app_three_services.json' file
    When I publish a dpk to the platform
    And I create a context.custom_installation var
    And I remove the last service from context.custom_installation
    And I install the app with custom_installation "True"
    And I wait "4"
    And I add att 'cooldownPeriod=500' to dpk 'service' in index '0'
    And I increment dpk version
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    Then I validate dpk autoscaler in composition for service in index '0'

================================================
File: tests/features/app_entity/test_app_get.feature
================================================
Feature: Test App get

    Background: Initialize
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        When I create a project by the name of "Project_test_app_get"
        And I have an app entity from "apps/app.json"
        And I publish a dpk to the platform
        And I install the app

    @testrail-C4524925
    @DAT-46452
    Scenario: Get app by name
        When I get the app by name
        Then I should get identical results as the json

    @testrail-C4524925
    @DAT-46452
    Scenario: Get app by id
        When I get the app by id
        Then I should get identical results as the json

    @testrail-C4524925
    @DAT-46452
    Scenario: Get app with invalid id
        When I get the app with invalid id
        Then I should get an exception error='404'

    @testrail-C4524925
    @DAT-46452
    Scenario: Get without parameters
        When I get the app without parameters
        Then I should get an exception error='400'

================================================
File: tests/features/app_entity/test_app_install.feature
================================================
Feature: App entity Install App

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_app_install"

  @testrail-C4524925
  @DAT-46453
  Scenario: Install a valid app
    Given I have an app entity from "apps/app.json"
    And publish the app
    When I install the app
    Then I should get the app with the same id
    When I install the app with exception
    Then I should get an exception error='400'
    And I uninstall the app


  @DAT-52471
  Scenario: Install app and validate configuration according to DPK
    Given I have an app entity from "apps/app_service_fields.json"
    And publish the app
    When I install the app
    And I get service by name "hello"
    Then I validate service configuration in dpk is equal to service from app
    And "service" has app scope

  @DAT-54655
  Scenario: Install a valid app
    Given I have an app entity from "apps/app.json"
    And publish the app
    When I install the app
    Then i can create pipeline function node from the app service

  @DAT-68027
  Scenario: Install a valid app with dataset component
    Given I have an app entity from "apps/app_dataset_component.json"
    And publish the app
    When I install the app
    Then The dataset component has been installed successfully

================================================
File: tests/features/app_entity/test_app_integrations.feature
================================================
Feature: publish a dpk with trigger

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset


  @DAT-70200
  Scenario: publishing a dpk with integrations
    Given I create "key_value" integration with name "init_integrations"
    Given I fetch the dpk from 'apps/app_service_init_intg.json' file
    When I set code path "init_integrations" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I add integration to dpk
    And I publish a dpk to the platform
    And I install the app
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    When I delete integration in context

  @DAT-70200
  Scenario: publishing a package with secrets
    Given I create "key_value" integration with name "init_service_integrations"
    And I create a package with secrets with entry point "init_integrations"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And service has integrations
    When I delete integration in context

================================================
File: tests/features/app_entity/test_app_model_integrations.feature
================================================
Feature: Models repository integration testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item


  @DAT-70200
  Scenario: test integration
    Given I create "key_value" integration with name "init_integrations"
    Given I fetch the dpk from 'apps/app_model_init_intg.json' file
    When I add integration to dpk
    When I set code path "init_integrations" to context
    When I create a dummy model package by the name of "dummymodel" with entry point "main_model.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "train" the model
    Then service metadata has a model id and operation "train"
    Then model status should be "trained" with execution "True" that has function "train_model"
    When I delete integration in context


================================================
File: tests/features/app_entity/test_app_status.feature
================================================
Feature: Test app status update

    Background: Initialize
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_app_status"
        And I have an app entity from "apps/app_service_fields.json"
        And I publish a dpk to the platform
        And I install the app
        And I get service by name "hello"
        And The service is active

    @DAT-68627
    Scenario: I update the app's status
        When I pause the app
        And I get service by name "hello"
        Then The deactivation should succeed
        And The service is inactive
        When I resume the app
        And I get service by name "hello"
        Then The activation should succeed
        And The service is active

================================================
File: tests/features/app_entity/test_app_uninstall.feature
================================================
Feature: Test App uninstall

    Background: Initialize
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_app_get"
        And I have an app entity from "apps/app.json"
        And I publish a dpk to the platform
        And I install the app

    @testrail-C4524925
    @DAT-46454
    Scenario: Uninstall the app
        Given I uninstall the app
        Then The app shouldn't be in listed

    @testrail-C4524925
    @DAT-46454
    Scenario: I uninstall an invalid app
        Given I uninstall not existed app
        Then I should get an exception error='404'

================================================
File: tests/features/app_entity/test_app_update.feature
================================================
Feature: Test app update

    Background: Initialize
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_app_get"
        And I have an app entity from "apps/app.json"
        And I publish a dpk to the platform
        And I install the app

    @testrail-C4524925
    @DAT-46455
    Scenario: I update the app
        When I update the app
        Then The update should success

================================================
File: tests/features/app_entity/test_app_with_map_var.feature
================================================
Feature: Test app vars map

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "app-vars-map"

  @DAT-80317
  @pipelines.delete
  Scenario: app with map var - Should create pipeline with values
    Given I fetch the dpk from 'apps/dependencies/baseDpkDep.json' file
    When I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I publish a dpk to the platform without random name "False"
    When I fetch the dpk from 'apps/app_with_dep_vars.json' file with fix template 'False'
    When I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app
    And create pipeline from app template
    Then pipeline has ids instead of vars
    And I clean the project

================================================
File: tests/features/app_integrations/test_integrations_in_module.feature
================================================
Feature: publish a dpk with service Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset


  @DAT-74467
  Scenario: publishing a dpk with integrations - integrations in module
    Given I create key value integration with key "nvidiaUser" value "inmodule"
    Given I fetch the dpk from 'app_with_integrations/module_service_integration.json' file
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I add integration to dpk
    And I publish a dpk to the platform
    And I install the app
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "inmodule"
    When I delete integration in context

================================================
File: tests/features/app_integrations/test_integrations_in_module_app.feature
================================================
Feature: publish a dpk with service Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset


  @DAT-74467
  Scenario: publishing a app with integrations - integrations in app module
    Given I create key value integration with key "nvidiaUser" value "inapp"
    Given I fetch the dpk from 'app_with_integrations/module_service_integration_without_value.json' file
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app with integration
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "inapp"
    When I delete integration in context

================================================
File: tests/features/app_integrations/test_integrations_in_service.feature
================================================
Feature: publish a dpk with service Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset


  @DAT-74467
  Scenario: publishing a dpk with integrations - integrations in service
    Given I create key value integration with key "nvidiaUser" value "inservice"
    Given I fetch the dpk from 'app_with_integrations/service_integration.json' file
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I add integration to dpk
    And I publish a dpk to the platform
    And I install the app
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "inservice"
    When I delete integration in context
    And I uninstall the app

  @DAT-75930
  Scenario: publishing a dpk with integrations optional - integrations in service should able to install
    Given I fetch the dpk from 'app_with_integrations/service_integration.json' file
    And I remove the "value" from integration from the dpk in "services" component in index 0
    And I add "optional=True" to integration from the dpk in "services" component in index 0
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app


================================================
File: tests/features/app_integrations/test_integrations_integrations_category.feature
================================================
Feature: publish a dpk with service Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset


  @DAT-74185
  Scenario: publishing a dpk with integrations - integrations in module use integration category
    Given I create key value integration with key "nvidiaUser" value "category"
    Given I fetch the dpk from 'app_with_integrations/module_service_integration_category.json' file
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I add integration to dpk
    And I publish a dpk to the platform
    And I install the app
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "category"
    When I delete integration in context

  @DAT-81128
  Scenario: publishing a dpk with integrations in scope node - integrations in module use integration category
    Given I publish a pipeline node dpk from file "apps/app_scope_node_with_init.json" and with code path "move_item"
    When I install the app
    Then i got the dpk required integration

================================================
File: tests/features/app_integrations/test_integrations_override.feature
================================================
Feature: publish a dpk with service Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset

  @DAT-74467
  Scenario: publishing a DPK with integrations - integrations override
    Given I create key value integration with key "nvidiaUser" value "indpk"
    Given I fetch the dpk from 'app_with_integrations/service_override_integrations.json' file
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I add integration to dpk
    And I publish a dpk to the platform
    Given I create key value integration with key "nvidiaUser" value "inapp"
    When I install the app with integration
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "inapp"
    When I delete integration in context

================================================
File: tests/features/app_integrations/test_model_integrations.feature
================================================
Feature: publish a dpk with model Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg-model"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset

  @DAT-76153
  Scenario: publishing a dpk with integrations optional true for model - Should be able to deploy model
    Given I fetch the dpk from 'app_with_integrations/module_model_integration.json' file
    And I remove the "value" from integration from the dpk in "integrations" component in index 0
    And I add "optional=True" to integration from the dpk in "integrations" component in index 0
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "model-num-1" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And I get last model in project
    And I "deploy" the model
    Then I have a model service



================================================
File: tests/features/app_integrations/test_sheare_integrations_in_module.feature
================================================
Feature: publish a dpk with service Integration

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-intg"
    And I create a dataset by the name of "integ" in the project
    And I upload item in "0000000162.jpg" to dataset


  @DAT-74467
  Scenario: publishing a dpk with integrations - integrations in module sharing services
    Given I create key value integration with key "nvidiaUser" value "indpk"
    Given I fetch the dpk from 'app_with_integrations/service_integration_share.json' file
    When I set code path "base_service_integration" to context
    And I pack directory by name "init_integrations"
    And I add codebase to dpk
    And I add integration to dpk
    And I publish a dpk to the platform
    And I install the app
    When I get service by name "run"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "indpk"
    When I get service by name "run2"
    Then I execute the service
    And Execution was executed and finished with status "success"
    And Execution output is "indpk"
    When I delete integration in context

================================================
File: tests/features/artifacts_repo/test_artifacts_delete.feature
================================================
@bot.create
Feature: Artifacts repository get artifact testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_artifacts_upload"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test_package" to "artifacts_upload"
    Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test-package" to "artifacts_upload"
    And I push "first" package
      | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None | modules=None |

  @packages.delete
  @testrail-C4523049
  @DAT-46456
  Scenario: Delete by name - item
    When I upload "1" artifacts to "package"
    When I delete artifact by "name"
    Then Artifact does not exist "name"

  @packages.delete
  @testrail-C4523049
  @DAT-46456
  Scenario: Delete with package  - item
    When I upload "1" artifacts to "package"
    When I delete artifact by "id"
    Then Artifact does not exist "id"

  @packages.delete
  @testrail-C4523049
  @DAT-46456
  Scenario: Delete with package name
    When I upload "1" artifacts to "package"
    When I delete artifact by "package_name"
    Then Artifact does not exist "package_name"

  @packages.delete
  @services.delete
  @testrail-C4523049
  @DAT-46456
  Scenario: Delete with execution
    Given There is a service by the name of "artifacts-upload" with module name "default_module" saved to context "service"
    And I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I create an execution with "Item"
      |sync=False|inputs=Item|
    And I upload "1" artifacts to "execution"
    When I delete artifact by "execution_id"
    Then Artifact does not exist "execution_id"


================================================
File: tests/features/artifacts_repo/test_artifacts_download.feature
================================================
@bot.create
Feature: Artifacts repository get artifact testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_artifacts_upload"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test_package" to "artifacts_upload"
    Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test-package" to "artifacts_upload"
    And I push "first" package
      | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None | modules=None |

  @packages.delete
  @testrail-C4523050
  @DAT-46457
  Scenario: Download by artifact name - item
    When I upload "1" artifacts to "package"
    And I download artifact by "name"
    Then Artifact "item" was downloaded successfully

  @packages.delete
  @testrail-C4523050
  @DAT-46457
  Scenario: Download by artifact id - item
    When I upload "1" artifacts to "package"
    And I download artifact by "id"
    Then Artifact "item" was downloaded successfully

  @packages.delete
  @testrail-C4523050
  @DAT-46457
  Scenario: Download by artifact id - folder
    Given Context "artifact_filepath" is "artifacts_repo/artifact_folder"
    When I upload "1" artifacts to "package"
    And I download artifact by "id"
    Then Artifact "folder" was downloaded successfully

  @packages.delete
  @testrail-C4523050
  @DAT-46457
  Scenario: Download with package name - item
    When I upload "1" artifacts to "package"
    And I download artifact by "package_name"
    Then Artifact "item" was downloaded successfully

  @packages.delete
  @services.delete
  @testrail-C4523050
  @DAT-46457
  Scenario: Download with execution id
    Given There is a service by the name of "artifacts-upload" with module name "default_module" saved to context "service"
    And Context has attribute execution_id = True
    And I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I create an execution with "Item"
      |sync=False|inputs=Item|
    And I upload "1" artifacts to "execution"
    And I get artifact by "execution_id"
    And I download artifact by "execution_id"
    Then Artifact "item" was downloaded successfully


================================================
File: tests/features/artifacts_repo/test_artifacts_get.feature
================================================
@bot.create
Feature: Artifacts repository get artifact testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_artifacts_upload"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test-package" to "artifacts_upload"
    Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test-package" to "artifacts_upload"
    And I push "first" package
      | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None | modules=None |

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Get by name - item
    When I upload "1" artifacts to "package"
    And I get artifact by "name"
    Then I receive an Artifact entity
    And Artifact received equals to the one uploaded

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Get by artifact name - non-existing
    When I upload "1" artifacts to "package"
    When I get artifact by "wrong_artifact_name"
    Then "NotFound" exception should be raised

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Get by package name - non-existing
    When I upload "1" artifacts to "package"
    When I get artifact by "wrong_package_name"
    Then "NotFound" exception should be raised

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Get by artifact id - non-existing
    When I upload "1" artifacts to "package"
    When I get artifact by "wrong_artifact_id"
    Then "BadRequest" exception should be raised

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Get by execution id - non-existing
    When I upload "1" artifacts to "package"
    When I get artifact by "wrong_execution_id"
    Then "NotFound" exception should be raised

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Use with package  - item
    When I upload "1" artifacts to "package"
    And I get artifact by "id"
    Then I receive an Artifact entity
    And Artifact received equals to the one uploaded

  @packages.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Use with package name
    When I upload "1" artifacts to "package_name"
    And I get artifact by "package_name"
    Then I receive an Artifact entity
    And Artifact received equals to the one uploaded

  @packages.delete
  @services.delete
  @testrail-C4523051
  @DAT-46458
  Scenario: Use with execution
    Given There is a service by the name of "artifacts-upload" with module name "default_module" saved to context "service"
    And I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I create an execution with "Item"
      |sync=False|inputs=Item|
    And I upload "1" artifacts to "execution"
    And I get artifact by "execution_id"
    Then I receive an Artifact entity
    And Artifact received equals to the one uploaded


================================================
File: tests/features/artifacts_repo/test_artifacts_list.feature
================================================
@bot.create
Feature: Artifacts repository list artifact testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_artifacts_upload"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test-package" to "artifacts_upload"
    Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
    And Directory "artifacts_upload" is empty
    When I generate package by the name of "test-package" to "artifacts_upload"
    And I push "first" package
      | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None | modules=None |

  @packages.delete
  @testrail-C4523052
  @DAT-46459
  Scenario: list package artifacts
    And I list Artifacts with "package_name"
    Then I receive artifacts list of "0" items
    When I upload "3" artifacts to "package"
    And I list Artifacts with "package_name"
    Then I receive artifacts list of "3" items

  @packages.delete
  @services.delete
  @testrail-C4523052
  @DAT-46459
  Scenario: Use with execution
    Given There is a service by the name of "artifacts-upload" with module name "default_module" saved to context "service"
    And I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I create an execution with "Item"
      |sync=False|inputs=Item|
    And I list Artifacts with "execution_id"
    Then I receive artifacts list of "0" items
    When I upload "3" artifacts to "execution"
    And I list Artifacts with "execution_id"
    Then I receive artifacts list of "3" items


================================================
File: tests/features/artifacts_repo/test_artifacts_upload.feature
================================================
@bot.create
Feature: Artifacts repository upload service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_artifacts_upload"
        And Directory "artifacts_upload" is empty
        When I generate package by the name of "test-package" to "artifacts_upload"

    @packages.delete
    @testrail-C4523053
    @DAT-46460
    Scenario: Use with package  - item
        Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
        And Directory "artifacts_upload" is empty
        When I generate package by the name of "test_package" to "artifacts_upload"
        And I push "first" package
            | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None |
        And I upload "1" artifacts to "package"
        Then I receive an artifact object

    @packages.delete
    @testrail-C4523053
    @DAT-46460
    Scenario: Use with package  - folder
        Given Context "artifact_filepath" is "artifacts_repo/artifact_folder"
        And Directory "artifacts_upload" is empty
        When I generate package by the name of "test_package" to "artifacts_upload"
        And I push "first" package
            | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None |
        And I upload "1" artifacts to "package"
        Then I receive an artifact object

    @packages.delete
    @testrail-C4523053
    @DAT-46460
    Scenario: Use with package name
        Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
        And Directory "artifacts_upload" is empty
        When I generate package by the name of "test_package" to "artifacts_upload"
        And I push "first" package
            | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None |
        And I upload "1" artifacts to "package_name"
        Then I receive an artifact object

     @packages.delete
     @services.delete
     @testrail-C4523053
     @DAT-46460
     Scenario: Use with execution
         Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
         And Directory "artifacts_upload" is empty
         When I generate package by the name of "test_package" to "artifacts_upload"
         And I push "first" package
             | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None | modules=None |
         Given There is a service by the name of "artifacts-upload" with module name "default_module" saved to context "service"
        And I create a dataset with a random name
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        When I create an execution with "Item"
          |sync=False|inputs=Item|
         And I upload "1" artifacts to "execution"
         Then I receive an artifact object

     @packages.delete
     @services.delete
     @testrail-C4523053
     @DAT-46460
     Scenario: Use with execution id
         Given Context "artifact_filepath" is "artifacts_repo/artifact_item.jpg"
         And Directory "artifacts_upload" is empty
         When I generate package by the name of "test_package" to "artifacts_upload"
         And I push "first" package
             | codebase_id=None | package_name=test-package | src_path=artifacts_upload | inputs=None | outputs=None | modules=None |
         Given There is a service by the name of "artifacts-upload" with module name "default_module" saved to context "service"
        And I create a dataset with a random name
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        When I create an execution with "Item"
          |sync=False|inputs=Item|
         And I upload "1" artifacts to "execution_id"
         Then I receive an artifact object



================================================
File: tests/features/assignments_repo/test_assignments_context.feature
================================================
Feature: Assignments repository Context testing

    Background: Initiate Platform Interface and create a projects and datasets
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "project1 project2"
        And I create datasets by the name of "dataset1 dataset2"
        And I set Dataset to Dataset 1
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        And I save dataset items to context
        When Add Members "annotator1@dataloop.ai" as "annotator" to project 1
        And Add Members "annotator2@dataloop.ai" as "annotator" to project 1
        And I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto | items=3 |
        And I append task to tasks
        And I get an Assignment

    @testrail-C4523054
    @DAT-46461
    Scenario: Get Assignment from the Project it belong to
        When I get the assignment from project number 1
        Then assignment Project_id is equal to project 1 id
        And assignment Project.id is equal to project 1 id
        And assignment dataset_id is equal to dataset 1 id
        And assignment dataset.id is equal to dataset 1 id
        And assignment task_id is equal to task 1 id
        And assignment task.id is equal to task 1 id

    @testrail-C4523054
    @DAT-46461
    Scenario: Get Assignment from the Project it not belong to
        When I get the assignment from project number 2
        Then assignment Project_id is equal to project 1 id
        And assignment Project.id is equal to project 1 id
        And assignment dataset_id is equal to dataset 1 id
        And assignment dataset.id is equal to dataset 1 id
        And assignment task_id is equal to task 1 id
        And assignment task.id is equal to task 1 id

    @testrail-C4523054
    @DAT-46461
    Scenario: Get Assignment from the Dataset it belong to
        When I get the assignment from dataset number 1
        Then assignment Project_id is equal to project 1 id
        And assignment Project.id is equal to project 1 id
        And assignment dataset_id is equal to dataset 1 id
        And assignment dataset.id is equal to dataset 1 id
        And assignment task_id is equal to task 1 id
        And assignment task.id is equal to task 1 id

    @testrail-C4523054
    @DAT-46461
    Scenario: Get Assignment from the Dataset it not belong to
        When I get the assignment from dataset number 2
        Then assignment Project_id is equal to project 1 id
        And assignment Project.id is equal to project 1 id
        And assignment dataset_id is equal to dataset 1 id
        And assignment dataset.id is equal to dataset 1 id
        And assignment task_id is equal to task 1 id
        And assignment task.id is equal to task 1 id

    @testrail-C4523054
    @DAT-46461
    Scenario: Get Assignment from the Task it belong to
        Then assignment Project_id is equal to project 1 id
        And assignment Project.id is equal to project 1 id
        And assignment dataset_id is equal to dataset 1 id
        And assignment dataset.id is equal to dataset 1 id
        And assignment task_id is equal to task 1 id
        And assignment task.id is equal to task 1 id

    @testrail-C4523054
    @DAT-46461
    Scenario: Get Assignment from the Task it not belong to
        Given  I set Dataset to Dataset 2
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        And I save dataset items to context
        When Add Members "annotator1@dataloop.ai" as "annotator" to project 2
        And Add Members "annotator2@dataloop.ai" as "annotator" to project 2
        And I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto | items=3 |
        And  I get Assignment by "id"
        Then assignment Project_id is equal to project 1 id
        And assignment Project.id is equal to project 1 id
        And assignment dataset_id is equal to dataset 1 id
        And assignment dataset.id is equal to dataset 1 id
        And assignment task_id is equal to task 1 id
        And assignment task.id is equal to task 1 id


================================================
File: tests/features/assignments_repo/test_assignments_create.feature
================================================
Feature: Assignments repository create method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "assignments_create"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        And I save dataset items to context
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"
        And Add Members "annotator3@dataloop.ai" as "annotator"

    @testrail-C4523055
    @DAT-46462
    Scenario: Create - minimum params
        When I create Task
            | task_name=min_params | due_date=auto |  assignee_ids=annotator1@dataloop.ai | items=3 |
        And I create an Assignment from "task" entity
            | assignee_id=annotator2@dataloop.ai | items=3 |
        Then I receive an assignment entity
        And Assignment has the correct attributes

    @testrail-C4523055
    @DAT-46462
    Scenario: Create - maximum params - filters
        When I create Task
            | task_name=min_params2 | due_date=auto |  assignee_ids=annotator1@dataloop.ai | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": true}]}} |
        When I create an Assignment from "task" entity
            | assignee_id=annotator2@dataloop.ai | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}],"$or": [{"metadata": {"user.good": true}},{"metadata": {"user.bad": true}}]}}| items=None |
        Then I receive an assignment entity
        And Assignment has the correct attributes

    @testrail-C4523055
    @DAT-46462
    Scenario: Create - maximum params - items
        When I create Task
            | task_name=min_params | due_date=auto |  assignee_ids=annotator1@dataloop.ai | items=3 |
        And I create an Assignment from "task" entity
            | assignee_id=annotator3@dataloop.ai | filters=None | items=3 |
        Then I receive an assignment entity
        And Assignment has the correct attributes


================================================
File: tests/features/assignments_repo/test_assignments_get.feature
================================================
Feature: Assignments repository get method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "assignments_get"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        And I save dataset items to context
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"
        And I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto | items=3 |
        And I get an Assignment

    @testrail-C4523056
    @DAT-46463
    Scenario: GET - id
        When I get assignment by "id"
        Then I get an assignment entity
        And Assignment received equals assignment created

    @testrail-C4523056
    @DAT-46463
    Scenario: GET - name
        When I get assignment by "name"
        Then I get an assignment entity
        And Assignment received equals assignment created


================================================
File: tests/features/assignments_repo/test_assignments_items_operations.feature
================================================
Feature: Assignments repository items operations method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "assignments_items_operations"
        And I create a dataset with a random name
        When Add Members "annotator1@dataloop.ai" as "annotator"


    @testrail-C4523057
    @DAT-46464
    Scenario: Get assignment items operation
        Given There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        And I save dataset items to context
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=annotator1@dataloop.ai  | items=3 |
        And I get an Assignment




================================================
File: tests/features/assignments_repo/test_assignments_list.feature
================================================
Feature: Assignments repository list method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "assignments_list"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        And I save dataset items to context
        When Add Members "annotator0@dataloop.ai" as "annotator"
        And Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"
        And I create Task
            | task_name=assignments_list | due_date=auto | assignee_ids=annotator0@dataloop.ai | items=2 |

    @testrail-C4523058
    @DAT-46465
    Scenario: List
        When I list assignments
        Then I receive a list of "1" assignments
        When I create an Assignment from "task" entity
            | assignee_id=annotator1@dataloop.ai | items=2 |
        And I list assignments
        Then I receive a list of "2" assignments
        When I create an Assignment from "task" entity
            | assignee_id=annotator2@dataloop.ai | items=2 |
        And I list assignments
        Then I receive a list of "3" assignments



================================================
File: tests/features/assignments_repo/test_assignments_reassign.feature
================================================
Feature: Assignments repository reassign method testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "assignments_reassign"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"
    And Add Members "new_annotator@dataloop.ai" as "annotator"
    And I create Task
      | task_name=reassign | due_date=auto | assignee_ids=annotator1@dataloop.ai | items=2 |
    And I create an Assignment from "task" entity
      | assignee_id=annotator2@dataloop.ai | items=2 |

  @testrail-C4523059
  @DAT-46466
  Scenario: Reassign
    When I reassign assignment to "new_annotator@dataloop.ai"
    Then Assignments was reassigned to "new_annotator@dataloop.ai"

  @testrail-C4523059
  @DAT-50651
  Scenario: Reassign with capital letters
    When I reassign assignment to "new_Annotator@dataloop.ai"
    Then Assignments was reassigned to "new_annotator@dataloop.ai"



================================================
File: tests/features/assignments_repo/test_assignments_redistribute.feature
================================================
Feature: Assignments repository redistribute method testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "assignments_redistribute"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"
    And Add Members "annotator3@dataloop.ai" as "annotator"
    And Add Members "annotator4@dataloop.ai" as "annotator"
    And I create Task
      | task_name=redistribute | due_date=auto | assignee_ids=auto |
    And I get the first assignment

  @testrail-C4523060
  @DAT-46467
  Scenario: redistribute
    When I redistribute assignment to "annotator3@dataloop.ai,annotator4@dataloop.ai"
    Then Assignments was redistributed to "annotator3@dataloop.ai,annotator4@dataloop.ai"

  @testrail-C4523060
  @DAT-50652
  Scenario: redistribute with capital letter
    When I redistribute assignment to "aNnotator3@dataloop.ai,Annotator4@dataloop.ai"
    Then Assignments was redistributed to "annotator3@dataloop.ai,annotator4@dataloop.ai"


================================================
File: tests/features/billing_repo/billing_daily/test_org_creation.feature
================================================

Feature: plan creation

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch

  @DAT-53898
  Scenario: Validate monthly free plan on ORG creation
    When I log in as a "superuser"
    When I create "org" name "free_plan"
    Then Validate plan "Type" is "Free"
    Then Validate plan "Period" is "monthly"



================================================
File: tests/features/billing_repo/billing_daily/test_org_deletion.feature
================================================

Feature: Delete org

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I log in as a "superuser"
    Given I create "org" name "org_to_delete"
    Given I create a project by the name of "org_to_delete_project"
    Given Add Members "user" as "owner"
    Given I update the project org
    And I log in as a "user"

  @DAT-37256
  Scenario: Delete org with FaaS services
    Given I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    And I log in as a "superuser"
    And I create a pipeline with code node
    And I delete the org
    Then Error message includes "Deleting the organization associated with your project cannot be done as long as there are FaaS services deployed in this project. To delete the organization, please delete all deployed FaaS services and try again."


================================================
File: tests/features/billing_repo/billing_daily/test_plan_resources.feature
================================================
Feature: Test Plan Resources

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch

  @DAT-53900
  Scenario: Validate given plan has expected resources


    Given I fetch "plans_resources.json" file from "billing"
    Given I get plans resources json
    Then I validate "Free" plan resources
    Then I validate "Basic" plan resources
    Then I validate "Standard" plan resources
    Then I validate "Pro" plan resources
    Then I validate "Pro Plus" plan resources

================================================
File: tests/features/billing_repo/billing_weekly/test_faas_blocking.feature
================================================
@bot.create
Feature: plan creation

  Background: Initiate Platform Interface

    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    Given I log in as a "superuser"
    Given I create "org" name "custom_subscription_budget"
    Given I create "project" name "custom_subscription"
    When I create "dataset" name "custom_subscription"
    When I update the project org
    When I create a bot by the name of "boty"
    Given Add Members "user" as "owner"
    Given I fetch "subscription_payload.json" file from "billing"


  @DAT-77314
  Scenario: Validate regular-xs, highmem-s, regular-m quota limit reached - with FF

    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-cpu-regular-xs=0.01 | account |
    When I delete the free subscription
    Given I fetch "enable-custom-subscription-blocking-FF.json" file from "billing"
    When I "create" enable-custom-subscription-blocking = True
    Given There is a package (pushed from "services/item") by the name of "services-log"
    When I create a service with autoscaler
      | service_name=service-xs | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-xs |
    Then I validate service has "1" instance up
    When I get analytics query "regular-xs" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I unable to activate service
    Then "Trial users do not have permission to create services" in error message
    Given I fetch "subscription_payload.json" file from "billing"
    Given I log in as a "superuser"
    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-cpu-highmem-s=0.01 | account |
    When I create a service with autoscaler
      | service_name=service-s | package=services-log-init | revision=None | config=None | runtime=None | pod_type=highmem-s |
    Then I validate service has "1" instance up
    When I get analytics query "highmem-s" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I unable to activate service
    Then "Trial users do not have permission to create services" in error message
    Given I log in as a "superuser"
    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-cpu-regular-m=0.01 | account |
    When I create a service with autoscaler
      | service_name=service-m | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-m |
    Then I validate service has "1" instance up
    When I get analytics query "regular-m" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I unable to activate service
    Then "Trial users do not have permission to create services" in error message

  @DAT-77315
  Scenario: Validate regular-xs, highmem-s, regular-m quota limit reached - without FF

    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-cpu-regular-xs=0.01 | account |
    When I delete the free subscription
    Given There is a package (pushed from "services/item") by the name of "services-log"
    When I create a service with autoscaler
      | service_name=service-xs | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-xs |
    Then I validate service has "1" instance up
    When I get analytics query "regular-xs" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I activate service
    Given I log in as a "superuser"
    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-cpu-highmem-s=0.01 | account |
    When I create a service with autoscaler
      | service_name=service-s | package=services-log-init | revision=None | config=None | runtime=None | pod_type=highmem-s |
    Then I validate service has "1" instance up
    When I get analytics query "highmem-s" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I activate service
    Given I log in as a "superuser"
    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-cpu-regular-m=0.01 | account |
    When I create a service with autoscaler
      | service_name=service-m | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-m |
    Then I validate service has "1" instance up
    When I get analytics query "regular-m" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I activate service


  @DAT-84593
  Scenario: Validate Compute Budget with multiple subscriptions

    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-budget=0.01 | account |
    When I delete the free subscription
    Given I fetch "enable-custom-subscription-blocking-FF.json" file from "billing"
    When I "create" enable-custom-subscription-blocking = True
    Given There is a package (pushed from "services/item") by the name of "services-log"
    When I create a service with autoscaler
      | service_name=service-s | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-s |
    Then I validate service has "1" instance up
    When I get analytics query "regular-s" for 120 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I unable to activate service
    Then "Trial users do not have permission to create services" in error message
    Given I log in as a "superuser"
    Given I fetch "subscription_payload.json" file from "billing"
    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-budget=0.01 | account |
    When I update quotas
    When I log in as a "user"
    When I create a service with autoscaler
      | service_name=service-m | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-m |
    Then I validate service has "1" instance up
    When I get analytics query "regular-m" for 60 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I unable to activate service
    Then "Trial users do not have permission to create services" in error message

  @DAT-84594
  Scenario: Validate Compute Budget with multiple subscriptions - without FF

    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-budget=0.01 | account |
    When I delete the free subscription
    Given There is a package (pushed from "services/item") by the name of "services-log"
    When I create a service with autoscaler
      | service_name=service-s | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-s |
    Then I validate service has "1" instance up
    When I get analytics query "regular-s" for 120 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I activate service
    Given I log in as a "superuser"
    Given I fetch "subscription_payload.json" file from "billing"
    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-budget=0.01 | account |
    When I update quotas
    When I log in as a "user"
    When I create a service with autoscaler
      | service_name=service-m | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-m |
    Then I validate service has "1" instance up
    When I get analytics query "regular-m" for 60 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I activate service

  @DAT-84595
  Scenario: Validate Compute Budget with few machines

    When I create custom subscription
      | annotation-tool-hours=777 | data-points=777 | api-calls=777 | hosted-storage=777 | compute-budget=0.098 | account |
    When I delete the free subscription
    Given I fetch "enable-custom-subscription-blocking-FF.json" file from "billing"
    When I "create" enable-custom-subscription-blocking = True
    Given There is a package (pushed from "services/item") by the name of "services-log"
    When I create a service with autoscaler
      | service_name=service-xs | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-xs |
    Then I validate service has "1" instance up
    When I get analytics query "regular-xs" for 120 seconds
    When I update quotas
    When I log in as a "user"
    Then I deactivate service named "service-xs"
    When I create a service with autoscaler
      | service_name=service-s | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-s |
    Then I validate service has "1" instance up
    When I get analytics query "regular-s" for 120 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I deactivate service named "service-s"
    When I create a service with autoscaler
      | service_name=service-m | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-m |
    Then I validate service has "1" instance up
    When I get analytics query "regular-m" for 120 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I deactivate service named "service-m"
    When I create a service with autoscaler
      | service_name=service-l | package=services-log-init | revision=None | config=None | runtime=None | pod_type=regular-l |
    Then I validate service has "1" instance up
    When I get analytics query "regular-l" for 120 seconds
    Given I log in as a "superuser"
    When I update quotas
    When I log in as a "user"
    Then I unable to activate service
    Then "Trial users do not have permission to create services" in error message







================================================
File: tests/features/bot_entity/test_bot_entity_methods.feature
================================================
Feature: Bot Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "bot_entity_methods"

    @testrail-C4523065
    @DAT-46472
    Scenario: Delete bot
        When I create a bot by the name of "some_bot"
        And I list bots in project
        Then I receive a bots list of "2"
        When I delete the created bot by "email"
        And I list bots in project
        Then I receive a bots list of "1"



================================================
File: tests/features/bots_repo/test_bots_create.feature
================================================
Feature: Bots repository create service testing

    Background: Background name
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "bot_entity_methods"

    @testrail-C4523061
    @DAT-46468
    Scenario: Create bot with legal name
        When I create a bot by the name of "some_bot"
        And I list bots in project
        Then a bot with name "some_bot" exists in bots list



================================================
File: tests/features/bots_repo/test_bots_delete.feature
================================================
Feature: Bots repository get service testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    Given I create a project by the name of "bot_delete"

  @testrail-C4523062
  @DAT-46469
  Scenario: Delete bot by email
    When I create a bot by the name of "some_bot"
    And I delete the created bot by "email"
    And I list bots in project
    Then I receive a bots list of "1"

  @testrail-C4523062
  @DAT-46469
  Scenario: Delete bot by id
    When I create a bot by the name of "some_bot"
    And I delete the created bot by "id"
    And I list bots in project
    Then I receive a bots list of "1"

  @testrail-C4523062
  @DAT-46469
  Scenario: Delete a non-existing bot
    When I try to delete a bot by the name of "Some Bot Name"
    Then "NotFound" exception should be raised

  @testrail-C4523062
  @services.delete
  @packages.delete
  @bot.create
  @DAT-48753
  Scenario: Delete a bot with active service should failed
    Given There is a package (pushed from "services/item") by the name of "services-create"
    When I create a bot by the name of "bot_service"
    And I create a service
      | service_name=services-create | package=services-create | revision=None | config=None | runtime=None | bot_user=bot_service |
    When I try to delete a bot by id
    Then "BadRequest" exception should be raised

  @testrail-C4523062
  @services.delete
  @packages.delete
  @bot.create
  @DAT-48753
  Scenario: Delete a bot from members endpoint with active service should failed
    Given There is a package (pushed from "services/item") by the name of "services-create"
    When I create a bot by the name of "bot_service_1"
    And I create a service
      | service_name=services-create | package=services-create | revision=None | config=None | runtime=None | bot_user=bot_service_1 |
    When I try to delete a member by email
    Then "BadRequest" exception should be raised

================================================
File: tests/features/bots_repo/test_bots_get.feature
================================================
Feature: Bots repository get service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "bots_get"


    @testrail-C4523063
    @DAT-46470
    Scenario: Get an existing bot by email
        When I create a bot by the name of "some_bot"
        When I get a bot by the name of "some_bot"
        Then Received bot equals created bot


================================================
File: tests/features/bots_repo/test_bots_list.feature
================================================
Feature: Bots repository list service testing

    Background: Background name
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "bots_list"

    @testrail-C4523064
    @DAT-46471
    Scenario: List one bot
        Given There are no bots in project
        When I create a bot by the name of "first_bot"
        And I list bots in project
        Then I receive a bots list of "1"

    @testrail-C4523064
    @DAT-46471
    Scenario: List two bot
        Given There are no bots in project
        When I create a bot by the name of "first_bot"
        And I create a bot by the name of "second_bot"
        And I list bots in project
        Then I receive a bots list of "2"



================================================
File: tests/features/cache/test_entites_get.feature
================================================
#Feature: cache get
#
#  Background: Initiate Platform Interface and create a project
#    When set binary cache size to 10
#    Given cache is on
#    Given Platform Interface is initialized as dlp and Environment is set according to git branch
#    Given I create a project by the name of "cachetest"
#
#
#  Scenario: Get an entities from cache
#    When I create a dataset with a random name
#    Then i make dataset get and i get hit
#    When I upload a file in path "assets_split/items_upload/0000000162.jpg"
#    Then i make item get and i get hit
#    Then I upload a annotation for the item
#    And i make annotation get and i get hit
#    When I delete the item by id
#    And I delete the dataset that was created by id
#    Then i make dataset get and i get miss
#    Then i make item get and i get miss
#    And i make annotation get and i get miss
#    Then cache is off
#
#  Scenario: download item
#    When I create a dataset with a random name
#    When I upload a file in path "assets_split/items_upload/0000000162.jpg"
#    Then I download the item
#    And I git cache bin hit
#    Then cache is off
#
#  Scenario: update item
#    When I create a dataset with a random name
#    When I upload a file in path "assets_split/items_upload/0000000162.jpg"
#    When I update item system metadata with system_metadata="True"
#    Then i make item get and i get hit
#    And Item was updated
#    Then cache is off
#
#  Scenario: list item
#    When I create a dataset with a random name
#    When I upload a file in path "assets_split/items_upload/0000000162.jpg"
#    When I list items
#    Then i make item get and i get hit
#    Then cache is off
#
#  Scenario: Get an bilk from cache and lru
#    When I create a dataset with a random name
#    Then i make dataset get and i get hit
#    When I upload a file in path "cache/100"
#    Then i make all item get and i get hit
#    And the lru is work
#    When I delete the dataset that was created by id
#    Then i make dataset get and i get miss
#    Then i make all item get and i get miss
#    Then cache is off

================================================
File: tests/features/checkout_testing/test_checkout.feature
================================================
 Feature: Checkouts

   Background: Background
     Given Platform Interface is initialized as dlp and Environment is set according to git branch
     And I create a project by the name of "items_download_batch"
     And Get feature entities
       |dataset|codebase|package|service|

   @packages.delete
   @services.delete
   @testrail-C4523066
   @DAT-46474
   Scenario: Feature entities
     Given Feature: I create a dataset by the name of "Dataset"
     When I checkout
       |project|
     Given Feature: There is a package and service
     And Done setting

   @testrail-C4523066
   @DAT-46474
   Scenario: Projects
     When I checkout
       |project|
     Then I am checked out
       |project|

   @testrail-C4523066
   @DAT-46474
   Scenario: Dataset
     When I checkout
       |dataset|
     Then I am checked out
       |dataset|

   @testrail-C4523066
   @DAT-46474
   Scenario: Package
     When I checkout
       |package|
     Then I am checked out
       |package|

    @testrail-C4523066
    @DAT-46474
    Scenario: Service
     When I checkout
       |service|
     Then I am checked out
       |service|



================================================
File: tests/features/cli_testing/cli_api.feature
================================================
#noinspection CucumberUndefinedStep
Feature: Cli Api

    Background: background
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I am logged in
        And I have context random number

    @testrail-C4523067
    @DAT-46475
    Scenario: Api info
        When I perform command:
            |api|info|
        Then I succeed
        And "environment" in output
        And "token" in output

    @testrail-C4523067
    @DAT-46475
    Scenario: Api info
        When I perform command:
            |api|setenv|-e|local|
        Then I succeed
        And "Platform environment: https://localhost:8443/api/v1" in output

    @testrail-C4523067
    @DAT-46475
    Scenario: Api info
        When I perform command:
            |api|setenv|-e|dev|
        Then I succeed
        And "Platform environment: https://dev-gate.dataloop.ai/api/v1" in output

    @setenv.reset
    @testrail-C4523067
    @DAT-46475
    Scenario: Api info
        When I perform command:
            |api|setenv|-e|some_env|
        Then I dont succeed
        And "Unknown platform environment: "some_env"" in output

================================================
File: tests/features/cli_testing/cli_datasets.feature
================================================
#noinspection CucumberUndefinedStep
Feature: Cli Datasets

    Background: background
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I am logged in
        And I have context random number

    @testrail-C4523068
    @DAT-46476
    Scenario: Datasets Create
        When I perform command:
            |projects|create|-p|to-delete-test-<random>_cli_datasets_project|
        Then I succeed
        And I wait "5"
        When I perform command:
            |datasets|create|-p|to-delete-test-<random>_cli_datasets_project|-d|test_<random>_cli_datasets_dataset|
        Then I succeed
        And I create a dataset by the name of "test_<random>_cli_datasets_dataset" in project "to-delete-test-<random>_cli_datasets_project"

    @testrail-C4523068
    @DAT-46476
    Scenario: Datasets list
        When I perform command:
            |datasets|ls|-p|to-delete-test-<random>_cli_datasets_project|
        Then I succeed
        And "test_<random>_cli_datasets_dataset" in output

    @testrail-C4523068
    @DAT-46476
    Scenario: Finally
        Given I delete the project by the name of "to-delete-test-<random>_cli_datasets_project"
        And I clean folder "<rel_path>/cli_dataset_download"


================================================
File: tests/features/cli_testing/cli_items.feature
================================================
#noinspection CucumberUndefinedStep
Feature: Cli Items

    Background: background
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I am logged in
        And I have context random number

    @testrail-C4523069
    @DAT-46477
    Scenario: Items ls
        When I perform command:
            |projects|create|-p|to-delete-test-<random>_cli_items_project|
        And I succeed
        Then I wait "4"
        When I perform command:
            |datasets|create|-p|to-delete-test-<random>_cli_items_project|-d|test_<random>_cli_items_dataset|
        And I succeed
        And I perform command:
            |items|ls|-p|to-delete-test-<random>_cli_items_project|-d|test_<random>_cli_items_dataset|
        Then I succeed

  @testrail-C4523069
  @DAT-46477
  Scenario: Items upload - maximum param given
        When I perform command:
            |items|upload|-p|to-delete-test-<random>_cli_items_project|-d|test_<random>_cli_items_dataset|-l|<rel_path>/upload_batch/to_upload|-lap|<rel_path>/annotations_for_cli_upload|-f|.jpg,.png|-r|/items|-rp|
        Then I succeed

    @testrail-C4523069
    @DAT-46477
    Scenario: Items upload - minimum param given
        When I perform command:
            |items|upload|-p|to-delete-test-<random>_cli_items_project|-d|test_<random>_cli_items_dataset|-l|<rel_path>/upload_batch/to_upload|-ow
        Then I succeed

    @testrail-C4523069
    @DAT-46477
    Scenario: Items download - maximum param given
        When I perform command:
            |items|download|-p|to-delete-test-<random>_cli_items_project|-d|test_<random>_cli_items_dataset|-l|<rel_path>/cli_dataset_download|-ao|mask,instance,json|-r|/**|-rp|-th|-1|-wt|
        Then I succeed

    @testrail-C4523069
    @DAT-46477
    Scenario: Items download - minimum param given
        When I perform command:
            |items|download|-p|to-delete-test-<random>_cli_items_project|-d|test_<random>_cli_items_dataset|-l|<rel_path>/cli_dataset_download|-ow
        Then I succeed

    @testrail-C4523069
    @DAT-46477
    Scenario: Finally
        Given I delete the project by the name of "to-delete-test-<random>_cli_items_project"


================================================
File: tests/features/cli_testing/cli_others.feature
================================================
#noinspection CucumberUndefinedStep
Feature: Cli Others

    Background: background
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I am logged in
        And I have context random number

    @testrail-C4523070
    @DAT-46478
    Scenario: Exit
        When I perform command:
            |exit|
        Then I succeed

    @testrail-C4523070
    @DAT-46478
    Scenario: Help
        When I perform command:
            |help|
        Then I succeed
        And "CLI for Dataloop" in output
        When I perform command:
            |-h|
        Then I succeed
        And "CLI for Dataloop" in output
        When I perform command:
            |--h|
        Then I succeed
        And "CLI for Dataloop" in output

    @testrail-C4523070
    @DAT-46478
    Scenario: Exit
        When I perform command:
            |version|
        Then I succeed
        And Version is correct


================================================
File: tests/features/cli_testing/cli_projects.feature
================================================
#noinspection CucumberUndefinedStep
Feature: Cli Projects

    Background: background
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I am logged in
        And I have context random number

    @testrail-C4523071
    @DAT-46479
    Scenario: Projects list
        When I perform command:
            |projects|ls|
        Then I succeed

    @testrail-C4523071
    @DAT-46479
    Scenario: Projects Create
        When I perform command:
            |projects|create|-p|to-delete-test-<random>_cli_project|
        Then I succeed
        And I create a project by the name of "to-delete-test-<random>_cli_project"
        And "to-delete-test-<random>_cli_project" in output

    @testrail-C4523071
    @DAT-46479
    Scenario: Finally
        Given I delete the project by the name of "to-delete-test-<random>_cli_project"


================================================
File: tests/features/code_base_entity/test_code_base_repo_methods.feature
================================================
Feature: Codebase Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebase_repo_methods"
        And I create a dataset with a random name
        And There is a Codebase directory with a python file in path "codebases_assets/codebase_entity"
        And I pack from project.codebases directory by name "codebase_name"

    @testrail-C4523078
    @DAT-46486
    Scenario: Unpack by name
        When I unpack a code base entity by the name of "codebase_name" to "codebases_assets/codebase_entity_unpack"
        Then Unpacked code base equal to code base in "codebases_assets/codebase_entity"

    @testrail-C4523078
    @DAT-46486
    Scenario: List all versions when 2 exist
        When I modify python file - (change version) in path "codebases_assets/codebase_entity/some_code.py"
        Given I pack from project.codebases directory by name "codebase_name"
        When I list versions of code base entity "codebase_name"
        Then I receive a list of "2" versions




================================================
File: tests/features/code_bases_repo/test_code_bases_get.feature
================================================
Feature: Codebases repository Get method

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebases_get"
        And I create a dataset with a random name

    @testrail-C4523073
    @DAT-46481
    Scenario: Get code base by name
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_get"
        When I pack directory by name "codebase_name1"
        And I modify python file - (change version) in path "codebases_assets/codebases_get/some_code.py"
        And I pack directory by name "codebase_name1"
        When I get by name version "1" of code base "codebase_name1"
        Then I receive a Codebase object
        And Codebase received equal code base packet

    @testrail-C4523073
    @DAT-46481
    Scenario: Get code base by id
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_get"
        When I pack directory by name "codebase_name2"
        And I modify python file - (change version) in path "codebases_assets/codebases_get/some_code.py"
        And I pack directory by name "codebase_name2"
        When I get by id version "1" of code base "codebase_name2"
        Then I receive a Codebase object
        And Codebase received equal code base packet

    @testrail-C4523073
    @DAT-46481
    Scenario: Get code base by id - latest
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_get"
        When I pack directory by name "codebase_name3"
        And I modify python file - (change version) in path "codebases_assets/codebases_get/some_code.py"
        And I pack directory by name "codebase_name3"
        When I get by id version "latest" of code base "codebase_name3"
        Then I receive a Codebase object
        And Codebase received equal code base packet

    @testrail-C4523073
    @DAT-46481
    Scenario: Get code base by name - no version given
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_get"
        When I pack directory by name "codebase_name4"
        And I modify python file - (change version) in path "codebases_assets/codebases_get/some_code.py"
        And I pack directory by name "codebase_name4"
        When I get a code base by name "codebase_name4"
        Then I receive a Codebase object
        And Codebase received equal code base packet

    @testrail-C4523073
    @DAT-46481
    Scenario: Get code base by id - all
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_get"
        When I pack directory by name "codebase_name5"
        And I modify python file - (change version) in path "codebases_assets/codebases_get/some_code.py"
        And I pack directory by name "codebase_name5"
        When I get by name version "all" of code base "codebase_name5"
        Then I receive a list of Codebase objects
        And Codebase list have length of "2"




================================================
File: tests/features/code_bases_repo/test_code_bases_init.feature
================================================
Feature: Codebases repository Init

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebases_init"
        And I create a dataset with a random name

    @testrail-C4523074
    @DAT-46482
    Scenario: Init code bases with param: dataset, project, client_api
        When I init code bases with params: project, dataset, client_api
        Then I receive a code bases repository object
        And Codebases project are equal
        And Codebases dataset equal "Dataset"

    @testrail-C4523074
    @DAT-46482
    Scenario: Init code bases with param: project, client_api
        When I init code bases with params: project, client_api
        Then I receive a code bases repository object
        And Codebases project are equal
        And Codebases dataset has name "Binaries"

#    @testrail-C4523074
     @DAT-46482
#    Scenario: Init code bases with param: client_api
#        When I try to init code bases with params: client_api
#        Then "BadRequest" exception should be raised

    @testrail-C4523074
    @DAT-46482
    Scenario: Init code bases with param: dataset, client_api
        When I init code bases with params: dataset, client_api
        Then I receive a code bases repository object
        And Codebases dataset equal "Dataset"




================================================
File: tests/features/code_bases_repo/test_code_bases_list.feature
================================================
Feature: Codebases repository List method

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebases_list"
        And I create a dataset with a random name

    @testrail-C4523075
    @DAT-46483
    Scenario: List all versions when 0 exist
        Given There are "0" code bases
        When I list all code bases
        Then I receive a list of "0" code bases

    @testrail-C4523075
    @DAT-46483
    Scenario: List all code bases when 1 exist
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_list"
        When I pack directory by name "codebase1_name"
        When I list all code bases
        Then I receive a list of "1" code bases

    @testrail-C4523075
    @DAT-46483
    Scenario: List all code bases when 2 exist
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_list"
        And I pack directory by name "codebase2_name"
        When I list all code bases
        Then I receive a list of "2" code bases

    @testrail-C4523075
    @DAT-46483
    Scenario: List all versions when 3 exist
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_list"
        And There are "2" code bases
        When I pack directory by name "codebase_name3"
        When I list all code bases
        Then I receive a list of "3" code bases




================================================
File: tests/features/code_bases_repo/test_code_bases_list_versions.feature
================================================
Feature: Codebases repository List Version method

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebases_list_versions"
        And I create a dataset with a random name

    @testrail-C4523076
    @DAT-46484
    Scenario: List all versions when 2 exist
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_list_versions"
        When I pack directory by name "codebase_name"
        And I modify python file - (change version) in path "codebases_assets/codebases_list_versions/some_code.py"
        And I pack directory by name "codebase_name"
        When I list versions of "codebase_name"
        Then I receive a list of "2" versions

    @testrail-C4523076
    @DAT-46484
    Scenario: List all versions when 1 exist
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_list_versions"
        When I pack directory by name "codebase_name1"
        When I list versions of "codebase_name1"
        Then I receive a list of "1" versions

    @testrail-C4523076
    @DAT-46484
    Scenario: List all versions when 0 exist
        When I list versions of "codebase_name2"
        Then I receive a list of "0" versions




================================================
File: tests/features/code_bases_repo/test_code_bases_unpack.feature
================================================
Feature: Codebases repository Unpack method

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebases_unpack"
        And I create a dataset with a random name

    @testrail-C4523077
    @DAT-46485
     Scenario: Unpack by name
         Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
         And I pack directory by name "codebase_name1"
         When I unpack a code base by the name of "codebase_name1" to "codebases_assets/codebase_unpack"
         Then Unpacked code base equal to code base in "codebases_assets/codebases_unpack"

    @testrail-C4523077
    @DAT-46485
     Scenario: Unpack by id
         Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
         And I pack directory by name "codebase_name2"
         When I unpack a code base by the id of "codebase_name2" to "codebases_assets/codebase_unpack"
         Then Unpacked code base equal to code base in "codebases_assets/codebases_unpack"

    @testrail-C4523077
    @DAT-46485
     Scenario: Unpack non-existing by name
         Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
         And I pack directory by name "codebase_name3"
         When I try to unpack a code base by the name of "some_name" to "codebases_assets/codebase_unpack"
         Then "NotFound" exception should be raised

    @testrail-C4523077
    @DAT-46485
     Scenario: Unpack by non-existing id
         Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
         And I pack directory by name "codebase_name4"
         When I try to unpack a code base by the id of "some_id" to "codebases_assets/codebase_unpack"
         Then "BadRequest" exception should be raised

    @testrail-C4523077
    @DAT-46485
     Scenario: Unpack - specific version
         Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
         And I pack directory by name "codebase_name5"
         And I modify python file - (change version) in path "codebases_assets/codebases_unpack/some_code.py"
         And I pack directory by name "codebase_name5"
         When I unpack a code base "codebase_name5" version "1" to "codebases_assets/codebase_unpack"
         Then Unpacked code base equal to code base in "codebases_assets/codebases_unpack"

    @testrail-C4523077
    @DAT-46485
     Scenario: Unpack - latest version
         Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
         And I pack directory by name "codebase_name6"
         And I modify python file - (change version) in path "codebases_assets/codebases_unpack/some_code.py"
         And I pack directory by name "codebase_name6"
         When I unpack a code base "codebase_name6" version "latest" to "codebases_assets/codebase_unpack"
         Then Unpacked code base equal to code base in "codebases_assets/codebases_unpack"

    @testrail-C4523077
    @DAT-46485
    Scenario: Unpack - all versions
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
        And I pack directory by name "codebase_name7"
        And I modify python file - (change version) in path "codebases_assets/codebases_unpack/some_code.py"
        And I pack directory by name "codebase_name7"
        When I unpack a code base "codebase_name7" version "all" to "codebases_assets/codebase_unpack"
        Then I receive all versions in "codebases_assets/codebase_unpack" and they are equal to versions in "codebases_assets/codebases_unpack/some_code.py"

    @testrail-C4523077
    @DAT-46485
    Scenario: Unpack - non-existing version
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_unpack"
        And I pack directory by name "codebase_name8"
        And I modify python file - (change version) in path "codebases_assets/codebases_unpack/some_code.py"
        And I pack directory by name "codebase_name8"
        When I try to unpack a code base "codebase_name8" version "5" to "codebases_assets/codebase_unpack"
        Then "NotFound" exception should be raised




================================================
File: tests/features/code_bases_repo/test_code_basess_pack.feature
================================================
Feature: Codebases repository Pack method

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "codebases_pack"
        And I create a dataset with a random name

    @testrail-C4523072
    @DAT-46480
    Scenario: Unpack a Codebase (init param: dataset, project, client_api)
        Given Directory "codebases_pack_unpack" is empty
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_pack"
        When I pack directory by name "codebase_name1"
        Then I receive a Codebase object
        And Codebase in host when downloaded to "codebases_pack_unpack" equals code base in path "codebases_assets/codebases_pack"

    @testrail-C4523072
    @DAT-46480
    Scenario: Pack a Codebase (init param: project, client_api)
        Given Directory "codebases_pack_unpack" is empty
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_pack"
        When I pack directory by name "codebase_name2"
        Then I receive a Codebase object
        And Dataset by the name of "Binaries" was created
        And Codebase in host in dataset "Binaries", when downloaded to "codebases_pack_unpack" equals code base in path "codebases_assets/codebases_pack"

    @testrail-C4523072
    @DAT-46480
    Scenario: Pack a Codebase - new version
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_pack"
        When I pack directory by name "codebase_name3"
        And I modify python file - (change version) in path "codebases_assets/codebases_pack/some_code.py"
        And I pack directory by name "codebase_name3"
        Then There should be "2" versions of the code base "codebase_name3" in host

    @testrail-C4523072
    @DAT-46480
    Scenario: Pack a Codebase - new version - nameless
        Given There is a Codebase directory with a python file in path "codebases_assets/codebases_pack"
        When I pack directory - nameless
        And I modify python file - (change version) in path "codebases_assets/codebases_pack/some_code.py"
        And I pack directory - nameless
        Then There should be "2" versions of the code base "codebase" in host




================================================
File: tests/features/command_entity/test_command.feature
================================================
Feature: command Entity repo - test using dataset clone feature

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "command_test"
        And I create a dataset with a random name

    @testrail-C4523079
    @DAT-46487
    Scenario: Use command with clone dataset
        Given There are "10" items
        When Dataset is cloning
        Then Cloned dataset has "10" items

    @testrail-C4523079
    @DAT-46487
    Scenario: command error  with clone dataset with same name
        When Dataset is cloning with same name get already exist error


  @testrail-C4523079
  @DAT-53225
  Scenario: Use command with clone dataset to existing dataset
    Given There are "10" items
    When I create another dataset with a random name
    And Dataset is cloning to existing dataset
    Then Cloned dataset has "10" items

  @DAT-53962
  Scenario: Use command with clone dataset to existing dataset with filter
    Given There are "11" items
    When I create another dataset with a random name
    And I create filters
    And I add field "filename" with values "/file1*" and operator "None"
    And Dataset is cloning to existing dataset
    Then Cloned dataset has "2" items

================================================
File: tests/features/compute/test_compute.feature
================================================
@skip_test
Feature: compute
#
#    Background: Initiate Platform Interface and create a project
#        Given Platform Interface is initialized as dlp and Environment is set according to git branch
#        And I create a project by the name of "compute_test"
#        And I create a compute
#        And I create a dataset with a random name
#        And There is a package (pushed from "services/item_with_init") by the name of "services-flow"
#        When I upload a file in path "assets_split/items_upload/0000000162.jpg"
#
#
#    @DAT-46487
#    Scenario: creating a faas compute cluster and try to deploy services
#        When i set driver feaureflag
#        And I deploy a service with init prams
#        Then I execute the service
#        And The execution success with the right output
#
#    @DAT-76181
#    Scenario: Set default
#        Given There are 2 drivers
#        And New Organization
#        And Organization has a project
#        And Organization has no default driver
#        And Services deployed in the project is not using the default driver
#        When I set default driver
#        Then Organization has a default driver user setting that uses secondary driver
#        And Services deployed in the project are using the secondary driver
#        And First service is also using the secondary driver




================================================
File: tests/features/compute/test_compute_archive.feature
================================================
Feature: Compute archive testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "compute_archive"


  @DAT-86651
  @restore_json_file
  @compute_serviceDriver.delete
  Scenario: SDK Test Compute archived - Create compute with the same name as archived compute Should success
    Given I fetch the compute from "computes/base_compute.json" file and update compute with params "True"
      | key           | value |
      | config.status | ready |
    When I try to create the compute from context.original_path
    Then I able to delete compute
    When I try to create the compute from context.original_path
    Then I validate no error in context
    When I get compute from the compute list by the name

================================================
File: tests/features/converter/test_converter_coco.feature
================================================
Feature: Converter coco format

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "coco_converter"
    And I create a dataset with a random name

  @testrail-C4523080
  @DAT-46488
  Scenario: Convert local coco dataset to dataloop
    Given There is a local "coco" dataset in "converter/coco/local_dataset"
    When I convert local "coco" dataset to "dataloop"
    Given Local path in "converter/coco/reverse" is clean
    When I reverse dataloop dataset to local "coco" in "converter/coco/reverse"
    Then local "coco" dataset in "converter/coco/local_dataset" is equal to reversed dataset in "converter/coco/reverse"

  @testrail-C4523080
  @DAT-46488
  Scenario: Convert local coco dataset with inner folders to dataloop
    Given There is a local "coco" dataset in "converter/coco/local_dataset_inner"
    When I convert local "coco" dataset to "dataloop"
    Given Local path in "converter/coco/reverse" is clean
    When I reverse dataloop dataset to local "coco" in "converter/coco/reverse"
    Then local "coco" dataset in "converter/coco/local_dataset_inner" is equal to reversed dataset in "converter/coco/reverse"

  @testrail-C4523080
  @DAT-46488
  Scenario: Upload dataset with inner folder using coco converter
    Given I use "COCO" converter to upload items with annotations to the dataset using the given params
      | Parameter              | Value                                                        |
      | local_items_path       | converter/coco/local_coco_with_folders/items                 |
      | local_annotations_path | converter/coco/local_coco_with_folders/annotations/coco.json |
    And   Local path in "converter/coco/local_downloaded_items" is clean
    And   I download the dataset items annotations in "COCO" format using the given params
      | Parameter  | Value                                 |
      | local_path | converter/coco/local_downloaded_items |
    Then  I use "COCO" format to compare the uploaded annotations with the downloaded annotations

  @testrail-C4533546
  @DAT-46488
  Scenario: Convert local coco dataset do not overwrite the existing labels
    Given There is a local "coco" dataset in "converter/coco/local_dataset"
    When I add single root Label "laptop"
    And I convert local "coco" dataset to "dataloop"
    Then The converter do not overwrite the existing label


  @DAT-47083
  Scenario: Convert local coco dataset to dataloop - with empty segmentation
    Given There is a local "coco" dataset in "converter/emptyseg"
    When I convert local "coco" dataset to "dataloop"
    Given Local path in "converter/emptyseg/reverse" is clean
    When I reverse dataloop dataset to local "coco" in "converter/emptyseg/reverse"
    Then local "coco" dataset in "converter/emptyseg" is equal to reversed dataset in "converter/emptyseg/reverse"


================================================
File: tests/features/converter/test_converter_dataloop.feature
================================================
Feature: Converter dataloop format

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "dataloop_converter"

   @converter.platform_dataset.delete
   @testrail-C4523081
   @DAT-46489
   Scenario: Convert platform dataloop dataset to yolo
       Given There is a local "dataloop" dataset in "converter/dataloop/local_dataset"
       And There is a platform dataloop dataset from "converter/dataloop/local_dataset"
       And Local path in "converter/dataloop/convert" is clean
       When I convert platform dataset to "yolo" in path "converter/dataloop/convert"
       Then Converted "yolo" dataset in "converter/dataloop/convert" is equal to dataset in "converter/yolo/dataloop_should_be"

   @converter.platform_dataset.delete
   @testrail-C4523081
   @DAT-46489
   Scenario: Convert platform dataloop dataset to voc
       Given There is a local "dataloop" dataset in "converter/dataloop/local_dataset"
       And There is a platform dataloop dataset from "converter/dataloop/local_dataset"
       And Local path in "converter/dataloop/convert" is clean
       When I convert platform dataset to "voc" in path "converter/dataloop/convert"
       Then Converted "voc" dataset in "converter/dataloop/convert/voc/images" is equal to dataset in "converter/voc/dataloop_should_be/voc/images"

  @converter.platform_dataset.delete
  @testrail-C4523081
  @DAT-46489
  Scenario: Convert platform dataloop dataset to coco
      Given There is a local "dataloop" dataset in "converter/dataloop/local_dataset"
      And There is a platform dataloop dataset from "converter/dataloop/local_dataset"
      And Local path in "converter/dataloop/convert" is clean
      Then I wait "30"
      When I convert platform dataset to "coco" in path "converter/dataloop/convert"
      Then Converted "coco" dataset in "converter/dataloop/convert/coco.json" is equal to dataset in "converter/coco/dataloop_should_be/coco.json"



================================================
File: tests/features/converter/test_converter_voc.feature
================================================
Feature: Converter voc format

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "voc_converter"
        And I create a dataset with a random name

    @testrail-C4523082
    @DAT-46490
    Scenario: Convert local voc dataset to dataloop
        Given There is a local "voc" dataset in "converter/voc/local_dataset_attr"
        When I convert local "voc" dataset to "dataloop"
        Given Local path in "converter/voc/reverse" is clean
        When I reverse dataloop dataset to local "voc" in "converter/voc/reverse"
        Then local "voc" dataset in "converter/voc/local_dataset_attr" is equal to reversed dataset in "converter/voc/reverse"

    @testrail-C4523082
    @DAT-46490
    Scenario: Upload dataset with inner folder using coco converter
        Given I use "VOC" converter to upload items with annotations to the dataset using the given params
            | Parameter              | Value                                                |
            | local_items_path       | converter/voc/local_voc_with_folders/items           |
            | local_annotations_path | converter/voc/local_voc_with_folders/annotations/voc |
        And   Local path in "converter/voc/local_downloaded_items" is clean
        And   I download the dataset items annotations in "VOC" format using the given params
            | Parameter  | Value                                |
            | local_path | converter/voc/local_downloaded_items |
        Then  I use "VOC" format to compare the uploaded annotations with the downloaded annotations


================================================
File: tests/features/converter/test_converter_yolo.feature
================================================
Feature: Converter yolo format

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "yolo_converter"
        And I create a dataset with a random name

    @testrail-C4523083
    @DAT-46491
    Scenario: Convert local yolo dataset to dataloop
        Given There is a local "yolo" dataset in "converter/yolo/local_dataset"
        When I convert local "yolo" dataset to "dataloop"
        Given Local path in "converter/yolo/reverse" is clean
        When I reverse dataloop dataset to local "yolo" in "converter/yolo/reverse"
        Then local "yolo" dataset in "converter/yolo/local_dataset" is equal to reversed dataset in "converter/yolo/reverse"

    @DAT-46491
    Scenario: Upload dataset with inner folder using coco converter
        Given I use "YOLO" converter to upload items with annotations to the dataset using the given params
            | Parameter              | Value                                                           |
            | local_items_path       | converter/yolo/local_yolo_with_folders/items                    |
            | local_labels_path      | converter/yolo/local_yolo_with_folders/annotations/labels.names |
            | local_annotations_path | converter/yolo/local_yolo_with_folders/annotations/yolo         |
        And   Local path in "converter/yolo/local_downloaded_items" is clean
        And   I download the dataset items annotations in "YOLO" format using the given params
            | Parameter  | Value                                 |
            | local_path | converter/yolo/local_downloaded_items |
        Then  I use "YOLO" format to compare the uploaded annotations with the downloaded annotations


================================================
File: tests/features/dataset_entity/test_add_labels_methods.feature
================================================
Feature: Add Labels include nested Labels

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_annotation_add"
        And I create a dataset with a random name
        And There is no label with the same label I plan to add

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single root Label with all parameters
        When I add new single label with all parameters
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single root Label with all parameters with no update_ontology parameter
        When I add new single label with all parameters with no update_ontology parameter
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single root Label with same label name twice
        When I add single root Label
        Then I add single root Label

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single root Label with Label name only
        When I add single root Label with Label name only
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single nested root Label with all parameters
        When I add single nested root Label with all parameters
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single nested Label with Label name only
        When I add single nested Label with Label name only
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single nested label using ontology.add_label when update_ontology is true
        When I add single nested label using ontology.add_label when update_ontology is true
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single nested label using ontology.add_label when update_ontology is false
        Then I add single nested label using ontology.add_label when update_ontology is false

    @testrail-C4523093
    @DAT-46504
    Scenario: Add single not nested label using ontology.add_label when update_ontology is false
        When I add single not nested label using ontology.add_label when update_ontology is false
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add labels of string type
        When I add labels of string type
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add labels of string type using ontology.add_labels when update_ontology is false
        When I add labels of string type using ontology.add_labels when update_ontology is false
        Then Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add and Update many labels
        When I add many labels
        Then Label has been added
        And  I update many labels
        And Label has been added
        And  I upsert many labels
        And Label has been added

    @testrail-C4523093
    @DAT-46504
    Scenario: Add many nested labels
        When I add many nested labels
        Then Label has been added



================================================
File: tests/features/dataset_entity/test_dataset_repo_methods.feature
================================================
Feature: Dataset Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "dataset_repo_methods"
        And I create a dataset with a random name

    @testrail-C4523094
    @DAT-46505
    Scenario: Download Annotations
        Given Labels in file: "labels.json" are uploaded to test Dataset
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
        And Item is annotated with annotations in file: "0162_annotations.json"
        And There is no folder by the name of "json" in assets folder
        When I download dataset entity annotations to assets
        Then I get a folder named "json" in assets folder
        And Annotations downloaded equal to the annotations uploaded

    @testrail-C4523094
    @DAT-46505
    Scenario: Download dataset with items
        Given Item in path "0000000162.png" is uploaded to "Dataset"
        And Labels in file: "labels.json" are uploaded to test Dataset
        And Item is annotated with annotations in file: "0162_annotations.json"
        And There are no folder or files in folder "downloaded_dataset"
        When I download dataset entity to "downloaded_dataset"
        Then Dataset downloaded to "downloaded_dataset" is equal to dataset in "downloaded_dataset-should_be"
        And There is no "log" file in folder "downloaded_dataset"

    @testrail-C4523094
    @DAT-46505
    Scenario: Delete dataset
        When I delete a dataset entity
        Then Dataset with same name does not exists

    @testrail-C4523094
    @DAT-46505
    Scenario: To Json
        Then Object "Dataset" to_json() equals to Platform json.


    @DAT-68486
    Scenario: unsearchablePaths
        When "add" unsearchable paths "metadata.test" to dataset
        Then "metadata.test" is "added" to dataset schema
        When "delete" unsearchable paths "metadata.test" to dataset
        Then "metadata.test" is "deleted" to dataset schema






================================================
File: tests/features/dataset_entity/test_directory_tree.feature
================================================
 Feature: Create test for dataset directory tree

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "dataset_dir_tree_test"

    @DAT-53904
    Scenario: Create dataset
        Given I create a dataset named "dir_tree_test"
        Then dataset.directory_tree.dir_names contains "/"
        And I upload an item by the name of "test_item.jpg"
        And I get an item thumbnail response
        And dataset.directory_tree.dir_names contains "/.dataloop"
        And dataset.directory_tree.dir_names contains "/.dataloop/thumbnails"




================================================
File: tests/features/datasets_repo/test_dataset_clone.feature
================================================
Feature: Test datasets clone method

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "clone_dataset"
    And I create a dataset with a random name

  @DAT-53963
  Scenario: Clone dataset to existing dataset using dataset.id
    Given There are "10" items
    When I create another dataset with a random name
    And I call datasets.clone using dataset.id
    Then Cloned dataset has "10" items

  @DAT-85613
    Scenario: Updated dataset - Dataset updater should be as given
        When I create a dataset with a random name
        And I clone a dataset
        Then Dataset attribute should be as given
            | creator=current_user | updated_by=current_user |
        When I update cloned dataset name to "New dataset name"
        Then Dataset attribute should be as given
            | updated_by=current_user |


================================================
File: tests/features/datasets_repo/test_dataset_context.feature
================================================
Feature: Datasets repository Context testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "project1 project2"


    @testrail-C4523091
    @DAT-46499
    Scenario: Get Dataset from the checkout Project it belong to
        Given I create datasets by the name of "dataset1 dataset2"
        When I checkout project number 1
        And I get a dataset number 1 from checkout project
        Then dataset Project.id is equal to project 1 id


    @testrail-C4523091
    @DAT-46499
    Scenario: Get Dataset from the checkout Project it is not belong to
        Given I create datasets by the name of "dataset1 dataset2"
        When I checkout project number 1
        When I get a dataset number 2 from checkout project
        Then dataset Project.id is equal to project 2 id


    @testrail-C4523091
    @DAT-46499
    Scenario: Get Dataset from the Project it is  belong to
        Given I create datasets by the name of "dataset1 dataset2"
        When I get a dataset number 1 from project number 1
        Then dataset Project.id is equal to project 1 id


    @testrail-C4523091
    @DAT-46499
    Scenario: Get Dataset from the Project it is not belong to
        Given I create datasets by the name of "dataset1 dataset2"
        When I get a dataset number 1 from project number 2
        Then dataset Project.id is equal to project 1 id



================================================
File: tests/features/datasets_repo/test_dataset_download.feature
================================================
Feature: Datasets repository download service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "datasets_download"
    When I add "free_text" attribute to ontology
      | key=1 | title=attr1 | scope=all |
    When I add "free_text" attribute to ontology
      | key=2 | title=attr2 | scope=all |

  @testrail-C4523092
  @DAT-46500
  Scenario: Download dataset with items
    Given I create a dataset with a random name
    And Item in path "0000000162.png" is uploaded to "Dataset"
    And Labels in file: "labels.json" are uploaded to test Dataset
    And Item is annotated with annotations in file: "0162_annotations.json"
    And There are no folder or files in folder "downloaded_dataset"
    When I download dataset to "downloaded_dataset"
    Then Dataset downloaded to "downloaded_dataset" is equal to dataset in "downloaded_dataset-should_be"
    And There is no "log" file in folder "downloaded_dataset"

  @testrail-C4523092
  @DAT-46500
  Scenario: Download dataset with dot in local path
    Given I create a dataset by the name of "dot.dataset1"
    And Item in path "0000000162.png" is uploaded to "Dataset"
    And Labels in file: "labels.json" are uploaded to test Dataset
    And Item is annotated with annotations in file: "0162_annotations.json"
    And There are no folder or files in folder "downloaded.dataset"
    When I download dataset to "downloaded.dataset"
    Then Dataset downloaded to "downloaded.dataset" is equal to dataset in "downloaded_dataset-should_be"
    And There is no "log" file in folder "downloaded.dataset"

  @testrail-C4523092
  @DAT-46500
  Scenario: Download dataset with Overwrite True
    Given I create a dataset with a random name
    And There are no items
    And I get "1" images of type "png" for the dataset
    When I upload all the images for the dataset
    And I download the dataset without Overwrite variable
    And I modify the downloaded item
    And I download the dataset with Overwrite "True"
    Then The dataset item will be "overwritten"

  @testrail-C4523092
  @DAT-46500
  Scenario: Download dataset with Overwrite False
    Given I create a dataset with a random name
    And There are no items
    And I get "1" images of type "png" for the dataset
    When I upload all the images for the dataset
    And I download the dataset without Overwrite variable
    And I modify the downloaded item
    And I download the dataset with Overwrite "False"
    Then The dataset item will be "not overwritten"

  @testrail-C4523092
  @DAT-46500
  Scenario: Download dataset with dot in local path without item folder
    Given I create a dataset by the name of "dot.datasetnoitem"
    And Item in path "0000000162.png" is uploaded to "Dataset"
    And Labels in file: "labels.json" are uploaded to test Dataset
    And Item is annotated with annotations in file: "0162_annotations.json"
    And There are no folder or files in folder "downloaded.dataset"
    When I download dataset to "downloaded.dataset" without item folder
    Then Dataset downloaded to "downloaded.dataset" is equal to dataset in "downloaded_dataset-should_be"
    And There is no "log" file in folder "downloaded.dataset"

  @testrail-C4523092
  @DAT-46500
  Scenario: Download dataset with json items
    Given I create a dataset with a random name
    And Item in path "linked_items-should_be/items/wrong-linked-item.json" is uploaded to "Dataset"
    And There are no folder or files in folder "linked_items"
    When I download dataset to "linked_items"
    Then The folder "linked_items" is equal to to "linked_items-should_be"
    And There is no "log" file in folder "linked_items"

  @testrail-C4533352
  @DAT-46500
  Scenario: Download dataset folder
    Given I create a dataset with a random name
    And There are no items
    And I upload an item by the name of "test_item.jpg"
    And I upload item by the name of "test_item.jpg" to a remote path "test"
    And Folder "test_items_download" is empty
    When I download dataset folder "/test" to "test_items_download"
    Then There are "1" files in "test_items_download"

  @testrail-C4533352
  @DAT-46500
  Scenario: Download dataset folder with sub folder
    Given I create a dataset with a random name
    And There are no items
    And I upload an item by the name of "test_item.jpg"
    And I upload item by the name of "test_item.jpg" to a remote path "test"
    And I upload item by the name of "test_item.jpg" to a remote path "test/subfolder"
    And Folder "test_items_download" is empty
    When I download dataset folder "/test" to "test_items_download"
    Then There are "2" files in "test_items_download"


================================================
File: tests/features/datasets_repo/test_dataset_upload_annotations.feature
================================================
Feature: Datasets repository download_annotations service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "datasets_download_annotations"
    And I create a dataset with a random name
    And Item in path "0000000162.png" is uploaded to "Dataset"
    When I add "free_text" attribute to ontology
      | key=1 | title=attr1 | scope=all |
    When I add "free_text" attribute to ontology
      | key=2 | title=attr2 | scope=all |

  Scenario: upload annotations
    Then I upload annotations to dataset
    And Annotations in item equal to the annotations uploaded


  Scenario: upload annotations use new end point
    When Item in path "0000000162.png" is uploaded to "Dataset" in remote path "/a/b/c"
    Then I upload annotations to dataset in new end point "/a/b/c"
    And Annotations in item equal to the annotations uploaded




================================================
File: tests/features/datasets_repo/test_dataset_upload_csv.feature
================================================
Feature: Datasets repository - upload items & annotations from csv

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_download_annotations"
        And I create a dataset with a random name

    @DAT-44230
    Scenario: upload csv
        Given I upload csv "mycsv_all.csv" to dataset
        Then description in csv "mycsv_all.csv" equal to the description uploaded
        Then metadata in csv "mycsv_all.csv" equal to the metadata uploaded






================================================
File: tests/features/datasets_repo/test_datasets_create.feature
================================================
@qa-nightly
Feature: Datasets repository create service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_create"

    @testrail-C4523084
    @DAT-46492
    Scenario: Create a dataset with a legal name
        Given There are no datasets
        When I create a dataset with a random name
        Then Dataset object with the same name should be exist
        And Dataset object with the same name should be exist in host

    # bug in platform dataset is created
    # Scenario: Create a dataset with an illegal name
    #     Given There are no datasets
    #     When When I try to create a dataset with a blank name
    #     Then "BadRequest" exception should be raised
    #     And There are no datasets

    @testrail-C4523084
    @DAT-46492
    Scenario: Create a dataset with an existing dataset name
        Given There are no datasets
        And I create a dataset with a random name
        When I try to create a dataset by the same name
        Then "BadRequest" exception should be raised
        And No dataset was created

    @testrail-C4523084
    @DAT-46492
    Scenario: Create a dataset with existing recipe
        When I create a dataset with a random name
        Then Dataset object with the same name should be exist
        And Dataset object with the same name should be exist in host
        And I create a dataset with existing recipe
        And dataset recipe is equal to the existing recipe

    @DAT-85612
    Scenario: Created dataset - Dataset creator should be as given
        When I create a dataset with a random name
        Then Dataset attribute should be as given
            | creator=current_user |

================================================
File: tests/features/datasets_repo/test_datasets_delete.feature
================================================
@qa-nightly
Feature: Datasets repository delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_delete"

    @testrail-C4523085
    @DAT-46493
    Scenario: Delete dataset by name
        Given There are no datasets
        And I create a dataset with a random name
        When I delete the dataset that was created by name
        Then Dataset with same name does not exists

    @testrail-C4523085
    @DAT-46493
    Scenario: Delete dataset by id
        Given There are no datasets
        And I create a dataset with a random name
        When I delete the dataset that was created by id
        Then Dataset with same name does not exists

    @testrail-C4523085
    @DAT-46493
    Scenario: Delete a non-existing dataset
        Given There are no datasets
        And I create a dataset with a random name
        When I try to delete a dataset by the name of "Some Dataset Name"
        Then "NotFound" exception should be raised
        And No dataset was deleted




================================================
File: tests/features/datasets_repo/test_datasets_download_annotations.feature
================================================
Feature: Datasets repository download_annotations service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_download_annotations"
        And I create a dataset with a random name
        When I add "free_text" attribute to ontology
            | key=1 | title=attr1 | scope=all |

    @testrail-C4523086
    @DAT-46494
    Scenario: Download existing annotations
        Given Item in path "0000000162.png" is uploaded to "Dataset"
        And There are a few annotations in the item
        And There is no folder by the name of "json" in assets folder
        When I download dataset annotations
        Then I get a folder named "json" in assets folder
        And Annotations downloaded equal to the annotations uploaded

    # Scenario: Download annotations when no annotation exist
    #     Given Item in path "0000000162.png" is uploaded to "Dataset"
    #     And There is no folder by the name of "json" in assets folder
    #     When I download dataset annotations
    #     Then I get a folder named "json" in assets folder
    #     And The folder named "json" in folder assets is empty





================================================
File: tests/features/datasets_repo/test_datasets_edit.feature
================================================
@qa-nightly
Feature: Datasets repository update service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_edit"

    @testrail-C4523087
    @DAT-46495
     Scenario: Change dataset name to a legal name
         Given There are no datasets
         And I create a dataset by the name of "Original_Dataset_Name"
         When I update dataset name to "New_Dataset_Name"
         Then I create a dataset by the name of "New_Dataset_Name" in host
         And There is no dataset by the name of "Original_Dataset_Name" in host
         And The dataset from host by the name of "New_Dataset_Name" is equal to the one created

      # Currently Not a bug
#     Scenario: Change dataset name to an illegal name
#         Given There are no datasets
#         And I create a dataset by the name of "Original_Dataset_Name"
#         When I try to update the "Original_Dataset_Name" name to a blank name
#         Then "BadRequest" exception should be raised

    @testrail-C4523087
    @DAT-46495
     Scenario: Change dataset name to an existing dataset name
         Given There are no datasets
         And I create a dataset by the name of "Existing_Dataset_Name"
         And I create a dataset by the name of "Dataset"
         When I try to update the "Dataset" name to "Existing_Dataset_Name"
         Then "BadRequest" exception should be raised
         And I create a dataset by the name of "Existing_Dataset_Name" in host
         And I create a dataset by the name of "Dataset" in host


    @DAT-80166
    Scenario: Update dataset metadata
        Given There are no datasets
        And Create "5" datasets in project with the prefix name "test"
        When I get dataset by name "test-4"
        And I update dataset metadata "user.test:'3'"
        And I create dataset filters by metadata - "user.test" = "3"
        And I get datasets list by params
            | filters=context.filters |
        Then I validate for "dataset" that the updated metadata is "user.test:3"


    @DAT-80166
    Scenario: Update dataset metadata.system
        Given There are no datasets
        And Create "5" datasets in project with the prefix name "test"
        When I get dataset by name "test-4"
        And I update dataset metadata "system.test:'3'"
        And I create dataset filters by metadata - "system.test" = "3"
        And I get datasets list by params
            | filters=context.filters |
        Then I validate for "dataset" that the updated metadata is "system.test:3"

================================================
File: tests/features/datasets_repo/test_datasets_get.feature
================================================
Feature: Datasets repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_get"

    @testrail-C4523088
    @DAT-46496
    Scenario: Get an existing dataset by name
        Given There are no datasets
        And I create a dataset with a random name
        When I get a dataset with the created name
        Then I get a dataset with the created name
        And The dataset I got is equal to the one created

    @testrail-C4523088
    @DAT-46496
    Scenario: Get an existing project by id
        Given There are no datasets
        And I create a dataset with a random name
        When I get a dataset by the id of the dataset "Dataset"
        Then I get a dataset with the created name
        And The dataset I got is equal to the one created

    @testrail-C4523088
    @DAT-46496
    Scenario: Get non-existing dataset by name
        Given There are no datasets
        When I try to get a dataset by the name of "Dataset"
        Then "NotFound" exception should be raised

    @testrail-C4523088
    @DAT-46496
    Scenario: Get non-existing dataset by id
        Given There are no datasets
        When I try to get a dataset by id
        Then "NotFound" exception should be raised




================================================
File: tests/features/datasets_repo/test_datasets_list.feature
================================================
Feature: Datasets repository list service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_list"

    @testrail-C4523089
    @DAT-46497
    Scenario: List all datasets when no dataset exists
        Given There are no datasets
        When I list all datasets
        Then I receive an empty datasets list

    @testrail-C4523089
    @DAT-46497
    Scenario: List all datasets when datasets exist
        Given There are no datasets
        And I create a dataset by the name of "Dataset" and count
        When I list all datasets
        Then I receive a datasets list of "1" dataset
        And The dataset in the list equals the dataset I created

    @DAT-63175
    Scenario: Test Limit the access to the system datasets
        Given There are no datasets
        Given I create a dataset by the name of "Dataset" and count
        When I list datasets "without" binaries dataset
        Then I receive a datasets list of "1" dataset

    @DAT-63176
    Scenario: Test access to the system datasets
        Given There are no datasets
        Given I create a dataset by the name of "Dataset" and count
        When I list datasets "with" binaries dataset
        Then I receive a datasets list of "2" dataset




================================================
File: tests/features/datasets_repo/test_datasets_lists.feature
================================================
Feature: Datasets repository lists testing

  Background: Initiate Platform Interface and create a project with datasets
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "datasets_list"
    And Create "5" datasets in project with the prefix name "test"

  @DAT-67880
  Scenario:  Get dataset from list by dataset_name
    When I get datasets list by params
      | name=test-2 |
    Then I receive a datasets list of "1" dataset

  @DAT-58070
  Scenario:  Get dataset from list by creator
    When I get datasets list by params
      | creator=current_user |
    Then I receive a datasets list of "5" dataset

  @DAT-67881
  Scenario:  Get dataset from list by filter metadata
    When I get dataset by name "test-1"
    And I update dataset metadata "user.test:'5'"
    When I create dataset filters by metadata - "user.test" = "5"
    And I get datasets list by params
      | filters=context.filters |
    Then I receive a datasets list of "1" dataset

  @DAT-67882
  Scenario:  Get dataset from list by dataset_name , creator and filter
    When I get dataset by name "test-4"
    And I update dataset metadata "user.test:'3'"
    When I create dataset filters by metadata - "user.test" = "3"
    When I get datasets list by params
      | filters=context.filters | name=test-4 | creator=current_user |
    Then I receive a datasets list of "1" dataset



================================================
File: tests/features/datasets_repo/test_datasets_storage.feature
================================================
@rc_only
Feature: Dataset storage testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "datasets_list"

  @testrail-C4523089
  @datasets.delete
  @drivers.delete
  @DAT-54449
  Scenario: List only storage datasets by driver id
    Given I create "s3" integration with name "test-aws-integration"
    And I create a dataset with a random name
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    Given I init Filters() using the given params
      | Parameter | Value   |
      | resource  | DATASET |
    When I call Filters.add() using the given params
      | Parameter | Value     |
      | field     | driver    |
      | values    | driver.id |
      | operator  | IN        |
    And I list datasets using context.filter
    Then I receive a datasets list of "1" dataset


================================================
File: tests/features/datasets_repo/tets_dataset_upload_labels.feature
================================================
Feature: Datasets repository upload labels

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "datasets_upload_labels"

    @testrail-C4530465
    @DAT-46503
    Scenario: Upload labels to dataset from csv
        When I create a dataset with a random name
        And I upload labels from csv file "label_in_csv.csv"
        Then I validate labels in recipe from file "label_in_csv.csv"


================================================
File: tests/features/documentation_tests/test_contributor_docs.feature
================================================
Feature: Contributor Roles SDK

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "contributor-project"

  @testrail-C4523096
  @DAT-46507
  Scenario: Contributor Roles SDK Scenario project owner
    When List Members
    Then Add Members "annotator1@dataloop.ai" as "owner"
    And Update Members "annotator1@dataloop.ai" to "engineer"
    And Remove Members "annotator1@dataloop.ai"

  @testrail-C4523096
  @DAT-46507
  Scenario: Contributor Roles SDK Scenario engineer
    When List Members
    Then Add Members "annotator1@dataloop.ai" as "engineer"
    And Update Members "annotator1@dataloop.ai" to "annotationManager"
    And Remove Members "annotator1@dataloop.ai"


  @testrail-C4523096
  @DAT-46507
  Scenario: Contributor Roles SDK Scenario annotation manager
    When List Members
    Then Add Members "annotator1@dataloop.ai" as "annotationManager"
    And Update Members "annotator1@dataloop.ai" to "annotator"
    And Remove Members "annotator1@dataloop.ai"

  @testrail-C4523096
  @DAT-46507
  Scenario: Contributor Roles SDK Scenario annotator
    When List Members
    Then Add Members "annotator1@dataloop.ai" as "annotator"
    And Update Members "annotator1@dataloop.ai" to "annotationManager"
    And Remove Members "annotator1@dataloop.ai"


================================================
File: tests/features/documentation_tests/test_dataset_docs.feature
================================================
Feature: Dataset SDK

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "my-project-dataset"
        And Create a Dataset "my-dataset-name"


    @testrail-C4523097
    @DAT-46508
    Scenario: Dataset SDK Scenario
        When Get Commands - Get Projects Datasets List
        Then Get Dataset by Name
        And Get a dataset by ID
        And Print a Dataset

    @testrail-C4523097
    @DAT-46508
    Scenario: Create and Manage Datasets
        When Clone Dataset "clone-dataset"
        And I upload a file in path "assets_split/items_upload/0000000162.jpg"
        And I clone an item
        Then Merge Datasets "merge-dataset"


================================================
File: tests/features/documentation_tests/test_projects_docs.feature
================================================
Feature: Projects repository create service testing

    Background: Background name
        Given Platform Interface is initialized as dlp and Environment is set according to git branch

    @testrail-C4523098
    @DAT-46509
    Scenario: Projects Commands
        When Create a Project "my-new-project"
        Then Get my projects
        And Get a project by name "my-new-project"
        And Get a project by project ID
        And Print a Project
        And Delete project by context.project






================================================
File: tests/features/documentation_tests/test_recipe_docs.feature
================================================
Feature: Recipe SDK

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "my-project-recipe"
        And Create a Dataset "my-dataset-name"

    @testrail-C4523099
    @DAT-46510
    Scenario: Recipe SDK Scenario
        When Get Recipe from List
        Then Get Recipe by ID
        And Recipe Clone
        And Delete Recipe
        And Recipe Switch

    @testrail-C4523099
    @DAT-46510
    Scenario: Add Labels by Dataset
        When View Datasets Labels
        Then Add one Label "person"
        And Add Multiple Labels "person", "animal", "object"
        And Add a single label "person" with a specific color (34, 6, 231)
        And Add a single label "person" with a specific color (34, 6, 231) and attributes ["big", "small"]
        And Add a single label "car" with an image "assets_split/items_upload/0000000162.jpg" and attributes ["white","black"]
        And Add Labels using Label object
        And Add a Label with children and attributes
        And Add multiple Labels with children and attributes "My-Recipe-name"


    @testrail-C4523099
    @DAT-46510
    Scenario: Add hierarchy labels with nested - Different options for hierarchy label creation.
        When Option A
        And Option B
        And Option C
        Then Create a Recipe From from a Label list "My-Recipe-name-1"
        And Update Label Features
        And Delete Labels by Dataset


================================================
File: tests/features/dpk_tests/dpk_comuteconfig_levels.feature
================================================
Feature: publish a dpk

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk_computeconfig_levels"
    And I create a dataset by the name of "model" in the project

  @DAT-65220
  Scenario: DPK with computeConfig on function , module - App should deployed with component config from function
    Given I fetch the dpk from 'apps/app_include_models.json' file
    When I add the context.dataset to the dpk model
    And I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I get computeConfig from path "apps/compute_config.json" named "module-level"
    And I add computeConfig to dpk on "modules" component in index "0"
    And I get computeConfig from path "apps/compute_config.json" named "function-level"
    And I add computeConfig to dpk on "functions" component in index "1" on module in index "0"
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    Then Model object with the same name should be exist
    When i "deploy" the model
    Then i have a model service
    Then I compare service config with context.compute_config_item
    And I uninstall the app
    And I delete model

  @DAT-65109
  Scenario: DPK with computeConfig on function , module and model - App should deployed with component config from model
    Given I fetch the dpk from 'apps/app_include_models.json' file
    When I add the context.dataset to the dpk model
    And I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I get computeConfig from path "apps/compute_config.json" named "module-level"
    And I add computeConfig to dpk on "modules" component in index "0"
    And I get computeConfig from path "apps/compute_config.json" named "function-level"
    And I add computeConfig to dpk on "functions" component in index "1" on module in index "0"
    And I get computeConfig from path "apps/compute_config.json" named "deploy-model"
    And I add computeConfig to dpk on "models" component in index "0" with operation "deploy"
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    Then Model object with the same name should be exist
    When i "deploy" the model
    Then i have a model service
    Then I compare service config with context.compute_config_item

================================================
File: tests/features/dpk_tests/dpk_flatten.feature
================================================
@skip_test
Feature: publish a dpk

    Background:
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_app_ins"
        And I fetch the dpk from 'apps/app-flatten.json' file


    Scenario: publishing a dpk
        When I add context to the dpk
        And I publish a dpk to the platform
        And I add pipeline template "pipeline_flow/context.json" to the dpk
        And  I install the app
        Then The pipeline template "context-pipeline-test" should be created


================================================
File: tests/features/dpk_tests/dpk_json_to_object.feature
================================================
Feature: Convert dpk entity to json and vice versa
  # Enter feature description here

    Background:
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_app_ins"

    @testrail-C4524925
    @DAT-46511
    Scenario: Converting valid app.json to entity
        When I fetch the dpk from 'apps/app.json' file
        Then I have a dpk entity
        And I have json object to compare
        And The dpk is filled with the same values

    @testrail-C4524925
    @DAT-46511
    Scenario: Converting a valid dpk entity to json object
        When I fetch the dpk from 'apps/app.json' file
        Then The dpk is filled with the same values


================================================
File: tests/features/dpk_tests/dpk_pipeline_custom_node.feature
================================================
Feature: publish a dpk with pipeline Custom Node

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-compare-model"

  @DAT-56172
  Scenario: Install dpk with specific dtlpy version - Service should be with the same version
    Given I fetch the dpk from 'apps/app_compare_models.json' file
    When I publish a dpk to the platform
    And I install the app
    And I wait "5"
    Given I create pipeline from json in path "pipelines_json/pipeline_compare_models_node.json"
    And I install pipeline in context
    When I wait "5"
    And I get service by name "compare-models-test"
    Then I validate service configuration in dpk is equal to service from app


================================================
File: tests/features/dpk_tests/dpk_pipeline_custom_node_long_name.feature
================================================
Feature: publish a dpk with pipeline Custom Node

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-long-name"

  @DAT-78090
  Scenario: Install dpk with long name - Service name should be valid
    Given I fetch the dpk from 'apps/dpk_pipeline_node_long_name.json' file
    When I publish a dpk to the platform
    And I install the app
    And I wait "5"
    Given I create pipeline from json in path "pipelines_json/pipeline_long_name_custom_name.json"
    And I install pipeline in context
    When I wait "5"
    And I get service by name "imdb-crawler-6b413285-a984-421f-96"
    Then I receive a Service entity

================================================
File: tests/features/dpk_tests/dpk_pipeline_custom_node_scope.feature
================================================
Feature: publish a dpk with pipeline Custom Node

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-scope-node"

  @DAT-62640
  Scenario: Install dpk with with scope nodes install service per node
    Given I fetch the dpk from 'apps/app_node_models.json' file
    When I publish a dpk to the platform
    And I install the app
    And I wait "5"
    Given I create pipeline from json in path "pipelines_json/pipeline_compare_app_node.json"
    And I install pipeline in context
    When I wait "5"
    And I list services in project
    Then I receive a Service list of "3" objects


  @DAT-62875
  @DAT-71931
  Scenario: Install dpk with with scope node - Execution should be success
    Given I fetch the dpk from 'apps/app_scope_node.json' file
    And I create a dataset with a random name
    When I set code path "move_item" to context
    And I pack directory by name "move_item"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app
    And I wait "5"
    Given I create pipeline from json in path "pipelines_json/pipeline_scope_node.json"
    And I install pipeline in context
    When I wait "5"
    And I upload item in "0000000162.jpg" to dataset
    Then I expect that pipeline execution has "1" success executions
    When I get the pipeline service
    And set context.published_dpk to context.dpk
    Then service has app scope

================================================
File: tests/features/dpk_tests/test_dpk_attributes.feature
================================================
Feature: DPK attributes api

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch

  @DAT-64115
  Scenario: Get dpk attributes
    When I send "get" gen_request with "dpk_attributes" params
    Then I validate attributes response in context.req

================================================
File: tests/features/dpk_tests/test_dpk_delete.feature
================================================
Feature: Delete dpk

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    When I create a project by the name of "delete_dpk"

  @DAT-62990
  Scenario: Creator should be able to delete dpk should
    When I fetch the dpk from 'apps/app.json' file
    And I publish a dpk to the platform
    And I delete published_dpk
    And I try get the "published_dpk" by id
    Then "NotFound" exception should be raised


  @DAT-65577
  Scenario: Delete dpk - Should delete all the dpk versions
    When I fetch the dpk from 'apps/app_include_models.json' file
    And I publish a dpk to the platform
    And I increment dpk version
    And I publish a dpk
    When I delete dpk with all revisions
    And I try get the "dpk" by id
    Then "NotFound" exception should be raised
    When I try get the "dpk" by name
    Then "NotFound" exception should be raised

================================================
File: tests/features/dpk_tests/test_dpk_get.feature
================================================
Feature: Get a dpk
    Background:
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        When I create a project by the name of "Project_test_dpk_get"
        And I fetch the dpk from 'apps/app.json' file
        And I publish a dpk to the platform

    @testrail-C4524925
    @DAT-46512
    Scenario: Get a valid dpk
        When I try get the "published_dpk" by id
        Then I have the same dpk as the published dpk

    @testrail-C4524925
    @DAT-46512
    Scenario: Get an invalid dpk
        When I get a dpk with invalid id
        Then I should get an exception

    @testrail-C4524925
    @DAT-46512
    Scenario: Get dpk by name
        When I get the dpk by name
        Then I have the same dpk as the published dpk

================================================
File: tests/features/dpk_tests/test_dpk_include_model.feature
================================================
Feature: publish a dpk

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_app_ins"
    And I create a dataset by the name of "model" in the project


  @DAT-50097
  Scenario: Publishing a dpk with model adapter
    Given I fetch the dpk from 'apps/app_include_models_adapter.json' file
    When I add the context.dataset to the dpk model
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    Then Model object with the same name should be exist
    And model status should be "deployed" with execution "False" that has function "run"
    And Model module_name should be "my-adapter"
    And I uninstall the app

  @DAT-50097
  Scenario: Publishing a dpk with model with subset
    Given I fetch the dpk from 'apps/app_include_filters_models.json' file
    When I add the context.dataset to the dpk model
    And I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    Then Model object with the same name should be exist
    And Model status should be "deployed" with execution "False" that has function "run"
    And I uninstall the app

  @DAT-50162
  Scenario: Publishing a dpk with model - Should be able to evaluate
    Given I fetch the dpk from 'apps/app_include_filters_models.json' file
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item
    When I add the context.dataset to the dpk model
    And I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    Then Model object with the same name should be exist
    And model status should be "deployed" with execution "False" that has function "run"
    When i "evaluate" the model
    Then model status should be "deployed" with execution "True" that has function "evaluate_model"
    And Dataset has a scores file
    When i call the precision recall api
    Then i should get a json response


  @DAT-50097
  Scenario: Publishing a dpk with model and compute
    Given I fetch the dpk from 'apps/app_include_compute_models.json' file
    When I add the context.dataset to the dpk model
    And I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    Then Model object with the same name should be exist
    When i "deploy" the model
    Then i have a model service
    Then I compare service config with dpk compute configuration for the operation "deploy"

================================================
File: tests/features/dpk_tests/test_dpk_include_multiple_models.feature
================================================
Feature: Publish multiple models using dpk

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "multiple_models_dpk"
    And I create a dataset by the name of "model" in the project

  @DAT-52048
  Scenario: Publishing a dpk with multiple models
    Given I fetch the dpk from 'apps/app_include_multiple_models.json' file
    When I add the context.dataset to the dpk model
    And I publish a dpk to the platform
    And  I install the app
    And I add models list to context.models and expect to get "2" models
    Then Model module_name should be "my-adapter-1,my-adapter-2"
    And I uninstall the app


================================================
File: tests/features/dpk_tests/test_dpk_include_trigger_cron.feature
================================================
Feature: publish a dpk with trigger

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-triggers"
    And I create a dataset by the name of "model" in the project


  @DAT-50148
  Scenario: publishing a dpk with cron trigger
    Given I fetch the dpk from 'apps/app_include_cron_trigger.json' file
    When I set code path "packages_get" to context
    And I pack directory by name "packages_get"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app
    And I set the trigger in the context
    Then I receive a CronTrigger entity
    When I list service executions
    Then I wait until I receive a list of "1" executions
    And I uninstall the app


  @DAT-52535
  Scenario: publishing a dpk with cron trigger and input
    Given I fetch the dpk from 'apps/app_include_cron_trigger_with_input.json' file
    When I update dpk dtlpy to current version for service in index 0
    When I set code path "triggers/cron_string" to context
    And I pack directory by name "cron_string"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app
    And I set the trigger in the context
    Then I receive a CronTrigger entity
    And Service was triggered on "string"
    And Execution was executed and finished with status "success"
    And I uninstall the app

================================================
File: tests/features/dpk_tests/test_dpk_include_trigger_event.feature
================================================
Feature: publish a dpk with trigger

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-triggers"
    And I create a dataset by the name of "model" in the project


  @DAT-65557
  Scenario: publishing a dpk with item event trigger
    Given I fetch the dpk from 'apps/app_include_trigger.json' file
    When I set code path "packages_get" to context
    And I pack directory by name "packages_get"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app
    And I set the trigger in the context
    And I upload item in "0000000162.jpg" to dataset
    And I set the execution in the context
    Then I receive a Trigger entity
    And Service was triggered on "item"
    And Execution was executed and finished with status "success"
    When I try to update trigger
      | active=False |
    Then Trigger attributes are modified
      | active=False |
    When I pause service in context
    Then I uninstall the app

================================================
File: tests/features/dpk_tests/test_dpk_include_trigger_event_filter.feature
================================================
Feature: publish a dpk with trigger

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-triggers"
    And I create a dataset by the name of "model" in the project


  @DAT-49643
  Scenario: publishing a dpk with item event trigger and filter
    Given I fetch the dpk from 'apps/app_include_filters_trigger.json' file
    When I set code path "packages_get" to context
    And I pack directory by name "packages_get"
    And I add codebase to dpk
    And I publish a dpk to the platform
    And I install the app
    And I set the trigger in the context
    And I upload item in "0000000162.png" to dataset
    And I set the execution in the context
    Then I receive a Trigger entity
    And Service was triggered on "item"
    And Execution was executed and finished with status "success"
    And I uninstall the app


================================================
File: tests/features/dpk_tests/test_dpk_list.feature
================================================
Feature: List the dpks

    Background:
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_app_get"
        And I fetch the dpk from 'apps/app.json' file
        When I publish a dpk to the platform
        And I publish a dpk to the platform


    @testrail-C4524925
    @DAT-46513
    Scenario: List the dpks
        When I list the dpks
        Then I should see at least 2 dpks



================================================
File: tests/features/dpk_tests/test_dpk_operation.feature
================================================
Feature: DPK with operation field

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk_model_operation"
    And I create a dataset by the name of "model" in the project

  @DAT-53253
  Scenario: Install dpk with 2 models one with operationType "deploy" - Should install only 1 service
    Given I fetch the dpk from 'apps/app_include_operation.json' file
    When I add the context.dataset to the dpk model
    And I publish a dpk to the platform
    And  I install the app
    And I list services in project
    Then I receive a Service list of "1" objects
    And I uninstall the app

  @DAT-57107
  Scenario: Install dpk with operation 'deploy' without compute - Should install only 1 service
    Given I fetch the dpk from 'apps/app_model_operation_type_without_compute.json' file
    When I add the context.dataset to the dpk model
    And I publish a dpk to the platform
    And  I install the app
    And I list services in project
    Then I receive a Service list of "1" objects
    And I uninstall the app

================================================
File: tests/features/dpk_tests/test_dpk_pipeline_template.feature
================================================
@rc_only
Feature: DPK Pipeline template testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_active_learning"
    And I create a dataset named "Upload-data"
    And I Add dataset to context.datasets
    And I create a dataset named "Ground-Truth"
    And I Add dataset to context.datasets

  @pipelines.delete
  @DAT-64404
  Scenario: Create an active learning pipeline template from dpk  - Should install the app
    When I validate global app by the name "Active Learning" is installed
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "ac-lr-package" with entry point "main.py"
    And I create a model from package by the name of "ac-lr-model" with status "trained" in index "0"
    When I get global dpk by name "active-learning"
    Given I fetch the dpk from 'apps/active_learning_template_pipe.json' file
    When I publish a dpk to the platform
    And I install the app






================================================
File: tests/features/dpk_tests/test_dpk_pipetemp_name.feature
================================================
Feature: DPK Pipeline template testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "update_dpk"


  @pipelines.delete
  @DAT-81804
  Scenario: publish two dpks with same pipeline name
    Given I create a dataset named "Upload-data"
    And I Add dataset to context.datasets
    Given I create a dataset named "Upload-data2"
    And I Add dataset to context.datasets
    When I fetch the dpk from 'apps/dpk_with_template_pipe.json' file
    And I publish a dpk to the platform
    When I install the app without custom_installation
    When I fetch the dpk from 'apps/dpk_with_template_pipe_two_datasets.json' file
    And I publish a dpk to the platform
    And I install the app without custom_installation


================================================
File: tests/features/dpk_tests/test_dpk_publish.feature
================================================
Feature: publish a dpk

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_app_ins"


  @testrail-C4524925
  @DAT-46514
  Scenario: publishing a dpk
    Given I fetch the dpk from 'apps/app.json' file
    When I publish a dpk to the platform
    Then The user defined properties should have the same values
    And id, name, createdAt, codebase, url and creator should have values

  @DAT-65868
  Scenario: publishing a dpk with invalid dpk
    Given I fetch the dpk from 'apps/app.json' file
    When I publish without context
    Then "BadRequest" exception should be raised

  @DAT-66447
  Scenario: Publish dpk with scope public - Should get 'invalid rules'
    Given I fetch the dpk from 'apps/dpk_scope_public.json' file
    When I try to publish a dpk to the platform
    Then "Forbidden" exception should be raised

================================================
File: tests/features/dpk_tests/test_dpk_pull.feature
================================================
Feature: Pull Dpk
  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_test_app_get"
    And I fetch the dpk from 'apps/app.json' file
    And I publish a dpk to the platform

  @testrail-C4524925
  @DAT-46515
  Scenario: I pull the dpk
    When I pull the dpk
    Then I should have a dpk file


================================================
File: tests/features/dpk_tests/test_dpk_update.feature
================================================
Feature: DPK Pipeline template testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "update_dpk"


  @pipelines.delete
  @DAT-76220
  Scenario: update dpk attributes
    Given I create a dataset named "Upload-data"
    And I Add dataset to context.datasets
    And I create a dataset named "Ground-Truth"
    And I Add dataset to context.datasets
    When I fetch the dpk from 'apps/dpk_with_template_pipe.json' file
    And I publish a dpk to the platform
    And set context.published_dpk to context.dpk
    And i update the dpk attr "display_name" with value "newDisplayName"
    And i update the dpk attr "icon" with value "newIcon"
    And i update the dpk attr "scope" with value "public"
    And i update the dpk attr "attributes" with value "{'status': 'updated'}"
    And i update the dpk pipe template preview
    And i update the dpk
    Then I should see the dpk updated successfully


  @DAT-76902
  @rc_only
  Scenario: Try to update a public DPK - Should failed for regular users
    When I get global dpk by name "pipeline_template_1"
    And I try to update the dpk
    Then "Permission denied" in error message with status code "403"


================================================
File: tests/features/dpk_tests/test_refs_delete.feature
================================================
Feature: Refs Block delete validation

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "refs_validation"

  @DAT-71436
  Scenario: Pause pipeline with faas , predict and code nodes - Should be able to delete the node services
    When get global model package
    And I create a model with a random name
    Given model is trained
    And a service
    And a dpk with custom node
    And an app
    And pipeline with model, service, code and custom nodes
    When I install pipeline
    And I pause pipeline in context
    And I pause the app
    Then I Should be able to uninstall service
    And I Should be able to delete model
    And I Should be able to uninstall app
    And I Should be able to delete dpk


================================================
File: tests/features/drivers_repo/test_aws_driver.feature
================================================
@rc_only
Feature: Driver repository testing - AWS

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "drivers_aws"
    And I create "s3" integration with name "test-aws-integration"


  @testrail-C4533706
  @datasets.delete
  @drivers.delete
  @DAT-49271
  Scenario: Create AWS Driver - Sync items - Should success
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items

  @testrail-C4533706
  @drivers.delete
  @DAT-49271
  Scenario: Delete AWS Driver without connected dataset
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    And I delete driver by the name "test-aws-driver"
    Then I validate driver "test-aws-driver" not longer in project drivers

  @testrail-C4533706
  @datasets.delete
  @drivers.delete
  @DAT-49271
  Scenario: Delete AWS Driver with connected dataset - Should return error
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    When I delete driver by the name "test-aws-driver"
    Then "BadRequest" exception should be raised


  @testrail-C4533706
  @datasets.delete
  @drivers.delete
  @DAT-49271
  Scenario: Create AWS Driver with path directory
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
      | path        | folder-1                     |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "4" items


  @testrail-C4533706
  @datasets.delete
  @drivers.delete
  @DAT-49271
  Scenario: Create AWS-sts Driver
    Given I create "aws-sts" integration with name "test-aws-sts-integration"
    When I create driver "s3" with the name "test-aws-sts-driver"
      | key         | value                 |
      | bucket_name | qa-sdk-sts-automation |
      | region      | eu-west-1             |
    Then I validate driver with the name "test-aws-sts-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items

  @datasets.delete
  @drivers.delete
  @DAT-85745
  Scenario: Create AWS Driver - Stream and upload item - Should success
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    And I stream Item by path "/img_1.jpg"
    When I upload item in "0000000162.jpg"
    Then I stream Item by path "/0000000162.jpg"
    When I delete the item by name
    Then I wait "12"
    And I validate driver dataset has "9" items

  @datasets.delete
  @drivers.delete
  @DAT-86789
  Scenario: Create AWS Driver - Stream and upload item using Dataloop platform the item should not be corrupted
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    When I upload item in "0000000162.jpg"
    When I create dataset "test-aws-to_delete" with driver entity
    And I sync dataset in context
    Then I use CRC to check original item in "0000000162.jpg" and streamed item from new dataset are not corrupted
    When I delete the item by name
    Then I wait "12"
    And I validate driver dataset has "9" items

  @datasets.delete
  @drivers.delete
  @DAT-87206
  Scenario: Create AWS Driver - and multiple sync it to a dataset
    When I create driver "s3" with the name "test-aws-driver"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-aws-driver" is created
    When I create dataset "test-aws" with driver entity
    And I sync dataset in context with is "False"
    Then I wait "0.5"
    When I sync dataset in context with is "False"
    Then I receive error with status code "423"
    Then "as it is already being synced." in error message
    Then I wait "5"

================================================
File: tests/features/drivers_repo/test_azure_driver.feature
================================================
@rc_only
Feature: Driver repository testing - AZURE

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "drivers_azure"
    And  I create "azureblob" integration with name "test-azure-integration"

  @testrail-C4536789
  @datasets.delete
  @drivers.delete
  @DAT-49272
  Scenario: Create Azure Blob Driver
    When I create driver "azureblob" with the name "test-azure-driver"
      | key         | value          |
      | bucket_name | sdk-automation |
    Then I validate driver with the name "test-azure-driver" is created
    When I create dataset "test-azure" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items

  @testrail-C4533706
  @drivers.delete
  @DAT-49272
  Scenario: Delete Azure Driver without connected dataset
    When I create driver "azureblob" with the name "test-azure-driver"
      | key         | value          |
      | bucket_name | sdk-automation |
    And I delete driver by the name "test-azure-driver"
    Then I validate driver "test-azure-driver" not longer in project drivers


  @testrail-C4533706
  @datasets.delete
  @drivers.delete
  @DAT-49272
  Scenario: Delete Azure Driver with connected dataset - Should return error
    When I create driver "azureblob" with the name "test-azure-driver"
      | key         | value          |
      | bucket_name | sdk-automation |
    Then I validate driver with the name "test-azure-driver" is created
    When I create dataset "test-azure" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    When I delete driver by the name "test-azure-driver"
    Then "BadRequest" exception should be raised


  @testrail-C4536789
  @datasets.delete
  @drivers.delete
  @DAT-49272
  Scenario: Create Azure DatalakeGe2 Driver
    Given I create "azuregen2" integration with name "test-azure-gen2-integration"
    When I create driver "azureDatalakeGen2" with the name "test-azure-gen2-driver"
      | key         | value               |
      | bucket_name | sdk-automation-gen2 |
    Then I validate driver with the name "test-azure-gen2-driver" is created
    When I create dataset "azure-datalake-gen2" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items

  @testrail-C4536789
  @datasets.delete
  @drivers.delete
  @DAT-49272
  Scenario: Create Azure Blob Driver with path directory
    When I create driver "azureblob" with the name "test-azure-driver"
      | key         | value          |
      | bucket_name | sdk-automation |
      | path        | folder-1       |
    Then I validate driver with the name "test-azure-driver" is created
    When I create dataset "test-azure" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "4" items


  @datasets.delete
  @drivers.delete
  @DAT-85746
  Scenario: Create Azure Blob Driver - Stream and upload item - Should success
    When I create driver "azureblob" with the name "test-azure-driver"
      | key         | value          |
      | bucket_name | sdk-automation |
    Then I validate driver with the name "test-azure-driver" is created
    When I create dataset "test-azure" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    And I stream Item by path "/img_1.jpg"
    When I upload item in "0000000162.jpg"
    Then I stream Item by path "/0000000162.jpg"
    When I delete the item by name
    Then I wait "12"
    And I validate driver dataset has "9" items

  @datasets.delete
  @drivers.delete
  @DAT-85747
  Scenario: Create Azure DatalakeGe2 Driver - Stream and upload item - Should success
    Given I create "azuregen2" integration with name "test-azure-gen2-integration"
    When I create driver "azureDatalakeGen2" with the name "test-azure-gen2-driver"
      | key         | value               |
      | bucket_name | sdk-automation-gen2 |
    Then I validate driver with the name "test-azure-gen2-driver" is created
    When I create dataset "azure-datalake-gen2" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    And I stream Item by path "/img_1.jpg"
    When I upload item in "0000000162.jpg"
    Then I stream Item by path "/0000000162.jpg"
    When I delete the item by name
    Then I wait "12"
    And I validate driver dataset has "9" items

================================================
File: tests/features/drivers_repo/test_drivers_providers.feature
================================================
Feature: Drivers provider end point testing

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch

  @DAT-85604
  Scenario: Get drivers by provider
    When I create a project by the name of "to-delete-test-project_create"
    When I get a project by the name of "to-delete-test-project_create"
    When I send "get" gen_request with "drivers_by_provider" params
    Then I validate attributes response in context.req

================================================
File: tests/features/drivers_repo/test_gcs_driver.feature
================================================
@rc_only
Feature: Driver repository testing - GCS

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "drivers_gcs"
    And I create "gcs" integration with name "test-gcs-integration"

  @testrail-C4536790
  @datasets.delete
  @drivers.delete
  @DAT-49273
  Scenario: Create GCS Driver and stream Item - Should success
    When I create driver "gcs" with the name "test-gcs-driver"
      | key         | value          |
      | bucket_name | sdk_automation |
    Then I validate driver with the name "test-gcs-driver" is created
    When I create dataset "test-gcs" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    And I stream Item by path "/img_1.jpg"

  @testrail-C4533706
  @drivers.delete
  @DAT-49273
  Scenario: Delete GCS Driver without connected dataset
    When I create driver "gcs" with the name "test-gcs-driver"
      | key         | value          |
      | bucket_name | sdk_automation |
    And I delete driver by the name "test-gcs-driver"
    Then I validate driver "test-gcs-driver" not longer in project drivers

  @testrail-C4533706
  @datasets.delete
  @drivers.delete
  @DAT-49273
  Scenario: Delete GCS Driver with connected dataset - Should return error
    When I create driver "gcs" with the name "test-gcs-driver"
      | key         | value          |
      | bucket_name | sdk_automation |
    Then I validate driver with the name "test-gcs-driver" is created
    When I create dataset "test-gcs" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    When I delete driver by the name "test-gcs-driver"
    Then "BadRequest" exception should be raised

  @testrail-C4536790
  @datasets.delete
  @drivers.delete
  @DAT-49273
  Scenario: Create GCS Driver with path directory
    When I create driver "gcs" with the name "test-gcs-driver"
      | key         | value          |
      | bucket_name | sdk_automation |
      | path        | folder-1       |
    Then I validate driver with the name "test-gcs-driver" is created
    When I create dataset "test-gcs" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "4" items


  @datasets.delete
  @drivers.delete
  @DAT-51487
  Scenario: GCS Driver Actions - Upload item - stream item - delete item - Should upload and delete the item
    When I create driver "gcs" with the name "test-gcs-driver"
      | key                   | value          |
      | bucket_name           | sdk_automation |
    Then I validate driver with the name "test-gcs-driver" is created
    When I create dataset "test-gcs" with driver entity
    And I sync dataset in context
    Then I validate driver dataset has "9" items
    When I upload item in "0000000162.jpg"
    Then I stream Item by path "/0000000162.jpg"
    When I delete the item by name
    Then I wait "12"
    And I validate driver dataset has "9" items

================================================
File: tests/features/execution_monitoring/execution_monitoring_terminate.feature
================================================
@bot.create
Feature: Execution Monitoring

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_execution_monitoring_terminate"

    @services.delete
    @packages.delete
    @testrail-C4523104
    @DAT-46522
    Scenario: Kill Thread
        When I push and deploy package with param "None" in "execution_monitoring/kill_thread"
        And I execute
        And I terminate execution
        Then Execution was terminated with error message "termination signal"

    @services.delete
    @packages.delete
    @testrail-C4523104
    @DAT-46522
    Scenario: Kill Process
        When I push and deploy package with param "None" in "execution_monitoring/run_as_process"
        And I execute
        And I terminate execution
        Then Execution was terminated with error message "killed"


================================================
File: tests/features/execution_monitoring/execution_monitoring_timeout.feature
================================================
@bot.create
Feature: Execution Monitoring

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_execution_monitoring_timeout"

    @services.delete
    @packages.delete
    @testrail-C4523104
    @DAT-46522
    Scenario: Timeout - failed
        When I push and deploy package with param "failed" in "execution_monitoring/timeout"
        And I execute
        Then Execution "failed" on timeout

    @services.delete
    @packages.delete
    @testrail-C4523104
    @DAT-46522
    Scenario: Timeout - rerun
        When I push and deploy package with param "rerun" in "execution_monitoring/timeout"
        And I execute
        Then Execution "rerun" on timeout


================================================
File: tests/features/executions_repo/test_execution_rerun.feature
================================================
@bot.create
Feature: Executions repository rerun execution

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "execution_rerun"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @DAT-52700
  Scenario: Rerun execution should take te latest package revision
    Given There is a package (pushed from "executions/item") by the name of "execution-rerun"
    And There is a service by the name of "executions-rerun" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed on "item"
    And I validate execution response params
      | key             | value |
      | packageRevision | 1.0.0 |
    When I update package
    And I update service to latest package revision
    And I rerun the execution
    Then I validate execution response params
      | key             | value |
      | packageRevision | 1.0.1 |
    Then Execution was executed on "item"


  @services.delete
  @packages.delete
  @DAT-79211
  Scenario: Rerun execution batch
    Given There is a package (pushed from "executions/item") by the name of "execution-rerun"
    And There is a service by the name of "executions-rerun" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed on "item"
    And I validate execution response params
      | key             | value |
      | packageRevision | 1.0.0 |
    When I update package
    And I update service to latest package revision
    And I rerun the execution with batch function
    Then I validate execution response params
      | key             | value |
      | packageRevision | 1.0.1 |
    Then Execution was executed on "item"






================================================
File: tests/features/executions_repo/test_executions_context.feature
================================================
@bot.create
Feature: Executions repository context testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "execution-context execution-context2"
        And I set Project to Project 1
        And There are no datasets
        And I create a dataset with a random name
        And There is a package (pushed from "executions/item") by the name of "execution-context"
        And I upload item in "0000000162.jpg" to dataset
        And There is a service by the name of "executions-get" with module name "default_module" saved to context "service"
        And I append service to services
        When I create an execution with "inputs"
            |sync=False|inputs=Item|

    @services.delete
    @packages.delete
    @testrail-C4523100
    @DAT-46517
    Scenario: Get Execution from the project it belong to
        When I get the execution from project number 1
        Then Execution Project_id is equal to project 1 id
        And Execution Project.id is equal to project 1 id
        And Execution Service_id is equal to service 1 id
        And Execution Service.id is equal to service 1 id

    @services.delete
    @packages.delete
    @testrail-C4523100
    @DAT-46517
    Scenario: Get Execution from the project it belong to
        When I get the execution from project number 2
        Then Execution Project_id is equal to project 1 id
        And Execution Project.id is equal to project 1 id
        And Execution Service_id is equal to service 1 id
        And Execution Service.id is equal to service 1 id

    @services.delete
    @packages.delete
    @testrail-C4523100
    @DAT-46517
    Scenario: Get Execution from the service it belong to
        When I get the execution from service number 1
        Then Execution Project_id is equal to project 1 id
        And Execution Project.id is equal to project 1 id
        And Execution Service_id is equal to service 1 id
        And Execution Service.id is equal to service 1 id

    @services.delete
    @packages.delete
    @testrail-C4523100
    @DAT-46517
    Scenario: Get Execution from the service it belong to
        Given There is a service by the name of "executions-context2" with module name "default_module" saved to context "service"
        And I append service to services
        When I get the execution from service number 2
        Then Execution Project_id is equal to project 1 id
        And Execution Project.id is equal to project 1 id
        And Execution Service_id is equal to service 1 id
        And Execution Service.id is equal to service 1 id


================================================
File: tests/features/executions_repo/test_executions_create.feature
================================================
@bot.create
Feature: Executions repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "execution_create"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @testrail-C4523101
  @DAT-46518
  Scenario: Created Item Execution - Execution input object - sync
    Given There is a package (pushed from "executions/item") by the name of "execution-create"
    And There is a service by the name of "executions-create" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed on "item"

  @services.delete
  @packages.delete
  @testrail-C4523101
  @DAT-46518
  Scenario: Created Item Execution - Execution input params - sync
    Given There is a package (pushed from "executions/item") by the name of "execution-create"
    And There is a service by the name of "executions-create" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed on "item"

  @services.delete
  @packages.delete
  @testrail-C4523101
  @DAT-46518
  Scenario: Created Item Execution - Execution input params - async
    Given There is a package (pushed from "executions/item") by the name of "execution-create"
    And There is a service by the name of "executions-create" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "params"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed on "item"

  @services.delete
  @packages.delete
  @testrail-C4523101
  @DAT-46518
  Scenario: Created Item Execution for multiple modules and functions - sync
    Given There is a package (pushed from "executions/multiple_modules_functions") by the name of "execution-create"
    And There is a service by the name of "executions-create-first" with module name "first" saved to context "first_service"
    And There is a service by the name of "executions-create-second" with module name "second" saved to context "second_service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution for all functions
    Then Execution was executed on item for all functions

  @services.delete
  @packages.delete
  @testrail-C4523101
  @DAT-46518
  Scenario: Created Item Execution - with sync true
    Given There is a package (pushed from "executions/item") by the name of "execution-create"
    And There is a service by the name of "executions-create" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=True | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed and finished with status "success"

  @services.delete
  @packages.delete
  @DAT-52909
  Scenario: Check Execution status finish - with sync true
    Given There is a package (pushed from "executions/status") by the name of "execution-create"
    And There is a service by the name of "executions-create" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=True | inputs=Item |
    Then I receive an Execution entity
    Then Execution was executed and finished with status "success"

  @services.delete
  @packages.delete
  @DAT-53072
  Scenario: Created Item Execution - with id that ends with e28
    Given A service that receives items input
    When I create an execution with "inputs"
      | sync=True | inputs=e28 |
    Then I receive an Execution entity
    Then Execution input is a valid itemId

  @DAT-85237
  Scenario: Create Execution from app model
    Given I create a dataset with a random name
    When I upload labels to dataset
    Given I upload an item by the name of "test_item.jpg"
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    When I create a model from package by the name of "test-model" with status "trained" in index "0"
    When I add a service to the DPK
    When I publish a dpk to the platform
    When I install the app
    And i fetch the model by the name "test-model"
    When i "deploy" the model
    When I run predict on the item with the model from the app
    When I execute the app service
    Then Both executions should have app and driverId


================================================
File: tests/features/executions_repo/test_executions_get.feature
================================================
@bot.create
Feature: Executions repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "execution_get"
        And I create a dataset with a random name
        And There is a package (pushed from "executions/item") by the name of "execution-get"
        And I upload item in "0000000162.jpg" to dataset
        And There is a service by the name of "executions-get" with module name "default_module" saved to context "service"
        When I create an execution with "inputs"
            |sync=False|inputs=Item|

    @services.delete
    @packages.delete
    @testrail-C4523102
    @DAT-46519
    Scenario: Get by id
        When I get execution by id
        Then I receive an Execution object
        And Execution received equals to execution created



================================================
File: tests/features/executions_repo/test_executions_list.feature
================================================
@qa-nightly
@bot.create
Feature: Executions repository list service testing

    @services.delete
    @packages.delete
    @testrail-C4523103
    @DAT-46520
    Scenario: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "execution_list"
        And I create a dataset with a random name
        And There is a package (pushed from "executions/item") by the name of "execution-list"
        And I upload item in "0000000162.jpg" to dataset
        And There is a service by the name of "executions-list" with module name "default_module" saved to context "service"
        When I list service executions
        Then I receive a list of "0" executions
        When I create an execution with "inputs"
            |sync=False|inputs=Item|
        And I list service executions
        Then I receive a list of "1" executions
        When I create an execution with "inputs"
            |sync=False|inputs=Item|
        And I list service executions
        Then I receive a list of "2" executions


================================================
File: tests/features/executions_repo/text_execution_creator.feature
================================================
@bot.create
Feature: Executions repository execution creator testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "execution_creator"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @testrail-C4533878
  @DAT-46521
  Scenario: Created Item Execution - Execution creator should be the current user
    Given There is a package (pushed from "executions/item") by the name of "execution-create"
    And There is a service by the name of "executions-creator" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    And I validate execution params
      | key     | value        |
      | creator | current_user |

  @services.delete
  @packages.delete
  @testrail-C4533878
  @DAT-46521
  Scenario: Created Item Execution by trigger event - Execution creator should be piper user
    Given There is a package (pushed from "executions/item") by the name of "execution-create"
    And There is a service by the name of "executions-creator" with module name "default_module" saved to context "service"
    When I create a trigger
      | name=triggers-create | filters=None | resource=Item | action=Created | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I upload item in "0000000162.jpg"
    And I get service execution by "item.id"
    Then I validate execution params
      | key     | value |
      | creator | piper |

  @pipelines.delete
  @testrail-C4533878
  @DAT-46521
  Scenario: Pipeline - Created Item Execution - Execution creator should be current user
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
    And I create "dataset" node with params
      | key      | value |
      | position | (2,2) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    And I upload item in "0000000162.jpg"
    And I execute pipeline with input type "Item"
    Then I validate pipeline execution params include node executions "True"
      | key     | value        |
      | creator | current_user |

  @pipelines.delete
  @testrail-C4533878
  @DAT-46521
  Scenario: Pipeline - Created Item Execution trigger event - Execution creator should be current user
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
    And I create "dataset" node with params
      | key      | value |
      | position | (2,2) |
    When I add and connect all nodes in list to pipeline entity
    And I add trigger to first node with params
      | key | value |
    And I install pipeline in context
    And I upload item in "0000000162.jpg"
    Then I wait "5"
    When I get pipeline cycle execution in index "0"
    Then I validate pipeline execution params include node executions "True"
      | key     | value |
      | creator | piper |



================================================
File: tests/features/features_vector_entity/test_features_create.feature
================================================
#Feature: Features vectors repository create service testing
#
#  Background: Initiate Platform Interface and create a project
#    Given Platform Interface is initialized as dlp and Environment is set according to git branch
#    And I create a project by the name of "feature_create"
#    And I create a dataset with a random name
#    And I upload an item by the name of "test_item.jpg"
#
#  @feature.delete
#  @feature_set.delete
#  @testrail-C4523105
#   @DAT-46523
#  Scenario: Create a Feature set
#    When I create a feature sets with a random name
#    When I create a feature
#    Then Feature object should be exist
#    And Feature object should be exist in host
#    Then FeatureSet object should be exist
#    And FeatureSet object should be exist in host
#
#  @feature.delete
#  @feature_set.delete
#  @testrail-C4523105
#   @DAT-46523
#  Scenario: List Feature set
#    Given There are no feature sets
#    When I create a feature sets with a random name
#    And I create a feature
#    Then FeatureSet list have len 1
#    And Feature list have len 1


================================================
File: tests/features/features_vector_entity/test_features_delete.feature
================================================
#Feature: Features vectors repository delete service testing
#
#    Background: Initiate Platform Interface and create a project
#        Given Platform Interface is initialized as dlp and Environment is set according to git branch
#        And I create a project by the name of "features_set_delete"
#        And I create a dataset with a random name
#        And I upload an item by the name of "test_item.jpg"
#
#    @testrail-C4523106
#     @DAT-46524
#    Scenario: Delete features set
#        Given There are no feature sets
#        When I create a feature sets with a random name
#        And I create a feature
#        When I delete the features that was created
#        Then features does not exists
#        When I delete the features set that was created
#        Then features set does not exists



================================================
File: tests/features/features_vector_entity/test_features_get.feature
================================================
#Feature: Features vectors repository get service testing
#
#    Background: Initiate Platform Interface and create a pipeline
#        Given Platform Interface is initialized as dlp and Environment is set according to git branch
#        And I create a project by the name of "feature_get"
#        And I create a dataset with a random name
#        And I upload an item by the name of "test_item.jpg"
#
#    @feature.delete
#    @feature_set.delete
#    @testrail-C4523107
#     @DAT-46525
#    Scenario: To Json
#        When I create a feature sets with a random name
#        When I create a feature
#        Then Object "Feature" to_json() equals to Platform json.
#        Then Object "Feature_Set" to_json() equals to Platform json.
#
#    @feature.delete
#    @feature_set.delete
#    @testrail-C4523107
#     @DAT-46525
#    Scenario: get Feature
#        When I create a feature sets with a random name
#        When I create a feature
#        And I get feature sets
#        And I get feature
#        Then It is equal to feature sets created
#        Then It is equal to feature created



================================================
File: tests/features/filters_entity/test_filters.feature
================================================
Feature: Items repository list service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "items_list"
    And I create a dataset with a random name

  @testrail-C4523108
  @DAT-46526
  Scenario: List with filters
    Given There are items, path = "filters/image.jpg"
      | directory={"/": 3, "/dir1/": 3, "/dir1/dir2/": 3} | annotated_label={"dog": 3, "cat": 3} | annotated_type={"box": 3, "polygon": 3} | name={"dog":3, "cat":3} | metadata={"user.good": 3, "user.bad": 3, "spe-cial.ke_ys": 3} |

        # single filter

    When I create filters
    And I add field "dir" with values "/dir1" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "filename" with values "/dir1/**" and operator "None"
    And I list items with filters
    Then I receive "6" items

    When I create filters
    And I add field "filename" with values "/**" and operator "None"
    And I list items with filters
    Then I receive "36" items

    When I create filters
    And I add field "filename" with values "/dir1/dir2/*" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I list items with filters
    Then I receive "12" items

    When I create filters
    And I add field "annotated" with values "False" and operator "None"
    And I list items with filters
    Then I receive "24" items

    When I create filters
    And I add field "name" with values "*dog*" and operator "None"
    And I list items with filters
    Then I receive "6" items


    When I create filters
    And I add field "name" with values "*cat*" and operator "None"
    And I list items with filters
    Then I receive "6" items


    When I create filters
    And I add field "metadata.user.good" with values "True" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "metadata.user.bad" with values "True" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "metadata.spe-cial.ke_ys" with values "True" and operator "exists"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "metadata.spe-cial.ke_ys" with values "False" and operator "exists"
    And I list items with filters
    Then I receive "33" items

    # single filter with join

    When I create filters
    And I add field "filename" with values "/**" and operator "None"
    And I join field "type" with values "box" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I join field "type" with values "segment" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "filename" with values "/**" and operator "None"
    And I join field "type" with values "box" and operator "ne"
    And I list items with filters
    Then I receive "9" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I join field "type" with values "segment" and operator "ne"
    And I list items with filters
    Then I receive "9" items

    When I create filters
    And I add field "filename" with values "/**" and operator "None"
    And I join field "label" with values "cat" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I join field "label" with values "dog" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "filename" with values "/**" and operator "None"
    And I join field "label" with values "cat" and operator "ne"
    And I list items with filters
    Then I receive "9" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I join field "label" with values "dog" and operator "ne"
    And I list items with filters
    Then I receive "9" items

    # multiple filters

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I add field "name" with values "*label*" and operator "None"
    And I list items with filters
    Then I receive "6" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I add field "name" with values "*type*" and operator "None"
    And I list items with filters
    Then I receive "6" items


    #  multiple filters with join

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"

    And I add field "name" with values "*label*" and operator "None"
    And I join field "label" with values "dog" and operator "ne"
    And I join field "type" with values "point" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I add field "name" with values "*label*" and operator "None"
    And I join field "label" with values "cat" and operator "ne"
    And I join field "type" with values "point" and operator "None"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I join field "type" with values "point" and operator "None"
    And I join field "label" with values "dog" and operator "ne"
    And I list items with filters
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I join field "type" with values "point" and operator "None"
    And I join field "label" with values "cat" and operator "ne"
    And I list items with filters
    Then I receive "3" items

    # update
    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I add field "name" with values "*label*" and operator "None"
    And I join field "label" with values "dog" and operator "ne"
    And I join field "type" with values "point" and operator "None"
    And I list items with filters
    Then I receive "3" items
    When I update items with filters, field "updated"
    And I create filters
    And I add field "metadata.user.updated" with values "True" and operator "None"
    Then I receive "3" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I list items with filters
    Then I receive "12" items
    When I update items with filters, field "annotated"
    And I create filters
    And I add field "metadata.user.annotated" with values "True" and operator "None"
    Then I receive "12" items

    # delete

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I add field "filename" with values "/**" and operator "None"
    And I add field "name" with values "*label*" and operator "None"
    And I join field "label" with values "dog" and operator "ne"
    And I join field "type" with values "point" and operator "None"
    And I list items with filters
    Then I receive "3" items
    When I delete items with filters
    And I list items with filters
    Then I receive "0" items
    When I create filters
    And I list items with filters
    Then I receive "33" items

    When I create filters
    And I add field "annotated" with values "True" and operator "None"
    And I list items with filters
    Then I receive "9" items
    When I delete items with filters
    And I create filters
    And I list items with filters
    Then I receive "24" items


================================================
File: tests/features/filters_entity/test_filters_singlestore.feature
================================================
Feature: Items advanced browser filters

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_list"
        And I create a dataset with a random name
        And Dataset ontology has attributes "attr1" and "attr2"
        Then Add Multiple Labels "dog", "cat", "object"




    @testrail-C4526499
    @DAT-46527
    Scenario: List with filters
        Given There are items, path = "filters/image.jpg"
            |directory={"/": 3, "/dir1/": 3, "/dir1/dir2/": 3}|annotated_label={"dog": 3, "cat": 3}|annotated_type={"box": 3, "polygon": 3}|name={"dog":3, "cat":3}|metadata={"user.good": 3, "user.bad": 3}|

#        Then I add attribute to items with box annotations

        When I create filters
        And I add field "metadata.system.size" with values "51200" and operator "gt"
        And I add field "metadata.system.size" with values "61440" and operator "lt"
        And I list items with filters
        Then I receive "33" items

#         single filter with join

        When I create filters
        And I add field "annotated" with values "True" and operator "None"
        And I add field "filename" with values "/*" and operator "None"
        And I join field "creator" with values "oa-test-1@dataloop.ai,oa-test-3@dataloop.ai,oa-test-4@dataloop.ai" and operator "in"
        And I join field "label" with values "cat" and operator "in"
        And I list items with filters
        Then I receive "3" items

        When I create filters
        And I convert "2" days ago to timestamp
        And I add field "createdAt" with values "timestamp" and operator "gt"
        And I add field "createdAt" with values "timestamp" and operator "lt"
        And I list items with filters
        Then I receive "33" items

#        When I create filters
#        And I join field "metadata.system.attributes" with values "{"1":"attr1"}" and operator "None"
#        And I list items with filters
#        Then I receive "3" items
#
#        When I create filters
#        And I join field "metadata.system.attributes" with values "{}" and operator "None"
#        And I list items with filters
#        Then I receive "9" items




================================================
File: tests/features/filters_entity/test_filters_task_browser.feature
================================================
Feature: Items task browser filters

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_list"
        And I create a dataset with a random name
        And Dataset ontology has attributes "attr1" and "attr2"
        Then Add Multiple Labels "dog", "cat", "object"
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"

    @testrail-C4526503
    @DAT-46528
    Scenario:
        Given There are items, path = "filters/image.jpg"
            |directory={"/": 3, "/dir1/": 3, "/dir1/dir2/": 3}|annotated_label={"dog": 3, "cat": 3}|annotated_type={"box": 3, "polygon": 3}|name={"dog":3, "cat":3}|metadata={"user.good": 3, "user.bad": 3}|

        # All tasks filter

        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        Then I receive a task entity
        When I create filters
        And I add field "metadata.system.refs.type" with values "task" and operator "eq"
        And I list items with filters
        Then I receive "33" items

        # No task assignment

        When I create filters
        And I add field "metadata.system.refs.type" with values "task" and operator "ne"
        And I list items with filters
        Then I receive "0" items

        # Specific task

        When I create filters
        And I add field "metadata.system.refs.id" with values "task.id" and operator "in"
        And I list items with filters
        Then I receive "33" items


        # Specific task with status from today

        When I create filters
        And I convert "2" days ago to timestamp
        And I update items status to default task actions
        And I use custom filter for Specific task and status from today
        And I list items with filters
        Then I receive "12" items

        # Specific task and annotation-type (join)

        When I create filters
        And I add field "metadata.system.refs.type" with values "task" and operator "eq"
        And I join field "label" with values "cat" and operator "ne"
        And I list items with filters
        Then I receive "9" items


================================================
File: tests/features/integrations_repo/test_ecr_intergration.feature
================================================
Feature: Integrations repository create testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "sdk_cross_integrations"
    And I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"

  @DAT-83866
  Scenario: ECR integration Flow
      Given There are no private registry integrations in the organization
      Given A deployed service with custom docker image from "ECR" private registry
      And I execute the service
      Then I should get an ImagePullBackOff error
      When I create an ECR integration
      And I pause and resume the service
      Then The execution should complete successfully
      When I delete the context integration
      And I pause and resume the service
      Given I execute the service
      Then I should get an ImagePullBackOff error


  @DAT-85247
  Scenario: GAR integration Flow
      Given There are no private registry integrations in the organization
      Given A deployed service with custom docker image from "GAR" private registry
      And I execute the service
      Then I should get an ImagePullBackOff error
      When I create an GAR integration
      And I pause and resume the service
      Then The execution should complete successfully
      When I delete the context integration
      And I pause and resume the service
      Given I execute the service
      Then I should get an ImagePullBackOff error


================================================
File: tests/features/integrations_repo/test_gcp_cross_project_intergration.feature
================================================
@rc_only
Feature: Integrations repository create testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "sdk_cross_integrations"

  @DAT-52122
  Scenario: Create GCP-cross-project integration and validate service account created
    Given I create "gcp-cross" integration with name "test-gcp-cross-project-integration"
    Then I validate integration with the name "test-gcp-cross-project-integration" is created
    And I validate gcp-cross-project has correct service account pattern

================================================
File: tests/features/integrations_repo/test_integration_create.feature
================================================
@rc_only
Feature: Integrations repository create testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "sdk_integrations"

  @testrail-C4533705
  @DAT-46529
  Scenario: Create AWS integration
    Given I create "s3" integration with name "test-aws-integration"
    Then I validate integration with the name "test-aws-integration" is created

  @testrail-C4533705
  @DAT-46529
  Scenario: Create GCS integration
    Given I create "gcs" integration with name "test-gcs-integration"
    Then I validate integration with the name "test-gcs-integration" is created

  @testrail-C4533705
  @DAT-46529
  Scenario: Create Azure-blob integration
    Given I create "azureblob" integration with name "test-azure-integration"
    Then I validate integration with the name "test-azure-integration" is created

  @testrail-C4533705
  @DAT-46529
  Scenario: Create Azure-gen2 integration
    Given I create "azuregen2" integration with name "test-azure-gen2-integration"
    Then I validate integration with the name "test-azure-gen2-integration" is created


  @testrail-C4533705
  @DAT-46529
  Scenario: Create AWS-sts integration
    Given I create "aws-sts" integration with name "test-aws-sts-integration"
    Then I validate integration with the name "test-aws-sts-integration" is created

================================================
File: tests/features/item_collections/item_collections.feature
================================================
Feature: Item Collections

  Background: Initialize Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_test_collections"
    And I create a dataset with a random name

  @DAT-85364
  Scenario: Create a new collection
    When I create a collection with the name "Justice League"
    Then The collection "Justice League" is created successfully

  @DAT-85365
  Scenario: Create a collection with a duplicate name
    Given I have an existing collection named "Justice League"
    When I try to create a collection with the name "Justice League"
    Then I receive an error stating that the collection name already exists

  @DAT-85366
  Scenario: Update an existing collection's name
    Given I have an existing collection named "Justice League"
    When I update the collection name to "Avengers"
    Then The collection name is updated to "Avengers"

  @DAT-85367
  Scenario: Delete an existing collection
    Given I have an existing collection named "Justice League"
    When I delete the collection "Justice League"
    Then The collection "Justice League" is deleted successfully

  @DAT-85368
  Scenario: Clone an existing collection
    Given I have an existing collection named "Justice League"
    When I clone the collection "Justice League"
    Then A new collection with the name "Justice League-clone-1" is created

  @DAT-85369
  Scenario: List all collections in the dataset
    Given I have multiple collections in the dataset
    When I list all collections
    Then I receive a list of all collections with their names and keys

  @DAT-85370
  Scenario: Assign an item to a collection
    Given I have an item in the dataset 
    When I assign the item to the collection "Justice League"
    Then The item is assigned to the collection "Justice League"

  @DAT-85735
  Scenario: Cloning a collection - the new collection should have the same items as the cloned collection
    When I create a collection with the name "Cloning Collection"
    Given I have an item in the dataset
    When I assign the item to the collection "Cloning Collection"
    When I clone the collection "Cloning Collection"
    Then The item is assigned to the collection "Cloning Collection-clone-1"

================================================
File: tests/features/item_entity/test_item_creator.feature
================================================
Feature: Item Creator testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "item_creator_project"

  @datasets.delete
  @drivers.delete
  @DAT-85611
  Scenario: Import an item from AWS and validate it has a creator
    Given I create "s3" integration with name "s3-test-qa"
    When I create driver "s3" with the name "test-item-creator-aws"
      | key         | value                        |
      | bucket_name | qa-sdk-automation-access-key |
      | region      | eu-west-1                    |
    Then I validate driver with the name "test-item-creator-aws" is created
    When I create dataset "test-aws" with driver entity
    When I import item "img_1.jpg" from "root"


  @datasets.delete
  @drivers.delete
  @DAT-85610
  Scenario: Import an item from GCS and validate it has a creator
    Given I create "gcs" integration with name "gcs-test-qa"
    When I create driver "gcs" with the name "test-item-creator-gcs"
      | key         | value          |
      | bucket_name | sdk_automation |
      | region      | eu-west-1      |
    Then I validate driver with the name "test-item-creator-gcs" is created
    When I create dataset "test-gcs" with driver entity
    When I import item "img_9.jpg" from "folder-1"


  @datasets.delete
  @drivers.delete
  @DAT-85822
  Scenario: Import an item from AZURE and validate it has a creator
    Given I create "azureblob" integration with name "azure-test-qa"
    When I create driver "azureblob" with the name "test-azure-driver"
      | key         | value          |
      | bucket_name | sdk-automation |
    Then I validate driver with the name "test-azure-driver" is created
    When I create dataset "test-azure" with driver entity
    When I import item "img_1.jpg" from "root"


================================================
File: tests/features/item_entity/test_item_description.feature
================================================
Feature: Item description testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project1"
        And I create a dataset with a random name
        And Item in path "0000000162.jpg" is uploaded to "Dataset"


    @testrail-C4532161
    @DAT-46544
    Scenario: Add description to item
        When  I Add description "Item description" to item
        Then  I validate item.description has "Item description" value

    @testrail-C4532161
    @DAT-46544
    Scenario: Remove field from root
        When  I Add description "Item description" to item
        Then  I validate item.description has "Item description" value
        Then  I remove description from the root
        Then  I validate item.description has "Item description" value
        And Return from and to Json functions to the original implementation

    @testrail-C4532161
    @DAT-46544
    Scenario: add field to the root
        When I add new field to the root
        And  new field do not added
        Then Return from and to Json functions to the original implementation

    @testrail-C4532161
    @DAT-46544
    Scenario: Delete root filed will set it to None
        When  I Add description "Item description" to item
        Then  I validate item.description has "Item description" value
        And   I remove item.description
        Then  I validate item.description is None

    @testrail-C4532161
    @DAT-46544
    Scenario: Removing from no-root
        When I update item system metadata with system_metadata="True"
        Then Then I receive an Item object
        And Item in host has modified metadata
        And I update the metadata
        And Item was modified metadata
        And I remove the added metadata
        And metadata was deleted

    @testrail-C4532161
    @DAT-46544
    Scenario: Set description - all options
        When I Add description "Item description1" to item with "item.description"
        Then I validate item.description has "Item description1" value
        When I Add description "Item description2" to item with "item.set_description"
        Then I validate item.description has "Item description2" value
        When I delete all the dataset items
        And  I Add description "Item description3" to item with "dataset.items.upload"
        Then I validate item.description has "Item description3" value
        When I Add description "Item description4" to item with "dataset.items.upload - overwrite=False"
        Then I validate item.description has "Item description3" value
        When I Add description "Item description5" to item with "dataset.items.upload - overwrite=True"
        Then I validate item.description has "Item description5" value

    @testrail-C4532720
    @DAT-46544
    Scenario: Upload annotation with description
        Given Labels in file: "assets_split/annotations_crud/labels.json" are uploaded to test Dataset
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        When I upload an annotation with description
        Then Annotation has description

    @DAT-76218
    Scenario: Add emoji to item description
        When I Add description "Test😀😂🥳" to item
        Then  I validate item.description has "Test\u1f600\u1f602\u1f973" value

================================================
File: tests/features/item_entity/test_item_move.feature
================================================
Feature: Item Move function testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project1"
        And I create a dataset with a random name
        And Item in path "0000000162.jpg" is uploaded to "Dataset"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new directory with / in the end without dot
        When  I move the item to "/new_dir/"
        Then  I insure that new full name is "/new_dir/0000000162.jpg"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new directory with / in the end with dot
        When  I move the item to "/new_dir.1/"
        Then  I insure that new full name is "/new_dir.1/0000000162.jpg"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new directory with / in the end t an existing directory
        Given I have directory with the name "/new_dir"
        When  I move the item to "/new_dir/"
        Then  I insure that new full name is "/new_dir/0000000162.jpg"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new directory without / in the end and with dot in the path
        Given I have directory with the name "/new_dir.1"
        When  I move the item to "/new_dir.1"
        Then  I insure that new full name is "/new_dir.1/0000000162.jpg"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new directory without / in the end  and without dot in the path
        Given I have directory with the name "/new_dir"
        When  I move the item to "/new_dir"
        Then  I insure that new full name is "/new_dir/0000000162.jpg"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new non existing directory  without dot
        When  I move the item to "/new_dir"
        Then  I insure that new full name is "/new_dir"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new  non existing directory  with dot
        When  I move the item to "/pic.jpg"
        Then  I insure that new full name is "/pic.jpg"


    @testrail-C4523122
    @DAT-46545
    Scenario: Move file to new directory without adding / on the beginning
        Given I have directory with the name "/new_dir"
        When  I move the item to "new_dir"
        Then  I insure that new full name is "/new_dir/0000000162.jpg"



================================================
File: tests/features/item_entity/test_item_repo_methods.feature
================================================
Feature: Item Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "item_repo_methods"
        And I create a dataset with a random name

    @testrail-C4523123
    @DAT-46546
    Scenario: Download item
        Given There are no items
        And I upload an item by the name of "test_item.jpg"
        And Folder "test_download_item_repo_methods" is empty
        When I download an item entity by the name of "/test_item.jpg" to "test_download_item_repo_methods"
        Then There are "1" files in "test_download_item_repo_methods"
        And Item is correctly downloaded to "test_download_item_repo_methods/test_item.jpg" (compared with "0000000162.jpg")

    @testrail-C4523123
    @DAT-46546
    Scenario: Delete item
        Given There are no items
        And I upload an item by the name of "test_item.jpg"
        When I delete the item
        Then There are no items

    @testrail-C4523123
    @DAT-46546
    Scenario: Update items name
        Given There is an item
        When I update item entity name to "/test_name.jpg"
        Then I receive an Item object with name "/test_name.jpg"
        And Item in host was changed to "/test_name.jpg"
        And Only name attributes was changed

    @testrail-C4523123
    @DAT-46546
    Scenario: To Json - not annotated item
        Given I upload item in path "assets_split/ann_json_to_object/0000000162.jpg" to dataset
        Then Object "Item" to_json() equals to Platform json.

    @testrail-C4523123
    @DAT-46546
    Scenario: To Json - annotated video
        Given I upload item in path "assets_split/ann_json_to_object/sample_video.mp4" to dataset
        When Item is annotated with annotations in file: "assets_split/ann_json_to_object/video_annotations.json"
        Then Item is annotated
        Then Object "Item" to_json() equals to Platform json.




================================================
File: tests/features/item_entity/test_item_src.feature
================================================
Feature: Get Item _src_item

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "item_src"
    And I create a dataset with a random name
    Given There are "3" items
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @DAT-54344
  Scenario: Get src_item from consensus item
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto | consensus_percentage=auto | consensus_assignees=auto | scoring=False |
    And I get a consensus item
    Then I validate _src_item is not None


  @DAT-54344
  Scenario: Get src_item from cloned item has correct src item
    Given Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I clone an item
    Then I validate cloned item has the correct src item


  @DAT-76433
  Scenario: clone failed etl item run image pre
    Given Item in path "faledetl.jpg" is uploaded to "Dataset"
    When I clone an item
    Then I validate image pre run on cloned item

================================================
File: tests/features/item_entity/test_item_thumbnail.feature
================================================
Feature: Item thumbnail feature testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "item-thumbnail"
    And I create a dataset with a random name
    And Item in path "0000000162.jpg" is uploaded to "Dataset"


  @DAT-56167
  Scenario: Update item thumbnail to null > open item thumbnail - Should get new thumbnail
    When I get an item thumbnail response
    And I get item thumbnail id
    And I update item "metadata.system.thumbnailId" with "None" system_metadata "True"
    Then I get an item thumbnail response
    And I validate item thumbnail id is "not-equal" to the previous thumbnail id



================================================
File: tests/features/item_entity/test_item_update_status.feature
================================================
Feature: Item Entity update item status

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "item_update_status"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @testrail-C4525318
  @DAT-46547
  Scenario: Update default status on items in task
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto |
    Then I receive a task entity
    When I update items status to default task actions
    Then I validate default items status in task

  @testrail-C4525318
  @DAT-46547
  Scenario: Update default status on items in second task with task id
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto |
    Then I receive a task entity
    When I update items status to default task actions with task id
    Then I validate default items status in task


  @testrail-C4525318
  @DAT-46547
  Scenario: Update Custom status on items in task
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto | available_actions=fix-label fix-ann fix-cor |
    Then I receive a task entity
    When I update items status to custom task actions "fix-label" "fix-ann" "fix-cor"
    Then I validate items status in task
#
#
#  @testrail-C4523167
   @DAT-46547
#  Scenario: Update default status on items in qa task
#    When I create Task
#      | task_name=min_params | due_date=auto | assignee_ids=auto |
#    Then I receive a task entity
#    When I update items status to default task actions
#    When I update items status to default qa task actions
#    Then I validate default items status in qa task
#
#
#  @testrail-C4523167
   @DAT-46547
#  Scenario: Update custom status on items in qa task
#    When I create Task
#      | task_name=min_params | due_date=auto | assignee_ids=auto |
#    Then I receive a task entity
#    When I update items status to default task actions
#    When I update items status to default qa task actions
#    Then I validate custom items status in qa task






================================================
File: tests/features/items_repo/test_download_and_upload_ndarray_item.feature
================================================
Feature: Upload and Download Numpy.Ndarray Item

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_test_annotation_add"
    And I create a dataset with a random name
    And I convert to Numpy.NdArray an item with the name "0000000162.jpg" and add it to context.array
    And I save item_metadata context.item_metadata
    And I remove log files

  @testrail-C4523109
  @DAT-46530
  Scenario: Upload jpg Numpy.NdArray item
    Given There are no items
    When I Upload an Numpy.NdArray (context.array) item with the name "0000000162_from_ndarray.jpg"
    Then Item is correctly uploaded to platform
    And  Log file does not exist

  @testrail-C4523109
  @DAT-46530
  Scenario: Upload png Numpy.NdArray item
    Given There are no items
    When I Upload an Numpy.NdArray (context.array) item with the name "0000000162_from_ndarray.png"
    Then Item is correctly uploaded to platform

  @testrail-C4523109
  @DAT-46530
  Scenario: Upload illegal Numpy.NdArray item
    Given There are no items
    When I Upload an Numpy.NdArray (context.array) item with the name "0000000162_from_ndarray.abc"
    Then There are no items

  @testrail-C4523109
  @DAT-46530
  Scenario: Download Image as Numpy.NdArray
    Given There are no items
    When I Upload an Numpy.NdArray (context.array) item with the name "0000000162_from_ndarray.jpg"
    And  I Download as Numpy.NdArray the uploaded item
    Then Download Numpy.NdArray item and context.array size equal

  @testrail-C4523109
  @DAT-46530
  Scenario: Download some images as Numpy.NdArray
    Given There are no items
    When I Upload an Numpy.NdArray (context.array) item with the name "0000000162_from_ndarray.jpg"
    And  I Download as Numpy.NdArray the uploaded item
    Then Download Numpy.NdArray item and context.array size equal

  @testrail-C4523109
  @DAT-46530
  Scenario: Try to Download video as Numpy.NdArray
    Given There is one .mp4 item
    When  Download as Numpy.NdArray the .mp4
    Then  Log file is exist with resource "item.id"



================================================
File: tests/features/items_repo/test_items_clone.feature
================================================
@qa-nightly
Feature: Items repository clone service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "items_get"
    And I create a dataset with a random name

  @DAT-83851
  Scenario: Clone item without fps metadata
    Given There is an item "without" "fps" in its metadata system
    When I clone the item
    Then The cloned item should trigger video preprocess function
    And The cloned item should have "fps" in its metadata

  @DAT-83851
  Scenario: Clone item with fps metadata
    Given There is an item "with" "fps" in its metadata system
    When I clone the item
    Then The cloned item should have "fps" in its metadata


================================================
File: tests/features/items_repo/test_items_context.feature
================================================
Feature: Items repository Context testing

    Background: Initiate Platform Interface and create a projects, datasets and Item
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "project1 project2"
        And I create datasets by the name of "dataset1 dataset2"
        And I set Dataset to Dataset 1
        And Item in path "0000000162.jpg" is uploaded to "Dataset"
#        And I append Item to Items
#        And I set Dataset to Dataset 2
#        And Item in path "0000000162.jpg" is uploaded to "Dataset"
#        And I set item to Item 2

    @testrail-C4523110
    @DAT-46531
    Scenario: Get Item from the Project it belong to
        When I get the item from project number 1
        Then item dataset_id is equal to dataset 1 id
        And item dataset.id is equal to dataset 1 id
#        after DAT-5663  will be ended
#        And item ProjectId is equal to project 1 id
        And item Project.id is equal to project 1 id

#   after DAT-5663  will be ended
#   Scenario: Get Item from the Project it not belong to
#        When I get the item from project number 2
#        Then item dataset_id is equal to dataset 1 id
#        And item dataset.id is equal to dataset 1 id
#        And item ProjectId is equal to project 1 id
#        And item Project.id is equal to project 1 id

    @testrail-C4523110
    @DAT-46531
    Scenario: Get Item from the Dataset it belong to
        When I get the item from dataset number 1
        Then item dataset_id is equal to dataset 1 id
        And item dataset.id is equal to dataset 1 id
#        after DAT-5663  will be ended
#        And item ProjectId is equal to project 1 id
        And item Project.id is equal to project 1 id

    @testrail-C4523110
    @DAT-46531
    Scenario: Get Item from the Dataset it not belong to
        When I get the item from dataset number 2
        Then item dataset_id is equal to dataset 1 id
        And item dataset.id is equal to dataset 1 id
#        after DAT-5663  will be ended
#        And item ProjectId is equal to project 1 id
#        And item Project.id is equal to project 1 id


================================================
File: tests/features/items_repo/test_items_delete.feature
================================================
@qa-nightly
Feature: Items repository delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_delete"
        And I create a dataset with a random name

    @testrail-C4523111
    @DAT-46532
    Scenario: Delete item by name
        Given There are no items
        And I upload an item by the name of "test_item.jpg"
        When I delete the item by name
        Then There are no items

    @testrail-C4523111
    @DAT-46532
    Scenario: Delete item by id
        Given There are no items
        And I upload an item by the name of "test_item.jpg"
        When I delete the item by id
        Then There are no items

    @testrail-C4523111
    @DAT-46532
    Scenario: Delete a non-existing item by name
        Given There are no items
        And I upload an item by the name of "test_item.jpg"
        When I try to delete an item by the name of "Some_item_name"
        Then "NotFound" exception should be raised
        And No item was deleted

    @testrail-C4523111
    @DAT-46532
    Scenario: Delete a non-existing item by id
        Given There are no items
        And I upload an item by the name of "test_item.jpg"
        When I try to delete an item by the id of "Some_id"
        Then "BadRequest" exception should be raised
        And No item was deleted




================================================
File: tests/features/items_repo/test_items_download.feature
================================================
Feature: Items repository download service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "items_download"
    And I create a dataset with a random name

    # Scenario: Download item by name - save locally
    #     Given There are no items
    #     And I upload an item by the name of "test_item.jpg"
    #     And Folder "test_items_download" is empty
    #     When I download an item by the name of "/test_item.jpg" to "test_items_download"
    #     Then There are "1" files in "test_items_download"
    #     And Item is correctly downloaded to "test_items_download/items/test_item.jpg" (compared with "0000000162.jpg")

  @testrail-C4533354
  @DAT-46533
  Scenario: Download item by id - save locally
    Given There are no items
    And I upload an item by the name of "test_item.jpg"
    And Folder "test_items_download" is empty
    When I download an item by the id to "test_items_download"
    Then There are "1" files in "test_items_download"

  @testrail-C4523112
  @DAT-46533
  Scenario: Download item by id - do not save locally
    Given There are no items
    And I upload an item by the name of "test_item.jpg"
    When I download without saving an item by the id of "/test_item.jpg"
    Then I receive item data
    When I upload item data by name of "test_item2.jpg"
    Then Item uploaded from data equals initial item uploaded

  @testrail-C4533353
  @DAT-46533
  Scenario: Download folder item
    Given There are no items
    And I upload item by the name of "test_item.jpg" to a remote path "test"
    And Folder "test_items_download" is empty
    When I download an item by the name of "/test" to "test_items_download"
    Then There are "1" files in "test_items_download"
    
  @testrail-C4533355
  @DAT-46533
  Scenario: Download item by id - do not save locally and create dir
      Given There are no items
      And I upload an item by the name of "test_item.jpg"
      When I download the item without saving and create folder
      Then file do not created



================================================
File: tests/features/items_repo/test_items_download_anotations.feature
================================================
Feature: Items repository download annotations testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "Project_item_annotations_download"
    And I create a dataset with a random name
    When I add "free_text" attribute to ontology
      | key=1 | title=attr1 | scope=all |

  @testrail-C4523113
  @DAT-46534
  Scenario: Download item annotations with mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations from item with "mask" to "downloaded_annotations/mask.jpg"
    Then Item annotation "mask" has been downloaded to "downloaded_annotations/mask"

  @testrail-C4523113
  @DAT-46534
  Scenario: Download item annotations with instance
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations from item with "instance" to "downloaded_annotations/instance.jpg"
    Then Item annotation "instance" has been downloaded to "downloaded_annotations/instance"

  @testrail-C4523113
  @DAT-46534
  Scenario: Download item annotations with json
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations from item with "json" to "downloaded_annotations/json.jpg"
    Then Item annotation "json" has been downloaded to "downloaded_annotations/json"

  @testrail-C4523113
  @DAT-46534
  Scenario: Download item annotations with img_mask
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations from item with "img_mask" to "downloaded_annotations/img_mask.jpg"
    Then Item annotation "img_mask" has been downloaded to "downloaded_annotations/img_mask"

  @testrail-C4523113
  @DAT-46534
  Scenario: Download item annotations with vtt
    Given Labels in file: "assets_split/annotations_download/labels.json" are uploaded to test Dataset
    And Item in path "assets_split/annotations_download/0000000162.jpg" is uploaded to "Dataset"
    And There are no files in folder "downloaded_annotations"
    And Item is annotated with annotations in file: "assets_split/annotations_download/0162_annotations.json"
    When I download items annotations from item with "vtt" to "downloaded_annotations/vtt.jpg"
    Then Item annotation "vtt" has been downloaded to "downloaded_annotations/vtt"



================================================
File: tests/features/items_repo/test_items_download_batch.feature
================================================
Feature: Items repository download_batch service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_download_batch"
        And I create a dataset with a random name
        And There are "2" Items

    @testrail-C4523114
    @DAT-46535
    Scenario: Download batch items to buffer
        When I download batched items to buffer
        And I delete all items from host
        Then I can upload items from buffer to host

    @testrail-C4523114
    @DAT-46535
    Scenario: Download items to local
        When I download items to local path "download_batch"
        Then Items are saved in "download_batch/image"




================================================
File: tests/features/items_repo/test_items_edit.feature
================================================
Feature: Items repository update service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "items_edit"
    And I create a dataset with a random name
    And There is an item

  @testrail-C4523115
  @DAT-46536
  Scenario: Update items name
    When I update items name to "/test_name.jpg"
    Then I receive an Item object with name "/test_name.jpg"
    And Item in host was changed to "/test_name.jpg"
    And Only name attributes was changed

  @testrail-C4523115
  @DAT-46536
  Scenario: Update items path
    Given And There is an item by the name of "/test_item.jpg"
    When I update items name to "/folder/test_item.jpg"
    Then I receive an Item object with name "/folder/test_item.jpg"
    And Item in host was changed to "/folder/test_item.jpg"
    And PageEntity has directory item "/folder"

  @testrail-C4523115
  @DAT-46536
  Scenario: Update item system metadata - with param system_metadata=True
    When I update item system metadata with system_metadata="True"
    Then Then I receive an Item object
    And Item in host has modified metadata
    And Only metadata was changed

  @testrail-C4523115
  @DAT-46536
  Scenario: Update item system metadata - with param system_metadata=False
    When I update item system metadata with system_metadata="False"
    Then Then I receive an Item object
    And Item in host was not changed


  @DAT-80521
  Scenario: Update item using update_values without filters - Should return error
    When I try to update item with params
      | key  | value |
      | item | None  |
    Then I receive error with status code "400"
    And "must provide either item or filters" in error message

  @DAT-80521
  Scenario: Update item using system_update_values without filters - Should return error
    When I try to update item with params
      | key           | value     |
      | update_values | {"a":"b"} |
    Then I receive error with status code "400"
    And "Cannot provide "update_values" or "system_update_values" with a specific "item" for an individual update." in error message

  @DAT-80700
  Scenario: Update item with system_update_values without system_metadata true - Should not update system
    When I create filters
    And I add field "annotated" with values "False" and operator "None"
    When I try to update item with params
      | key                  | value           |
      | item                 | None            |
      | filters              | context.filters |
      | system_update_values | {"c":"d"}       |
    Then I validate "{"c":"d"}" not in item system metadata

  @DAT-80701
  Scenario: Update item with update_values and system_update_values and system_metadata true - Should update item
    When I create filters
    And I add field "annotated" with values "False" and operator "None"
    When I try to update item with params
      | key                  | value           |
      | item                 | None            |
      | filters              | context.filters |
      | update_values        | {"a":"b"}       |
      | system_update_values | {"c":"d"}       |
      | system_metadata      | True            |
    Then I validate for "item" that the updated metadata is "user.a:b"
    Then I validate for "item" that the updated metadata is "system.c:d"



================================================
File: tests/features/items_repo/test_items_get.feature
================================================
@qa-nightly
Feature: Items repository get service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "items_get"
    And I create a dataset with a random name

  @testrail-C4523116
  @DAT-46537
  Scenario: Get an existing item by id
    Given There is an item
    When I get the item by id
    Then I receive an Item object
    And The item I received equals the item I uploaded

  @testrail-C4523116
  @DAT-46537
  Scenario: Get an non-existing item by id
    When I try to get item by "some_id"
    Then "NotFound" exception should be raised

  @testrail-C4523116
  @DAT-46537
  Scenario: Get an existing item by remote path
    Given There is an item
    When I get the item by remote path "/0000000162.jpg"
    Then I receive an Item object
    And The item I received equals the item I uploaded

  @testrail-C4523116
  @DAT-46537
  Scenario: Get a non-existing item by remote path
    When I try to get an item by remote path "/some_path"
    Then "NotFound" exception should be raised

  @testrail-C4523116
  @DAT-46537
  Scenario: Use get service with neither filename or remote path
    When I try to use get services with no params
    Then "BadRequest" exception should be raised

  @testrail-C4523116
  @DAT-46537
  Scenario: Get an existing item by filename when 2 files with the same name exist
    Given There are 2 items by the name of "0000000162.jpg"
    When I try to get an item by remote path "**/0000000162.jpg"
    Then "NotFound" exception should be raised

  @testrail-C4523116
  @DAT-46537
  Scenario: Get dataset items by dataset Id
    Given There are 2 items by the name of "0000000162.jpg"
    Then I get items by dataset Id


  @DAT-86214
  Scenario: Get an existing item by remote path with no type
    When I upload item in "notype"
    When I get the item by remote path "/notype"
    Then I receive an Item object
    And The item I received equals the item I uploaded

================================================
File: tests/features/items_repo/test_items_list.feature
================================================
@qa-nightly
Feature: Items repository list service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_list"
        And I create a dataset with a random name

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items
        Given There is an item
        When I list items
        Then I receive a PageEntity object
        And PageEntity items has length of "1"
        And Item in PageEntity items equals item uploaded

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items - with size
        Given There are "10" items
        When I list items with size of "5"
        Then I receive a PageEntity object
        And PageEntity items has length of "5"
        And PageEntity items has next page
        And PageEntity next page items has length of "5"
        And PageEntity items does not have next page

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items - with offset
        Given There are "10" items
        When I list items with offset of "1" and size of "5"
        Then I receive a PageEntity object
        And PageEntity items has length of "5"
        And PageEntity items does not have next page

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items - with query - filename
        Given There are "10" items
        And There is one item by the name of "test_name.jpg"
        When I list items with query filename="/test_name.jpg"
        Then I receive a PageEntity object
        And PageEntity items has length of "1"
        And PageEntity item received equal to item uploaded with name "test_name"

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items - with query - filepath
        Given There are "5" items
        And There are "5" items in remote path "/folder"
        When I list items with query filename="/folder/*"
        Then I receive a PageEntity object
        And PageEntity items has length of "5"
        And PageEntity items received have "/folder" in the filename

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items - with query - mimetypes png
        Given There are "5" .jpg items
        And There is one .png item
        When I list items with query mimetypes="*png"
        Then I receive a PageEntity object
        And PageEntity items has length of "1"
        And And PageEntity item received equal to .png item uploadede

    @testrail-C4523117
    @DAT-46538
    Scenario: List dataset items - with query - mimetypes video
        Given There are "5" .jpg items
        And There is one .mp4 item
        When I list items with query mimetypes="video*"
        Then I receive a PageEntity object
        And PageEntity items has length of "1"
        And And PageEntity item received equal to .mp4 item uploadede




================================================
File: tests/features/items_repo/test_items_set_items_entity.feature
================================================
Feature: Items repository set_items_entity service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "set_items_entity"
        And I create a dataset with a random name

    @testrail-C4523118
    @DAT-46539
    Scenario: Change items entity to legal entity
        When I change entity to "Artifact"
        Then Items item entity is "Artifact"
        When I change entity to "Item"
        Then Items item entity is "Item"
        When I change entity to "Codebase"
        Then Items item entity is "Codebase"

    @testrail-C4523118
    @DAT-46539
    Scenario: Change items entity to legal entity
        When I try to change entity to "Dataset"
        Then "Forbidden" exception should be raised




================================================
File: tests/features/items_repo/test_items_upload.feature
================================================
Feature: Items repository upload service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_upload"
        And I create a dataset with a random name

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item
        When I upload a file in path "assets_split/items_upload/0000000162.jpg"
        Then Item exist in host
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item to a specific remote path
        When I upload file in path "assets_split/items_upload/0000000162.jpg" to remote path "/folder"
        Then Item exist in host
        And Item in host is in folder "/folder"
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item with a specific remote name set via the buffer interface
        When I upload file in path "assets_split/items_upload/0000000162.jpg" with remote name "file.jpg" set via the buffer interface
        Then Item exist in host
        And Item in host has name "file.jpg"
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item with a specific remote name
        When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "file.jpg"
        Then Item exist in host
        And Item in host has name "file.jpg"
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item with a specific remote name to a specific remote path
        When I upload the file from path "assets_split/items_upload/0000000162.jpg" with remote name "file.jpg" to remote path "/folder"
        Then Item exist in host
        And Item in host is in folder "/folder"
        And Item in host has name "file.jpg"
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item - overwrite
        Given Item in path "assets_split/items_upload/0000000162.jpg" is uploaded to "Dataset"
        When I upload with "overwrite" a file in path "assets_split/items_upload/0000000162.jpg"
        Then Item exist in host
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item - merge
        Given Item in path "assets_split/items_upload/0000000162.jpg" is uploaded to "Dataset"
        When I upload with "merge" a file in path "assets_split/items_upload/0000000162.jpg"
        Then Item exist in host
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"
        And Item was merged to host

#    ToDo - remove - this is no longer forbidden
#    Scenario: Upload an item with an illegal name
#        When I try to upload file in path "assets_split/items_upload/0000000162.jpg" to remote path "/fol.der"
#        Then Number of error files should be larger by one

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a non-existing file
        When I try to upload file in path "non-existing-path/file.jpg"
        Then "NotFound" exception should be raised

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload items from buffer
        Given There are "3" items
        And I download items to buffer
        And I delete all items from host
        When I upload items from buffer to host
        Then There are "3" items in host

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload item from buffer with specific remote name
        When I use a buffer to upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "file.jpg"
        Then Item exist in host
        And Item in host has name "file.jpg"
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"

#     # TODO - add tests for upload with remote name: url, link, multiview, similarity
#     Scenario: Upload item from URL with specific remote name
#         When I upload a file from a URL "http://some.domain/some_file.png" with remote name "file.png"
#         Then Item exist in host
#         And Item in host has name "file.jpg"
#         And Upload method returned an Item object
#         And Item object from host equals item uploaded

    @testrail-C4523119
    @DAT-46540
    Scenario: Upload a single item with description
        When I upload the file in path "assets_split/items_upload/0000000162.jpg" with description "description"
        Then Item exist in host
        And Upload method returned an Item object
        And Item object from host equals item uploaded
        And Item in host when downloaded to "test_items_upload_downloaded_item" equals item in "assets_split/items_upload/0000000162.jpg"
        And I validate item.description has "description" value


================================================
File: tests/features/items_repo/test_items_upload_batch.feature
================================================
Feature: Items repository upload_batch service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_upload_batch"
        And I create a dataset with a random name

    @testrail-C4523120
    @DAT-46541
    Scenario: Upload items batch
        When I upload item batch from "upload_batch/to_upload"
        And I download items to local path "upload_batch/to_compare"
        Then Items in "upload_batch/to_upload" should equal items in "upload_batch/to_compare/image"




================================================
File: tests/features/items_repo/test_items_upload_dataframe.feature
================================================
Feature: Items repository upload_batch service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "items_upload_batch"
        And I create a dataset with a random name

    @testrail-C4523121
    @DAT-46542
    Scenario: Upload items from pandas data frame
        When I upload item using data frame from "upload_batch/to_upload"
        And I download items to local path "upload_dataframe/to_compare"
        Then Items in "upload_batch/to_upload" should equal items in "upload_dataframe/to_compare/items"
        And Items should have metadata




================================================
File: tests/features/items_repo/test_upload_and_download_images.feature
================================================
Feature: Upload and Download Images

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "delete_images_upload_download"
        And I create a dataset with a random name

    @testrail-C4529127
    @DAT-46543
    Scenario: Upload and Download Images PNG
        Given There are no items
        And I get "10" images of type "png"
        When I upload all the images
        And I download all the images
        Then The images werent changed

    @testrail-C4529127
    @DAT-46543
    Scenario: Upload and Download Images JPG
        Given There are no items
        And I get "10" images of type "jpg"
        When I upload all the images
        And I download all the images
        Then The images werent changed

    @testrail-C4529127
    @DAT-46543
    Scenario: Upload and Download Images BMP
        Given There are no items
        And I get "10" images of type "bmp"
        When I upload all the images
        And I download all the images
        Then The images werent changed

    @testrail-C4529127
    @DAT-46543
    Scenario: Download item with Overwrite True
        Given There are no items
        And I get "1" images of type "png"
        When I upload all the images
        And I overwrite "1" images of type "png"
        And I download the item with Overwrite "True"
        Then The images will be "overwritten"

    @testrail-C4529127
    @DAT-46543
    Scenario: Download item with Overwrite False
        Given There are no items
        And I get "1" images of type "png"
        When I upload all the images
        And I overwrite "1" images of type "png"
        And I download the item with Overwrite "False"
        Then The images will be "not overwritten"

    @DAT-54335
    Scenario: Download item with Overwrite True path
        Given I upload item by the name of "test_item.jpg" to a remote path "test/1/2/3"
        When I download the item with Overwrite value "True" to path "qa_22.3"
        Then check that the new download will be with the same path "qa_22.3"

================================================
File: tests/features/ml_subsets/test_ml_subsets.feature
================================================
Feature: ML Subsets functionalities

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_ml_subsets"
        And I create a dataset with a random name
        When There are "12" items

    @DAT-83214
    Scenario: Split ML subsets with default percentages
        When I split dataset items into ML subsets with default percentages
        Then Items are splitted according to the default ratio

    @DAT-83216
    Scenario: Assign 'train' subset to selected items
        Given I select 3 specific items from the dataset
        When I assign the 'train' subset to those selected items
        Then Those items have train subset assigned

    @DAT-83217
    Scenario: Remove subsets from selected items
        Given I select 3 specific items from the dataset
        When I remove subsets from those selected items
        Then Those items have no ML subset assigned

    @DAT-83218
    Scenario: Assign subset to a single item
        Given I have a single item from the dataset
        When I assign the 'validation' subset to this item at the item level
        Then The item has 'validation' subset assigned

    @DAT-83219
    Scenario: Remove subset from a single item
        Given I have a single item with a subset assigned
        When I remove the subset from the item
        Then The item has no ML subset assigned

    @DAT-83220
    Scenario: Check item current subset
        Given I have a single item with 'test' subset assigned
        When I get the current subset of the item
        Then The result is 'test'

    @DAT-83221
    Scenario: Get items missing ML subset
        Given Some items in the dataset have subsets assigned and some do not
        When I get items missing ML subset
        Then I receive a list of item IDs with no ML subset assigned


================================================
File: tests/features/models_repo/test_dpk_model_genai.feature
================================================
Feature: Test dpk with gen model

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk-gen-model"

  @DAT-73514
  @models.delete
  Scenario: Create gen model with LLM
    Given I publish a model dpk from file "model_dpk/basicModelDpkGen.json" package "dummymodel" with status "trained"
    When I install the app without custom_installation
    And I get last model in project
    And "model" has app scope
    And model should be with mltype "LLM"

  @DAT-73514
  Scenario: Auto update app model should be LLM
    Given I publish a model dpk from file "model_dpk/basicModelDpk.json" package "dummymodel" with status "trained"
    When I install the app without custom_installation
    And I update app auto update to "True"
    And I get last model in project
    And I "deploy" the model
    And I get the dpk by name
    And i update dpk attribute "Gen AI" to "LLM"
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    And I get last model in project
    And I get service in index "-1"
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope
    And service runnerImage is "jjanzic/docker-python3-opencv"
    And model status should be "deployed" with execution "False" that has function "None"
    And model should be with mltype "LLM"





================================================
File: tests/features/models_repo/test_model_verification.feature
================================================
Feature: App Verification error

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item


  @DAT-83113
  Scenario: DPK model with wrong values - App should failed to installed with correct error
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file and update dpk with params 'True'
      | key                            | value |
      | components.models.0.outputType | Image |
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I publish a dpk to the platform
    And I try to install the app
    Then "FailedDependency" exception should be raised
    And app is not installed in the project
    And "error installing an app: Validation Failed" in error message
    And "model.outputType" in error message
    And ""value":"Image"" in error message


================================================
File: tests/features/models_repo/test_models_clone_1.feature
================================================
Feature: Models repository clone testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @DAT-55187
  Scenario: test clone model
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    When i "evaluate" the model
    Then model status should be "deployed" with execution "True" that has function "evaluate_model"
    When i "evaluate" the model
    Then model status should be "deployed" with execution "True" that has function "evaluate_model"
    Then model metadata should include operation "evaluate" with filed "datasets" and length "1"
    When I clone a model
    Then model input should be equal "image", and output should be equal "box"
    Then model do not have operation "evaluate"
    And I clean the project

  @DAT-65866
  Scenario: test clone model failed
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "failed" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When I clone the model
    Then model status should be "pre-trained"
    And I clean the project

  @DAT-77435
  Scenario: test clone model artifact
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    And I upload artifact in "0000000162.jpg"
    When I clone the model
    Then src model and clone model should have the same artifact name and different id
    And I clean the project

================================================
File: tests/features/models_repo/test_models_clone_2.feature
================================================
Feature: Models repository clone testing 2

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_configuration"
    And I create a dataset by the name of "model" in the project
    When I upload labels to dataset

  @DAT-65220
  Scenario: Clone model with custom configuration - Should clone also the configuration
    Given I fetch the dpk from 'apps/app_include_models.json' file
    When I add the context.dataset to the dpk model
    And I set code path "models_flow" to context
    And I pack directory by name "model_flow"
    And I add codebase to dpk
    And I get computeConfig from path "apps/compute_config.json" named "module-level"
    And I add computeConfig to dpk on "modules" component in index "0"
    And I get computeConfig from path "apps/compute_config.json" named "function-level"
    And I add computeConfig to dpk on "functions" component in index "2" on module in index "0"
    And I publish a dpk to the platform
    And  I install the app
    And I set the model in the context
    When I clone a model
    Then Model object with the same name should be exist
    When i "train" the model
    Then i have a model service
    Then I compare service config with context.compute_config_item

================================================
File: tests/features/models_repo/test_models_context.feature
================================================
Feature: Models repository flow testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item


  @DAT-50829
  Scenario: test model - failed
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "modelfaild" with entry point "failedmain.py"
    And I create a model from package by the name of "test-model-failed" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model-failed"
    When i "train" the model
    Then model status should be "failed" with execution "True" that has function "train_model"


  @DAT-52904
  Scenario: test flow model - initPrams
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "initmodel" with entry point "main.py"
    And I create a model from package by the name of "test-model-init" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model-init"
    When i train the model with init param model none
    Then model status should be "trained" with execution "True" that has function "train_model"


================================================
File: tests/features/models_repo/test_models_create.feature
================================================
Feature: Models repository create testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"

  @testrail-C4523165
  @DAT-46548
  Scenario: Create a model with a legal name
    When get global model package
    And I create a model with a random name
    Then Model object with the same name should be exist
    And Model object with the same name should be exist in host

  @testrail-C4523165
  @DAT-46548
  Scenario: Rename model entity
    When get global model package
    And I create a model with a random name
    When I rename model to "some_other_name"
    Then model name is "some_other_name"
    Then "model" has updatedBy field

  @DAT-44734
  Scenario: Create a two models with the same bot
    When get global model package
    And I create a model with a random name
    And I update model status to "trained"
    And I deploy the model
    And I create a model with a random name
    And I train the model
    Then The project have only one bot

  @DAT-44695
  Scenario: delete clone model artifact
    When get global model package
    And I create a model with a random name
    Then Model object with the same name should be exist
    When I upload an artifact "0000000162.jpg" to the model
    And I clone the model
    And I delete the clone model
    Then artifact is exist in the host

  @DAT-45101
  Scenario: Create a model without dataset
    When get global model package
    And I create a model without dataset
    Then Model object with the same name should be exist
    And Model object with the same name should be exist in host

  @DAT-45099
  Scenario: Create a model with filter
    When get global model package
    And I create a model with a random name
    Then Model filter should not be empty

  @DAT-45099
  Scenario: Create a model without filter
    When get global model package
    And I create a model without filter
    And I train the model
    Then "BadRequest" exception should be raised


================================================
File: tests/features/models_repo/test_models_delete.feature
================================================
Feature: Models repository delete testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt-delete"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @DAT-55103
  Scenario: test model delete
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "deploy" the model
    And i add service id "1234" to model metadata
    And I delete model
    Then model is deleted

================================================
File: tests/features/models_repo/test_models_dpk_updated.feature
================================================
Feature: Test model created from dpk with updated app version

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_service_updated"


  @DAT-72492
  Scenario: Update dpk and app version with customInstallation - Shouldn't create new model and
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "created"
    When I get the dpk by name
    And I create a context.custom_installation var
    And I install the app with custom_installation "True"
    And I update app auto update to "True"
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    Then I expect to get "1" models in project

  @DAT-72492
  Scenario: Update dpk and app version without customInstallation - Shouldn't create new model
    Given There are no models in project
    And I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "created"
    When I get the dpk by name
    And I create a context.custom_installation var
    And I install the app with custom_installation "False"
    And I update app auto update to "True"
    And I publish a dpk
    And I wait for app version to be updated according to dpk version
    Then I expect to get "1" models in project

  @DAT-77777
  Scenario: Model with generated string - Publish new dpk version - Should not create new model
    Given There are no models in project
    And I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "created"
    When I install the app
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "created"
    When I install the app
    And I get the dpk by name
    And I get last model in project
    And I update app auto update to "True"
    Then The model name not changed
    When I publish a dpk
    And I wait for app version to be updated according to dpk version
    Then I expect to get "2" models in project

================================================
File: tests/features/models_repo/test_models_embed_datasets.feature
================================================
Feature: Models repository embed datasets testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt-embed"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item
    Given I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @DAT-81076
  Scenario: test model with embedded datasets
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    When I run model embed datasets
    Then command status is "success"
    And model service has "2" executions and "2" triggers
    Given I upload an item by the name of "test_item2.jpg"
    Then model service has "3" executions and "2" triggers
    And i clean the project

  @DAT-81076
  Scenario: test model with embedded datasets twice
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    When I run model embed datasets
    Then command status is "success"
    And model service has "4" executions and "4" triggers
    When I run model embed datasets
    Then command status is "success"
    And model service has "8" executions and "4" triggers
    And i clean the project


  @DAT-81076
  Scenario: test model with embedded datasets - command failed
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    When i change model status to "trained"
    When I run model embed datasets
    Then command massage is in model
    And model service has "0" executions and "0" triggers
    And i clean the project

================================================
File: tests/features/models_repo/test_models_flow.feature
================================================
Feature: Models repository flow testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @DAT-48263
  @DAT-51111
  @DAT-51143
  @DAT-51144
  @DAT-51145
  @DAT-54702
  Scenario: test flow model
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "train" the model
    Then service metadata has a model id and operation "train"
    Then model status should be "trained" with execution "True" that has function "train_model"
    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    When i "evaluate" the model
    Then service metadata has a model id and operation "evaluate"
    Then model status should be "deployed" with execution "True" that has function "evaluate_model"
    And Dataset has a scores file
    When i call the precision recall api
    Then i should get a json response
    Then Log "model training" is in model.log() with operation "train"
    Then Log "model prediction" is in model.log() with operation "evaluate"


================================================
File: tests/features/models_repo/test_models_list.feature
================================================
Feature: Model repository query testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "models_list"
    And I create a dataset by the name of "models_dataset" in the project

  @testrail-C4525320
  @DAT-46549
  Scenario: List by model name
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "model-num-1" with status "trained" in index "0"
    And I create a model from package by the name of "model-num-2" with status "trained" in index "1"
    And I add the context.dataset to the dpk model
    And I publish a dpk to the platform
    And I install the app
    When I list models with filter field "name" and values "model-num-1"
    Then I get "1" entities




================================================
File: tests/features/models_repo/test_models_service_delete.feature
================================================
Feature: Models repository delete flow testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @DAT-56475
  Scenario: test model delete services
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "train" the model
    Then service metadata has a model id and operation "train"
    Then model status should be "trained" with execution "True" that has function "train_model"
    When I get service by id
    Then Service is archived

    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    And i have a model service
    When I delete service by "id"
    When I get service by id
    Then Service is archived

    When i "evaluate" the model
    Then model status should be "trained" with execution "True" that has function "evaluate_model"
    And service metadata has a model id and operation "evaluate"
    When I delete service by "id"
    When I get service by id
    Then Service is archived

================================================
File: tests/features/models_repo/test_models_service_update.feature
================================================
Feature: Models repository flow testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item


  @DAT-53071
  Scenario: test evaluate service updated
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model-eve" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model-eve"
    And i add a Service config runtime
      | runnerImage=ImageA | podType=regular-xs |
    When i "evaluate" the model
    Then check service runtime
      | runnerImage=ImageA | podType=regular-xs |
    When i add a Service config runtime
      | runnerImage=ImageB | podType=regular-s |
    When i "evaluate" the model
    Then check service runtime
      | runnerImage=ImageB | podType=regular-s |


================================================
File: tests/features/models_repo/test_models_service_update_archive.feature
================================================
Feature: Models repository delete flow testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_mgmt"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item


  @DAT-57724
  Scenario: test model update archive services
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    When i "train" the model
    Then service metadata has a model id and operation "train"
    Then model status should be "trained" with execution "True" that has function "train_model"
    When I get service by id
    Then Service is archived
    When I set service in context
    When I get service revisions
    And I change service "concurrency" to "17"
    And I update service
    Then Service received equals service changed except for "runtime.concurrency"
    When I change service "archive" to "False"
    When I update service
    Then "BadRequest" exception should be raised

================================================
File: tests/features/models_repo/test_models_services_updated.feature
================================================
Feature: Test app umbrella - Update app should add app refs

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_service_updated"


  @DAT-71922
  Scenario: Update app with model train service - Should create train service with latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "created"
    When I install the app
    And I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And i fetch the model by the name "test-model"
    And I "train" the model
    And I get service from context.execution
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope

  @DAT-71923
  @services.delete
  Scenario: Update app with model evaluate service - Should create evaluate service with latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And I get last model in project
    And I "evaluate" the model
    And I get service from context.execution
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope


  @DAT-71993
  Scenario: Update app with cloned model deploy service - Should update deploy service to latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And I get last model in project
    And I clone a model and set status "trained"
    And I "deploy" the model
    And I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And I get last model in project
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope


  @DAT-71997
  Scenario: Update app with cloned model train service - Should create train service with latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "created"
    When I install the app
    And I get last model in project
    And I clone a model
    And I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And I get last model in project
    And I "train" the model
    And I get service from context.execution
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And "service" has app scope

================================================
File: tests/features/models_repo/test_models_train.feature
================================================
Feature: Models repository train function testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model-train"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @DAT-68045
  @DAT-69061
  Scenario: Train model without mandatory params - Should return correct errors
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I remove attributes "datasetId,labels" from dpk model in index "0"
    Given I publish a dpk to the platform
    When dpk has base id
    And i save dpk base id
    And I install the app
    And i fetch the model by the name "test-model"
    Then "model" has app scope
    When I "train" the model with exception "True"
    Then "Must provide a dataset to train model" in error message
    And I clean the project

  @DAT-68045
  @DAT-69061
  Scenario: Train model without mandatory params - Should return correct errors
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    And I remove attributes "labels" from dpk model in index "0"
    Given I publish a dpk to the platform
    When dpk has base id
    And i save dpk base id
    And I install the app
    And i fetch the model by the name "test-model"
    Then "model" has app scope
    When I "train" the model with exception "True"
    Then "Must provide labels to train model" in error message
    And I clean the project


  @DAT-68045
  @DAT-69061
  Scenario: Train model without mandatory params - Should return correct errors
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "created" in index "0"
    Given I publish a dpk to the platform
    When dpk has base id
    And i save dpk base id
    And I install the app
    And i fetch the model by the name "test-model"
    Then "model" has app scope
    When I "train" the model with exception "True"
    And I "train" the model with exception "True"
    Then service metadata has a model id and operation "train"
    When i fetch the model by the name "test-model"
    Then "model" has app scope
    And I clean the project

================================================
File: tests/features/models_repo/test_models_updated_dpk.feature
================================================
Feature: DPK single Id

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "model_updated_dpk"


  @DAT-70420
  Scenario: DPK - Models > Update dpk codebase and update app - Model should point to new dpk version and service updated
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And I get the dpk by name
    And I set code path "packages_get" to context
    And I pack directory by name "packages_get"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And i fetch the model by the name "test-model"
    And I "deploy" the model
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "model" has app scope
    And I clean the project


  @DAT-71180
  Scenario: DPK - Models > Update dpk model labels and update app - Model labels should not updated
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel"
    When I install the app without custom_installation
    And I get the dpk by name
    And I remove attributes "labels" from dpk model in index "0"
    And I add att 'labels=["1"]' to dpk 'model' in index '0'
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    When i fetch the model by the name "test-model"
    Then "model" has app scope
    Then I validate the context.model has the attribute "labels" with value "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
    And I clean the project



================================================
File: tests/features/models_repo/test_models_validation.feature
================================================
#Feature: Models create validation
#
#  Background:
#    Given Platform Interface is initialized as dlp and Environment is set according to git branch
#    And I create a project by the name of "model_create_validation"
#    And I create a dataset by the name of "model" in the project
#
#  @DAT-66505
#  Scenario: Install dpk with 2 models with same name - Should failed to install app
#    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
#    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
#    And I create a model from package by the name of "test-model" with status "trained" in index "0"
#    And I create a model from package by the name of "test-model" with status "trained" in index "1"
#    And I add the context.dataset to the dpk model
#    And I publish a dpk to the platform
#    And I install the app with exception
#    Then "BadRequest" exception should be raised
#    And "error creating an app , models already exist, Model with this name already exists" in error message
#


================================================
File: tests/features/notifications/notification_code_base.feature
================================================
@qa-nightly
@bot.create
@rc_only
Feature: Executions repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "codebase_error"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @DAT-44914
  Scenario: Code base error should raise notification
    Given There is a package (pushed from "faas/initError") by the name of "codebase-error"
    And There is a service by the name of "codebase-error" with module name "default_module" saved to context "service"
    And I delete service code base
    When Service minimum scale is "1"
    Then I receive "CodebaseError" notification with resource "service.id"
    And Service is deactivated by system

================================================
File: tests/features/notifications/notification_docker.feature
================================================
#@qa-nightly
#@bot.create
#Feature: Executions repository create service testing
#
#  Background: Initiate Platform Interface and create a project
#    Given Platform Interface is initialized as dlp and Environment is set according to git branch
#    And I create a project by the name of "imagepullback_error"
#    And I create a dataset with a random name
#
#  @services.delete
#  @packages.delete
#  @DAT-46786
#  Scenario: Docker Image error should raise notification
#    Given There is a package (pushed from "faas/initError") by the name of "imagepullback-error"
#    And There is a service by the name of "imagepullback-error" with module name "default_module" saved to context "service"
#    And Service has wrong docker image
#    When Service minimum scale is "1"
#    Then I receive "ImagePullBackOff" notification with resource "service.id"
#    And Service is deactivated by system


================================================
File: tests/features/notifications/notification_import_error.feature
================================================
@qa-nightly
@bot.create
@rc_only
Feature: Executions repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "import_error"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @DAT-46784
  Scenario: Init error should raise notification
    Given There is a package (pushed from "faas/importError") by the name of "import-error"
    And There is a service by the name of "import-error" with module name "default_module" saved to context "service"
    When Service minimum scale is "1"
    Then I receive "RequirementsError" notification with resource "service.id"
    And Service is deactivated by system


================================================
File: tests/features/notifications/notification_init_error.feature
================================================
@qa-nightly
@bot.create
@rc_only
Feature: Executions repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "init_error"
    And I create a dataset with a random name


  @services.delete
  @packages.delete
  @DAT-46785
  Scenario: Import error should raise notification
    Given There is a package (pushed from "faas/initError") by the name of "init-error"
    And There is a service by the name of "init-error" with module name "default_module" saved to context "service"
    When Service minimum scale is "1"
    Then I receive "InitError" notification with resource "service.id"
    And Service is deactivated by system

================================================
File: tests/features/notifications/notification_oom_error.feature
================================================
@qa-nightly
@bot.create
@rc_only
Feature: Executions repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "oom_error"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @DAT-25330
  Scenario: Init error should raise notification
    Given There is a package (pushed from "faas/oomError") by the name of "oom-error"
    And There is a service by the name of "oom-error" with module name "default_module" saved to context "service"
    And I upload item in "0000000162.jpg" to dataset
    When I create an execution with "inputs"
      | sync=False | inputs=Item |
    Then I receive an Execution entity
    Then I receive "ExecutionFailed" notification with resource "execution.id"
    And Execution was executed and finished with status "failed" and message "Execution has reached max attempts"




================================================
File: tests/features/notifications/notification_requirements.feature
================================================
@qa-nightly
@bot.create
@rc_only
Feature: Executions repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "requirements_error"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @DAT-46788
  Scenario: Requirements error should raise notification
    Given There is a package (pushed from "faas/initError") by the name of "requirements-error"
    And There is a service by the name of "requirements-error" with module name "default_module" saved to context "service"
    And I add bad requirements to service
    When Service minimum scale is "1"
    Then I receive "RequirementsError" notification with resource "service.id"
    And Service is deactivated by system

================================================
File: tests/features/ontologies_repo/test_ontologies_create.feature
================================================
Feature: Ontologies repository create service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "ontologies_create"
        And I create a dataset with a random name

    @testrail-C4523127
    @DAT-46550
    Scenario: Create ontology
        When I create a new ontology with labels from file "labels.json"
        And I update dataset ontology to the one created
        Then Dataset ontology in host equal ontology uploaded

    @testrail-C4523127
    @DAT-46550
    Scenario: Create ontology - no project id
        When I create a new ontology with no projectIds, with labels from file "labels.json"
        And I update dataset ontology to the one created
        Then Dataset ontology in host equal ontology uploaded

    @testrail-C4523127
    @DAT-46550
    Scenario: Create ontology with attributes
        When I create a new ontology with labels from file "labels.json" and attributes "['attr1', 'attr2']"
        And I update dataset ontology to the one created
        Then Dataset ontology in host equal ontology uploaded

    # not working properly
    # Scenario: Create ontology - other project id
    #     Given There is another project by the name of "other_project"
    #     When I create a new ontology with labels and project id of "other_project" from file "labels.json"
    #     And I try to update dataset ontology to the one created
    #     Then "Forbidden" exception should be raised

    @testrail-C4523127
    @DAT-46550
    Scenario: Create ontology - wrong project id
        When I try create a new ontology with labels and "some_project_id" from file "labels.json"
        Then "Forbidden" exception should be raised





================================================
File: tests/features/ontologies_repo/test_ontologies_delete.feature
================================================
Feature: Ontologies repository Delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "ontologies_delete"
        And I create a dataset with a random name
        And Dataset has ontology

    @testrail-C4523128
    @DAT-46551
    Scenario: Delete existing ontology by id
        When I delete ontology by id
        Then Ontology does not exist in dataset

    @testrail-C4523128
    @DAT-46551
    Scenario: Delete non-existing ontology by id
        When I try to delete ontology by "some_id"
        Then "BadRequest" exception should be raised




================================================
File: tests/features/ontologies_repo/test_ontologies_edit.feature
================================================
Feature: Ontologies repository Update service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "ontologies_edit"
        And I create a dataset with a random name
        And Dataset has ontology

    @testrail-C4523129
    @DAT-46552
    Scenario: Update existig ontology labels
        When I update ontology with labels from file "labels.json"
        Then Dataset ontology in host equal ontology uploaded

    @testrail-C4523129
    @DAT-46552
    Scenario: Update existig ontology attributes
        When I update ontology with labels from file "labels.json"
        When I update ontology attributes to "attr1", "attr2"
        Then Dataset ontology in host have an attributes

    @testrail-C4523129
    @DAT-46552
    Scenario: Update existig ontology metadata system
        When I update ontology system metadata
        Then Dataset ontology in host equal ontology uploaded




================================================
File: tests/features/ontologies_repo/test_ontologies_get.feature
================================================
Feature: Ontologies repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "ontologies_get"
        And I create a dataset with a random name

    @testrail-C4523130
    @DAT-46553
    Scenario: Get an existing ontology by id
        Given Dataset has ontology
        When I get a ontology by id
        Then I get an Ontology object

    @testrail-C4523130
    @DAT-46553
    Scenario: Get non-existing Ontology by id
        Given Dataset has ontology
        When I try to get Ontology by "some_id"
        Then "BadRequest" exception should be raised




================================================
File: tests/features/ontology_entity/test_ontology_attributes.feature
================================================
Feature: Ontology Entity attributes testing

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "ontology_attributes"
    And I create a dataset with a random name
    And Dataset has ontology
    And Classes in file: "classes_new.json" are uploaded to test Dataset


  @testrail-C4529142
  @DAT-46554
  Scenario: Add attributes to ontology
    When I add "checkbox" attribute to ontology
      | key=1 | title=test1 | values=['a','b','c'] |
    Then I validate attribute "checkbox" added to ontology
    When I add "radio_button" attribute to ontology
      | key=2 | title=test2 | values=['a','b','c'] | scope=all |
    Then I validate attribute "radio_button" added to ontology
    When I add "slider" attribute to ontology
      | key=3 | title=test3 | scope=all | attribute_range=0,10,1 |
    Then I validate attribute "slider" added to ontology
    When I add "yes_no" attribute to ontology
      | key=4 | title=test4 | scope=all |
    Then I validate attribute "yes_no" added to ontology
    When I add "free_text" attribute to ontology
      | key=5 | title=test5 | scope=all |
    Then I validate attribute "free_text" added to ontology


  @testrail-C4529142
  @DAT-46554
  Scenario: Delete ontology attributes
    When I add "checkbox" attribute to ontology
      | key=1 | title=test1 | values=['a','b','c'] | scope=all |
    Then I delete attributes with key "1" in ontology
    When I add "checkbox" attribute to ontology
      | key=1 | title=test1 | values=['a','b','c'] | scope=all |
    And I add "slider" attribute to ontology
      | key=3 | title=test3 | scope=all | attribute_range=0,10,1 |
    And I add "free_text" attribute to ontology
      | key=5 | title=test5 | scope=all |
    Then I delete all attributes in ontology


  @testrail-C4529142
  @DAT-46554
  Scenario: Attributes Edge cases
        # I Add attribute with all params
    When I add "checkbox" attribute to ontology
      | key=1 | title=test1 | values=['a','b','c'] | scope=['Person','Wheel'] | optional=False |
    Then I validate attribute "checkbox" added to ontology
        # I Add attribute with int value
    When I add "checkbox" attribute to ontology
      | key=1 | title=test1 | values=[1] |
    Then I receive error with status code "400"


================================================
File: tests/features/ontology_entity/test_ontology_bamba_icon.feature
================================================
Feature: Ontology Labels with icon

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "ontology_bamba_feature"
        And I create a dataset with a random name
        And Dataset has ontology

    @testrail-C4532189
    @DAT-46555
    Scenario: Add images to labels from different folders
        When I add label "1" to ontology with "labels/1/bamba-icon.jpg"
        When I add label "2" to ontology with "labels/2/bamba-icon.jpg"
        Then I validate dataset labels images are different


================================================
File: tests/features/ontology_entity/test_ontology_repo_methods.feature
================================================
Feature: Ontology Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "ontology_repo_methods"
        And I create a dataset with a random name
        And Dataset has ontology

    @testrail-C4523131
    @DAT-46556
    Scenario: Add label to ontology
        When I add label to ontology
        And I update ontology entity
        Then Ontology in host has label

    @testrail-C4523131
    @DAT-46556
    Scenario: Add labels to ontology and to_root() testing
        When I update ontology entity with labels from file "labels.json"
        Then Dataset ontology in host has labels uploaded from "labels.json"

    @testrail-C4523131
    @DAT-46556
    Scenario: Delete item
        When I delete ontology entity
        Then Ontology does not exist in dataset

    @testrail-C4523131
    @DAT-46556
    Scenario: Update existing ontology labels
        When I update ontology entity with labels from file "labels.json"
        Then Dataset ontology in host equal ontology uploaded

    @testrail-C4523131
    @DAT-46556
    Scenario: Update existing ontology metadata system
        When I update ontology entity system metadata
        Then Dataset ontology in host equal ontology uploaded





================================================
File: tests/features/packages_entity/packages_get.feature
================================================
Feature: Packages entity method testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_get"
    And Directory "packages_get" is empty
    When I generate package by the name of "test-package" to "packages_get"

  @packages.delete
  @testrail-C4523132
  @DAT-46557
  Scenario: To Json
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_get | inputs=None | outputs=None |
    When I get package by the name of "test-package"
    Then I get a package entity
    And Object "Package" to_json() equals to Platform json.

  @packages.delete
  @DAT-56141
  Scenario: test input description
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_get | inputs=itemWithDescription | outputs=None |
    When I get package by the name of "test-package"
    Then I get a package entity
    And Object "Package" to_json() equals to Platform json.
    Then I verify PackageModule params
      | key                                | value |
      | functions[0].inputs[0].description | item  |
    When I update PackageModule function "1" input "1" use "package"
      | key         | value  |
      | description | item_1 |
    When I update package
    When i update the context module from package
    Then I verify PackageModule params
      | key                                | value  |
      | functions[0].inputs[0].description | item_1 |

================================================
File: tests/features/packages_entity/test_package_module.feature
================================================
Feature: Packages entity method testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_package_module"
    And Directory "packages_get" is empty
    When I generate package by the name of "test-package" to "packages_get"

  @packages.delete
  @DAT-51155
  Scenario: Update PackageModule - Should be updated with expected values
    When I create PackageModule with params
      | name=package_module | entry_point=main.py | functions=[{'name':'run','inputs':'item'}] |
    And I update PackageModule function "1" input "1" use "module"
      | key  | value  |
      | name | item_1 |
    Then I verify PackageModule params
      | key                         | value  |
      | functions[0].inputs[0].name | item_1 |

================================================
File: tests/features/packages_flow/packages_flow.feature
================================================
@bot.create
Feature: Packages Flow

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_packages_flow"
        And I create a dataset with a random name
        And Directory "package_flow/package_directory" is empty

    @services.delete
    @packages.delete
    @testrail-C4523133
    @DAT-46558
    Scenario: Standard Flow
        When I generate package by the name of "None" to "package_flow/package_directory"
        And I upload a file in path "package_flow/package_assets/picture1.jpg"
        And I copy all relevant files from "package_flow/package_assets" to "package_flow/package_directory"
        And I test local package in "package_flow/package_directory"
        And I push and deploy package in "package_flow/package_directory"
        And I upload item in "package_flow/package_assets/picture2.jpg" to dataset
        Then Item "1" annotations equal annotations in "package_flow/package_assets/annotations1.json"
        And Item "2" annotations equal annotations in "package_flow/package_assets/annotations2.json"


================================================
File: tests/features/packages_repo/package_context.feature
================================================
Feature: Packages repository context service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_context"

  @packages.delete
  @testrail-C4523134
  @DAT-46566
  Scenario: Create package
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_context | inputs=None | outputs=None |
    Then I receive package entity
    And Package entity equals local package in "packages_context"



================================================
File: tests/features/packages_repo/package_slot.feature
================================================
Feature: Packages repository create slot testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_slot"
    And Directory "packages_create" is empty
    When I generate package by the name of "test-package" to "packages_create"

  @packages.delete
  @testrail-C4528974
  @DAT-46567
  Scenario: Add UI slot to exist package
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=None | outputs=None |
    Then I receive package entity
    When I add UI slot to the package
    Then I validate slot is added to the package


  @packages.delete
  @testrail-C4528974
  @DAT-46567
  Scenario: Add UI slot to exist package with all scopes
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=item,dataset,annotation,task | outputs=None |
    Then I receive package entity
    When I add UI slot to the package with all scopes
    Then I validate slot is added to the package



================================================
File: tests/features/packages_repo/packages_create.feature
================================================
@qa-nightly
Feature: Packages repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_create"
    And Directory "packages_create" is empty
    When I generate package by the name of "test-package" to "packages_create"

  @packages.delete
  @testrail-C4523134
  @DAT-46559
  Scenario: Create package
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=None | outputs=None |
    Then I receive package entity
    And Package entity equals local package in "packages_create"

  @packages.delete
  @DAT-46046
  Scenario: Create package
    When I push "module-input-conflict" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=item,item | outputs=None |
    Then "Conflict" exception should be raised

  @packages.delete
  @DAT-46046
  Scenario: Create package
    When I push "module-output-conflict" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=item | outputs=item,item |
    Then "Conflict" exception should be raised

  @packages.delete
  @DAT-46046
  Scenario: Create package
    When I push "module-input-value-error" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=None | outputs=item,item |
    Then "ValueError" exception should be raised



================================================
File: tests/features/packages_repo/packages_delete.feature
================================================
Feature: Packages repository delete testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_delete"
    And Directory "packages_create" is empty
    When I generate package by the name of "test-package" to "packages_create"

  @testrail-C4532690
  @DAT-46560
  Scenario: Create package
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=None | outputs=None |
    Then I receive package entity
    And I delete package



================================================
File: tests/features/packages_repo/packages_generate.feature
================================================
Feature: Packages repository generate service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_generate"

  @testrail-C4523135
  @DAT-46561
  Scenario: Generate package - no given path - nameless
    Given Directory "packages_generate/to_generate" is empty
    And cwd is "packages_generate/to_generate"
    When I generate package by the name of "None" to "None"
    And cwd goes back to original
    Then package local files in "packages_generate/to_generate" equal package local files in "packages_generate/to_compare_nameless"

  @testrail-C4523135
  @DAT-46561
  Scenario: Generate package - no given path - by name
    Given Directory "packages_generate/to_generate" is empty
    And cwd is "packages_generate/to_generate"
    When I generate package by the name of "test-package" to "None"
    And cwd goes back to original
    Then Package local files in "packages_generate/to_generate" equal package local files in "packages_generate/to_compare_test_package"

  @testrail-C4523135
  @DAT-46561
  Scenario: Generate package by name
    Given Directory "packages_generate/to_generate" is empty
    When I generate package by the name of "test-package" to "packages_generate/to_generate"
    Then Package local files in "packages_generate/to_generate" equal package local files in "packages_generate/to_compare_test_package"

  @testrail-C4523135
  @DAT-46561
  Scenario: Generate package - nameless
    Given Directory "packages_generate/to_generate" is empty
    When I generate package by the name of "None" to "packages_generate/to_generate"
    Then Package local files in "packages_generate/to_generate" equal package local files in "packages_generate/to_compare_nameless"



================================================
File: tests/features/packages_repo/packages_get.feature
================================================
Feature: Packages repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_packages_get"
        And Directory "packages_get" is empty
        When I generate package by the name of "test-package" to "packages_get"

    @packages.delete
    @testrail-C4523136
    @DAT-46557
    Scenario: Get package by name
        When I push "first" package
            |codebase_id=None|package_name=test-package|src_path=packages_get|inputs=None|outputs=None|
        When I get package by the name of "test-package"
        Then I get a package entity
        And It is equal to package created

    @packages.delete
    @testrail-C4523136
    @DAT-46557
    Scenario: Get package by id
        When I push "first" package
            |codebase_id=None|package_name=test-package|src_path=packages_get|inputs=None|outputs=None|
        When I get package by id
        Then I get a package entity
        And It is equal to package created



================================================
File: tests/features/packages_repo/packages_list.feature
================================================
Feature: Packages repository list service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_packages_list"
        And Directory "packages_list" is empty
        When I generate package by the name of "test-package" to "packages_list"

    @testrail-C4523137
    @DAT-46563
    Scenario: list packages when 0 exist
        When I list all project packages
        Then I receive a list of "0" packages

    @packages.delete
    @testrail-C4523137
    @DAT-46563
    Scenario: list packages when 1 exist
        When I push "first" package
            |codebase_id=None|package_name=None|src_path=packages_list|inputs=None|outputs=None|
        Then I receive package entity
        And Package entity equals local package in "packages_generate/to_compare_test_package"
        When I list all project packages
        Then I receive a list of "1" packages




================================================
File: tests/features/packages_repo/packages_name_validation.feature
================================================
Feature: Packages repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_create"

  @testrail-C4523138
  @DAT-46564
  Scenario: Create package with illegal name
    When I try to push package
      | codebase_id=None | package_name=illegal_name | src_path=packages_create | inputs=None | outputs=None |
    Then "BadRequest" exception should be raised
    And Name is invalid

  @testrail-C4523138
  @DAT-46564
  Scenario: Validate names
    When I validate name "1package"
    Then Name is invalid
    When I validate name "my_package"
    Then Name is invalid
    When I validate name "-package"
    Then Name is invalid
    When I validate name "package-"
    Then Name is invalid
    When I validate name "Package"
    Then Name is invalid
    When I validate name "packageabcderthjgkldkfjghrnfhdbchfnvjghgj"
    Then Name is invalid
    When I validate name "package"
    Then Name is valid
    When I validate name "my-package"
    Then Name is valid
    When I validate name "package-number-1"
    Then Name is valid


================================================
File: tests/features/packages_repo/packages_push.feature
================================================
Feature: Packages repository push service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_packages_push"
        And Directory "packages_push" is empty
        When I generate package by the name of "test-package" to "packages_push"

    @packages.delete
    @testrail-C4523139
    @DAT-46565
    Scenario: Push local package - no params
        When I push "first" package
            |codebase_id=None|package_name=None|src_path=packages_push|inputs=None|outputs=None|
        Then I receive package entity
        And Package entity equals local package in "packages_generate/to_compare_test_package"
        When I modify python file - (change version) in path "packages_push/main.py"
        And I push "second" package
            |codebase_id=None|package_name=None|src_path=packages_push|inputs=None|outputs=None|
        Then I receive another package entity
        And 1st package and 2nd package only differ in code base id


================================================
File: tests/features/packages_repo/test_package_git_ignore.feature
================================================
Feature: Packages repository git ignore testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_packages_git_ignore"

  @packages.delete
  @testrail-C4532670
  @DAT-46569
  Scenario: Create package with git-ignore file
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_git_ignore | inputs=None | outputs=None |
    Then I receive package entity
    And Package entity equals local package in "packages_git_ignore_expected"



================================================
File: tests/features/packages_repo/test_package_revision.feature
================================================
@bot.create
Feature: Packages revision testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_package_revision"
    And Directory "packages_create" is empty
    When I generate package by the name of "test-package" to "packages_create"

  @packages.delete
  @testrail-C4532574
  @DAT-46570
  Scenario: Update package revision Should update the package version
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=None | outputs=None |
    Then I receive package entity
    And I expect package version to be "1.0.0" and revision list size "1"
    When I update package
    Then I expect package version to be "1.0.1" and revision list size "2"
    When I update package
    Then I expect package version to be "1.0.2" and revision list size "3"


  @services.delete
  @packages.delete
  @testrail-C4532574
  @DAT-46570
  Scenario: Update package revision and install specific service version
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_create | inputs=None | outputs=None |
    Then I receive package entity
    When I update package
    And I update package
    Then I expect package version to be "1.0.2" and revision list size "3"
    When I create a service
      | service_name=services-get | package=services-get | revision=1.0.1 | config=None | runtime=None |
    Then I validate service version is "1.0.1"


  @pipelines.delete
  @testrail-C4532574
  @DAT-46570
  Scenario: Update pipeline and update code node - package revision should updated
    Given I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    And I create a pipeline with code node
    Then I wait "4"
    And I pause pipeline in context
    And I wait "4"
    When I update pipeline code node
    Then I install pipeline in context
    And I wait "4"
    And I validate pipeline code-node service is with the correct version "1.0.1"


  @pipelines.delete
  @testrail-C4532574
  @DAT-46570
  Scenario: Update pipeline with code node not update the package revision
    Given I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    And I create a pipeline with code and task node
    Then I wait "4"
    And I pause pipeline in context
    And I wait "4"
    And I install pipeline in context
    And I validate pipeline code-node service is with the correct version "1.0.0"



================================================
File: tests/features/packages_repo/test_packages_context.feature
================================================
Feature: Packages repository Context testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create projects by the name of "project1 project2"
    And I set Project to Project 1
    And Directory "packages_get" is empty
    When I generate package by the name of "test-package" to "packages_get"

  @packages.delete
  @testrail-C4523140
  @DAT-46568
  Scenario: Get Package from the project it belong to
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_get | inputs=None | outputs=None |
    When I get the package from project number 1
    Then Package project_id is equal to project 1 id
    And Package project.id is equal to project 1 id

  @testrail-C4523140
  @DAT-46568
  Scenario: Get Package from the project it is not belong to
    When I push "first" package
      | codebase_id=None | package_name=test-package | src_path=packages_get | inputs=None | outputs=None |
    When I get the package from project number 2
    Then Package project_id is equal to project 1 id
    And Package project.id is equal to project 1 id


================================================
File: tests/features/pipeline_entity/pipeline_active_learning.feature
================================================
@rc_only
Feature: Pipeline active learning testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_active_learning"
    And I create a dataset named "Upload-data"
    And I Add dataset to context.datasets
    When I upload labels to dataset
    Given I create a dataset named "Ground-Truth"
    And I Add dataset to context.datasets
    When I upload labels to dataset

  @pipelines.delete
  @DAT-62553
  Scenario: Create an active learning pipeline flow - Pipeline created from json
    When I validate global app by the name "Active Learning" is installed
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "ac-lr-package" with entry point "main.py"
    And I create a model from package by the name of "ac-lr-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "ac-lr-model"
    And I get global dpk by name "active-learning"
    Given I create pipeline from json in path "pipelines_json/active_learning.json"
    When I install pipeline in context
    And I upload items in the path "images_numbers/1_50" to the dataset in index "0"
    And I validate all items is annotated in dataset in index "0"
    And I get task by pipeline task node
    Then I expect Task created with "50" items
    When I have box annotation_definition with params
      | key    | value    |
      | label  | "Number" |
      | top    | 177      |
      | left   | 172      |
      | bottom | 303      |
      | right  | 322      |
    And I update items annotations in task with context.annotation_definition
    And I update items status to default task actions
    And Dataset in index "1" have "50" items
    When I get last model in project
    Then model metadata should include operation "deploy" with filed "services" and length "1"
    When I execute pipeline with input type "None"
    Then I expect that pipeline execution has "6" success executions
    When I get last model in project
    Then model metadata should include operation "evaluate" with filed "datasets" and length "1"





================================================
File: tests/features/pipeline_entity/pipeline_context.feature
================================================
@bot.create
Feature: Pipeline contex testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_flow"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @DAT-45831
  Scenario: pipeline with context
    Given I have a custom "context" pipeline from json
    When I upload item in "0000000162.jpg" to pipe dataset
    And I wait for item to enter task
    And I update item status to "completed" with task id
    Then Context should have all required properties


  @pipelines.delete
  @services.delete
  @packages.delete
  @DAT-56555
  Scenario: Faas and Code node with trigger Item_status - Should get correct pipeline.context
    Given There is a package (pushed from "pipeline_faas_services/context") by the name of "node-context"
    When I deploy a service
      | service_name=node-context | package=node-context | revision=None | config=None | runtime=None |
    Given I create pipeline from json in path "pipelines_json/pipeline_context_trigger_item_status.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I get task by pipeline task node
    And I wait for item to enter task
    And I update item status to "completed" with task id
    Then Context should have all required properties

================================================
File: tests/features/pipeline_entity/pipeline_delete.feature
================================================
Feature: Pipeline repository get service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_delete"
        And Directory "pipeline_delete" is empty

    @testrail-C4523141
    @DAT-46572
    Scenario: Delete pipeline by name
        When I create a pipeline with name "pipelinedelete"
        When I delete a pipeline by the name of "pipelinedelete"
        Then There are no pipeline by the name of "pipelinedelete"

    @testrail-C4523141
    @DAT-46572
    Scenario: Delete pipeline by id
        When I create a pipeline with name "pipelinedeleteid"
        When I delete a pipeline by the id
        Then There are no pipeline by the name of "pipelinedeleteid"

    @testrail-C4523141
    @DAT-46572
    Scenario: Delete a non-existing pipeline
        When I try to delete a pipeline by the name of "SomepipelineName"
        Then "NotFound" exception should be raised


================================================
File: tests/features/pipeline_entity/pipeline_execute.feature
================================================
Feature: Pipeline entity method testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_flow"
    And I create a dataset with a random name
    And I upload item in "0000000162.jpg" to dataset



  @pipelines.delete
  @testrail-C4533385
  @DAT-46573
  Scenario: pipeline execute batch
    Given I upload item by the name of "test_item.jpg" to a remote path "test"
    And I upload item by the name of "test_item2.jpg" to a remote path "test"
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I create a pipeline with name "test_pipeline_execute"
    And I add a code node to the pipeline
    When I execute the pipeline batch items
    Then pipeline execution are success in "2" items

  @services.delete
  @packages.delete
  @testrail-C4533386
  @DAT-46573
  Scenario: service execute batch
    Given I upload item by the name of "test_item.jpg" to a remote path "test"
    And I upload item by the name of "test_item2.jpg" to a remote path "test"
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    Given There is a package (pushed from "services/item_with_init") by the name of "services-flow"
    When I deploy a service with init prams
    And I execute the service batch items
    Then service execution are success in "2" items

  @pipelines.delete
  @DAT-80459
  Scenario: Execute pipeline from a specific node
    Given I install a pipeline with 2 dataset nodes
    When I execute the second node which is not the root node
    Then Then pipeline should start from the requested node


================================================
File: tests/features/pipeline_entity/pipeline_flow.feature
================================================
Feature: Pipeline entity method testing

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_flow"
        And I create a dataset with a random name
        When I create a new plain recipe
        And I update dataset recipe to the new recipe


    @pipelines.delete
    @testrail-C4523142
    @DAT-46574
    Scenario: pipeline flow
        When I create a package and service to pipeline
        And I create a pipeline from json
        And I upload item in "0000000162.jpg" to pipe dataset
        Then verify pipeline flow result



================================================
File: tests/features/pipeline_entity/pipeline_get.feature
================================================
Feature: Pipeline entity method testing

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_get"
        And Directory "pipeline_get" is empty

    @pipelines.delete
    @testrail-C4523143
    @DAT-46575
    Scenario: To Json
        When I create a pipeline with name "testpipeline"
        Then Object "Pipeline" to_json() equals to Platform json.

    @pipelines.delete
    @testrail-C4523143
    @DAT-46575
    Scenario: get pipeline
        When I create a pipeline with name "testpipeline"
        And I get pipeline by the name of "testpipeline"
        Then I get a pipeline entity
        And It is equal to pipeline created



================================================
File: tests/features/pipeline_entity/pipeline_list.feature
================================================
Feature: Pipeline entity method testing

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_list"
        And Directory "pipeline_list" is empty

    @pipelines.delete
    @testrail-C4523144
    @DAT-46576
    Scenario: test list pipeline
        When i list a project pipelines i get "0"
        And I create a pipeline with name "testpipelinelist"
        And i list a project pipelines i get "1"


================================================
File: tests/features/pipeline_entity/pipeline_multiple_add_to_task.feature
================================================
Feature: Pipeline entity method testing recomplete items

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_recomplete"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @services.delete
  @DAT-44712
  Scenario: Same item enters a task node twice
    Given a pipeline with same item enters task twice
    When I upload item in "0000000162.jpg" to pipe dataset
    When I execute pipeline on item
    When I wait for item to enter task
    Then I update item status to "complete" with task id
    Then Cycle should be completed


================================================
File: tests/features/pipeline_entity/pipeline_node_input_handling.feature
================================================
Feature: Pipeline node input handling

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_flow"


    @DAT-54491
    @pipelines.delete
    Scenario: First node passes input and the second node function has default input in case of missing input
        When I build a pipeline where the second node handles missing input
        And I execute pipeline with input type: "String" and input value: "string_from_root"
        Then Pipeline has "1" cycle executions
        And Cycle completed with save "True"
        And Cycle "1" status is "success"
        And Cycle "1" node "2" execution "1" single output is: "string_from_root"

    @DAT-54490
    @pipelines.delete
    Scenario: First node passes input and the second node function has no default input in case of missing input
        When I build a pipeline where the second node does not handle missing input
        And I execute pipeline with input type: "String" and input value: "string_from_root"
        Then Pipeline has "1" cycle executions
        And Cycle completed with save "True"
        And Cycle "1" status is "success"
        And Cycle "1" node "2" execution "1" single output is: "string_from_root"

    @DAT-54491
    @pipelines.delete
    Scenario: First node does not pass input and the second node function has default input in case of missing input
        When I build a pipeline where the second node handles missing input
        And I execute pipeline with input type: "String" and input value: "return_none"
        Then Pipeline has "1" cycle executions
        And Cycle completed with save "True"
        And Cycle "1" status is "success"
        And Cycle "1" node "2" execution "1" single output is: "default_string"

    @DAT-54490
    @pipelines.delete
    Scenario: First node does not pass input and the second node function has no default input in case of missing input
        When I build a pipeline where the second node does not handle missing input
        And I execute pipeline with input type: "String" and input value: "return_none"
        Then Pipeline has "1" cycle executions
        And Cycle completed with save "True"
        And Cycle "1" status is "failed"



================================================
File: tests/features/pipeline_entity/pipeline_output_list.feature
================================================
Feature: Pipeline entity output list method testing

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_outputs"
        And I create a dataset with a random name
        When I create a new plain recipe
        And I update dataset recipe to the new recipe

    @pipelines.delete
    @testrail-C4528760
    @DAT-46578
    Scenario: pipeline task node get items list
        When There are "9" items
        And I create a pipeline with code and task node
        And I upload item in "0000000162.jpg" to pipe dataset
        Then verify pipeline output result of "10" items


================================================
File: tests/features/pipeline_entity/pipeline_recomplete_status.feature
================================================
Feature: Pipeline entity method testing recomplete items

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_recomplete"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4529141
  @DAT-46579
  Scenario: pipeline re-complete status for task
    When I create a pipeline dataset, task "annotation" and dataset nodes - repeatable "True"
    And I upload item in "0000000162.jpg" to pipe dataset
    Then I wait for item to enter task
    And I update item status to "complete" with task id
    Then I wait "20"
    And I remove specific "complete" from item with task id
    Then I wait "20"
    And I update item status to "complete" with task id
    And I expect that pipeline execution has "4" success executions


  @pipelines.delete
  @testrail-C4529141
  @DAT-46579
  Scenario: pipeline re-complete status for qa task
    When I create a pipeline dataset, task "qa" and dataset nodes - repeatable "True"
    And I upload item in "0000000162.jpg" to pipe dataset
    Then I wait for item to enter task
    And I update item status to "approve" with task id
    Then I wait "20"
    And I remove specific "approve" from item with task id
    Then I wait "20"
    And I update item status to "approve" with task id
    And I expect that pipeline execution has "4" success executions


  @pipelines.delete
  @testrail-C4529141
  @DAT-46579
  Scenario: pipeline re-complete status for repeatable false task
    When I create a pipeline dataset, task "annotation" and dataset nodes - repeatable "False"
    And I upload item in "0000000162.jpg" to pipe dataset
    Then I wait "20"
    And I update item status to "complete" with task id
    Then I wait "20"
    And I remove specific "complete" from item with task id
    Then I wait "20"
    And I update item status to "complete" with task id
    And I expect that pipeline execution has "3" success executions



================================================
File: tests/features/pipeline_entity/pipeline_refs.feature
================================================
Feature: Test pipeline refs

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline-refs"

  @DAT-64799
  @DAT-71435
  Scenario: Test full pipeline refs + cleanup
    When get global model package
    And I create a model with a random name
    Given model is trained
    And a service
    And a dpk with custom node
    And an app
    And pipeline with model, service, code and custom nodes
    When I install pipeline
    Then service should have pipeline refs and cannot be uninstall
    And model should have pipeline refs and cannot be deleted
    And code node package should have pipeline refs
    And code node service should have pipeline refs and cannot be uninstall
    And app should have pipeline refs and cannot be uninstall
    And dpk should have pipeline refs and cannot be deleted
    When I delete pipeline
    Then service should not have pipeline refs and uninstall service "True"
    And model should not have pipeline refs and delete model "True"
    And app should not have pipeline refs and uninstall app "True"
    And dpk should not have pipeline refs and delete dpk "True"

  @DAT-70831
  @pipelines.delete
  Scenario: Pipeline refs of services created in pipeline runtime
    Given I create a dataset with a random name
    When I upload labels to dataset
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model-trained" with status "trained" in index "0"
    And I create a model from package by the name of "test-model-created" with status "created" in index "1"
    And I publish a dpk to the platform
    And I install the app
    Given I have a pipeline with train node and evaluate node
    When I install pipeline
    When I execute pipeline without input
    Given I pause pipeline when executions are created
    When I install pipeline
    Then model services should still have refs


================================================
File: tests/features/pipeline_entity/pipeline_rerun_cycles_1.feature
================================================
Feature: Pipeline entity method testing - rerun cycle

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_rerun"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"
    When I build a pipeline with dynamic node status


  @pipelines.delete
  @testrail-C4536833
  @DAT-70139
  Scenario: Multiple outputs - rerun cycle
    Given I create pipeline from json in path "pipelines_json/output_integers_input_integer_1000.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    Then Pipeline has "1" cycle executions
    And I expect that pipeline execution has "1001" success executions
    When rerun the cycle from the beginning
    And I wait "20"
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun

  @pipelines.delete
  @DAT-70139
  Scenario: rerun pipeline cycle from the beginning root case
    When I execute pipeline on item
    Then Cycle completed with save "True"
    When rerun the cycle from the beginning
    Then Cycle completed with save "True"
    When rerun the cycle from the beginning
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun

#  @pipelines.delete
#  Scenario: rerun pipeline cycle from the beginning task
#    When I create a package and service to pipeline
#    And I create a pipeline from sdk
#    And I upload item in "0000000162.jpg" to pipe dataset
#    Then verify pipeline flow result
#    When rerun the cycle from the beginning
#    And I wait "20"
#    Then the pipeline cycle should be rerun
#    And Cycle status should be "in-progress"


  @pipelines.delete
  @DAT-70139
  Scenario: rerun pipeline cycle from executions
    When I execute pipeline on item
    Then Cycle completed with save "True"
    When rerun the cycle from the execution
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun

================================================
File: tests/features/pipeline_entity/pipeline_rerun_cycles_2.feature
================================================
Feature: Pipeline entity method testing - rerun cycle

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_rerun"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"
    When I build a pipeline with dynamic node status


  @pipelines.delete
  @DAT-70140
  Scenario: rerun pipeline cycle from the beginning root case
    When I execute pipeline on item
    Then Cycle completed with save "True"
    When rerun the cycle from the beginning
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun


  @pipelines.delete
  @DAT-70140
  Scenario: rerun pipeline cycle from the beginning trigger case
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "2.jpg"
    Then Cycle completed with save "True"
    When rerun the cycle from the beginning
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun


  @pipelines.delete
  @DAT-70140
  Scenario: rerun pipeline Execution
    When I execute pipeline on item
    Then Cycle completed with save "True"
    When rerun the cycle from the "2" node
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun

  @pipelines.delete
  @DAT-70140
  Scenario: rerun pipeline Execution from the failed node
    When I execute pipeline on item
    Then Cycle completed with save "True"
    When rerun the cycle from the failed node
    And I wait "20"
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun
    Then Cycle completed with save "True"
    When rerun the cycle from the failed node
    Then the pipeline cycle should not change


================================================
File: tests/features/pipeline_entity/pipeline_rerun_cycles_3.feature
================================================
Feature: Pipeline entity method testing - rerun cycle

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_rerun_3"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"


  @pipelines.delete
  @DAT-52397
  Scenario: Rerun pipeline with multiple inputs - Should have input cache
    Given I create pipeline from json in path "pipelines_json/rerun_multiple_inputs.json"
    And I install pipeline in context
    When I execute pipeline on item
    Then Cycle completed with save "True"
    When rerun the cycle from the "3" node
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun



================================================
File: tests/features/pipeline_entity/pipeline_rerun_cycles_4.feature
================================================
Feature: Pipeline entity method testing - rerun cycle

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_rerun_4"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"


  @pipelines.delete
  @DAT-52626
  Scenario: Rerun pipeline - with task node (start-node) should remove item status after rerun
    Given I create pipeline from json in path "pipelines_json/task_dataset_nodes.json"
    And I install pipeline in context
    When I execute pipeline on item
    And I get task by pipeline task node
    And I wait for item to enter task
    And I update item status to "completed" with task id
    Then Cycle completed with save "True"
    When rerun the cycle from the beginning
    And I wait for item status to be "completed" with action "deleted"
    And I update item status to "completed" with task id
    Then Cycle completed with save "False"
    Then the pipeline cycle should be rerun


  @pipelines.delete
  @DAT-52626
  Scenario: Rerun pipeline - with task node (last-node) should remove item status after rerun
    Given I create pipeline from json in path "pipelines_json/dataset_task_nodes.json"
    And I install pipeline in context
    When I execute pipeline on item
    And I get task by pipeline task node
    And I wait for item to enter task
    And I update item status to "completed" with task id
    Then Cycle completed with save "True"
    When rerun the cycle from the "2" node
    And I wait for item status to be "completed" with action "deleted"
    And I update item status to "completed" with task id
    Then Cycle completed with save "False"



================================================
File: tests/features/pipeline_entity/pipeline_sdk.feature
================================================
Feature: Pipeline entity method testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_flow"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @services.delete
  @testrail-C4525314
  @DAT-46580
  Scenario: pipeline flow
    When I create a package and service to pipeline
    And I create a pipeline from sdk
    And I upload item in "0000000162.jpg" to pipe dataset
    Then verify pipeline flow result
    Then I expect that pipeline execution has "3" success executions

  @pipelines.delete
  @testrail-C4525314
  @DAT-46580
  Scenario: pipeline flow pipeline trigger
    When I create a pipeline from sdk with pipeline trigger
    And I upload item in "0000000162.jpg" to pipe dataset
    Then I expect that pipeline execution has "2" success executions

  @pipelines.delete
  @testrail-C4525314
  @DAT-46580
  Scenario: pipeline delete use sdk
    Given I create pipeline from json in path "pipelines_json/pipeline_sanity.json"
    And I install pipeline in context
    When I remove node by the name "code" from pipeline
    And check pipeline nodes

  @pipelines.delete
  @testrail-C4533330
  @DAT-46580
  Scenario: pipeline with dataset task with new recipe
    When I create a new plain recipe
    When I create a pipeline with task node and new recipe
    Then I install pipeline in context


================================================
File: tests/features/pipeline_entity/pipeline_service_uninstall.feature
================================================
Feature: Pipeline service uninstall

  Background: Initiate Platform Interface
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_flow"
    And I create a dataset with a random name
    And I upload item by the name of "test_item.jpg" to a remote path "test"
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @DAT-56574
  @pipelines.delete
  Scenario: pipeline execute batch
    When I create a pipeline with name "test_pipeline_1"
    And I add a code node to the pipeline
    And I install pipeline
    And I pause service
    And I execute the pipeline batch items
    And I try to uninstall service
    Then "Unable to delete service" in error message
    When I terminate an execution
    And I pause pipeline in context
    And I uninstall service

================================================
File: tests/features/pipeline_entity/pipeline_start_node.feature
================================================
Feature: Pipeline entity method testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_flow"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4532486
  @DAT-46581
  Scenario: pipeline flow
    When I create a package and service to pipeline
    And I create a pipeline from sdk
    And I add a node and connect it to the start node
    Then New node is the start node

  @pipelines.delete
  @DAT-79613
  Scenario: Cron trigger on regular node (not start node)
    Given pipeline with 2 nodes
    And the node which is not the start node has a cron trigger
    When installing the pipeline
    Then the relevant node should be executed



================================================
File: tests/features/pipeline_entity/pipeline_tasks_loop.feature
================================================
Feature: Pipeline contex testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_flow"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @DAT-81338
  Scenario: pipeline with tasks that has loop - should create all tasks
    Given I create pipeline from json in path "pipelines_json/pipeline_tasks_loop.json"
    And I install pipeline in context
    When I list Tasks by param "project_ids" value "current_project"
    Then I receive a list of "2" tasks

================================================
File: tests/features/pipeline_entity/pipeline_update.feature
================================================
Feature: Pipeline update testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_update"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe

  @pipelines.delete
  @testrail-C4523145
  @DAT-46582
  Scenario: Update pipeline with description
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
    And I create "dataset" node with params
      | key      | value |
      | position | (2,2) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    And I pause pipeline in context
    And I update pipeline description
    Then Pipeline received equals Pipeline changed except for "description"
    Then "update_pipeline" has updatedBy field


  @pipelines.delete
  @DAT-52687
  Scenario: Try to update pipeline with dataset node input output loop - Should failed to update
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
    When I update node input output to infinite loop
    Then "Failed to update pipeline: Infinite Loop detected for node "random_dataset" in error message

  @pipelines.delete
  @DAT-52687
  Scenario: Try to update pipeline with node input output loop - Should failed to update
    Given I create pipeline with the name "pipeline"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    When I update node input output to infinite loop
    Then "Failed to update pipeline: Infinite Loop detected for node "codenode" in error message


  @pipelines.delete
  @DAT-52687
  Scenario: Try to create pipeline with code node input output loop - Should failed to update
    Given I create pipeline from json in path "pipelines_json/infinite_loop.json"
    Then "Failed to create pipeline: Infinite Loop detected for node "random_dataset" in error message

================================================
File: tests/features/pipeline_entity/pipline_reset.feature
================================================
Feature: Pipeline entity statistics reset testing

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_statistics_reset"
        And I create a dataset with a random name
        When I create a new plain recipe
        And I update dataset recipe to the new recipe

    @pipelines.delete
    @testrail-C4530417
    @DAT-46583
    Scenario: reset running pipeline without force
        When I create a package and service to pipeline
        And I create a pipeline from json
        And I upload item in "0000000162.jpg" to pipe dataset
        Then I try to reset statistics with stop_if_running "False"

    @pipelines.delete
    @testrail-C4530417
    @DAT-46583
    Scenario: reset running pipeline with force
        When I create a package and service to pipeline
        And I create a pipeline from json
        And I upload item in "0000000162.jpg" to pipe dataset
        Then I try to reset statistics with stop_if_running "True"


================================================
File: tests/features/pipeline_entity/test_pipeline_actions.feature
================================================
Feature: Pipeline entity method testing actions outputs

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_actions"
        And I create a dataset with a random name
        When I create a new plain recipe
        And I update dataset recipe to the new recipe


    @pipelines.delete
    @testrail-C4533387
    @DAT-48153
    Scenario: Pipeline with code node - Item should filtered by first output port
        When I create custom pipeline for code node with progress.update(action="first-output")
        And I upload item in "0000000162.jpg" to dataset
        Then I wait "5"
        And I expect that pipeline execution has "2" success executions
        And I pause pipeline in context


    @pipelines.delete
    @testrail-C4533387
    @DAT-48153
    Scenario: Pipeline with code node - Item should filtered by second output port
        When I create custom pipeline for code node with progress.update(action="second-output")
        And I upload item in "0000000162.jpg" to dataset
        Then I wait "5"
        And I expect that pipeline execution has "2" success executions
        And I pause pipeline in context


    @pipelines.delete
    @testrail-C4533387
    @DAT-48153
    Scenario: Pipeline with code node - Null input shouldn't failed the cycle pipeline
        Given I create pipeline from json in path "pipelines_json/variable_input_null.json"
        And I install pipeline in context
        And I upload item in "0000000162.jpg" to dataset
        When I execute pipeline with input type "Item"
        Then I wait "5"
        And I expect that pipeline execution has "2" success executions

================================================
File: tests/features/pipeline_entity/test_pipeline_app_refs.feature
================================================
Feature: Test app umbrella refs - Pipeline nodes

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline-refs"

  @DAT-71917
  @pipelines.delete
  Scenario: Update app with custom node scope node - Should update the node service to latest app version
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    When I install the app
    Given I create pipeline from json in path "pipelines_json/pipeline_scope_node.json"
    And I install pipeline in context
    When I get the pipeline service
    And I try get the "published_dpk" by id
    Then "service" has app scope
    When I try get the "published_dpk" by id
    And I set code path "update_metadata" to context
    And I pack directory by name "update_metadata"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And I get the pipeline service
    Then "service" has app scope

  @pipelines.delete
  Scenario: Update app with clone model nodes - Should update the nodes service to latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And I get the dpk by name
    And I set code path "model_temp_1" to context
    And I pack directory by name "model_temp_1"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And i fetch the model by the name "test-model"
    And I clone a model and set status "created"
    Given I create pipeline from json in path "pipelines_json/train_evaluate_nodes.json"
    And I install pipeline in context
    When I execute pipeline with input type "Model"
    Then I expect that pipeline execution has "2" success executions
    And I wait "4"
    And Dataset has a scores file


================================================
File: tests/features/pipeline_entity/test_pipeline_code_node.feature
================================================
@bot.create
Feature: Pipeline code node testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "code_node"
    And I create a dataset with a random name

  @pipelines.delete
  @DAT-51870
  Scenario: Pipeline - Update code node name and install - should installed successfully
    Given I create pipeline with the name "pipeline"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    And I pause pipeline in context
    And I update pipeline attributes with params
      | key                      | value  |
      | nodes[0].outputs[0].name | item_1 |
    And I get pipeline in context by id
    Then I validate pipeline attributes with params
      | key                      | value  |
      | nodes[0].outputs[0].name | item_1 |
    When I install pipeline in context
    Then Pipeline status is "Installed"

================================================
File: tests/features/pipeline_entity/test_pipeline_connectors.feature
================================================
Feature: Pipeline resource connectors testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_connectors"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe

  @pipelines.delete
  @testrail-C4533573
  @DAT-46585
  @DAT-52903
  Scenario: Code node with 2 outputs - Should pass the output to correct input
    When I create a pipeline with code nodes with 2 outputs and code node with 2 inputs
    And I upload item in "0000000162.jpg" to dataset
    Then I expect that pipeline execution has "2" success executions
    And I pause pipeline in context


================================================
File: tests/features/pipeline_entity/test_pipeline_dataset_node_trigger_to_pipeline.feature
================================================
Feature: Pipeline entity method testing - Dataset node data execution

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_dataset_node_data_execution"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"


  @pipelines.delete
  @DAT-70527
  Scenario: Successful dataset node data execution - without filter
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
      | load_existing_data | true |
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    Then The dataset node is marked as triggered
    And The dataset node is assigned with a command id
    And The pipeline has been executed "1" times
    And The uploaded item has "2" executions

  @pipelines.delete
  @DAT-80468
  Scenario: Successful dataset node data execution - without filter - with variable
    Given I have a pipeline with 2 dataset nodes with variables and triggerToPipeline=true
    When I install pipeline in context
    Then All dataset items should run


  @pipelines.delete
  @DAT-70527
  Scenario: Successful dataset node data execution - with filter
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
      | load_existing_data | true |
      | data_filters       | {"$and": [{"hidden": false}, {"type": "file"}]} |
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    Then The dataset node is marked as triggered
    And The dataset node has data_filters
    And The dataset node is assigned with a command id
    And The pipeline has been executed "1" times
    And The uploaded item has "2" executions


    @pipelines.delete
    @DAT-70527
    Scenario: Failed dataset node data execution - In-port connection error
      Given I create pipeline with the name "pipeline"
      And I create "code" node with params
        | key      | value |
        | position | (1,1) |
      And I create "dataset" node with params
        | key      | value |
        | position | (1,1) |
        | load_existing_data | true |
      And I create "code" node with params
        | key      | value |
        | position | (1,1) |
      When I add and connect all nodes in list to pipeline entity
      And I try to install pipeline in context
      Then I should get an exception error='400'
      And The pipeline has been executed "0" times
      And The uploaded item has "0" executions


  @pipelines.delete
  @DAT-70527
  Scenario: Successful dataset node data execution - Should execute only once
    Given I create pipeline with the name "pipeline"
    And I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
      | load_existing_data | true |
    And I create "code" node with params
      | key      | value |
      | position | (2,2) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    Then The dataset node is marked as triggered
    And The dataset node is assigned with a command id
    And The pipeline has been executed "1" times
    And The uploaded item has "2" executions
    When I pause pipeline in context
    And I install pipeline in context
    Then The pipeline has been executed "1" times
    And The uploaded item has "2" executions

================================================
File: tests/features/pipeline_entity/test_pipeline_evalaute_node.feature
================================================
Feature: Pipeline evaluate node Testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_evaluate_node"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item

  @pipelines.delete
  @DAT-56168
  Scenario: Create pipeline with evaluate node - Execution should success
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "package-evaluate-node" with entry point "main.py"
    And I create a model from package by the name of "model-evaluate-node" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "model-evaluate-node"
    When i "deploy" the model
    Then model status should be "deployed" with execution "False" that has function "None"
    Given I create pipeline from json in path "pipelines_json/pipeline_evaluate_node.json"
    And I install pipeline in context
    When I execute pipeline with input type "None"
    Then I expect that pipeline execution has "1" success executions
    And I wait "4"
    And Dataset has a scores file

================================================
File: tests/features/pipeline_entity/test_pipeline_filters.feature
================================================
Feature: Pipeline resource testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_resource"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe

  @pipelines.delete
  @testrail-C4528946
  @DAT-46586
  Scenario: Filter pipeline connector with dataset resource and dataset id values
    When I create filters
    And I add "dataset" filter with "id" and "dataset.id"
    And I create a pipeline with dataset resources
    And I upload item in "0000000162.jpg" to dataset
    Then I expect that pipeline execution has "2" success executions
    And I pause pipeline in context


  @pipelines.delete
  @testrail-C4528946
  @DAT-46586
  Scenario: Filter pipeline connector with dataset resource and random values
    When I create filters
    And I add "dataset" filter with "id" and "123456"
    And I create a pipeline with dataset resources
    And I upload item in "0000000162.jpg" to dataset
    Then I expect that pipeline execution has "1" success executions
    And I pause pipeline in context



================================================
File: tests/features/pipeline_entity/test_pipeline_install.feature
================================================
@qa-nightly
Feature: Pipeline entity method testing re-installed pipeline

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_install"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4533549
  @DAT-46587
  Scenario: Remove code node from pipeline should be able to install again
    When I create a pipeline with code node
    And I pause pipeline in context
    And I delete current nodes and add dataset nodes to pipeline
    Then Pipeline status is "Installed"





================================================
File: tests/features/pipeline_entity/test_pipeline_many_to_one.feature
================================================
Feature: Pipeline resource multiples outputs testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_limit_many_to_one"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4536833
  @DAT-70138
  Scenario: Multiple outputs - Limit many to one - 1000 execution - Cycle status should get success
    Given I create pipeline from json in path "pipelines_json/output_integers_input_integer_1000.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    Then Pipeline has "1" cycle executions
    And I expect that pipeline execution has "1001" success executions


  @pipelines.delete
  @testrail-C4536833
  @DAT-70138
  Scenario: Multiple outputs - Limit many to one - 1001 execution - Cycle status should get failed
    Given I create pipeline from json in path "pipelines_json/output_integers_input_integer_1001.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    When I get pipeline execution in index "0"
    Then Pipeline has "1" cycle executions
    And I validate Cycle execution status is "failed"



================================================
File: tests/features/pipeline_entity/test_pipeline_ml_node.feature
================================================
@bot.create
Feature: Pipeline ml node testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "ml_node"
    And I create a dataset with a random name

  @pipelines.delete
  @DAT-54797
  Scenario: Pipeline - ml predict node updated variable
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel" with entry point "main.py"
    And I create a model from package by the name of "test-model" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model"
    Given Pipeline which have a model variable and predict ml node that reference to this model variable file "ml_predict.json"
    When I install pipeline in context
    Then i have a model service
    When I clone the model
    And i fetch the model by the name "clone_test-model"
    And I update the model variable in pipeline to reference to this model
    Then I wait "3"
    And the model service id updated

  @pipelines.delete
  @services.delete
  @DAT-55427
  Scenario: Pipeline - ml predict node updated variable with old deploy model
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "dummymodel-1" with entry point "main.py"
    And I create a model from package by the name of "test-model-1" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "test-model-1"
    Given Pipeline which have a model variable and predict ml node that reference to this model variable file "ml_predict.json"
    When I install pipeline in context
    Then i have a model service
    When i "deploy" the model
    When I clone the model
    And i fetch the model by the name "clone_test-model-1"
    And I update the model variable in pipeline to reference to this model
    Then I wait "3"
    Then i have a model service
    Then models with the names "test-model-1,test-model-2" status "deployed"

================================================
File: tests/features/pipeline_entity/test_pipeline_model_compute_configs.feature
================================================
Feature: Test pipeline models compute configs

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline-compute-configs"
    And I create a dataset with a random name
    When I upload labels to dataset


  @DAT-66085
  @pipelines.delete
  Scenario: Test pipeline models compute configs - base case
    Given a dpk with models and compute configs
    And an app
    And pipeline with train model
    And models are set in context
    When I install pipeline
    And I execute pipeline on model with compute configs
    Then service should use model train compute config
    When I execute pipeline on model with config in module
    Then service should use model module compute config
    When I execute pipeline on model with config in function
    Then service should use model function compute config


================================================
File: tests/features/pipeline_entity/test_pipeline_multiple_inputs.feature
================================================
Feature: Pipeline resource multiples inputs testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_multiple_inputs"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4536827
  @DAT-46588
  Scenario: Multiple inputs - Input with connector and input with default param - Should use the default param
    Given I create pipeline from json in path "pipelines_json/inputs_default_and_connector.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    Then Pipeline has "1" cycle executions
    And I expect that pipeline execution has "2" success executions
    And I validate pipeline executions params
      | key   | value                                                                        |
      | input | {"item": {"item_id": "item.id"}, "dataset_id": "dataset.id", "folder": None} |
      | input | {"item": {"item_id": "item.id"}, "text" : "Hello World"}                     |

  @pipelines.delete
  @testrail-C4536827
  @DAT-46588
  Scenario: Multiple inputs - Input with connector and input with connector - Should wait for two inputs
    Given I create pipeline from json in path "pipelines_json/inputs_wait_for_two.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    Then Pipeline has "1" cycle executions
    And I expect that pipeline execution has "4" success executions
    And I validate pipeline executions params
      | key   | value                                                                        |
      | input | {"item": {"item_id": "item.id"}, "dataset_id": "dataset.id", "folder": None} |
      | input | {"item": {"item_id": "item.id"}, "dataset_id": "dataset.id", "folder": None} |
      | input | {"item": {"item_id": "item.id"}, "dataset_id": "dataset.id", "folder": None} |
      | input | {"item": {"item_id": "item.id"}, "item_1": {"item_id": "item.id"}}           |




================================================
File: tests/features/pipeline_entity/test_pipeline_multiple_inputs_list.feature
================================================
Feature: Pipeline resource multiples inputs testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_multiple_inputs_list"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4536827
  @DAT-46588
  Scenario: Multiple inputs - Input with connector and input-list with connector - Should duplicate executions according to list
    Given I create pipeline from json in path "pipelines_json/inputs_item_and_annotations.json"
    And I install pipeline in context
    And Item in path "assets_split/annotations_upload/0000000162.jpg" is uploaded to "Dataset"
    When Item is annotated with annotations in file: "assets_split/annotations_upload/annotations_new.json"
    And I execute pipeline with input type "Item"
    Then Pipeline has "1" cycle executions
    And I expect that pipeline execution has "9" success executions


  @pipelines.delete
  @testrail-C4536827
  @DAT-46588
  Scenario: Multiple inputs - Input-list with connector and input-list with connector - with same length - Should create one execution
    Given There are "4" items
    And I create pipeline from json in path "pipelines_json/inputs_items_list_same_size.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    Then Pipeline has "1" cycle executions
    And I expect that pipeline execution has "6" success executions


  @pipelines.delete
  @testrail-C4536827
  @DAT-46588
  Scenario: Multiple inputs - Input-list with connector and input-list with connector - with different length - Should failed
    Given There are "4" items
    And I create pipeline from json in path "pipelines_json/inputs_items_list_different_size.json"
    And I install pipeline in context
    When I upload item in "0000000162.jpg" to pipe dataset
    And I execute pipeline with input type "Item"
    And I get pipeline execution in index "0"
    Then I validate Cycle execution status is "failed"



================================================
File: tests/features/pipeline_entity/test_pipeline_predict_node.feature
================================================
Feature: Pipeline predict node Testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_predict_node"
    And I create a dataset with a random name
    When I upload labels to dataset

  @pipelines.delete
  @DAT-56147
  Scenario: Create pipeline with predict node - Execution should success
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "package-predict-node" with entry point "main.py"
    And I create a model from package by the name of "model-predict-node" with status "trained" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "model-predict-node"
    Given I create pipeline from json in path "pipelines_json/pipeline_predict_node.json"
    And I install pipeline in context
    And Item in path "0000000162.jpg" is uploaded to "Dataset"
    When I execute pipeline on item
    Then I expect that pipeline execution has "1" success executions
    And I validate item is annotated


================================================
File: tests/features/pipeline_entity/test_pipeline_pulling_task.feature
================================================
Feature: Pipeline pulling task testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_resource"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe


  @pipelines.delete
  @testrail-C4532851
  @DAT-46589
  Scenario: Create pipeline with pulling task - receive the correct task params
    When I create pipeline with pulling task with type "annotation" node and dataset node
      | key                | value |
      | batch_size         | 5     |
      | max_batch_workload | 7     |
      | priority           | LOW   |
    And I get task by pipeline task node
    Then I validate pulling task created equal to pipeline task node


  @pipelines.delete
  @testrail-C4532851
  @DAT-46589
  Scenario: Create pipeline with pulling qa-task - receive the correct qa-task params
    When I create pipeline with pulling task with type "qa" node and dataset node
      | key                | value |
      | batch_size         | 5     |
      | max_batch_workload | 7     |
      | priority           | LOW   |
    And I get task by pipeline task node
    Then I validate pulling task created equal to pipeline task node


  @pipelines.delete
  @testrail-C4532851
  @DAT-46589
  Scenario: Create pipeline with wrong pulling task params - Should rise the correct error
    When I create pipeline with pulling task with type "annotation" node and dataset node
      | key                | value |
      | batch_size         | 10    |
      | max_batch_workload | 7     |
      | priority           | LOW   |
    Then I expect pipeline error to be "Failed to create a task"

  @pipelines.delete
  @testrail-C4532851
  @DAT-46589
  Scenario: Create pipeline with pulling task - receive the correct task params
    When I create pipeline with pulling task with type "annotation" node and dataset node
      | key                | value |
      | batch_size         | 5     |
      | max_batch_workload | 7     |
      | priority           | LOW   |
    And I get task by pipeline task node
    Given There are "10" items
    Then I wait "45"
    When I get Task items by "name"
    Then I receive task items list of "10" items
    And I receive a list of "5" items for each assignment




================================================
File: tests/features/pipeline_entity/test_pipeline_sdk_sanity.feature
================================================
# Feature: Pipeline entity method testing

#  Background: Initiate Platform Interface and create a pipeline
#    Given Platform Interface is initialized as dlp and Environment is set according to git branch
#    And I create a project by the name of "test_pipeline_sanity"
#    And I create a dataset with a random name
#    When I create a new plain recipe
#    And I update dataset recipe to the new recipe
#    Then Add Members "annotator1@dataloop.ai"
#
#  @pipelines.delete
#  @testrail-C4525314
#    @DAT-46590
#  Scenario: pipeline sanity all nodes type
#    When I create a package and service to pipeline
#    And I create a pipeline from pipeline-sanity
#    And There are "20" items
#    Then I wait "10"
#    When I update items status to custom task actions "fix-label" "fix-ann" "fix-cor"
#    Then I wait "7"
#    When I update items status to default task actions
#    Then I wait "7"
#    When I update items status to default qa task actions
#    Then I wait "7"
#    Then verify pipeline sanity result
#
#  @pipelines.delete
##  @testrail-C4525314
#    @DAT-46590
#  Scenario: pipeline delete use sdk
#    When I create a package and service to pipeline
#    And I create a pipeline from json
#    And I update the pipeline nodes
#    And check pipeline nodes


================================================
File: tests/features/pipeline_entity/test_pipeline_secrets.feature
================================================
@bot.create
Feature: Pipeline secrets

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_secret"
    And I create a dataset with a random name

  @pipelines.delete
  @DAT-74670
  Scenario: Pipeline secret should saved on the service code node
    Given I create a dataset with a random name
    And I create pipeline with the name "pipeline"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    And I create "key_value" integration with name "pipeline_secret"
    When I add integration to pipeline secrets and update pipeline
    And I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    And I get service in index "0"
    Then Integration display on service secrets

================================================
File: tests/features/pipeline_entity/test_pipeline_task_node.feature
================================================
Feature: Pipeline entity method testing - Task node execution

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_task_execute_twice"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"


  @pipelines.delete
  @DAT-52654
  Scenario: Execute task node twice with the same item - Should remove the assignment status
    Given I create pipeline with the name "pipeline"
    And I create "task" node with params
      | key      | value |
      | position | (1,1) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    When I execute pipeline on item
    And I get task by pipeline task node
    And I wait for item to enter task
    And I update item status to "complete" with task id
    And I execute pipeline on item
    Then I wait for item status to be "complete" with action "deleted"


  @pipelines.delete
  @DAT-63245
  Scenario: Create task node with item and delete node shouldn't delete task
    Given I create pipeline with the name "pipeline"
    And I create "task" node with params
      | key      | value |
      | position | (1,1) |
    When I add and connect all nodes in list to pipeline entity
    And I install pipeline in context
    And I execute pipeline on item
    And I get task by pipeline task node
    And I wait for item to enter task
    And I pause pipeline in context
    And I delete all nodes
    And I wait "8"
    Then I get task by pipeline task node


  @DAT-73857
  @pipelines.delete
  Scenario: Pipeline with many to one task node
    Given I create a dataset with a random name
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "1.jpg"
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "2.jpg"
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "3.jpg"
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "4.jpg"
    When I upload the file in path "assets_split/items_upload/0000000162.jpg" with remote name "5.jpg"
    Given a pipeline with task node that receives many to one input
    Given I install pipeline in context
    Given I execute the pipeline on item
    When I set status on some of the input items
    Then cycle should be inProgress and task node should be inProgress
    When I set status on all input items
    Then cycle should be completed and task node should be completed

================================================
File: tests/features/pipeline_entity/test_pipeline_train_node.feature
================================================
Feature: Pipeline train node Testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_train_node"
    And I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item


  @pipelines.delete
  @DAT-55921
  Scenario: Create pipeline with train node - Execution should success
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "package-train-node" with entry point "main.py"
    And I create a model from package by the name of "model-train-node" with status "created" in index "0"
    And I publish a dpk to the platform
    And I install the app
    And i fetch the model by the name "model-train-node"
    Given I create pipeline from json in path "pipelines_json/pipeline_train_node.json"
    And I install pipeline in context
    When I execute pipeline with input type "None"
    Then I expect that pipeline execution has "1" success executions



================================================
File: tests/features/pipeline_entity/test_pipeline_update_view_mode.feature
================================================
Feature: Test app umbrella refs - Pipeline nodes

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline-refs"

  @DAT-84467
  @pipelines.delete
  Scenario: Update app with custom node scope node - Should update the node service to latest app version
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel" with status "trained"
    When I install the app
    And i fetch the model by the name "test-model"
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    When I install the app
    Given I create pipeline from json in path "pipelines_json/pipeline_scope_node.json"
    When I add a code node to the pipeline
    And I add a predict node to the pipeline
    And i update runnerImage "python:3.10" to pipeline node with type "code"
    And i update runnerImage "python:3.10" to pipeline node with type "custom"
    And i update runnerImage "python:3.10" to pipeline node with type "ml"
    And I install pipeline in context
    And i get the service for the pipeline node with type "code"
    Then service runnerImage is "python:3.10"
    When i get the service for the pipeline node with type "custom"
    Then service runnerImage is "python:3.10"
    When i get the service for the pipeline node with type "ml"
    Then service runnerImage is "python:3.10"
    When i update runnerImage "python:3.11" to pipeline node with type "code"
    And i get the service for the pipeline node with type "code"
    Then service runnerImage is "python:3.11"
    When i update runnerImage "python:3.11" to pipeline node with type "custom"
    And i get the service for the pipeline node with type "custom"
    Then service runnerImage is "python:3.11"
    When i update runnerImage "python:3.11" to pipeline node with type "ml"
    And i get the service for the pipeline node with type "ml"
    Then service runnerImage is "python:3.11"

================================================
File: tests/features/pipeline_entity/test_pipeline_validation.feature
================================================
Feature: Pipeline entity method testing - rerun cycle

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "pipeline_validation"


  @pipelines.delete
  @DAT-54410
  Scenario: Pipeline validation - Input output resource missmatch - Should failed to update
    Given I create pipeline with the name "input-output"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    And I create "code" node with params
      | key                | value   |
      | position           | (2,2)   |
      | input_type         | Dataset |
      | input_name         | dataset |
      | input_display_name | dataset |
    When I add and connect all nodes in list to pipeline entity
    Then I receive error with status code "400"
    And "Invalid input specified, Failed to update pipeline: connection source and target ports must be of the same type" in error message


  @pipelines.delete
  @DAT-54411
  Scenario: Pipeline validation - Output without action - port with action - Should failed to update
    Given I create pipeline with the name "output-action"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    And I create "code" node with params
      | key      | value |
      | position | (2,2) |
    When I add and connect all nodes in list to pipeline entity
    And I add action "test" to connection in index "0"
    Then I receive error with status code "400"
    And "Invalid input specified, Cannot read properties of undefined (reading 'includes')" in error message

  @pipelines.delete
  @DAT-54412
  Scenario: Pipeline validation - Try to add another start node - Should failed to update
    Given I create pipeline with the name "multiple-start"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    And I create "code" node with params
      | key      | value |
      | position | (2,2) |
    When I add all nodes in list to pipeline entity
    When I update node in index "1" to start node
    Then I receive error with status code "400"
    And "Invalid input specified, Pipeline may only include 1 root node" in error message


  @pipelines.delete
  @DAT-54772
  Scenario: Pipeline validation - At least one node with wrong validation - Should failed to install
    Given I create pipeline with the name "at-least-one"
    And I create a dataset with a random name
    When I create a new plain recipe
    And I update dataset recipe to the new recipe
    Given I create "dataset" node with params
      | key      | value |
      | position | (1,1) |
    And I create "task" node with params
      | key      | value |
      | position | (2,2) |
    And I create "dataset" node with params
      | key      | value |
      | position | (3,3) |
    When I update pipeline context.node "dataset_id" with "None"
    And I add and connect all nodes in list to pipeline entity
    And I try to install pipeline in context
    Then I receive error with status code "400"
    And "must have a datasetId" in error message

================================================
File: tests/features/pipeline_resume/pipeline_resume.feature
================================================
Feature: Resuming Pipeline

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_flow"
        And I create a dataset with a random name

    @services.delete
    @pipelines.delete
    @DAT-85194
    Scenario: Resuming pipeline
        Given I create a package and service to pipeline
        And I have a resumable pipeline
        And I install pipeline in context
        And Faas node service is paused
        And Item in path "assets_split/annotations_crud/0000000162.jpg" is uploaded to "Dataset"
        And Item reached task node
        When I pause pipeline in context
        And I complete item in task
        And I wait "10"
        Then Next nodes should not be executed
        When I resume pipeline with resume option "resumeExistingCycles"
        Then Next nodes should be executed
        When I pause pipeline in context
        And I wait "5"
        Given Faas node execution is in queue
        When Faas node service is resumed
        And Faas node service has completed
        And I resume pipeline with resume option "resumeExistingCycles"
        Then Item proceeded to next node

================================================
File: tests/features/platform_urls/test_platform_urls.feature
================================================
Feature: Platform Urls Tests

    Background: Background name
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "platform_urls"
        And I create a dataset with a random name
        And Item in path "0000000162.jpg" is uploaded to "Dataset"

    @testrail-C4532781
    @DAT-46592
    Scenario: Check project platform_url
        Then  I validate the platform url "project.platform_url" works

    @testrail-C4532781
    @DAT-46592
    Scenario: Check dataset platform_url
        Then  I validate the platform url "dataset.platform_url" works
        Then  I validate the platform url "project.dataset.platform_url" works

    @testrail-C4532781
    @DAT-46592
    Scenario: Check item platform_url
        Then  I validate the platform url "item.platform_url" works
        Then  I validate the platform url "dataset.item.platform_url" works
        Then  I validate the platform url "project.dataset.item.platform_url" works


================================================
File: tests/features/project_entity/test_project_repo_methods.feature
================================================
Feature: Project Entity repo services

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "project_repo_methods"

    @testrail-C4523150
    @DAT-46598
    Scenario: Delete project
        When I delete a project Entity
        Then Project entity was deleted

    @testrail-C4523150
    @DAT-46598
    Scenario: Update projects name
        When I change project name to "new_project_name"
        Then Project in host has name "new_project_name"

    @testrail-C4523150
    @DAT-46598
    Scenario: To Json
        When I reclaim project
        Then Object "Project" to_json() equals to Platform json.





================================================
File: tests/features/projects_repo/test_project_integrations.feature
================================================
@qa-nightly
@rc_only
Feature: Projects repository create integration testing

  Background: Background name
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "integration-key_value"

  @testrail-C4532976
  @DAT-46597
  Scenario: Create key value integration in project
    Given I create "key_value" integration with name "sdk_automation_key_value"
    Then I validate integration with the name "sdk_automation_key_value" is created

  @testrail-C4532976
  @DAT-46597
  Scenario: Delete key value integration in project
    Given I create "key_value" integration with name "sdk_automation_to_delete"
    When I delete integration in context
    Then I validate integration not in integrations list by context.integration_name

  @DAT-76494
  Scenario: Create key value integration in project with valid key-value metadata
    When I create "key_value" integration with name "sdk_automation_key_value_valid" and metadata "{'provider': 'test_provider', 'ref': 'test_ref'}"
    Then I validate integration with the name "sdk_automation_key_value_valid" is created

  @DAT-76495
  Scenario: Create key value integration in project with invalid key-value metadata
    When I create "key_value" integration with name "sdk_automation_key_value_invalid" and metadata "{'hello': 'world', 'key': 'key'}"
    Then I receive error with status code "400"
    And "Invalid metadata fields for key-value integration" in error message

  @DAT-76497
  Scenario: Create gcp-cross integration in project with key-value metadata
    When I create "gcp-cross" integration with name "sdk_automation_gcp_cross_invalid" and metadata "{'provider': 'test_provider', 'ref': 'test_ref'}"
    Then I receive error with status code "400"
    And "Invalid input specified" in error message


================================================
File: tests/features/projects_repo/test_projects_create.feature
================================================
@qa-nightly
Feature: Projects repository create service testing

    Background: Background name
        Given Platform Interface is initialized as dlp and Environment is set according to git branch

    @testrail-C4523146
    @DAT-46593
    Scenario: Create project with legal name
        When I create a project by the name of "to-delete-test-project_create"
        Then Project object by the name of "to-delete-test-project_create" should be created
        And Project should exist in host by the name of "to-delete-test-project_create"


    @testrail-C4523146
    @DAT-46593
    Scenario: Create project with an existing project name
        When I try to create a project by the name of "to-delete-test-project_create_same_name"
        When I try to create a project by the name of "to-delete-test-project_create_same_name"
        Then "BadRequest" exception should be raised
        And Error message includes "Failed to create project"


    @testrail-C4523146
    @DAT-46593
    Scenario: Create project with illegal special characters
        When I try to create a project by the name of "¤¶§!#$%%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}~ÇüéâäàåçêëèïîìæÆôöòûùÿ¢£¥PƒáíóóúñÑ¿¬½¼¡«»¦ßµ±°•·²€„…†‡ˆ‰Š‹Œ‘’“”–—˜™š›œŸ¨©®¯³´¸¹¾ÀÁÂÃÄÅÈÉÊËÌÍÎÏÐÒÓÔÕÖ×ØÙÚÛÜÝÞãðõ÷øüýþ"
        Then "BadRequest" exception should be raised


    @testrail-C4523146
    @DAT-46593
    Scenario: Create project with legal special characters
        When I create a project by the name of "ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*()_+-[]|.,"
        Then Project object by the name of "ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*()_+-[]|.," should be created
        And Project should exist in host by the name of "ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*()_+-[]|.,"



================================================
File: tests/features/projects_repo/test_projects_delete.feature
================================================
@qa-nightly
Feature: Projects repository get service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch

    @testrail-C4523147
    @DAT-46594
    Scenario: Delete project by name
        Given I create a project by the name of "project_delete"
        When I delete a project by the name of "project_delete"
        Then There are no projects by the name of "project_delete"

    @testrail-C4523147
    @DAT-46594
    Scenario: Delete project by id
        When I create a project by the name of "project_delete_id"
        When I delete a project by the id of "project_delete_id"
        Then There are no projects by the name of "project_delete_id"

    @testrail-C4523147
    @DAT-46594
    Scenario: Delete a non-existing project
        When I try to delete a project by the name of "Some Project Name"
        Then "NotFound" exception should be raised


================================================
File: tests/features/projects_repo/test_projects_get.feature
================================================
@qa-nightly
Feature: Projects repository get service testing

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch

    @testrail-C4523148
    @DAT-46595
    Scenario: Get an existing project by name
        Given I create a project by the name of "projects_get"
        When I get a project by the name of "projects_get"
        Then I get a project by the name of "projects_get"
        And The project I got is equal to the one created

   @testrail-C4523148
   @DAT-46595
   Scenario: Get an existing project by id
       Given I create a project by the name of "projects_get"
       When I get a project by the id of Project
       Then I get a project by the name of "projects_get"
       And The project I got is equal to the one created

   @testrail-C4523148
   @DAT-46595
   Scenario: Get non-existing project by name
       When I try to get a project by the name of "some project"
       Then "NotFound" exception should be raised

   @testrail-C4523148
   @DAT-46595
   Scenario: Get non-existing project by id
       When I try to get a project by id
       Then "NotFound" exception should be raised



================================================
File: tests/features/projects_repo/test_projects_list.feature
================================================
@qa-nightly
Feature: Projects repository list service testing

    Background: Background name
        Given Platform Interface is initialized as dlp and Environment is set according to git branch

    @testrail-C4523149
    @DAT-46596
    Scenario: List all projects when projects exist
        Given I create a project by the name of "projects_list"
        When I list all projects
        Then The project in the projects list equals the project I created



================================================
File: tests/features/recipe_entity/test_recipe_repo_methods.feature
================================================
Feature: Recipes entity methods testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "recipes_entity_methods"
        And I create a dataset with a random name

    @testrail-C4523155
    @DAT-46603
    Scenario: To Json
        Given Dataset has Recipes
        Then Object "Recipe" to_json() equals to Platform json.

    @testrail-C4523155
    @DAT-46603
    Scenario: add instruction
        Given Dataset has Recipes
        Then Add instruction "sample.pdf" to Recipe
        And instruction are exist

    @testrail-C4523155
    @DAT-46603
    Scenario: add instruction fail
        Given Dataset has Recipes
        Then Add instruction "0000000162.jpg" to Recipe
        And "BadRequest" exception should be raised


================================================
File: tests/features/recipes_repo/test_recipes_clone.feature
================================================
Feature: Recipes repository clone service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "project1 project2"
        And I create datasets by the name of "dataset1 dataset2"
        And I set Dataset to Dataset 1
        And I set dataset recipe and ontology to context
        When I add new label "111" to dataset 1
        And I add new label "222" to dataset 1


    @testrail-C4523151
    @DAT-46599
    Scenario: Clone recipe and Ontology
        When I clone recipe from  dataset 1 to dataset 2 with ontology
        And I add new label "333" to dataset 2
        Then I verify that Dataset 1 has 2 labels
        And I verify that Dataset 2 has 3 labels


    @testrail-C4523151
    @DAT-46599
    Scenario: Clone recipe without Ontology
        When I clone recipe from  dataset 1 to dataset 2 without ontology
        And I add new label "333" to dataset 2
        Then I verify that Dataset 1 has 3 labels
        And I verify that Dataset 2 has 3 labels


================================================
File: tests/features/recipes_repo/test_recipes_create.feature
================================================
@qa-nightly
Feature: Recipes repository create service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "Project_test_recipes_create"
        And I create a dataset with a random name

    @testrail-C4523152
    @DAT-46600
    Scenario: Create new recipe - plain
        When I create a new plain recipe
        And I update dataset recipe to the new recipe
        Then Dataset recipe in host equals the one created

    @testrail-C4523152
    @DAT-46600
    Scenario: Create new recipe - labels and attributes
        When I create a new recipe with param labels from "labels.json" and attributes: "attr1", "attr2"
        And I update dataset recipe to the new recipe
        Then Dataset recipe in host equals the one created
        And Dataset ontology in host has labels from "labels.json" and attributes: "attr1", "attr2"

    @testrail-C4523152
    @DAT-46600
    Scenario: Create new recipe - with existing ontology id
        When I create a new plain recipe with existing ontology id
        And I update dataset recipe to the new recipe
        Then Dataset recipe in host equals the one created

    @DAT-46014
    Scenario: Create new recipe with the checkout project
        When I checkout project
        And I create a new project recipe
        Then recipe in host is exist

    @DAT-68612
    Scenario: list recipes
        When I create a new plain recipe with name "#recipe1"
        And I query recipes by "title" "#recipe1"
        Then recipe in host is exist

================================================
File: tests/features/recipes_repo/test_recipes_delete.feature
================================================
Feature: Recipes repository Delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "recipes_delete"
        And I create a dataset with a random name

    @testrail-C4523153
    @DAT-46601
    Scenario: Delete recipe
        Given Dataset has Recipes
        When I delete recipe
        Then Dataset has no recipes




================================================
File: tests/features/recipes_repo/test_recipes_edit.feature
================================================
Feature: Recipes repository Update service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "recipes_edit"
        And I create a dataset with a random name

    @testrail-C4523154
    @DAT-46602
    Scenario: Update recipe
        When I update recipe
        Then Recipe in host equals recipe edited




================================================
File: tests/features/service_driver_repo/test_service_driver.feature
================================================
Feature: Service driver testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service_driver_test"


  @DAT-86911
  @restore_json_file
  @compute_serviceDriver.delete
  Scenario: Create compute - Service driver should created before bootstrap
    Given I fetch the compute from "computes/base_compute.json" file and update compute with params "True"
      | key           | value |
      | config.status | ready |
    When I try to create the compute from context.original_path
    Then I validate compute service driver is "created"

  @DAT-87190
  @restore_json_file
  @compute_serviceDriver.delete
  Scenario: Create compute - Service driver should archived if compute is archived
    Given I fetch the compute from "computes/base_compute.json" file and update compute with params "True"
      | key           | value |
      | config.status | None  |
    When I try to create the compute from context.original_path
    And I get compute from the compute list by the name with archived "True"
    Then I validate compute service driver is "archived"
    And I get archived service driver

================================================
File: tests/features/services_entity/test_service_debug_mode.feature
================================================
@bot.create
Feature: Service entity debug mode

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-debug"
    And There is a package (pushed from "services/items") by the name of "services-debug"

  @services.delete
  @packages.delete
  @testrail-C4533885
  @DAT-46606
  Scenario: Update service to debug mode - Should return <html> in response and executions should success
    Given I create a dataset with a random name
    And There are "5" items
    When I create a service
      | service_name=services-debug | package=services-debug | revision=None | config=None | runtime=None |
    And I send "post" gen_request with "debug_p" params
    And I send "post" gen_request with "debug_a" params
    Then I validate service has "1" instance up
    And I validate gen_request "get" with "debug_check" params and interval: "20" tries: "5" response "true"
    Then I call service.execute() on items in dataset
    When I send "post" gen_request with "terminate" params
    Then Execution was executed and finished with status "success"


  @services.delete
  @packages.delete
  @testrail-C4533885
  @DAT-46606
  Scenario: Update service to regular mode - Should remove the replica
    When I create a service
      | service_name=services-debug | package=services-debug | revision=None | config=None | runtime=None |
    And I send "post" gen_request with "debug_p" params
    And I send "post" gen_request with "debug_a" params
    Then I validate service has "1" instance up
    When I send "post" gen_request with "terminate" params
    Then I validate service has "0" instance up


================================================
File: tests/features/services_entity/test_service_debug_runtime.feature
================================================
@bot.create
Feature: Service entity debug mode

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-debug"
    And There is a package (pushed from "services/items") by the name of "services-debug"


  @services.delete
  @packages.delete
  @DAT-54523
  Scenario: Update service to debug mode and using custom docker- Should return <html> in response and executions should success
    Given I create a dataset with a random name
    And There are "5" items
    When I create a service
      | service_name=services-debug | package=services-debug | revision=None | config=None | runtime={"runnerImage": "python:3.10"} |
    And I send "post" gen_request with "debug_p" params
    And I send "post" gen_request with "debug_a" params
    Then I validate service has "1" instance up
    And I validate gen_request "get" with "debug_check" params and interval: "30" tries: "20" response "true"
    Then I call service.execute() on items in dataset
    When I send "post" gen_request with "terminate" params
    Then Execution was executed and finished with status "success"

================================================
File: tests/features/services_entity/test_service_items_input.feature
================================================
@bot.create
Feature: Service repository Execute items input

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "service-items-input"
        And I create a dataset with a random name
        And There is a package (pushed from "services/items") by the name of "services-items"
        And There are "5" items

    @services.delete
    @packages.delete
    @testrail-C4533094
    @DAT-46607
    Scenario: Create Service
        When I create a service
            |service_name=services-items|package=services-items|revision=None|config=None|runtime=None|
        Then I call service.execute() on items in dataset
        And Execution was executed and finished with status "success"



================================================
File: tests/features/services_entity/test_service_machine_types_1.feature
================================================
@bot.create
Feature: Service entity Regular Machine types

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-regular-machine-types"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-item"
    And I upload item in path "0000000162.jpg" to dataset

  @services.delete
  @packages.delete
  @DAT-57087
  Scenario: Create Service with pod type - regular-xs
    When I create a service
      | service_name=services-item | package=services-item | revision=None | config=None | runtime=None | pod_type=regular-xs |
    Then I execute the service
    And Execution was executed and finished with status "success"

  @services.delete
  @packages.delete
  @DAT-57088
  Scenario: Create Service with pod type - regular-s
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=regular-s |
    Then I execute the service
    And Execution was executed and finished with status "success"



================================================
File: tests/features/services_entity/test_service_machine_types_2.feature
================================================
@bot.create
Feature: Service entity Highmem Machine types

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-highmem-machine-types"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-item"
    And I upload item in path "0000000162.jpg" to dataset


  @services.delete
  @packages.delete
  @DAT-57091
  Scenario: Create Service with pod type - highmem-xs
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=highmem-xs |
    Then I execute the service
    And Execution was executed and finished with status "success"

  @services.delete
  @packages.delete
  @DAT-57092
  Scenario: Create Service with pod type - highmem-s
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=highmem-s |
    Then I execute the service
    And Execution was executed and finished with status "success"


================================================
File: tests/features/services_entity/test_service_machine_types_3.feature
================================================
@bot.create
Feature: Service entity Regular Machine types

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-regular-machine-types"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-item"
    And I upload item in path "0000000162.jpg" to dataset

  @services.delete
  @packages.delete
  @DAT-57089
  Scenario: Create Service with pod type - regular-m
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=regular-m |
    Then I execute the service
    And Execution was executed and finished with status "success"

  @services.delete
  @packages.delete
  @DAT-57090
  Scenario: Create Service with pod type - regular-l
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=regular-l |
    Then I execute the service
    And Execution was executed and finished with status "success"


================================================
File: tests/features/services_entity/test_service_machine_types_4.feature
================================================
@bot.create
Feature: Service entity Highmem Machine types

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-highmem-machine-types"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-item"
    And I upload item in path "0000000162.jpg" to dataset


  @services.delete
  @packages.delete
  @DAT-57093
  Scenario: Create Service with pod type - highmem-m
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=highmem-m |
    Then I execute the service
    And Execution was executed and finished with status "success"

  @services.delete
  @packages.delete
  @DAT-57094
  Scenario: Create Service with pod type - highmem-l
    When I create a service
      | service_name=services-items | package=services-items | revision=None | config=None | runtime=None | pod_type=highmem-l |
    Then I execute the service
    And Execution was executed and finished with status "success"

================================================
File: tests/features/services_entity/test_service_preemptible.feature
================================================
@bot.create
Feature: Services entity preemptible attribute

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "services_preemptible"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-preemptible"

  @services.delete
  @packages.delete
  @testrail-C4533503
  @DAT-46608
  Scenario: Update execution_timeout - Preemptible should updated to False
    When I create a service
      | service_name=services-preemptible | package=services-preemptible | revision=None | config=None | runtime=None | execution_timeout=360000 |
    Then I expect preemptible value to be "False"
    And Object "Service" to_json() equals to Platform json.


  @services.delete
  @packages.delete
  @testrail-C4533503
  @DAT-46608
  Scenario: Update max_attempts - Preemptible should updated to False
    When I create a service
      | service_name=services-preemptible | package=services-preemptible | revision=None | config=None | runtime=None | max_attempts=1 |
    Then I expect preemptible value to be "False"
    And Object "Service" to_json() equals to Platform json.





================================================
File: tests/features/services_entity/test_service_status.feature
================================================
@rc_only
Feature: Service status testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "services-status"

  @DAT-81805
  Scenario: Get service status from different drivers - Should be able to get services instances
    Given I create pipeline from json in path "pipelines_json/pipeline_service_status.json"
    And I install pipeline in context
    When I get service by name "run-1"
    Then I validate service has "1" instance up and replicaId include service name "True"
    When I get service by name "run-2"
    Then I validate service has "1" instance up and replicaId include service name "True" num_try 3



================================================
File: tests/features/services_entity/test_services_flow.feature
================================================
@bot.create
Feature: Service repository Context testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services-flow"
        And There are no datasets
        And I create a dataset with a random name
        And There is a package (pushed from "services/item_with_init") by the name of "services-flow"
        When I upload a file in path "assets_split/items_upload/0000000162.jpg"

    @services.delete
    @packages.delete
    @testrail-C4532909
    @DAT-46604
    Scenario: Get Service from the project it belong to
        When I deploy a service with init prams
        Then I execute the service
        Then The execution success with the right output



================================================
File: tests/features/services_entity/test_services_get.feature
================================================
@bot.create
Feature: Services entity methods testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services_get"
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-get"
        When I create a service
            |service_name=services-get|package=services-get|revision=None|config=None|runtime=None|

    @services.delete
    @packages.delete
    @testrail-C4523156
    @DAT-46605
    Scenario: To Json
        Then Object "Service" to_json() equals to Platform json.





================================================
File: tests/features/services_repo/test_service_output_limit.feature
================================================
@bot.create
Feature: Services repository output limit service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service_output_limit"
    And I create a dataset with a random name
    And There is a package (pushed from "services/output_limit") by the name of "services-limit"

  @services.delete
  @packages.delete
  @DAT-54723
  Scenario: Service should response failed on 4 mb output limit
    When I create a service
      | service_name=services-limit | package=services-limit | revision=None | config=None | runtime=None |
    Then I receive a Service entity
    When I execute service on "5" with type "Integer" with name "size"
    Then "The function execution was successful, but the output couldn't be saved because it was larger than the maximum size allowed of 4.0MB" in error message

================================================
File: tests/features/services_repo/test_services_app_refs.feature
================================================
Feature: Test app umbrella refs - App services

  Background: Initialize
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-app-refs"

  @DAT-72119
  Scenario: Update app with app service - Should update the app service to latest app version
    Given I fetch the dpk from 'apps/app_move_item_service.json' file
    When I publish a dpk
    And I install the app
    And I set code path "move_item" to context
    And I pack directory by name "move_item"
    And I add codebase to dpk
    And I set code path "move_item" to context
    And I pack directory by name "move_item"
    And I add codebase to dpk
    And I publish a dpk
    And I increment app dpk_version
    And I update an app
    And I get service in index "0"
    Then I validate service response params
      | key             | value |
      | packageRevision | 1.0.1 |
    And "service" has app scope


================================================
File: tests/features/services_repo/test_services_context.feature
================================================
@bot.create
Feature: Service repository Context testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "services-get services-get2"
        And I set Project to Project 1
        And There are no datasets
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-get"
        And I append package to packages
        When I create a service
            |service_name=services-get|package=services-get|revision=None|config=None|runtime=None|

    @services.delete
    @packages.delete
    @testrail-C4523157
    @DAT-46609
    Scenario: Get Service from the project it belong to
        When I get the service from project number 1
        Then Service Project_id is equal to project 1 id
        And Service Project.id is equal to project 1 id
        And Service Package_id is equal to package 1 id
        And Service Package.id is equal to package 1 id


    @services.delete
    @packages.delete
    @testrail-C4523157
    @DAT-46609
    Scenario: Get Service from the project it not belong to
        When I get the service from project number 2
        Then Service Project_id is equal to project 1 id
        And Service Project.id is equal to project 1 id
        And Service Package_id is equal to package 1 id
        And Service Package.id is equal to package 1 id


    @services.delete
    @packages.delete
    @testrail-C4523157
    @DAT-46609
    Scenario: Get Service from the package it belong to
        When I get the service from package number 1
        Then Service Project_id is equal to project 1 id
        And Service Project.id is equal to project 1 id
        And Service Package_id is equal to package 1 id
        And Service Package.id is equal to package 1 id


    @services.delete
    @packages.delete
    @testrail-C4523157
    @DAT-46609
    Scenario: Get Service from the package it belong to
        Given There is a package (pushed from "services/item") by the name of "services-get"
        And I append package to packages
        When I get the service from package number 2
        Then Service Project_id is equal to project 1 id
        And Service Project.id is equal to project 1 id
        And Service Package_id is equal to package 1 id
        And Service Package.id is equal to package 1 id



================================================
File: tests/features/services_repo/test_services_crashloop.feature
================================================
@bot.create
Feature: Services repository crashloopbackoff

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "services_crashloop"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @DAT-52810
  Scenario: Service with one init error should not be deactivated
    Given I upload item in "0000000162.jpg" to dataset
    Given Service that restart once in init
    When service is deployed with num replicas > 0
    Then I receive "InitError" notification with resource "service.id"
    Then service should stay active




================================================
File: tests/features/services_repo/test_services_create.feature
================================================
@bot.create
Feature: Services repository create service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services_create"
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-create"

    @services.delete
    @packages.delete
    @testrail-C4523158
    @DAT-46610
    Scenario: Create Service
        When I create a service
            |service_name=services-create|package=services-create|revision=None|config=None|runtime=None|
        Then I receive a Service entity



================================================
File: tests/features/services_repo/test_services_delete.feature
================================================
@bot.create
Feature: Services repository delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services_delete"
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-delete"
        And There is a service by the name of "services-delete" with module name "default_module" saved to context "service"

    @packages.delete
    @testrail-C4523159
    @DAT-46611
    Scenario: Delete by id
        When I delete service by "id"
        Then There are no services

    @packages.delete
    @testrail-C4523159
    @DAT-46611
    Scenario: Delete by name
        When I delete service by "name"
        Then There are no services

    @packages.delete
    @testrail-C4523159
    @DAT-46611
    Scenario: Delete by entity
        When I delete service by "entity"
        Then There are no services


================================================
File: tests/features/services_repo/test_services_deploy.feature
================================================
@bot.create
Feature: Services repository deploy service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services_deploy"
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-deploy"


    @services.delete
    @packages.delete
    @testrail-C4523160
    @DAT-46612
    Scenario: Deploy
        Given There are no services
        When I deploy a service
            |service_name=services-deploy|package=services-deploy|revision=None|config=None|runtime=None|
        Then I receive a Service entity
        When I deploy a service
            |service_name=services-deploy|package=services-deploy|revision=None|config={"new": "config"}|runtime=None|
        Then I receive a Service entity
        And There is only one service


    @services.delete
    @packages.delete
    @testrail-C4523160
    @DAT-46612
    Scenario: Deploy service from function with tabs
        When I deploy a service from function "service-with-tabs"
        Then I receive a Service entity


================================================
File: tests/features/services_repo/test_services_get.feature
================================================
@bot.create
Feature: Services repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services_get"
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-get"
        When I create a service
            |service_name=services-get|package=services-get|revision=None|config=None|runtime=None|

    @services.delete
    @packages.delete
    @testrail-C4523161
    @DAT-46605
    Scenario: Get by id
        When I get service by id
        Then I receive a Service entity
        And Service received equals to service created

    @services.delete
    @packages.delete
    @testrail-C4523161
    @DAT-46605
    Scenario: Get by name
        When I get service by name
        Then I receive a Service entity
        And Service received equals to service created




================================================
File: tests/features/services_repo/test_services_list.feature
================================================
@bot.create
Feature: Services repository list service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "services_list"
        And I create a dataset with a random name
        And There is a package (pushed from "services/item") by the name of "services-list"

    @services.delete
    @packages.delete
    @testrail-C4523162
    @DAT-46614
    Scenario: List when none exist
        When I list services
        Then I receive a Service list of "0" objects

    @services.delete
    @packages.delete
    @testrail-C4523162
    @DAT-46614
    Scenario: List when 1 exist
        When I create a service
            |service_name=services-list|package=services-list|revision=None|config=None|runtime=None|
        And I list services
        Then I receive a Service list of "1" objects

    @services.delete
    @packages.delete
    @testrail-C4523162
    @DAT-46614
    Scenario: List when 2 exist
        When I create a service
            |service_name=services-list-1|package=services-list|revision=None|config=None|runtime=None|
        And I create a service
            |service_name=services-list-2|package=services-list|revision=None|config=None|runtime=None|
        And I list services
        Then I receive a Service list of "2" objects


================================================
File: tests/features/services_repo/test_services_logs.feature
================================================
@bot.create
Feature: Services repository logs testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "services_log"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-log"

  @services.delete
  @packages.delete
  @testrail-C4523163
  @DAT-46615
  Scenario: Log
    When I create a service
      | service_name=services-log | package=services-log | revision=None | config=None | runtime=None |
    Then I receive a Service entity
    And I upload an item by the name of "test_item.jpg"
    And I run a service execute for the item
    And Log "THIS LOG LINE SHOULD BE IN LOGS" is in service.log()

  @services.delete
  @packages.delete
  @testrail-C4523163
  @DAT-46615
  Scenario: Log init
    When I create a service with autoscaler
      | service_name=services-log-init | package=services-log-init | revision=None | config=None | runtime=None |
    Then I receive a Service entity
    And Log "THIS LOG LINE SHOULD BE IN LOGS" is in service.log()

  @services.delete
  @packages.delete
  @testrail-C4533401
  @DAT-46615
  Scenario: Requirements errors
    When Add requirements "noRequirements" to package
    And I create a service with autoscaler
      | service_name=services-log-init | package=services-log-init | revision=1.0.1 | config=None | runtime=None |
    Then I receive a Service entity
    And Log "ERROR: No matching distribution found for" is in service.log()


================================================
File: tests/features/services_repo/test_services_long_short_term_1.feature
================================================
Feature: Test service mode update 1
  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-long-short-term"
    And I create a dataset with a random name
    And I upload item in "0000000162.jpg" to dataset

  @DAT-68557
  Scenario: Updating service from long-term to short term multiple times
    Given I have a paused "short-term" service with concurrency "5"
    And I run "100" executions and activate the service
    When I update the service back and forth "4" times from long-term to short-term
    And I expect all executions to be successful and no execution should have ran twice

================================================
File: tests/features/services_repo/test_services_long_short_term_2.feature
================================================
Feature: Test service mode update 2
  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "service-long-short-term"
    And I create a dataset with a random name
    And I upload item in "0000000162.jpg" to dataset

  @DAT-68557
  Scenario: Updating service from long-term to short term multiple times
    Given I have a paused "long-term" service with concurrency "5"
    And I run "100" executions and activate the service
    When I update the service back and forth "4" times from long-term to short-term
    And I expect all executions to be successful and no execution should have ran twice

================================================
File: tests/features/services_repo/test_services_slot.feature
================================================
@bot.create
Feature: Packages repository create slot testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_ui_slot"
    And I create a dataset with a random name
    And There is a package (pushed from "services/item") by the name of "services-create"


  @services.delete
  @packages.delete
  @testrail-C4532930
  @DAT-46616
  Scenario: Activate UI slot in service - Slot should create in settings
    When I create a service
      | service_name=services-create | package=services-create | revision=None | config=None | runtime=None |
    And I get package entity from service
    And I add UI slot to the package
    Then I validate slot is added to the package
    When I update service to latest package revision
    And I activate UI slot in service
    Then I get setting for context service


  @services.delete
  @packages.delete
  @testrail-C4532930
  @DAT-46616
  Scenario: Update UI slot display_name Should updated in settings
    When I create a service
      | service_name=services-create | package=services-create | revision=None | config=None | runtime=None |
    And I get package entity from service
    And I add UI slot to the package
    When I update service to latest package revision
    And I activate UI slot in service
    And I update UI slot display_name to "new-ui-slot"
    And I update service to latest package revision
    Then I get setting for context service
    And I validate service UI slot is equal to settings


  @services.delete
  @packages.delete
  @testrail-C4532930
  @DAT-46616
  Scenario: Update package and UI slot with new function Should updated in settings
    When I create a service
      | service_name=services-create | package=services-create | revision=None | config=None | runtime=None |
    And I get package entity from service
    And I add UI slot to the package
    And I update service to latest package revision
    And I activate UI slot in service
    And I add new function to package
    And I add new function to UI slot
    And I update service to latest package revision
    And I activate UI slot in service
    Then I get setting for context service
    And I validate service UI slot is equal to settings




================================================
File: tests/features/services_repo/test_services_update.feature
================================================
@bot.create
Feature: Services repository update service testing

     Background: Initiate Platform Interface and create a project
         Given Platform Interface is initialized as dlp and Environment is set according to git branch
         And I create a project by the name of "services_update"
         And I create a dataset with a random name
         And There is a package (pushed from "services/item") by the name of "services-update"
         And There is a service by the name of "services-update" with module name "default_module" saved to context "service"

     @services.delete
     @packages.delete
     @testrail-C4523164
     @DAT-46617
     Scenario: Update service
         When I get service revisions
         And I change service "concurrency" to "17"
         And I update service
         Then Service received equals service changed except for "runtime.concurrency"
         Then "service_update" has updatedBy field




================================================
File: tests/features/services_repo/test_services_update_force.feature
================================================
@bot.create
Feature: Services repository update with force=True service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "services_update"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @testrail-C4532327
  @DAT-46618
  Scenario: Update service with force - Execution should stop immediately
    Given There is a package (pushed from "services/long_term") by the name of "services-update"
    And There is a service with max_attempts of "1" by the name of "services-update-force" with module name "default_module" saved to context "service"
    And I execute service
    And Execution is running
    When I update service with force="True"
    Then Execution stopped immediately


  @services.delete
  @packages.delete
  @DAT-70898
  Scenario: Update service with force - Execution logs should restart
    Given There is a package (pushed from "services/execution_progress") by the name of "execution-progress"
    And There is service by the name of "executions-create" with module name "default_module" and execution_timeout of "30" saved to context "service"
    And I execute service
    And Execution is running
    When I update service with force="True"
    Then I expect execution status progress include "10" in "percentComplete" with a frequency of "2"


================================================
File: tests/features/settings/test_settings.feature
================================================
Feature: Settings Context

  Background: Background name
    Given Platform Interface is initialized as dlp and Environment is set according to git branch

  @testrail-C4529105
  @DAT-46619
  Scenario: check box rotation settings
    When I create two project and datasets by the name of "settings_test1" "settings_test2"
    And I upload item in "0000000162.jpg" to both datasets
    And i upload annotations to both items
    And add settings to the project
    Then check if geo in the first item and in the second are difference

  @testrail-C4532534
  @DAT-46619
  Scenario: check get setting by id
    Given I create a project by the name of "to-delete-test-setting-get-id"
    When add settings to the project
    And I get setting by "id"
    Then I check setting got is equal to the one created

  @testrail-C4532533
  @DAT-46619
  Scenario: check get setting by name
    When I create a project by the name of "to-delete-test-setting-get-name"
    When add settings to the project
    And I get setting by "name"
    Then I check setting got is equal to the one created

  @DAT-46364
  Scenario Outline: check settings type validation
    When I create a project by the name of "to-delete-test-setting-validate_type"
    When I add settings to the project with wrong "<value>" type
    Then I expect the correct exception to be thrown
    Examples: Type
      | value   |
      | 'boaz'  |
      | 123     |


================================================
File: tests/features/solution/test_dependency_active_learning.feature
================================================
@rc_only
Feature: DPK with dependencies

  Background: Login to the platform and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "active_learning_dependencies"

  @DAT-69442
  Scenario: E2E active learning using dependencies
    Given I fetch dpk active learning pipeline template from file with params
      | key          | value                                   |
      | file_name    | apps/active_learning_template_pipe.json |
      | package_name | ac-lr-package                           |
      | entry_point  | main.py                                 |
      | model_name   | ac-lr-model                             |
      | status       | trained                                 |
      | index        | 0                                       |
    When I add dependency to the dpk with params
      | key       | value           |
      | name      | active-learning |
      | mandatory | True            |
    And I publish a dpk to the platform
    Given I fetch the dpk from 'apps/dependencies/solution_default.json' file
    When I add dependency to the dpk with params
      | key       | value                          |
      | name      | context.published_dpks[0].name |
      | mandatory | True                           |
    When I add dependency to the dpk with params
      | key        | value                                            |
      | name       | yolov8                                           |
      | components | {"models":[{"name":"yolov8","mandatory": True}]} |
    And I add dependency to the dpk with params
      | key       | value                |
      | name      | hf-furniture-dataset |
      | mandatory | True                 |
    And I publish a dpk to the platform
    When I install the app
    Then I validate app dependencies are installed
    And I validate app dependencies have "usedBy" relation refs



================================================
File: tests/features/solution/test_dependency_refs.feature
================================================
Feature: DPK with dependencies

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk_dependency_1"

  @DAT-69194
  Scenario: DPK with dependencies App 'usedBy' refs cases - Install and Uninstall
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    When I install the app
    Given I fetch the dpk from 'apps/dependencies/solution_default.json' file
    When I add dependency to the dpk with params
      | key  | value                      |
      | name | context.published_dpk.name |
    And I publish a dpk to the platform
    And I install the app
    Then I validate app dependencies have "usedBy" relation refs "only"
    When I uninstall the app
    Then I validate app dependencies not have "usedBy" relation refs
    When I uninstall the app

  @DAT-69195
  Scenario: DPK with dependencies App clean-up refs cases - Uninstall and remove dpk
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    And I fetch the dpk from 'apps/dependencies/solution_default.json' file
    When I add dependency to the dpk with params
      | key  | value                      |
      | name | context.published_dpk.name |
    And I publish a dpk to the platform
    Then I validate dpk dependencies have "usedBy" relation refs
    When I install the app
    Then I validate app dependencies have "createdBy" relation refs
    And I validate app dependencies have "usedBy" relation refs
    When I uninstall the app
    Then I validate app dependencies not installed
    When I delete published_dpk
    Then I validate dpk dependencies not have "usedBy" relation refs



================================================
File: tests/features/solution/test_dependency_sanity.feature
================================================
Feature: DPK with dependencies

  Background:
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "dpk_dependency_1"

  @DAT-69005
  Scenario: DPK with dependencies - App and apps should be deployed according to dependencies
    Given I publish a pipeline node dpk from file "apps/app_scope_node.json" and with code path "move_item"
    Given I publish a model dpk from file "model_dpk/modelsDpks.json" package "dummymodel"
    And I fetch the dpk from 'apps/dependencies/solution_default.json' file
    When I add dependency to the dpk with params
      | key  | value                          |
      | name | context.published_dpks[0].name |
    When I add dependency to the dpk with params
      | key  | value                          |
      | name | context.published_dpks[1].name |
    And I publish a dpk to the platform
    Then I validate dpk dependencies have "usedBy" relation refs
    When I install the app
    Then I validate app dependencies are installed
    And I validate app dependencies have "createdBy" relation refs
    And I validate app dependencies have "usedBy" relation refs



================================================
File: tests/features/steps/bring_steps.py
================================================
import os

# set var for assets
os.environ['DATALOOP_TEST_ASSETS'] = os.path.join(os.getcwd(), 'tests', 'assets')

from tests.features.steps.projects_repo import test_projects_create
from tests.features.steps.projects_repo import test_projects_list
from tests.features.steps.projects_repo import test_projects_get
from tests.features.steps.projects_repo import test_projects_delete
from tests.features.steps.projects_repo import test_project_integrations

from tests.features.steps.project_entity import test_project_repo_methods

from tests.features.steps.datasets_repo import test_datasets_create
from tests.features.steps.datasets_repo import test_datasets_list
from tests.features.steps.datasets_repo import test_datasets_get
from tests.features.steps.datasets_repo import test_datasets_delete
from tests.features.steps.datasets_repo import test_datasets_update
from tests.features.steps.datasets_repo import test_datasets_readonly
from tests.features.steps.datasets_repo import test_datasets_download_annotations
from tests.features.steps.datasets_repo import test_dataset_upload_annotations
from tests.features.steps.datasets_repo import test_dataset_upload_csv
from tests.features.steps.datasets_repo import test_datasets_download
from tests.features.steps.datasets_repo import test_dataset_context
from tests.features.steps.datasets_repo import tets_dataset_upload_labels

from tests.features.steps.dataset_entity import test_dataset_repo_methods
from tests.features.steps.dataset_entity import test_add_labels_methods

from tests.features.steps.annotations_repo import test_annotations_get
from tests.features.steps.annotations_repo import test_annotations_list
from tests.features.steps.annotations_repo import test_annotations_delete
from tests.features.steps.annotations_repo import test_annotations_download
from tests.features.steps.annotations_repo import test_annotations_update
from tests.features.steps.annotations_repo import test_annotations_upload
from tests.features.steps.annotations_repo import test_annotations_show
from tests.features.steps.annotations_repo import test_annotations_draw
from tests.features.steps.annotations_repo import test_annotations_context

from tests.features.steps.annotation_entity import test_annotation_repo_methods
from tests.features.steps.annotation_entity import test_upload_annotations
from tests.features.steps.annotation_entity import test_annotation_json_to_object
from tests.features.steps.annotation_entity import test_segmentation_to_box

from tests.features.steps.annotation_collection import test_annotation_collection

from tests.features.steps.items_repo import test_items_get
from tests.features.steps.items_repo import test_items_clone
from tests.features.steps.items_repo import test_items_upload
from tests.features.steps.items_repo import test_items_list
from tests.features.steps.items_repo import test_items_update
from tests.features.steps.items_repo import test_items_set_items_entity
from tests.features.steps.items_repo import test_items_get_all_items
from tests.features.steps.items_repo import test_items_download_batch
from tests.features.steps.items_repo import test_items_upload_batch
from tests.features.steps.items_repo import test_items_upload_dataframe
from tests.features.steps.items_repo import test_items_delete
from tests.features.steps.items_repo import test_items_download
from tests.features.steps.items_repo import test_download_and_upload_ndarray_item
from tests.features.steps.items_repo import test_items_context

from tests.features.steps.item_entity import test_item_repo_methods
from tests.features.steps.item_entity import test_item_move
from tests.features.steps.item_entity import test_item_update_status
from tests.features.steps.item_entity import test_item_description
from tests.features.steps.item_entity import test_item_attributes

from tests.features.steps.ontologies_repo import test_ontologies_create
from tests.features.steps.ontologies_repo import test_ontologies_get
from tests.features.steps.ontologies_repo import test_ontologies_delete
from tests.features.steps.ontologies_repo import test_ontologies_update

from tests.features.steps.ontology_entity import test_ontology_repo_methods
from tests.features.steps.ontology_entity import test_ontology_attributes
from tests.features.steps.ontology_entity import test_ontology_bamba_icon

from tests.features.steps.recipes_repo import test_recipes_create
from tests.features.steps.recipes_repo import test_recipes_update
from tests.features.steps.recipes_repo import test_recipes_delete
from tests.features.steps.recipes_repo import test_recipes_clone

from tests.features.steps.codebases_repo import test_codebases_pack
from tests.features.steps.codebases_repo import test_codebases_init
from tests.features.steps.codebases_repo import test_codebases_unpack
from tests.features.steps.codebases_repo import test_codebases_get
from tests.features.steps.codebases_repo import test_codebases_list_versions
from tests.features.steps.codebases_repo import test_codebases_list

from tests.features.steps.codebase_entity import test_codebase_repo_methods

from tests.features.steps.command_entity import test_command

from tests.features.steps.utilities import platform_interface_steps

from tests.features.steps.cli_testing import cli_projects
from tests.features.steps.cli_testing import cli_datasets
from tests.features.steps.cli_testing import cli_others

from tests.features.steps.checkout_testing import test_checkout

from tests.features.steps.packages_repo import packages_generate
from tests.features.steps.packages_repo import packages_push
from tests.features.steps.packages_repo import packages_get
from tests.features.steps.packages_repo import packages_list
from tests.features.steps.packages_repo import test_packages_context
from tests.features.steps.packages_repo import packages_name_validation
from tests.features.steps.packages_flow import packages_flow
from tests.features.steps.packages_repo import package_slot
from tests.features.steps.packages_repo import packages_delete
from tests.features.steps.packages_repo import test_package_module

from tests.features.steps.triggers_repo import test_triggers_create
from tests.features.steps.triggers_repo import test_triggers_get
from tests.features.steps.triggers_repo import test_triggers_list
from tests.features.steps.triggers_repo import test_triggers_update
from tests.features.steps.triggers_repo import test_triggers_delete
from tests.features.steps.triggers_repo import test_triggers_context
from tests.features.steps.triggers_repo import test_triggers_item_update
from tests.features.steps.triggers_repo import test_triggers_annotation_update
from tests.features.steps.triggers_repo import test_pipeline_triggers_create

from tests.features.steps.service_entity import test_service_debug_mode

from tests.features.steps.services_repo import test_services_deploy
from tests.features.steps.services_repo import test_services_create
from tests.features.steps.services_repo import test_services_get
from tests.features.steps.services_repo import test_services_delete
from tests.features.steps.services_repo import test_services_list
from tests.features.steps.services_repo import test_services_update
from tests.features.steps.services_repo import test_services_context
from tests.features.steps.services_repo import test_services_slot
from tests.features.steps.services_repo import test_services_crashloop
from tests.features.steps.services_repo import test_service_validation

from tests.features.steps.filters_entity import test_filters

from tests.features.steps.executions_repo import test_executions_get
from tests.features.steps.executions_repo import test_executions_create
from tests.features.steps.executions_repo import test_executions_list
from tests.features.steps.executions_repo import test_executions_multiple
from tests.features.steps.executions_repo import test_executions_context
from tests.features.steps.execution_monitoring import test_execution_monitoring
from tests.features.steps.executions_repo import test_execution_rerun
from tests.features.steps.executions_repo import test_execution_validation

from tests.features.steps.bots_repo import test_bots_create
from tests.features.steps.bots_repo import test_bots_list
from tests.features.steps.bots_repo import test_bots_get
from tests.features.steps.bots_repo import test_bots_delete

from tests.features.steps.artifacts_repo import test_artifacts_upload
from tests.features.steps.artifacts_repo import test_artifacts_get
from tests.features.steps.artifacts_repo import test_artifacts_list
from tests.features.steps.artifacts_repo import test_artifacts_download
from tests.features.steps.artifacts_repo import test_artifacts_delete

from tests.features.steps.tasks_repo import test_tasks_create
from tests.features.steps.tasks_repo import test_tasks_get
from tests.features.steps.tasks_repo import test_tasks_list
from tests.features.steps.tasks_repo import test_tasks_delete
from tests.features.steps.tasks_repo import test_tasks_qa_task
from tests.features.steps.tasks_repo import test_tasks_add_and_get_items
from tests.features.steps.tasks_repo import test_task_context
from tests.features.steps.tasks_repo import test_task_priority
from tests.features.steps.tasks_repo import test_task_update

from tests.features.steps.assignments_repo import test_assignments_create
from tests.features.steps.assignments_repo import test_assignments_get
from tests.features.steps.assignments_repo import test_assignments_list
from tests.features.steps.assignments_repo import test_assignments_reassign
from tests.features.steps.assignments_repo import test_assignments_redistribute
from tests.features.steps.assignments_repo import test_assignments_items_operations
from tests.features.steps.assignments_repo import test_assignments_context

from tests.features.steps.assignment_entity import test_assignment_update

from tests.features.steps.converter import converter

from tests.features.steps.models_repo import test_models_create
from tests.features.steps.models_repo import test_models_list
from tests.features.steps.models_repo import test_models_delete
from tests.features.steps.models_repo import test_model_flow
from tests.features.steps.models_repo import test_models_get

from tests.features.steps.model_entity import test_model_name

from tests.features.steps.pipeline_entity import pipeline_get, pipeline_delete, pipeline_update, pipeline_flow, pipeline_output_list, pipeline_execute
from tests.features.steps.pipeline_resume import pipeline_resume
from tests.features.steps.pipeline_entity import test_pipeline_interface
from tests.features.steps.pipeline_entity import test_pipeline_dataset_node_trigger_to_pipeline
from tests.features.steps.pipeline_entity import test_pipeline_execution
from tests.features.steps.pipeline_entity import pipeline_validation
from tests.features.steps.pipeline_entity import test_pipeline_refs
from tests.features.steps.pipeline_entity import test_pipeline_model_compute_configs

from tests.features.steps.features_vectors import test_features_create, test_features_delete
from tests.features.steps.pipeline_entity import test_pipeline_pulling_task
from tests.features.steps.pipeline_entity import test_pipeline_actions
from tests.features.steps.pipeline_entity import test_pipeline_install
from tests.features.steps.pipeline_entity import test_pipeline_connectors
from tests.features.steps.pipeline_entity import test_pipeline_task_node

from tests.features.steps.documentation_tests import test_projects_docs
from tests.features.steps.documentation_tests import test_contributor_docs
from tests.features.steps.documentation_tests import test_recipe_docs
from tests.features.steps.documentation_tests import test_dataset_docs

from tests.features.steps.test_cache import test_cache
from tests.features.steps.settings_context import test_settings_context

from tests.features.steps.pipeline_entity import pipeline_reset
from tests.features.steps.pipeline_entity import pipeline_rerun_cycles

from tests.features.steps.items_repo import test_upload_and_download_images

from tests.features.steps.annotations_repo import test_rotated_box_points
from tests.features.steps.app_entity import test_app_install, test_app_uninstall, test_app_update, test_app_get, test_app_status
from tests.features.steps.dpk_tests import dpk_json_to_object, test_dpk_publish, test_dpk_list, \
    test_dpk_pull, test_dpk_get, test_dpk_delete, test_dpk_interface, test_dpk_update, test_dpk_validation
from tests.features.steps.app_entity import test_app_validate
from tests.features.steps.app_entity import test_app_interface
from tests.features.steps.app_entity import app_with_fs_panel


from tests.features.steps.webm_converter import test_failed_video_message

from tests.features.steps.platform_urls import test_platform_urls

from tests.features.steps.annotation_entity import test_annotation_description

from tests.features.steps.annotations_repo import test_note_annotation_with_messages

from tests.features.steps.annotations_repo import test_annotations_format_json

from tests.features.steps.annotations_repo import test_annotations_adding_multiple_frames

from tests.features.steps.annotation_entity import test_segmentation_to_polygon

# Interface steps locations
from tests.features.steps.annotation_entity import annotation_entity_interface

from tests.features.steps.annotations_repo import annotations_interface
from tests.features.steps.annotations_repo import image_annotations_interface
from tests.features.steps.annotations_repo import video_annotations_interface
from tests.features.steps.annotations_repo import audio_annotations_interface
from tests.features.steps.annotations_repo import text_annotations_interface

from tests.features.steps.converter import conveters_interface

from tests.features.steps.dataset_entity import dataset_entity_interface

from tests.features.steps.datasets_repo import datasets_interface

from tests.features.steps.filters_entity import filters_interface

from tests.features.steps.item_entity import item_entity_interface

from tests.features.steps.items_repo import items_interface

from tests.features.steps.projects_repo import projects_interface

from tests.features.steps.annotations_repo import test_annotations_adding_multiple_frames
from tests.features.steps.drivers_repo import test_drivers_create
from tests.features.steps.drivers_repo import test_drivers_delete
from tests.features.steps.notifications import notifications

from tests.features.steps.annotations_repo import test_annotations_format_json
from tests.features.steps.xray import test_xray
from tests.features.steps.pipeline_entity import pipeline_ml
from tests.features.steps.dataset_entity import test_directory_tree

from tests.features.steps.billing_steps import object_creation_steps
from tests.features.steps.billing_steps import test_get_plan_steps
from tests.features.steps.billing_steps import test_delete_org
from tests.features.steps.billing_steps import test_plan_resources
from tests.features.steps.billing_steps import Log_in

from tests.features.steps.pipeline_entity import pipeline_node_input_handling
from tests.features.steps.compute import compute
from tests.features.steps.compute import ecr_integrations
from tests.features.steps.compute import test_compute_create
from tests.features.steps.compute import test_compute_inteface
from tests.features.steps.compute import test_compute_get
from tests.features.steps.compute import test_compute_delete

from tests.features.steps.ml_subsets import test_ml_subsets

from tests.features.steps.item_collections import test_item_collections

from tests.features.steps.service_driver_repo import test_service_driver_get


================================================
File: tests/features/steps/fixtures.py
================================================
import time

from behave import fixture
import os
import json
import datetime
import dtlpy as dl
from operator import attrgetter
import re


@fixture
def compare_dir_recursive(dir_a, dir_b):
    equal = True

    dir_a_items = [item for item in os.listdir(dir_a) if item != 'folder_keeper']
    dir_b_items = [item for item in os.listdir(dir_b) if item != 'folder_keeper']
    if '__pycache__' in dir_a_items:
        dir_a_items.remove('__pycache__')
    if len(dir_a_items) != len(dir_b_items):
        return False

    for item in dir_a_items:
        if os.path.isdir(os.path.join(dir_a, item)):
            equal = compare_dir_recursive(os.path.join(dir_a, item), os.path.join(dir_b, item))
        else:
            if item.lower().endswith('.json'):
                with open(os.path.join(dir_a, item), 'r') as f:
                    data_a = json.load(f)
                with open(os.path.join(dir_a, item), 'w') as f:
                    json.dump(data_a, f, indent=2, sort_keys=True)
                with open(os.path.join(dir_b, item), 'r') as f:
                    data_b = json.load(f)
                with open(os.path.join(dir_b, item), 'w') as f:
                    json.dump(data_b, f, indent=2, sort_keys=True)
            equal = compare_files(os.path.join(dir_a, item), os.path.join(dir_b, item))
        if not equal:
            return False

    return equal


def compare_files(fpath1, fpath2):
    with open(fpath1, 'r') as file1:
        with open(fpath2, 'r') as file2:
            same = set(file1).difference(file2)

    if same:
        return False
    else:
        return True


def get_value(params, context):
    key = params[0]
    val = params[1]
    if val == 'None':
        val = None
    elif val == 'auto':
        if key == 'due_date':
            val = datetime.datetime.today().timestamp() + 24 * 60 * 60
        elif key == 'assignee_ids':
            val = ['annotator1@dataloop.ai', 'annotator2@dataloop.ai']
        elif key == 'workload':
            val = context.dl.Workload.generate(['annotator1@dataloop.ai', 'annotator2@dataloop.ai'])
        elif key == 'dataset':
            val = context.dataset
        elif key == 'task_owner':
            val = 'owner@dataloop.ai'
        elif key == 'recipe_id':
            val = context.dataset.get_recipe_ids()[0]
        elif key == 'project_id':
            val = context.project.id
        elif key == 'task_id':
            val = context.task.id
        elif key == 'batch_size':
            val = 5
        elif key == 'max_batch_workload':
            val = 7
        elif key == 'allowed_assignees':
            val = ['annotator1@dataloop.ai', 'annotator2@dataloop.ai']
        elif key == 'consensus_task_type':
            val = 'consensus'
        elif key == 'consensus_percentage':
            val = 100
        elif key == 'consensus_assignees':
            val = 2
        elif key == 'scoring':
            val = False

    elif val == 'current_user':
        if key == 'creator':
            val = dl.info()['user_email']
        elif key == 'updated_by':
            val = dl.info()['user_email']
    if key == 'filters' and val == 'context.filters':
        val = context.filters
    elif key == 'filters' and val is not None and not isinstance(val, dict):
        filters = context.dl.Filters()
        filters.custom_filter = json.loads(val)
        val = filters
    elif key == 'items' and val is not None:
        if len(context.items_in_dataset) >= int(val):
            items = list()
            for _ in range(int(val)):
                items.append(context.items_in_dataset.pop())
            val = items
        else:
            raise Exception('Not enough items in dataset')
    elif key == 'metadata' and val is not None:
        val = json.loads(val)
    elif key == 'due_date':
        if val == 'next_week':
            val = datetime.datetime.today().timestamp() + (7 * 24 * 60 * 60)
    elif key == 'project_id':
        if val == 'second':
            val = context.second_project.id
    elif key == 'recipe_id':
        if val == 'second':
            assert hasattr(context, "second_dataset"), "TEST FAILED: Test should have second_dataset"
            val = context.second_dataset.get_recipe_ids()[0]
    elif key == 'assignee_ids':
        if val and "[" in val:
            val = eval(val)
    elif key == 'available_actions':
        action_status = val.split()
        available_actions_list = list()
        for action in action_status:
            available_actions_list.append(context.dl.ItemAction(action=action, display_name=action, color='#2ef16c'))
        val = available_actions_list
    elif key == 'batch_size':
        val = int(val)
    elif key == 'max_batch_workload':
        val = int(val)
    elif key == 'consensus_task_type':
        val = str(val)
    elif key == 'consensus_percentage':
        val = int(val)
    elif key == 'consensus_assignees':
        val = int(val)
    elif key == 'scoring':
        val = eval(val)

    return val


def get_assignment_value(params, context):
    key = params[0]
    val = params[1]
    if val == 'None':
        val = None
    elif val == 'auto':
        if key == 'assignee_id':
            val = 'annotator2@dataloop.ai'
        elif key == 'dataset':
            val = context.dataset
        elif key == 'project_id':
            val = context.project.id
        elif key == 'task_id':
            val = context.task.id

    if key == 'filters' and val is not None and not isinstance(val, dict):
        filters = context.dl.Filters()
        filters.custom_filter = json.loads(val)
        val = filters
    elif key == 'items' and val is not None:
        if len(context.items_in_dataset) >= int(val):
            items = list()
            for _ in range(int(val)):
                items.append(context.items_in_dataset.pop())
            val = items
        else:
            raise Exception('Not enough items in dataset')
    elif key == 'metadata' and val is not None:
        val = json.loads(val)
    elif key == 'project_id':
        if val == 'second':
            val = context.second_project.id

    return val


def get_package_io(params, context):
    val = list()
    for key in params:
        if key == 'item':
            val.append(context.dl.FunctionIO(type=context.dl.PACKAGE_INPUT_TYPE_ITEM, name=key))
        elif key == 'annotation':
            val.append(context.dl.FunctionIO(type=context.dl.PACKAGE_INPUT_TYPE_ANNOTATION, name=key))
        elif key == 'dataset':
            val.append(context.dl.FunctionIO(type=context.dl.PACKAGE_INPUT_TYPE_DATASET, name=key))
        elif key == 'task':
            val.append(context.dl.FunctionIO(type=context.dl.PACKAGE_INPUT_TYPE_TASK, name=key))
        elif key == 'assignment':
            val.append(context.dl.FunctionIO(type=context.dl.PACKAGE_INPUT_TYPE_ASSIGNMENT, name=key))
        elif key == 'itemWithDescription':
            val.append(context.dl.FunctionIO(type=context.dl.PACKAGE_INPUT_TYPE_ITEM, name='item', description='item'))

    return val


def access_nested_dictionary_key(dict_input, keys):
    # using for loop to access nested dictionary key safely
    val = dict_input
    for key in keys:
        if key in val:
            if isinstance(val[key], list):
                if val[key]:
                    val = val[key][0]
                else:
                    val = None
                    break
            else:
                val = val[key]
        else:
            val = None
            break
    return val


def update_dtlpy_version(json_obj):
    """
    For DPK
    Check if component attribute from the list 'component_attributes' is contained the keys: 'versions' / 'runtime'.
    if True - Check if the obj keys value is 'dtlpy_version' - if True - update to current sdk version
    if False - Skip to next component attribute
    Return: json_obj
    """
    component_attributes = ["services", "computeConfigs", "modules", "versions"]
    for component_att in component_attributes:
        val = access_nested_dictionary_key(json_obj, ['components', component_att, 'versions', 'dtlpy'])
        if val:
            for obj in json_obj['components'][component_att]:
                if obj['versions']['dtlpy'] == "dtlpy_version":
                    obj['versions'].update({"dtlpy": dl.__version__})

        val = access_nested_dictionary_key(json_obj, ['components', component_att, 'runtime', 'runnerImage'])
        if val:
            for obj in json_obj['components'][component_att]:
                if "dtlpy_version" in obj['runtime']["runnerImage"]:
                    obj['runtime'].update({"runnerImage": f"dataloop_runner-cpu/main:{dl.__version__}.latest"})

        val = access_nested_dictionary_key(json_obj, [component_att, 'dtlpy'])
        if val:
            if json_obj[component_att]['dtlpy'] == "dtlpy_version":
                json_obj[component_att].update({"dtlpy": dl.__version__})

    return json_obj


def gen_request(context, method=None, req=None, num_try=None, interval=None, expected_response=None):
    if req.get('path', None) and ".id" in req.get('path'):
        replace_field = req.get('path').split("/")[2]
        req['path'] = req['path'].replace(replace_field, attrgetter(replace_field)(context))
    for i in range(num_try):
        success, response = dl.client_api.gen_request(req_type=method,
                                                      path=req.get('path', None),
                                                      data=req.get('data', None),
                                                      json_req=req.get('json_req', None))
        if success:
            if expected_response and expected_response in response.text:
                break
        dl.logger.warning("Number of tries {}".format(i + 1))
        time.sleep(interval)

    if not success:
        raise dl.exceptions.PlatformException(response)
    return response


def update_nested_structure(context, d):
    """
    Recursively search for a value in a nested dictionary or list and update it.
    """
    if isinstance(d, dict):
        for key, value in d.items():
            if ".id" in value:
                d[key] = attrgetter(value)(context)
                return True
            elif isinstance(value, (dict, list)):
                if update_nested_structure(context, value):
                    return True
    elif isinstance(d, list):
        for index, item in enumerate(d):
            if ".id" in item:
                d[index] = attrgetter(item)(context)
                return True
            elif isinstance(item, (dict, list)):
                if update_nested_structure(context, item):
                    return True
    return True


def update_nested_dict(target_dict: dict, updates: dict):
    """
    Updates the target_dict based on the paths and values provided in updates.

        :param target_dict: The original dictionary to be updated.
        :param updates: A dictionary where keys are dot-separated paths, and values are the new values.
    """
    for path, value in updates.items():
        keys = path.split('.')
        current = target_dict
        for key in keys[:-1]:  # Traverse to the second-to-last key
            if key.isdigit():  # If the key is an integer (for lists), convert it
                key = int(key)
            current = current[key]
        # Handle the last key
        final_key = keys[-1]
        if final_key.isdigit():  # If the final key is an integer (for lists), convert it
            final_key = int(final_key)
        current[final_key] = value


def remove_key_from_nested_dict(d, path_key):
    """
    Recursively search for a key in a nested dictionary or list and remove it
    :param d: dictionary or list
    :param path_key: dot-separated path to the key to remove
    """
    keys = path_key.split('.')
    current = d
    for key in keys[:-1]:  # Traverse to the second-to-last key
        if key.isdigit():  # If the key is an integer (for lists), convert it
            key = int(key)
        current = current[key]
    # Handle the last key
    final_key = keys[-1]
    if final_key.isdigit():  # If the final key is an integer (for lists), convert it
        final_key = int(final_key)
    del current[final_key]


================================================
File: tests/features/steps/annotation_collection/test_annotation_collection.py
================================================
import behave
import random as r
import time


@behave.given(u"I get item annotation collection")
def step_impl(context):
    context.annotation_collection = context.item.annotations.list()


@behave.given(u'I change all annotations label to "{label}"')
def step_impl(context, label):
    for ann in context.annotation_collection:
        for i_frame, frame in ann.frames.items():
            ann.set_frame(i_frame)
            ann.label = label


@behave.given(u'I change all image annotations label to "{label}"')
def step_impl(context, label):
    for ann in context.annotation_collection:
        ann.label = label


@behave.when(u"I update annotation collection")
def step_impl(context):
    context.annotation_collection.update()
    time.sleep(7)


@behave.then(u'Annotations in host have label "{label}"')
def step_impl(context, label):
    context.annotation_collection = context.item.annotations.list()
    for ann in context.annotation_collection:
        for i_frame, frame in ann.frames.items():
            ann.set_frame(i_frame)
            assert ann.label == label


@behave.then(u'Image annotations in host have label "{label}"')
def step_impl(context, label):
    context.annotation_collection = context.item.annotations.list()
    for ann in context.annotation_collection:
        assert ann.label == label


@behave.when(u"I delete annotation collection")
def step_impl(context):
    context.annotation_collection.delete()


@behave.then(u"Item in host has no annotations")
def step_impl(context):
    context.annotation_collection = context.item.annotations.list()
    assert len(context.annotation_collection) == 0


@behave.given(u"I create item annotation collection")
def step_impl(context):
    context.annotation_collection = context.item.annotations.list()


@behave.then(u'"{num}" annotations are upload')
def step_impl(context, num):
    num_ann = int(num)
    assert len(context.item.annotations.list().annotations) == num_ann, "Number of annotations is not as expected"


@behave.when(u"I add a gis annotations to item")
def step_impl(context):
    box = context.dl.Gis(
        annotation_type=context.dl.GisType.BOX,
        geo=[
            [
                [
                    -118.33545020696846,
                    33.82643304226775
                ],
                [
                    -118.33544677833854,
                    33.82643304226775
                ],
                [
                    -118.33544677833854,
                    33.826421352507026
                ],
                [
                    -118.33545020696846,
                    33.826421352507026
                ],
                [
                    -118.33545020696846,
                    33.82643304226775
                ]
            ]
        ], label='car')

    polyline = context.dl.Gis(
        annotation_type=context.dl.GisType.POLYLINE,
        geo=[
            [
                -118.33542547032158,
                33.82643633447882
            ],
            [
                -118.33540897336161,
                33.82643356752507
            ],
            [
                -118.33540897005454,
                33.82642871822452
            ],
            [
                -118.33541921967488,
                33.826426863335726
            ],
            [
                -118.33542344940071,
                33.826436077343644
            ],
            [
                -118.33542639231051,
                33.82643044674003
            ],
            [
                -118.33541932625792,
                33.8264323433901
            ],
            [
                -118.33542031749415,
                33.82644421336565
            ],
            [
                -118.33542236477875,
                33.82643168077253
            ]
        ], label='car')
    point = context.dl.Gis(
        annotation_type=context.dl.GisType.POINT,
        geo=[
            -118.33540793707832,
            33.82642046242373
        ],
        label='car', )

    polygon = context.dl.Gis(
        annotation_type=context.dl.GisType.POLYGON,
        geo=[
            [
                [
                    -118.33543603201835,
                    33.82641747569975
                ],
                [
                    -118.33541563389792,
                    33.82641375053887
                ],
                [
                    -118.33542170322828,
                    33.82639922240139
                ],
                [
                    -118.33544032988657,
                    33.8264016410096
                ]
            ]
        ], label='car'
    )

    builder = context.item.annotations.builder()
    builder.add(annotation_definition=box)
    builder.add(annotation_definition=point)
    builder.add(annotation_definition=polyline)
    builder.add(annotation_definition=polygon)
    context.item.annotations.upload(annotations=builder)


@behave.given(u"I add a few annotations to image")
def step_impl(context):
    context.dataset = context.dataset.update()
    context.item = context.item.update()
    labels = context.dataset.labels
    height = context.item.height
    width = context.item.width
    if height is None:
        height = 768
    if width is None:
        width = 1536

    # point
    annotation_definition = context.dl.Point(
        x=r.randrange(0, width),
        y=r.randrange(0, height),
        label=r.choice(labels).tag,
        attributes={"1": "attr1"},
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)
    # box
    top = r.randrange(0, height)
    left = r.randrange(0, width)
    right = left + 100
    bottom = top + 100
    annotation_definition = context.dl.Box(
        top=top,
        left=left,
        right=right,
        bottom=bottom,
        label=r.choice(labels).tag,
        attributes={"1": "attr1", "2": "attr2"},
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)
    # poliygon
    geo = [
        (r.randrange(0, width), r.randrange(0, height)),
        (r.randrange(0, width), r.randrange(0, height)),
        (r.randrange(0, width), r.randrange(0, height)),
        (r.randrange(0, width), r.randrange(0, height)),
        (r.randrange(0, width), r.randrange(0, height)),
        (r.randrange(0, width), r.randrange(0, height)),
    ]
    annotation_definition = context.dl.Polygon(
        geo=geo, label=r.choice(labels).tag, attributes={"1": "attr1", "2": "attr2"}
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)
    # ellipse
    annotation_definition = context.dl.Ellipse(
        x=r.randrange(0, width),
        y=r.randrange(0, height),
        rx=r.randrange(0, 20),
        ry=r.randrange(0, 20),
        angle=r.randrange(0, 100),
        label=r.choice(labels).tag,
        attributes={"2": "attr2"},
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)


@behave.when(u"I upload annotation collection")
def step_impl(context):
    context.annotation_collection.upload()


@behave.then(u"Annotations in host equal annotations uploaded")
def step_impl(context):
    context.annotation_collection_get = context.item.annotations.list()
    assert len(context.annotation_collection_get) == len(context.annotation_collection)
    for ann_get in context.annotation_collection_get:
        for ann in context.annotation_collection:
            if ann.type == ann_get.type:
                break
        assert ann.label == ann_get.label
        assert ann.attributes == ann_get.attributes
        assert ann.coordinates == ann_get.coordinates


@behave.given(u"I add a few annotations to video")
def step_impl(context):
    context.dataset = context.dataset.update()
    context.item = context.item.update()
    labels = context.dataset.labels
    height = 1088
    width = 1920

    # point
    annotation_definition = context.dl.Point(
        x=r.randrange(0, width),
        y=r.randrange(0, height),
        label=r.choice(labels).tag,
        attributes={"1": "attr1"},
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)
    # box
    top = r.randrange(0, height)
    left = r.randrange(0, width)
    right = left + 100
    bottom = top + 100
    annotation_definition = context.dl.Box(
        top=top,
        left=left,
        right=right,
        bottom=bottom,
        label=r.choice(labels).tag,
        attributes={"1": "attr1", "2": "attr2"},
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)
    # ellipse
    annotation_definition = context.dl.Ellipse(
        x=r.randrange(0, width),
        y=r.randrange(0, height),
        rx=20,
        ry=10,
        angle=r.randrange(0, 100),
        label=r.choice(labels).tag,
        attributes={"2": "attr2"},
    )
    context.annotation_collection.add(annotation_definition=annotation_definition)


@behave.given(u"I add a few frames to annotations")
def step_impl(context):
    for ann in context.annotation_collection:
        for i in range(20):
            if ann.type == "box":
                top = ann.top + (i * 10)
                left = ann.left + (i * 10)
                right = ann.right + (i * 10)
                bottom = ann.bottom + (i * 10)
                annotation_definition = context.dl.Box(
                    top=top,
                    left=left,
                    right=right,
                    bottom=bottom,
                    label=ann.label,
                    attributes=ann.attributes,
                )
            elif ann.type == "point":
                x = ann.x + (i * 10)
                y = ann.y + (i * 10)
                annotation_definition = context.dl.Point(
                    x=x, y=y, label=ann.label, attributes=ann.attributes
                )
            elif ann.type == "ellipse":
                x = ann.x + (i * 10)
                y = ann.y + (i * 10)
                rx = ann.rx
                ry = ann.ry
                angle = ann.angle
                annotation_definition = context.dl.Ellipse(
                    x=x,
                    y=y,
                    rx=rx,
                    ry=ry,
                    angle=angle,
                    label=ann.label,
                    attributes=ann.attributes,
                )
            ann.add_frame(annotation_definition=annotation_definition, frame_num=i * 10)


================================================
File: tests/features/steps/annotation_entity/annotation_entity_interface.py
================================================
import behave
import os


@behave.when(u'I call Annotation.download() using the given params')
def step_impl(context):
    context.filepath = None
    context.annotation_format = None
    context.height = None
    context.width = None
    context.thickness = 1
    context.with_text = False
    context.alpha = 1

    annotation_format_list = {
        "JSON": context.dl.VIEW_ANNOTATION_OPTIONS_JSON,
        "MASK": context.dl.VIEW_ANNOTATION_OPTIONS_MASK,
        "INSTANCE": context.dl.VIEW_ANNOTATION_OPTIONS_INSTANCE,
        "ANNOTATION_ON_IMAGE": context.dl.VIEW_ANNOTATION_OPTIONS_ANNOTATION_ON_IMAGE,
        "VTT": context.dl.VIEW_ANNOTATION_OPTIONS_VTT,
        "OBJECT_ID": context.dl.VIEW_ANNOTATION_OPTIONS_OBJECT_ID
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "filepath":
            context.filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "annotation_format":
            context.annotation_format = annotation_format_list[parameter.cells[1]]

        if parameter.cells[0] == "height":
            context.height = float(parameter.cells[1])

        if parameter.cells[0] == "width":
            context.width = float(parameter.cells[1])

        if parameter.cells[0] == "thickness":
            context.thickness = float(parameter.cells[1])

        if parameter.cells[0] == "with_text":
            context.with_text = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "alpha":
            context.alpha = float(parameter.cells[1])

    if context.annotation_format is None:
        context.annotation.download(
            filepath=context.filepath,
            height=context.height,
            width=context.width,
            thickness=context.thickness,
            with_text=context.with_text,
            alpha=context.alpha
        )
    else:
        context.annotation.download(
            filepath=context.filepath,
            annotation_format=context.annotation_format,
            height=context.height,
            width=context.width,
            thickness=context.thickness,
            with_text=context.with_text,
            alpha=context.alpha
        )


@behave.when(u'I update annotation attributes with params')
def step_impl(context):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']

    context.annotation.attributes = params
    context.annotation = context.annotation.update(True)


@behave.when(u'I update annotation attributes to empty dict')
def step_impl(context):
    context.annotation.attributes = {}
    context.annotation = context.annotation.update(True)


================================================
File: tests/features/steps/annotation_entity/test_annotation_description.py
================================================
import behave


@behave.when(u'I add description "{text}" to the annotation')
def step_impl(context, text):
    context.annotation.description = text
    context.annotation.update()


@behave.then(u'I validate annotation.description has "{text}" value')
def step_impl(context, text):
    if text == "None":
        text = None

    context.annotation = context.item.annotations.get(annotation_id=context.annotation.id)
    assert context.annotation.description == text


@behave.when(u'I remove description from the annotation')
def step_impl(context):
    context.annotation.description = None
    context.annotation.update()


================================================
File: tests/features/steps/annotation_entity/test_annotation_json_to_object.py
================================================
import logging
import behave
import json
import random as r
import time
import dictdiffer


@behave.then(u'Object "{entity}" to_json() equals to Platform json.')
def step_impl(context, entity):
    if entity != 'Annotations':
        entity = entity.lower()
        if entity == 'feature_set':
            url_path = '/features/sets/{}'.format(getattr(context, entity).id)
        elif entity == 'feature':
            url_path = '/features/vectors/{}'.format(getattr(context, entity).id)
        else:
            url_path = "/{}s/{}".format(entity, getattr(context, entity).id)
        success, response = getattr(context, entity)._client_api.gen_request(
            req_type="get",
            path=url_path
        )

        entity_to_json = getattr(context, entity).to_json()
        response = response.json()

        assert success
        if entity == 'item':
            if 'metadata' in response and 'system' in response['metadata']:
                response['metadata']['system'].pop('executionLogs', None)
            if 'metadata' in entity_to_json and 'system' in entity_to_json['metadata']:
                entity_to_json['metadata']['system'].pop('executionLogs', None)
        elif entity == 'service':
            if 'runtime' in response:
                if 'autoscaler' not in response['runtime']:
                    response['runtime']['autoscaler'] = None
        elif entity == 'dataset':
            entity_to_json.pop('export', None)
            response.pop('export', None)
        elif entity == 'execution':
            entity_to_json.pop('status')
            response.pop('status')
            entity_to_json.pop('latestStatus')
            response.pop('latestStatus')
            entity_to_json.pop('statusLog')
            response.pop('statusLog')
            entity_to_json.pop('updatedAt')
            response.pop('updatedAt')
        elif entity == 'project':
            entity_to_json.pop('isBlocked', None)
            response.pop('isBlocked', None)
        elif entity == 'package':
            if 'modules' in entity_to_json:
                for module in entity_to_json['modules']:
                    module.pop('computeConfig', None)

        if entity_to_json != response:
            assert False, "TEST FAILED: Different in response and entity_to_json.\n{}".format(
                list(dictdiffer.diff(response, entity_to_json)))
    else:
        annotations_list = context.item.annotations.list()
        for ann in annotations_list:
            success, response = context.item._client_api.gen_request(
                req_type="get",
                path="/datasets/%s/items/%s/annotations/%s"
                     % (context.item.dataset.id, context.item.id, ann.id),
            )
            ann_json = ann.to_json()
            response = response.json()
            assert success

            if ann_json.get('hash', None) is not None and ann_json.get('hash', None).startswith('NO'):
                ann_json.pop('hash', None)
                response.pop('hash', None)

            if ann_json.get('attributes', None) is None:
                ann_json.pop('attributes', None)

            # 'segment', 'polyline'  remove metadata because response has no metadata
            # 'box', 'point', 'ellipse' sdk remove the system metadata if empty
            if ann.type in ['segment', 'polyline', 'box', 'point', 'ellipse']:
                if 'metadata' in ann_json:
                    ann_json.pop('metadata')
                if 'metadata' in response:
                    response.pop('metadata')

            # compare json
            if response != ann_json:
                assert False, "TEST FAILED: Different in response and ann_json.\n{}".format(
                    list(dictdiffer.diff(response, ann_json)))


@behave.when(u"I create a blank annotation to item")
def step_impl(context):
    context.dataset = context.dataset.update()
    context.item = context.item.update()
    if context.item.fps is None:
        context.item.fps = 25
    logging.warning('item fps is: {}'.format(context.item.fps))
    context.annotation = context.dl.Annotation.new(item=context.item)


@behave.when(u"I add annotation to item using add annotation method")
def step_impl(context):
    context.dataset = context.dataset.update()
    item = context.item.update()
    labels = context.dataset.labels
    height = item.height
    if height is None:
        height = 768
    width = item.width
    if width is None:
        width = 1536

    # box
    top = r.randrange(0, height)
    left = r.randrange(0, width)
    right = left + 100
    bottom = top + 100
    annotation_definition = context.dl.Box(
        top=top,
        left=left,
        right=right,
        bottom=bottom,
        label=r.choice(labels).tag,
        attributes={"1": "attr1", "2": "attr2"},
    )
    context.annotation = context.dl.Annotation.new(
        annotation_definition=annotation_definition, item=context.item
    )

@behave.when(u"I add annotation to item using add annotation method with empty dict attrs")
def step_impl(context):
    context.dataset = context.dataset.update()
    item = context.item.update()
    labels = context.dataset.labels
    height = item.height
    if height is None:
        height = 768
    width = item.width
    if width is None:
        width = 1536

    # box
    top = r.randrange(0, height)
    left = r.randrange(0, width)
    right = left + 100
    bottom = top + 100
    annotation_definition = context.dl.Box(
        top=top,
        left=left,
        right=right,
        bottom=bottom,
        label=r.choice(labels).tag,
        attributes={},
    )
    context.annotation = context.dl.Annotation.new(
        annotation_definition=annotation_definition, item=context.item
    )

@behave.then(u"I validate annotation has all expected attributes")
def step_impl(context):
    item = context.dl.items.get(item_id=context.item.id)
    anns = item.annotations.list()
    for ann in anns:
        assert ann.attributes in [["attr1", "attr2"], {}, {'4': "true"}], f"TEST FAILED: attributes not exist - {ann.attributes}"

@behave.when(u'I add annotation with attrs "{key}" "{value}" to item using add annotation method')
def step_impl(context, key, value):
    context.dataset = context.dataset.update()
    item = context.item.update()
    labels = context.dataset.labels
    height = item.height
    if height is None:
        height = 768
    width = item.width
    if width is None:
        width = 1536

    # box
    top = r.randrange(0, height)
    left = r.randrange(0, width)
    right = left + 100
    bottom = top + 100
    annotation_definition = context.dl.Box(
        top=top,
        left=left,
        right=right,
        bottom=bottom,
        label=r.choice(labels).tag
    )
    context.annotation = context.dl.Annotation.new(
        annotation_definition=annotation_definition, item=context.item
    )
    context.annotation.attributes = {key: value}


@behave.then(u'I validate annotation has attribute "{key}" with value "{value}"')
def step_impl(context, key, value):
    assert context.annotation.attributes[key] == value, "TEST FAILED: attribute value not equal"


@behave.then(u'I validate annotation has no attributes')
def step_impl(context):
    assert context.annotation.attributes == {}, f"TEST FAILED: attribute value not equal - {context.annotation.attributes}"


@behave.when(u"I add annotation to audio using add annotation method")
def step_impl(context):
    context.dataset = context.dataset.update()
    item = context.item.update()
    labels = context.dataset.labels
    builder = item.annotations.builder()

    builder.add(annotation_definition=context.dl.Subtitle(text="this is a test", label=r.choice(labels).tag),
                start_time=10,
                end_time=12,
                object_id=1)

    item.annotations.upload(builder)


@behave.when(u"I upload annotation created")
def step_impl(context):
    logging.warning('item fps is: {}'.format(context.item.fps))
    context.annotation = context.annotation.upload()


@behave.when(u"I add some frames to annotation")
def step_impl(context):
    if context.item.fps is None:
        context.item.fps = 25
    ann = context.annotation
    for i in range(20):
        top = ann.top + (i * 10)
        left = ann.left + (i * 10)
        right = ann.right + (i * 10)
        bottom = ann.bottom + (i * 10)
        annotation_definition = context.dl.Box(
            top=top,
            left=left,
            right=right,
            bottom=bottom,
            label=ann.label,
            attributes=ann.attributes,
        )
        frame_num = i * 10
        ann.add_frame(annotation_definition=annotation_definition, frame_num=frame_num)


@behave.then(u"Item in host has annotation added")
def step_impl(context):
    if context.item.fps is None:
        context.item.fps = 25
        context.item.update(system_metadata=True)
        time.sleep(4)
        context.item = context.dataset.items.get(item_id=context.item.id)
    if context.item.fps is None:
        context.item.fps = 25
        time.sleep(1)
    try:
        annotations = context.item.annotations.list()
        context.annotation_get = annotations[0]
    except:
        logging.error(context.item.to_json())
    assert context.annotation_get.label == context.annotation.label
    assert context.annotation_get.attributes == context.annotation.attributes
    assert context.annotation_get.coordinates == context.annotation.coordinates
    assert context.annotation_get.to_json() == context.annotation.to_json()
    if 'snapshots_' in context.annotation_get.metadata['system']:
        assert len(context.annotation_get.metadata['system']['snapshots_']) == 19


@behave.then(u"audio in host has annotation added")
def step_impl(context):
    context.item = context.dataset.items.get(item_id=context.item.id)
    try:
        annotations = context.item.annotations.list()
        context.annotation_get = annotations[0]
    except:
        logging.error(context.item.to_json())
    assert context.annotation_get.start_time == 10
    assert context.annotation_get.end_time == 12


@behave.when(u"I add frames to annotation")
def step_impl(context):
    ann = context.annotation
    top = 100
    left = 100
    right = left + 100
    bottom = top + 100
    label = context.dataset.labels[0]
    for i in range(20):
        top = top + (i * 10)
        left = left + (i * 10)
        right = right + (i * 10)
        bottom = bottom + (i * 10)
        annotation_definition = context.dl.Box(
            top=top, left=left, right=right, bottom=bottom, label=label.tag
        )
        frame_num = i * 10
        ann.add_frame(annotation_definition=annotation_definition, frame_num=frame_num)


@behave.then(u"Item in host has video annotation added")
def step_impl(context):
    context.annotation_get = context.item.annotations.list()[0]
    assert context.annotation_get.label == context.annotation.label
    assert context.annotation_get.attributes == context.annotation.attributes
    assert context.annotation_get.coordinates == context.annotation.coordinates
    # can check frames since the video decoder
    # assert len(context.annotation_get.frames) == len(context.annotation.frames)


@behave.when(u"I create a false fixed annotation in video")
def step_impl(context):
    system = context.item.metadata.get('system', dict())
    nb_frames = system.get('nb_frames', None)

    num_tries = 60
    interval_time = 5
    found_frames = False

    if not nb_frames:
        for i in range(num_tries):
            time.sleep(interval_time)
            nb_frames = system.get('ffmpeg', dict()).get('nb_read_frames', None)
            if nb_frames:
                nb_frames = int(nb_frames)
                found_frames = True
                break

    assert isinstance(nb_frames,
                      int), f"TEST FAILED: nb_frames is not defined after {round(num_tries * interval_time / 60, 1)} minutes"
    ann = context.annotation
    label = context.dataset.labels[0]
    for frame in range(nb_frames):
        annotation_definition = context.dl.Box(
            top=100, left=100, right=500, bottom=500, label=label.tag
        )
        frame_num = frame
        ann.add_frame(annotation_definition=annotation_definition, frame_num=frame_num, fixed=False)

    return found_frames


@behave.then(u"Video has annotation without snapshots")
def step_impl(context):
    assert context.item.annotations.list()[0].metadata['system']['snapshots_'] == []


@behave.when(u"I get annotation using dl")
def step_impl(context):
    context.dl_annotation = context.dl.annotations.get(annotation_id=context.annotation.id)


@behave.then(u"I validate annotation have frames")
def step_impl(context):
    assert context.dl_annotation.start_time == 0.0
    assert context.dl_annotation.end_time == 7.6
    assert context.dl_annotation.start_frame == 0
    assert context.dl_annotation.end_frame == 190
    assert context.dl_annotation.to_json()['metadata']['system']['snapshots_'] == \
           context.annotation.to_json()['metadata']['system']['snapshots_']


@behave.when(u'I update annotation start time to "{start_time}"')
def step_impl(context, start_time):
    context.start_time = float(start_time)
    context.annotation_get = context.item.annotations.list()[0]
    context.annotation_get.start_time = context.start_time
    context.annotation_get = context.annotation_get.update(True)
    context.start_frame = context.annotation_get.start_frame


@behave.then(u"I validate snapshot has the correct start frame")
def step_impl(context):
    assert context.annotation_get.metadata['system']['snapshots_'][0][
        'frame'], "TEST FAILED: Missing frame in snapshots_ \n{}".format(context.annotation_get.metadata['system'])
    assert context.annotation_get.metadata['system']['snapshots_'][0][
               'frame'] > context.start_frame, "TEST FAILED: Wrong start frame"


@behave.when(u'I update annotation start time "{start_time}" end time "{end_time}"')
def step_impl(context, start_time, end_time):
    context.start_time = float(start_time)
    context.end_time = float(end_time)
    context.annotation_get = context.item.annotations.list()[0]
    context.annotation_get.start_time = context.start_time
    context.annotation_get.end_time = context.end_time
    context.annotation_get = context.annotation_get.update(True)
    context.start_time = context.annotation_get.start_time
    context.end_time = context.annotation_get.end_time


@behave.then(u"I validate audio has the correct start and end time")
def step_impl(context):
    assert context.annotation_get.metadata['system'][
               'startTime'] == context.start_time, "TEST FAILED: failed to update startTime from {} to {}".format(
        context.annotation_get.metadata['system']['startTime'], context.start_time)
    assert context.annotation_get.metadata['system'][
               'endTime'] == context.end_time, "TEST FAILED: failed to update endTime from {} to {}".format(
        context.annotation_get.metadata['system']['endTime'],
        context.end_time)


@behave.when(u"I add class annotation to item using add annotation method")
def step_impl(context):
    labels = context.dataset.labels
    context.label = r.choice(labels).tag
    context.annotation_definition = context.dl.Classification(
        label=context.label,
        attributes={"1": "attr1", "2": "attr2"},
    )
    context.annotation = context.dl.Annotation.new(
        annotation_definition=context.annotation_definition, item=context.item, end_time=1
    )


@behave.when(u'I set frame "{frame}" annotation attributes')
def step_impl(context, frame):
    context.new_attributes = {"1": "attr1", "2": "attr4"}
    context.old_attributes = context.annotation_definition.attributes
    annotation_definition = context.dl.Classification(
        label=context.label,
        attributes=context.new_attributes,
    )
    context.annotation.add_frame(annotation_definition=annotation_definition, frame_num=int(frame))


@behave.then(u'I validity "{frames}" has the updated attributes')
def step_impl(context, frames):
    context.annotation_get = context.item.annotations.list()[0]
    assert context.annotation_get.attributes == context.old_attributes, "TEST FAILED: attributes not updated"
    assert context.annotation_get.frames[
               int(frames)].attributes == context.new_attributes, "TEST FAILED: attributes not updated"


================================================
File: tests/features/steps/annotation_entity/test_annotation_repo_methods.py
================================================
import behave
import os
import random


@behave.when(u"I delete entity annotation x")
def step_impl(context):
    context.annotation_x.delete()


@behave.when(u'I download Entity annotation x with "{annotation_format}" to "{img_filepath}"')
def step_impl(context, img_filepath, annotation_format):
    import cv2
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], img_filepath)
    if not os.path.isdir(os.path.dirname(path)):
        os.makedirs(os.path.dirname(path), exist_ok=True)
    img_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], '0000000162.jpg')
    img = cv2.imread(img_path)
    dimensions = img.shape
    height = dimensions[0]
    width = dimensions[1]
    context.annotation_x.download(
        filepath=path,
        annotation_format=annotation_format,
        height=height,
        width=width,
        thickness=1,
        with_text=False)


@behave.given(u'I change annotation x label to "{new_label}"')
def step_impl(context, new_label):
    context.annotation_x.label = new_label


@behave.given(u'I add field "{new_field}" to annotation x system metadata')
def step_impl(context, new_field):
    context.annotation_x.metadata = dict()
    context.annotation_x.metadata["system"] = new_field


@behave.when(u"I update annotation entity")
def step_impl(context):
    context.annotation_x.update(system_metadata=True)


@behave.then(u'Annotation x in host has label "{new_label}"')
def step_impl(context, new_label):
    assert context.annotation_get.label == new_label


# @behave.then(u'Annotation x in host has field "{new_field}" in system metadata')
# def step_impl(context, new_field):
#     assert new_field in context.annotation_get.metadata['system']


@behave.when(u"I get annotation x from host")
def step_impl(context):
    context.annotation_get = context.item.annotations.get(
        annotation_id=context.annotation_x.id
    )


@behave.when(u"I upload annotation")
def step_impl(context):
    annotation = {
        "type": "box",
        "label": "car",
        "attributes": ["Occlusion2"],
        "coordinates": [
            {"x": 714.086399, "y": 194.834737},
            {"x": 812.802021, "y": 243.433196},
        ]
    }
    context.item.annotations.upload(annotations=annotation)
    context.annotation = context.item.annotations.list()[0]
    context.item.annotations.delete(context.annotation)
    assert len(context.item.annotations.list()) == 0


@behave.when(u"I upload random x annotations")
def step_impl(context):
    annotation = {
        "type": "box",
        "label": "car",
        "metadata": {
          "system": {
            "attributes": {"1":  "Occlusion2"}
          }
        },
        "coordinates": [
            {"x": random.randrange(0, 500), "y": random.randrange(0, 500)},
            {"x": random.randrange(0, 500), "y": random.randrange(0, 500)},
        ]
    }
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)
    context.annotation_count = random.randrange(1, len(context.dataset.items.list()))

    itemList = context.dataset.items.list().items
    for i in range(context.annotation_count):
        context.item = itemList[i]
        context.item.annotations.upload(annotations=annotation)


@behave.then(u"analytic should say I have x annotations")
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)
    assert (context.annotation_count == context.dataset.annotated)


@behave.then(u"Item in host have annotation uploaded")
def step_impl(context):
    context.annotation.upload()
    host_annotation = context.item.annotations.list()[0]
    assert host_annotation.label == context.annotation.label
    assert host_annotation.type == context.annotation.type


@behave.given(u'I create an annotation')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)
    labels = context.dataset.labels
    context.annotation_point = context.dl.Annotation.new(item=context.item,
                                                         annotation_definition=context.dl.Point(x=100, y=150,
                                                                                                label=labels[0].tag,
                                                                                                attributes={"1": "attr1"}))
    context.annotation_box = context.dl.Annotation.new(item=context.item,
                                                       annotation_definition=context.dl.Box(left=100, top=200,
                                                                                            right=140, bottom=120,
                                                                                            label=labels[1].tag))
    context.annotation_ellipse = context.dl.Annotation.new(item=context.item,
                                                           annotation_definition=context.dl.Ellipse(x=300, y=300,
                                                                                                    angle=50, ry=20,
                                                                                                    rx=10,
                                                                                                    label=labels[2].tag,
                                                                                                    attributes={"1": "attr2"}))
    context.annotation_polygon = context.dl.Annotation.new(item=context.item, annotation_definition=context.dl.Polygon(
        geo=[(300, 300), (320, 200), (350, 400), (300, 300)], label=labels[0].tag))
    context.num_annotations = 4


@behave.when(u'I upload annotation entity to item')
def step_impl(context):
    context.annotation_point.upload()
    context.annotation_box.upload()
    context.annotation_ellipse.upload()
    context.annotation_polygon.upload()


@behave.then(u'Item in host has annotation entity created')
def step_impl(context):
    annotations = context.item.annotations.list()
    assert len(annotations) == context.num_annotations, 'Missing annotation: uploaded: {}, found on item: {}'.format(
        len(annotations), context.num_annotations)
    for ann in annotations:
        if ann.type == 'box':
            assert context.annotation_box.label == ann.label
            # TODO
            # assert context.annotation_box.coordinates == ann.coordinates
            assert context.annotation_box.attributes == ann.attributes
        if ann.type == 'point':
            assert context.annotation_point.label == ann.label
            assert context.annotation_point.coordinates == ann.coordinates
            assert context.annotation_point.attributes == ann.attributes
        if ann.type == 'ellipse':
            assert context.annotation_ellipse.label == ann.label
            assert context.annotation_ellipse.coordinates == ann.coordinates
            assert context.annotation_ellipse.attributes == ann.attributes
        if ann.type == 'segment':
            assert context.annotation_polygon.label == ann.label
            assert context.annotation_polygon.coordinates == ann.coordinates
            assert context.annotation_polygon.attributes == ann.attributes


@behave.given(u'I create a video annotation')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)
    labels = context.dataset.labels
    x = 200
    y = 200
    context.annotation = context.dl.Annotation.new(item=context.item,
                                                   annotation_definition=context.dl.Point(x=200, y=200,
                                                                                          label=labels[1].tag))
    ann = context.annotation
    for i in range(300):
        context.annotation.add_frame(annotation_definition=context.dl.Point(x=x + i,
                                                                            y=y + i,
                                                                            label=ann.label))


@behave.when(u'I upload video annotation entity to item')
def step_impl(context):
    context.annotation.upload()


@behave.then(u'Item in host has video annotation entity created')
def step_impl(context):
    context.item = context.item.update()
    annotation = context.item.annotations.list()[0]
    if 'coordinateVersion' in annotation.metadata.get('system', {}) and 'coordinateVersion' not in context.annotation.metadata.get('system', {}):
        annotation.metadata['system'].pop('coordinateVersion')
    assert annotation.to_json()['metadata'] == context.annotation.to_json()['metadata']
    assert annotation.type == context.annotation.type
    assert annotation.label == context.annotation.label


@behave.when(u'I create video annotation by frames')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)
    labels = context.dataset.labels
    x = 200
    y = 200
    context.annotation = context.dl.Annotation.new(item=context.item)
    for i_frame in range(20):
        context.annotation.add_frame(annotation_definition=context.dl.Point(x=x, y=y, label=labels[1].tag),
                                     frame_num=i_frame,
                                     fixed=False)
    context.annotation = context.annotation.upload()


@behave.when(u'I add same frames to video annotation')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)
    labels = context.dataset.labels
    x = 200
    y = 200
    for i_frame in range(20, 40):
        context.annotation.add_frame(annotation_definition=context.dl.Point(x=x, y=y, label=labels[1].tag),
                                     frame_num=i_frame,
                                     fixed=False)
    context.annotation.update(True)


@behave.then(u'annotation do not have snapshots')
def step_impl(context):
    assert context.annotation.metadata['system']['snapshots_'] == [], 'Snapshots should be empty'
    assert context.annotation.to_json()['metadata']['system']['snapshots_'] == [], 'Snapshots should be empty'


================================================
File: tests/features/steps/annotation_entity/test_segmentation_to_box.py
================================================
from behave import given, then, when
import numpy as np
import dtlpy as dl


def mask_from_circle(h, w, center, radius):
    x, y = np.ogrid[:h, :w]
    dist_from_center = np.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2)

    mask = dist_from_center <= radius
    return mask


@given(u'I have a segmentation annotation')
def step_impl(context):
    builder = context.item.annotations.builder()
    builder.add(
        annotation_definition=context.dl.Segmentation.from_polygon(
            geo=np.array([[100, 200], [150, 200], [300, 100], [200, 400]]),
            label='Person',
            shape=(context.item.height, context.item.width)))
    annotations = builder.upload()
    context.annotation = annotations[0]


@given(u'I have a multi segmentation annotations')
def step_impl(context):
    builder = context.item.annotations.builder()

    mask1 = mask_from_circle(context.item.height, context.item.width, center=(200, 200), radius=100)
    mask2 = mask_from_circle(context.item.height, context.item.width, center=(400, 400), radius=100)
    mask = np.ma.mask_or(mask1, mask2)

    builder.add(annotation_definition=context.dl.Segmentation(mask, label='person'))
    annotations = builder.upload()
    context.annotation = annotations[0]


@when(u'I execute to_box function on segmentation annotation')
def step_impl(context):
    bbox = context.annotation.annotation_definition.to_box()
    builder = context.item.annotations.builder()
    builder.add(annotation_definition=bbox)
    annotations = builder.upload()
    context.bboxes = annotations


@when(u'I create Box annotation with  from_segmentation function with mask')
def step_impl(context):
    bbox = dl.Box.from_segmentation(context.annotation.annotation_definition.geo,
                                    context.annotation.label)
    builder = context.item.annotations.builder()
    builder.add(annotation_definition=bbox)
    annotations = builder.upload()
    context.bboxes = annotations


@then(u'Box will be generate')
def step_impl(context):
    should_be_geo = [[100, 100], [300, 400]]
    assert should_be_geo == context.bboxes[0].geo


@then(u'Boxes will be generate')
def step_impl(context):
    should_be_geo1 = [[100, 100], [300, 300]]
    should_be_geo2 = [[300, 300], [500, 500]]
    assert should_be_geo1 == context.bboxes[0].geo
    assert should_be_geo2 == context.bboxes[1].geo


@then(u'annotation color is set to recipe color')
def step_impl(context):
    context.dataset = dl.datasets.get(dataset_id=context.dataset.id)
    context.annotation = context.dataset.annotations.get(annotation_id=context.annotation.id)
    assert context.annotation.color == context.dataset._get_ontology().color_map[context.annotation.label], 'annotation color is not set to recipe color'


================================================
File: tests/features/steps/annotation_entity/test_segmentation_to_polygon.py
================================================
import behave
import os
import numpy as np


@behave.when(u'I call Polygon.from_segmentation() using "{max_instances}" nax_instances')
def step_impl(context, max_instances):
    polygon = context.dl.Polygon.from_segmentation(context.annotation.annotation_definition.geo,
                                                   context.annotation.label,
                                                   max_instances=int(max_instances))
    builder = context.item.annotations.builder()
    builder.add(annotation_definition=polygon)
    annotations = builder.upload().annotations
    context.polygons = annotations


@behave.then(u'The polygon will match to the json file "{json_file_path}"')
def step_impl(context, json_file_path):
    polygons_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], json_file_path)
    annotations = context.item.annotations.upload(polygons_path).annotations

    for i in range(len(annotations)):
        assert np.array_equal(context.polygons[i].geo, annotations[i].geo)


================================================
File: tests/features/steps/annotation_entity/test_upload_annotations.py
================================================
from behave import given, when, then
import os
import json
from PIL import Image
import numpy as np
import dtlpy as dl


@given(u'Dataset ontology has attributes "{first_attribute}" and "{second_attribute}"')
def step_impl(context, first_attribute, second_attribute):
    context.recipe = context.dataset.recipes.list()[0]
    context.ontology = context.recipe.ontologies.list()[0]
    context.ontology.attributes = [first_attribute, second_attribute]
    context.ontology = context.recipe.ontologies.update(context.ontology)


@when(u'Item is annotated with annotations in file: "{annotations_path}"')
def step_impl(context, annotations_path):
    annotations_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], annotations_path)

    with open(annotations_path, 'r') as f:
        context.annotations = json.load(f)
    context.item.annotations.upload(context.annotations)
    context.last_response = context.item._client_api.last_response.json()


@then(u'Item video annotations in host equal annotations in file "{annotations_path}"')
def step_impl(context, annotations_path):
    annotations_get = context.item.annotations.list()
    for ann in annotations_get:
        metadata = ann.to_json()['metadata']
        ann = {'type': ann.type,
               'attributes': ann.attributes,
               'label': ann.label,
               'coordinates': ann.coordinates,
               'metadata': metadata}

        if 'coordinateVersion' in ann['metadata']['system']:
            ann['metadata']['system'].pop('coordinateVersion')

        assert ann in context.annotations


@then(u'Item annotations in host equal annotations in file "{annotations_path}"')
def step_impl(context, annotations_path):
    annotations_list = context.item.annotations.list()
    anns = list()
    for ann in context.annotations:
        ann = {'type': ann['type'],
               'attributes': ann.get('metadata', {}).get('system', {}).get('attributes'),
               'label': ann['label'],
               'coordinates': ann['coordinates']}
        anns.append(ann)
    for ann in annotations_list:
        ann = {'type': ann.type,
               'attributes': ann.attributes,
               'label': ann.label,
               'coordinates': ann.coordinates}
        assert ann in anns


@when(u'I draw all annotations to image "{image_path}"')
def step_impl(context, image_path):
    image_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], image_path)
    annotations_get = context.item.annotations.list()
    context.item = context.dataset.items.get(item_id=context.item.id)
    # try loading from numpy
    # img_arr = np.asarray(Image.open(image_path))
    img_arr = np.load(image_path + '.npy')

    for ann in annotations_get:
        img_arr = ann.show(image=img_arr)
    context.image_drawn = img_arr


@then(u'Image drawn to "{drawn_image_path}" equal image in "{should_be_path}"')
def step_impl(context, should_be_path, drawn_image_path):
    should_be_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], should_be_path)
    drawn_image_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], drawn_image_path)

    # try loading from numpy
    # im_drawn = np.asarray(Image.open(drawn_image_path))
    # im_should = np.asarray(Image.open(should_be_path))
    im_drawn = np.load(drawn_image_path + '.npy')
    im_should = np.load(should_be_path + '.npy')

    assert (im_drawn == im_should).all


@when(u'I draw to image in "{im_path}" all annotations with param "{annotation_format}"')
def step_impl(context, annotation_format, im_path):
    im_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], im_path)
    filters = dl.Filters(resource=dl.FiltersResource.ANNOTATION)
    filters.sort_by(field='label', value=dl.FiltersOrderByDirection.ASCENDING)
    filters.sort_by(field='createdAt', value=dl.FiltersOrderByDirection.DESCENDING)
    annotations_get = context.item.annotations.list(filters=filters)
    context.item = context.dataset.items.get(item_id=context.item.id)

    # make sure we have height and width 
    if context.item.height is None:
        context.item.height = 768
    if context.item.width is None:
        context.item.width = 1536

    # try loading from numpy
    # context.img_arr = np.asarray(Image.open(im_path)))
    context.img_arr = np.load(im_path + '.npy')

    context.img_arr = context.img_arr.copy()
    for ann in annotations_get:
        context.img_arr = ann.show(image=context.img_arr, annotation_format=annotation_format)
    context.mask = context.img_arr


@then(u'Annotations drawn equal numpy in "{should_be_path}"')
def step_impl(context, should_be_path):
    should_be_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], should_be_path)
    if not np.array_equal(context.mask, np.load(should_be_path)):
        np.save(should_be_path.replace('.npy', '_wrong.npy'), context.mask)
        assert False


@given(u'Classes in file: "{labels_path}" are uploaded to test Dataset')
def step_impl(context, labels_path):
    labels_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], labels_path)
    context.recipe = context.dataset.recipes.list()[0]
    context.ontology = context.recipe.ontologies.list()[0]
    with open(labels_path, 'r', encoding="utf8") as f:
        context.labels = json.load(f)
    context.ontology.add_labels(label_list=context.labels)
    context.ontology = context.ontology.update(system_metadata=True)
    for label in context.ontology.labels:
        assert label.to_root() in context.labels
    assert len(context.ontology.labels) == len(context.labels)


================================================
File: tests/features/steps/annotations_repo/annotations_interface.py
================================================
import behave
import os
import json
from time import sleep


@behave.given(u'I upload annotation in the path "{annotation_path}" to the item')
def step_impl(context, annotation_path):
    context.annotation_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], annotation_path)
    context.annotation = context.item.annotations.upload(annotations=context.annotation_path)[0]


@behave.given(u'I upload note annotation to the item with the params')
def step_impl(context):
    # Used to get item height and width from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    context.left = 50
    context.top = 50
    context.right = 100
    context.bottom = 100
    context.label = "label1"
    context.attributes = None
    context.assignee = context.dl.info()['user_email']
    context.creator = context.dl.info()['user_email']
    context.description = None
    context.messages = None

    context.object_id = None
    context.frame_num = None
    context.end_frame_num = None

    for parameter in context.table.rows:
        # Annotation params
        if parameter.cells[0] == "left":
            context.left = int(parameter.cells[1])

        if parameter.cells[0] == "top":
            context.top = int(parameter.cells[1])

        if parameter.cells[0] == "right":
            context.right = int(parameter.cells[1])

        if parameter.cells[0] == "bottom":
            context.bottom = int(parameter.cells[1])

        if parameter.cells[0] == "label":
            context.label = parameter.cells[1]

        if parameter.cells[0] == "attributes":
            context.attributes = eval(parameter.cells[1])

        if parameter.cells[0] == "assignee":
            context.assignee = parameter.cells[1]

        if parameter.cells[0] == "creator":
            context.creator = parameter.cells[1]

        if parameter.cells[0] == "description":
            context.description = parameter.cells[1]

        if parameter.cells[0] == "messages":
            context.messages = eval(parameter.cells[1])

        # Builder params
        if parameter.cells[0] == "object_id":
            context.object_id = int(parameter.cells[1])

        if parameter.cells[0] == "frame_num":
            context.frame_num = int(parameter.cells[1])

        if parameter.cells[0] == "end_frame_num":
            context.end_frame_num = int(parameter.cells[1])

    note_annotation = context.dl.Note(
        left=context.left,
        top=context.top,
        right=context.right,
        bottom=context.bottom,
        label=context.label,
        attributes=context.attributes,
        assignee=context.assignee,
        creator=context.creator,
        description=context.description,
        messages=context.messages
    )

    builder = context.item.annotations.builder()
    builder.add(note_annotation, object_id=context.object_id, frame_num=context.frame_num, end_frame_num=context.end_frame_num, fixed=False)
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


@behave.when(u'I upload x annotations to item')
def step_impl(context, x):
    """
    Search keywords: Upload annotations | Add annotations
    """
    context.dataset.add_label(label_name="Box")
    builder = context.item.annotations.builder()
    for i in range(x):
        builder.add(annotation_definition=context.dl.Box(top=10 + i,
                                                         left=10 + i,
                                                         bottom=100 + i,
                                                         right=100 + i,
                                                         label='Box'))
    context.item.annotations.upload(builder)


@behave.when(u'I have box annotation_definition with params')
def step_impl(context):
    params = dict()
    for row in context.table:
        params[row['key']] = eval(row['value'])

    keys = ['label', 'top', 'left', 'bottom', 'right']
    for key in keys:
        if key not in params.keys():
            assert False, f"TEST FAILED: Missing {key} in {params.keys()}"

    context.annotation_definition = context.dl.Box(
        label=params['label'],
        top=params['top'],
        left=params['left'],
        bottom=params['bottom'],
        right=params['right']
    )


================================================
File: tests/features/steps/annotations_repo/audio_annotations_interface.py
================================================
import behave
from time import sleep


@behave.given(u'I upload "{annotation_type}" annotation to the audio item')
def step_impl(context, annotation_type):
    # Used to get item data from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    annotations_definitions_list = {
        "subtitle": context.dl.Subtitle(
            text="text1",
            label="label1",
            attributes={"1": "subtitle1"}
        )
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type])
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


@behave.given(u'I upload "{annotation_type}" annotation with description "{annotation_description}" to the audio item')
def step_impl(context, annotation_type, annotation_description):
    # Used to get item data from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    annotations_definitions_list = {
        "subtitle": context.dl.Subtitle(
            text="text1",
            label="label1",
            attributes={"1": "subtitle1"},
            description=annotation_description
        )
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type])
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


================================================
File: tests/features/steps/annotations_repo/image_annotations_interface.py
================================================
import time

import behave
import numpy as np
from time import sleep


@behave.given(u'I upload "{annotation_type}" annotation to the image item')
def step_impl(context, annotation_type):
    # Used to get item height and width from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    mask = None
    if annotation_type == "semantic segmentation":
        mask = np.zeros(shape=(context.item.height, context.item.width), dtype=np.uint8)
        mask[50:50, 150:200] = 1

    annotations_definitions_list = {
        "classification": context.dl.Classification(
            label="label1",
            attributes={"1": "classification1"}
        ),
        "point": context.dl.Point(
            x=10,
            y=10,
            label="label1",
            attributes={"1": "point1"}
        ),
        "box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "box1"}
        ),
        "rotated box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            angle=45,
            label="label1",
            attributes={"1": "box1"}
        ),
        "cube": context.dl.Cube(
            front_bl=[50, 100],
            front_br=[100, 100],
            front_tr=[100, 50],
            front_tl=[50, 50],
            back_bl=[25, 75],
            back_br=[75, 75],
            back_tr=[75, 25],
            back_tl=[25, 25],
            label="label1",
            attributes={"1": "cube1"}
        ),
        "semantic segmentation": context.dl.Segmentation(
            geo=mask,
            label="label1",
            attributes={"1": "semantic1"}
        ),
        "polygon": context.dl.Polygon(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polygon1"}
        ),
        "polyline": context.dl.Polyline(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polyline1"}
        ),
        "ellipse": context.dl.Ellipse(
            x=100,
            y=100,
            rx=50,
            ry=75,
            angle=0,
            label="label1",
            attributes={"1": "ellipse1"}
        ),
        "note": context.dl.Note(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "note1"},
            assignee=context.dl.info()['user_email'],
            creator=context.dl.info()['user_email']
        ),
        "gis-box": context.dl.Gis(
            annotation_type=context.dl.GisType.BOX,
            geo=[
                [
                    [
                        -118.33545020696846,
                        33.82643304226775
                    ],
                    [
                        -118.33544677833854,
                        33.82643304226775
                    ],
                    [
                        -118.33544677833854,
                        33.826421352507026
                    ],
                    [
                        -118.33545020696846,
                        33.826421352507026
                    ],
                    [
                        -118.33545020696846,
                        33.82643304226775
                    ]
                ]
            ], label='gisbox')
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type])
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


@behave.given(u'I upload "{annotation_type}" annotation with description "{annotation_description}" to the image item')
def step_impl(context, annotation_type, annotation_description):
    # Used to get item height and width from the backend
    num_try = 12
    interval = 5
    success = False

    for i in range(num_try):
        time.sleep(interval)
        context.item = context.dl.items.get(item_id=context.item.id)
        if context.item.height:
            success = True
            break
    assert success, f"TEST FAILED: Failed to get item height. {context.item.metadata} after {round(num_try * interval / 60, 1)} minutes"

    mask = np.zeros(shape=(context.item.height, context.item.width), dtype=np.uint8)
    mask[50:50, 150:200] = 1

    annotations_definitions_list = {
        "classification": context.dl.Classification(
            label="label1",
            attributes={"1": "classification1"},
            description=annotation_description
        ),
        "point": context.dl.Point(
            x=10,
            y=10,
            label="label1",
            attributes={"1": "point1"},
            description=annotation_description
        ),
        "box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "box1"},
            description=annotation_description
        ),
        "rotated box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            angle=45,
            label="label1",
            attributes={"1": "box1"},
            description=annotation_description
        ),
        "cube": context.dl.Cube(
            front_bl=[50, 100],
            front_br=[100, 100],
            front_tr=[100, 50],
            front_tl=[50, 50],
            back_bl=[25, 75],
            back_br=[75, 75],
            back_tr=[75, 25],
            back_tl=[25, 25],
            label="label1",
            attributes={"1": "cube1"},
            description=annotation_description
        ),
        "semantic segmentation": context.dl.Segmentation(
            geo=mask,
            label="label1",
            attributes={"1": "semantic1"},
            description=annotation_description
        ),
        "polygon": context.dl.Polygon(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polygon1"},
            description=annotation_description
        ),
        "polyline": context.dl.Polyline(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polyline1"},
            description=annotation_description
        ),
        "ellipse": context.dl.Ellipse(
            x=100,
            y=100,
            rx=50,
            ry=75,
            angle=0,
            label="label1",
            attributes={"1": "ellipse1"},
            description=annotation_description
        ),
        "note": context.dl.Note(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "note1"},
            assignee=context.dl.info()['user_email'],
            creator=context.dl.info()['user_email'],
            description=annotation_description
        )
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type])
    context.annotation = context.item.annotations.upload(annotations=builder)[0]
    print()


================================================
File: tests/features/steps/annotations_repo/test_annotations_adding_multiple_frames.py
================================================
import behave
from time import sleep


@behave.when(u'I upload "{number_of_annotations}" annotation with the same object id')
def step_impl(context, number_of_annotations):
    # Used to get item data from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    builder = context.item.annotations.builder()
    for i in range(int(number_of_annotations)):
        builder.add(annotation_definition=context.dl.Box(left=50 + i * 15,
                                                         top=50 + i * 15,
                                                         right=250 + i * 15,
                                                         bottom=250 + i * 15,
                                                         label='label1'),
                    object_visible=True,
                    object_id=1,
                    frame_num=i + 10 * i)
    context.item.annotations.upload(builder)


@behave.then(u'I check that I got "{number_of_keyframes}" keyframes')
def step_impl(context, number_of_keyframes):
    frames = context.item.annotations.list()[0].frames

    for i in range(int(number_of_keyframes)):
        current_frame = frames[i + 10 * i]

        left = current_frame.coordinates[0]['x']
        top = current_frame.coordinates[0]['y']
        right = current_frame.coordinates[1]['x']
        bottom = current_frame.coordinates[1]['y']

        assert left == 50 + i * 15
        assert top == 50 + i * 15
        assert right == 250 + i * 15
        assert bottom == 250 + i * 15


@behave.when(u'I update the "{number_of_keyframes}" frame attribute of the annotation')
def step_impl(context, number_of_keyframes):
    context.annotation = context.item.annotations.list()[0]
    context.annotation.frames[int(number_of_keyframes)].attributes = {"1": "a"}
    context.annotation = context.annotation.update(True)


@behave.then(u'I the only frame "{number_of_keyframes}" attribute is updated')
def step_impl(context, number_of_keyframes):
    assert context.annotation.frames[int(number_of_keyframes)].attributes == {"1": "a"}, "The attribute is not updated"
    assert not context.annotation.frames[0].attributes, "The attribute is not updated"


================================================
File: tests/features/steps/annotations_repo/test_annotations_context.py
================================================
import behave


@behave.given(u'I append item to Items')
def step_impl(context):
    if not hasattr(context, "items"):
        context.items = list()
    context.items.append(context.item)


@behave.when(u'I get the annotation from item number {item_index}')
def step_impl(context, item_index):
    context.annotation = context.items[int(item_index) - 1].annotations.get(annotation_id=context.annotation.id)


@behave.when(u'I get the annotation from dataset number {dataset_index}')
def step_impl(context, dataset_index):
    context.annotation = context.datasets[int(dataset_index) - 1].annotations.get(annotation_id=context.annotation.id)


@behave.then(u'Annotation dataset_id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.annotation.dataset_id == context.datasets[int(dataset_index)-1].id


@behave.then(u'Annotation dataset.id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.annotation.dataset.id == context.datasets[int(dataset_index)-1].id


@behave.then(u'Annotation item_id is equal to item {item_index} id')
def step_impl(context, item_index):
    assert context.annotation.item_id == context.items[int(item_index)-1].id


@behave.then(u'Annotation item.id is equal to item {item_index} id')
def step_impl(context, item_index):
    assert context.annotation.item.id == context.items[int(item_index)-1].id


================================================
File: tests/features/steps/annotations_repo/test_annotations_delete.py
================================================
import behave


@behave.when(u'I delete a annotation x')
def step_impl(context):
    context.item.annotations.delete(annotation_id=context.annotation_x.id)


@behave.when(u'I try to delete a non-existing annotation')
def step_impl(context):
    try:
        context.project.datasets.delete(dataset_name='some_name',
                                        sure=True,
                                        really=True)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'No annotation was deleted')
def step_impl(context):
    context.annotations_list = context.item.annotations.list()
    assert len(context.annotations_list) == len(context.annotations)


@behave.then(u'Annotation x does not exist in item')
def step_impl(context):
    annotations_list = context.item.annotations.list()
    assert context.annotation_x not in annotations_list


@behave.given(u'I count other Annotation except "{annotation_type}" using "{entity}" entity')
def step_impl(context, annotation_type, entity):
    filters = context.dl.Filters(resource=context.dl.FiltersResource.ANNOTATION, field='type', values=annotation_type)
    if entity == 'dataset':
        annotations_list_filtered = context.dataset.annotations.list(filters=filters)
        annotations_list_all = context.dataset.annotations.list()
    elif entity == 'item':
        annotations_list_filtered = context.item.annotations.list(filters=filters)
        annotations_list_all = context.dataset.annotations.list()
    else:
        raise ValueError("Entity {} does not supported".format(entity))

    context.other_annotation_before_delete = len(annotations_list_all) - len(annotations_list_filtered)
    a = 5


@behave.when(u'I delete annotation from type "{annotation_type}" using "{entity}" entity')
def step_impl(context, annotation_type, entity):
    filters = context.dl.Filters(resource=context.dl.FiltersResource.ANNOTATION, field='type', values=annotation_type)
    if entity == 'dataset':
        context.dataset.annotations.delete(filters=filters)
    elif entity == 'item':
        context.item.annotations.delete(filters=filters)
    else:
        raise ValueError("Entity {} does not supported".format(entity))


@behave.then(u'I verify that I has the right number of annotations')
def step_impl(context):
    filters = context.dl.Filters(resource=context.dl.FiltersResource.ANNOTATION)
    annotations = context.dataset.annotations.list(filters=filters)
    assert len(annotations) == context.other_annotation_before_delete


@behave.when(u'I delete annotation')
def step_impl(context):
    context.annotation.delete()


================================================
File: tests/features/steps/annotations_repo/test_annotations_download.py
================================================
import shutil

import behave
import os
import dtlpy as dl


@behave.when(u'I download items annotations with "{ann_type}" to "{path}"')
def step_impl(context, ann_type, path):
    if context.item.height is None:
        context.item.height = 768
    if context.item.width is None:
        context.item.width = 1536
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
    if ann_type == 'img_mask':
        context.item.annotations.download(
            filepath=path,
            img_filepath=context.item_path,
            annotation_format=ann_type,
            thickness=1
        )
    elif ann_type == 'default':
        context.item.annotations.download(
            filepath=path,
            thickness=1)
    else:
        context.item.annotations.download(
            filepath=path,
            annotation_format=ann_type,
            thickness=1)
    context.annotation_download_filepath = path


@behave.when(u'I download items annotations from item with "{ann_type}" to "{path}"')
def step_impl(context, ann_type, path):
    if context.item.height is None:
        context.item.height = 768
    if context.item.width is None:
        context.item.width = 1536
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
    context.item = dl.items.get(item_id=context.item.id)
    context.item.download(
        local_path=path,
        annotation_options=ann_type,
        thickness=1
    )


@behave.given(u'There are no files in folder "{folder_path}"')
def step_impl(context, folder_path):
    folder_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], folder_path)
    if os.path.isdir(folder_path):
        # on git there are no empty folder so only if the folder exists check that it is empty
        items = os.listdir(folder_path)
        for item in items:
            if item != 'folder_keeper':
                path = os.path.join(folder_path, item)
                if os.path.isdir(path):
                    shutil.rmtree(path)
                else:
                    os.remove(path)
        dirs = os.listdir(folder_path)
        dirs.pop(dirs.index('folder_keeper'))
        assert len(dirs) == 0


@behave.then(u'Item annotation "{file_type}" has been downloaded to "{folder_path}"')
def step_impl(context, file_type, folder_path):
    folder_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], folder_path)
    items = os.listdir(folder_path)
    if file_type == 'json':
        file = file_type + '.json'
    elif file_type == 'vtt':
        file = file_type + '.vtt'
    else:
        file = file_type + '.png'
    assert file in items


@behave.then(u'video Item annotation "{file_type}" has been downloaded to "{folder_path}"')
def step_impl(context, file_type, folder_path):
    folder_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], folder_path)
    items = os.listdir(folder_path)
    if file_type == 'json':
        file = file_type + '.json'
    else:
        file = file_type + '.mp4'
    assert file in items


# @behave.then(u'"{file_type}" is correctly downloaded (compared with "{file_to_compare}")')
# def step_impl(context, file_type, file_to_compare):
#     path = 'downloaded_annotations'
#     path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
#     file_to_compare = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_to_compare)

#     file = file_type.lower() + '.png'
#     original = cv2.imread(file_to_compare)
#     downloaded = cv2.imread(os.path.join(path, file))
#     if original.shape == downloaded.shape:
#         difference = cv2.subtract(original, downloaded)
#         b, g, r = cv2.split(difference)
#         if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:
#             assert True
#         else:
#             assert False
#     else:
#         assert False

@behave.then(u'I download the annotation to "{item_path}" with "{annotation_type}" type')
def step_impl(context, item_path, annotation_type):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.annotation_filepath = context.annotation_x_get.download(filepath=item_path,
                                                                    annotation_format=annotation_type)


@behave.then(u'annotation file exist in the path "{annotation_filepath}"')
def step_impl(context, annotation_filepath):
    files = os.listdir(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], annotation_filepath))
    assert os.path.basename(context.annotation_filepath) in files


@behave.then(u'I download the items annotations with ViewAnnotationOptions "{file_type}" enum to find "{item_path}"')
def step_impl(context, file_type, item_path):
    folder_path = ""

    if file_type != "None":
        folder_path = os.path.dirname(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path))

    if file_type == 'JSON':
        context.dataset.download_annotations(annotation_options=dl.ViewAnnotationOptions.JSON, local_path=folder_path)
        file = os.path.basename(item_path)
        items = os.path.join(folder_path, "json")
    elif file_type == 'VTT':
        context.dataset.download_annotations(annotation_options=dl.ViewAnnotationOptions.VTT, local_path=folder_path)
        file = os.path.basename(item_path)
        items = os.path.join(folder_path, "vtt")
    elif file_type == 'MASK':
        context.dataset.download_annotations(annotation_options=dl.ViewAnnotationOptions.MASK, local_path=folder_path)
        file = os.path.basename(item_path)
        items = os.path.join(folder_path, "mask")
    else:
        try:
            context.dataset.download_annotations(annotation_options=dl.ViewAnnotationOptions)
            assert False

        except Exception as e:
            assert folder_path in e.args[1]
            return  # END

    items = os.listdir(items)
    assert file in items


@behave.when(u'I download dataset annotations with "{ann_type}" to "{path}"')
def step_impl(context, ann_type, path):
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
    context.dataset.download_annotations(
        local_path=path,
        annotation_options=ann_type,
    )


@behave.then(u'dataset "{folder_name}" folder has been downloaded to "{folder_path}"')
def step_impl(context, folder_name, folder_path):
    folder_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], folder_path)
    items = os.listdir(folder_path)
    assert folder_name in items
    count_files = os.listdir(os.path.join(folder_path, folder_name))
    assert len(count_files) == 1


@behave.then(u'Validate annotation file has "{ann_count}" annotations')
def step_impl(context, ann_count):
    if not hasattr(context, 'annotation_download_filepath'):
        raise Exception(
            'annotation_download_filepath not found in context, please run: I download items annotations with "{ann_type}" to "{path}"')

    import json
    with open(context.annotation_download_filepath, 'r') as f:
        annotation_file = json.load(f)

    assert len(annotation_file['annotations']) == int(ann_count), f"TEST FAILED: expected {ann_count} annotations, got {len(annotation_file['annotations'])}"


================================================
File: tests/features/steps/annotations_repo/test_annotations_draw.py
================================================
import behave
import os
import numpy as np


@behave.when(u'I draw items annotations with param "{annotation_format}" to image in "{im_path}"')
def step_impl(context, annotation_format, im_path):
    im_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], im_path)
    context.img_arr = np.load(im_path + '.npy')

    context.img_arr = context.img_arr.copy()
    context.item = context.item.update()
    annotation_collection = context.item.annotations.list()
    # sort to avoid test fail on order of drawing
    annotation_collection.annotations = sorted(annotation_collection.annotations, key=lambda x: x.top)

    context.mask = annotation_collection.show(height=768,
                                              width=1536,
                                              thickness=1,
                                              with_text=False,
                                              annotation_format=annotation_format,
                                              image=context.img_arr)


================================================
File: tests/features/steps/annotations_repo/test_annotations_format_json.py
================================================
import behave
import os
import filecmp
import json


@behave.then(u'I compare json annotations between the files in dirs')
def step_impl(context):
    context.annotation_json_dir = None
    context.item_json_dir = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "annotation_json_dir":
            context.annotation_json_dir = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "item_json_dir":
            context.item_json_dir = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

    # Function to compare annotation and item json files
    def compare_directory_annotations_json_files(dir1, dir2):
        dirs_cmp = filecmp.dircmp(dir1, dir2)
        if len(dirs_cmp.left_only) > 0 or len(dirs_cmp.right_only) > 0 or len(dirs_cmp.funny_files) > 0:
            return False

        # Compare files data
        for item in os.listdir(dir1):
            if item.endswith('.json'):
                file1_path = os.path.join(dir1, item)
                file2_path = os.path.join(dir2, item)

                with open(file1_path, 'r') as annotations_file1:
                    file1_json = json.load(annotations_file1)

                with open(file2_path, 'r') as annotations_file2:
                    file2_json = json.load(annotations_file2)

                # Comparing the sorted files
                assert file1_json == file2_json['annotations'][0], "mismatch data on file: " + item

        for common_dir in dirs_cmp.common_dirs:
            new_dir1 = os.path.join(dir1, common_dir)
            new_dir2 = os.path.join(dir2, common_dir)
            if not compare_directory_annotations_json_files(new_dir1, new_dir2):
                return False
        return True

    assert compare_directory_annotations_json_files(context.annotation_json_dir, context.item_json_dir)


@behave.then(u'I compare json metadata and annotationsCount between the files in dirs')
def step_impl(context):
    context.item_json_dir = None
    context.dataset_json_dir = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "item_json_dir":
            context.item_json_dir = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "dataset_json_dir":
            context.dataset_json_dir = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

    # Function to compare item and dataset json files
    def compare_directory_metadata_json_files(dir1, dir2):
        dirs_cmp = filecmp.dircmp(dir1, dir2)
        if len(dirs_cmp.left_only) > 0 or len(dirs_cmp.right_only) > 0 or len(dirs_cmp.funny_files) > 0:
            return False

        # Compare files data
        for item in os.listdir(dir1):
            if item.endswith('.json'):
                file1_path = os.path.join(dir1, item)
                file2_path = os.path.join(dir2, item)

                with open(file1_path, 'r') as annotations_file1:
                    file1_json = json.load(annotations_file1)

                with open(file2_path, 'r') as annotations_file2:
                    file2_json = json.load(annotations_file2)

                # Comparing the sorted files
                assert file1_json["metadata"] == file2_json["metadata"], "mismatch metadata on file: " + item
                assert len(file1_json["annotations"]) == file2_json["annotationsCount"], "mismatch annotationsCount on file: " + item

                if len(file1_json["annotations"]) == 0:
                    assert file2_json["annotated"] is False, "mismatch annotated on file: " + item
                else:
                    assert file2_json["annotated"] is True, "mismatch annotated on file: " + item

        for common_dir in dirs_cmp.common_dirs:
            new_dir1 = os.path.join(dir1, common_dir)
            new_dir2 = os.path.join(dir2, common_dir)
            if not compare_directory_metadata_json_files(new_dir1, new_dir2):
                return False
        return True

    assert compare_directory_metadata_json_files(context.item_json_dir, context.dataset_json_dir)


================================================
File: tests/features/steps/annotations_repo/test_annotations_get.py
================================================
import behave
import json
import os


@behave.given(u'Item is annotated with annotations in file: "{annotations_path}"')
def step_impl(context, annotations_path):
    annotations_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], annotations_path)

    with open(annotations_path, 'r') as f:
        context.annotations = json.load(f)
    if not isinstance(context.annotations, list):
        context.annotations = context.annotations['annotations']
    context.item.annotations.upload(context.annotations)


@behave.given(u'Labels in file: "{labels_file_path}" are uploaded to test Dataset')
def step_impl(context, labels_file_path):
    labels_file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], labels_file_path)
    with open(labels_file_path, 'r') as f:
        context.labels = json.load(f)
    context.recipe_id = context.dataset.metadata['system']['recipes'][0]
    context.recipe = context.dataset.recipes.get(context.recipe_id)
    context.ontology_id = context.recipe.ontology_ids[0]
    context.ontology = context.recipe.ontologies.get(context.ontology_id)
    for label in context.labels:
        context.ontology.labels.append(context.dl.Label.from_root(label))
    context.ontology.attributes = ['Occlusion1', 'Occlusion2']
    context.recipe.ontologies.update(ontology=context.ontology, system_metadata=True)


@behave.given(u'There is annotation x')
def step_impl(context):
    context.annotation_x = context.item.annotations.list()[0]


@behave.when(u'I get the annotation by id')
def step_impl(context):
    context.annotation_x_get = context.item.annotations.get(context.annotation_x.id)


@behave.then(u'I receive an Annotation object')
def step_impl(context):
    assert type(context.annotation_x_get) == context.dl.Annotation


@behave.then(u'Annotation received equals to annotation x')
def step_impl(context):
    assert context.annotation_x.to_json() == context.annotation_x_get.to_json()


@behave.when(u'I try to get a non-existing annotation')
def step_impl(context):
    try:
        context.item.annotations.get('some_id')
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u'I get AnnotationCollection from json "{annotations_path}"')
def step_impl(context, annotations_path):
    annotations_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], annotations_path)

    try:
        context.annotation_collection = context.dl.AnnotationCollection.from_json_file(filepath=annotations_path, item=context.item)
    except Exception as e:
        context.error = e
        assert False, context.error


================================================
File: tests/features/steps/annotations_repo/test_annotations_list.py
================================================
import behave


@behave.given(u'There are no annotations')
def step_impl(context):
    assert True


@behave.when(u'I list all annotations')
def step_impl(context):
    context.annotations_list = context.item.annotations.list()


@behave.then(u'I receive a list of all annotations')
def step_impl(context):
    assert len(context.annotations_list) == len(context.annotations)


@behave.then(u'The annotations in the list equals the annotations uploaded')
def step_impl(context):
    for annotation in context.annotations_list:
        ann = {'type': annotation.type,
               'label': annotation.label,
               'metadata': { 'system': {'attributes': annotation.attributes}},
               'coordinates': annotation.coordinates}
        # remove 'z' value to match file
        for coordinate in ann['coordinates']:
            coordinate.pop('z')
        assert ann in context.annotations


@behave.then(u'I receive an empty annotations list')
def step_impl(context):
    assert len(context.annotations_list) == 0


================================================
File: tests/features/steps/annotations_repo/test_annotations_show.py
================================================
import behave
import os
import numpy as np


@behave.when(u'I show items annotations with param "{annotation_format}"')
def step_impl(context, annotation_format):
    context.item = context.item.update()
    annotation_collection = context.item.annotations.list()

    annotation_collection.annotations = sorted(annotation_collection.annotations, key=lambda x: x.top)
    context.mask = annotation_collection.show(height=768,
                                              width=1536,
                                              thickness=1,
                                              with_text=False,
                                              annotation_format=annotation_format)


@behave.then(u'I receive annotations mask and it is equal to mask in "{should_be_path}"')
def step_impl(context, should_be_path):
    should_be_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], should_be_path)
    if not np.array_equal(context.mask, np.load(should_be_path)):
        np.save(should_be_path.replace('.npy', '_wrong.npy'), context.mask)
        assert False


@behave.when(u'Every annotation has an object id')
def step_impl(context):
    context.annotaitons = context.item.annotations.list()
    types = ['ellipse', 'segment', 'box', 'point', 'polyline']
    for ann in context.annotaitons:
        if ann.type == 'polyline':
            ann.object_id = 2
        else:
            ann.object_id = types.index(ann.type) + 1
    context.annotaitons = context.annotaitons.update()


================================================
File: tests/features/steps/annotations_repo/test_annotations_update.py
================================================
import behave
import time


@behave.given(u"I remove annotations attributes")
def step_impl(context):
    context.annotations_get = context.item.annotations.list()
    for annotation in context.annotations_get:
        annotation.attributes = {}


@behave.when(u"I update annotations")
def step_impl(context):
    context.item.annotations.update(
        annotations=context.annotations_get.annotations,
        system_metadata=True
    )
    time.sleep(7)


@behave.then(u"Item annotations has no attributes")
def step_impl(context):
    annotations_get = context.item.annotations.list()
    for annotation in annotations_get:
        assert not annotation.attributes


@behave.given(u"I change annotations attributes to non-existing attributes")
def step_impl(context):
    context.annotations_get = context.item.annotations.list()
    for annotation in context.annotations_get:
        annotation.attributes = ["some_attribute_name"]


@behave.given(u'I change all annotations types to "{type_name}"')
def step_impl(context, type_name):
    context.annotations_get = context.item.annotations.list()
    for annotation in context.annotations_get:
        annotation.type = type_name


@behave.then(u'All item annotations have type "{type_name}"')
def step_impl(context, type_name):
    annotations_get = context.item.annotations.list()
    for annotation in annotations_get:
        assert annotation.type == type_name


@behave.given(u'I change all annotations labels to "{label_name}"')
def step_impl(context, label_name):
    context.annotations_get = context.item.annotations.list()
    for annotation in context.annotations_get:
        annotation.label = label_name


@behave.then(u'All item annotations have label "{label_name}"')
def step_impl(context, label_name):
    annotations_get = context.item.annotations.list()
    for annotation in annotations_get:
        assert annotation.label == label_name


@behave.when(u"I try to update annotations")
def step_impl(context):
    annotation_ids = list()
    for annotation in context.annotations_get:
        annotation_ids.append(annotation.id)
    try:
        context.item.annotations.update(
            annotations=context.annotations_get,
            annotations_ids=annotation_ids,
            system_metadata=True,
        )
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u"I change annotation values")
def step_impl(context):
    context.annotation_x.attributes = list()
    context.annotation_x.label = "person"


@behave.when(u"I update annotation")
def step_impl(context):
    context.item.annotations.update(
        annotations=context.annotation_x,
        system_metadata=True,
    )


@behave.then(u"Annotation should be updated")
def step_impl(context):
    annotation_get = context.item.annotations.get(context.annotation_x.id)
    assert not annotation_get.attributes
    assert annotation_get.label == "person"


@behave.given(u'I add "{raise_value}" to annotation coordinates')
def step_impl(context, raise_value):
    raise_value = int(raise_value)
    context.annotation_x.top += raise_value
    context.annotation_x.right += raise_value
    context.annotation_x.left += raise_value
    context.annotation_x.bottom += raise_value


@behave.given(u'I set start frame to "{start_frame}" and end frame to "{end_frame}"')
def step_impl(context, start_frame, end_frame):
    start_frame = int(start_frame)
    end_frame = int(end_frame)
    context.annotation_x.start_frame = start_frame
    context.annotation_x.end_frame = end_frame


@behave.then(u'annotation x coordinates should be changed accordingly')
def step_impl(context):
    annotation_get = context.item.annotations.get(context.annotation_x.id)
    assert annotation_get.coordinates == context.annotation_x.coordinates


@behave.then(u'annotation x metadata should be changed accordingly')
def step_impl(context):
    annotation_get = context.item.annotations.get(context.annotation_x.id)
    assert annotation_get.start_time == context.annotation_x.start_time
    assert annotation_get.end_time == context.annotation_x.end_time


@behave.when(u'I get the only annotation')
def step_impl(context):
    context.annotation = context.item.annotations.list()[0]


@behave.then(u'Annotation snapshots equal to platform snapshots')
def step_impl(context):
    ann = context.annotation
    assert len(ann.frames.actual_keys()) == 5
    time.sleep(10)
    vals = list(ann.frames.values())
    assert len(vals) == 46
    platform_snapshots = context.annotation._platform_dict['metadata']['system']['snapshots_']
    ann_snapshot = ann.to_json()['metadata']['system']['snapshots_']
    assert len(platform_snapshots) == len(ann_snapshot)
    for platform_snap in platform_snapshots:
        ann_snap = [snap for snap in ann_snapshot if snap['frame'] == platform_snap['frame']][0]
        assert ann_snap
        assert ann_snap['frame'] == platform_snap['frame']
        assert ann_snap['data'] == platform_snap['data']


@behave.when(u'I update items annotations in task with context.annotation_definition')
def step_impl(context):
    filters = context.dl.Filters()
    filters.add(field='annotated', values=True)
    pages = context.task.get_items(filters=filters)
    for item in pages.all():
        for annotation in item.annotations.list():
            annotation.annotation_definition = context.annotation_definition
            annotation.update()


@behave.then(u'I verify that I have annotations')
def step_impl(context):
    filters = context.filters
    annotations = context.dataset.annotations.list(filters=filters)
    assert annotations.items_count != 0



================================================
File: tests/features/steps/annotations_repo/test_annotations_upload.py
================================================
import behave
import json
import os


@behave.when(u'I upload annotations from file: "{file_path}" with merge "{merge_ann}"')
def step_impl(context, file_path, merge_ann):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    merge_ann = True if merge_ann == 'True' else False
    with open(file_path, "r") as f:
        context.annotations = json.load(f)["annotations"]
    context.item.annotations.upload(context.annotations, merge=merge_ann)


@behave.when(u'I save binary annotation coordinates')
def step_impl(context):
    context.item = context.project.items.get(item_id=context.item.id)
    annotations = context.item.annotations.list()
    for annotation in annotations:
        if annotation.type == 'binary':
            context.binary_annotation = annotation

@behave.then(u'binary annotation has been merged')
def step_impl(context):
    context.item = context.project.items.get(item_id=context.item.id)
    annotations = context.item.annotations.list()
    for annotation in annotations:
        if annotation.type == 'binary':
            assert annotation.coordinates != context.binary_annotation.coordinates, 'Binary annotation has not been merged'
            assert annotation.id == context.binary_annotation.id, 'Binary annotation has not been merged'
            assert annotation.label == context.binary_annotation.label, 'Binary annotation has not been merged'
            break

@behave.then(u"Item should have annotations uploaded")
def step_impl(context):
    annotations_list = context.item.annotations.list()
    for annotation in annotations_list:
        ann = {
            "type": annotation.type,
            "label": annotation.label,
            "coordinates": annotation.coordinates,
        }
        if annotation.attributes:
            ann['metadata'] = { "system": { 'attributes': annotation.attributes} }
        # remove 'z' value to match file
        if annotation.type != 'gis':
            for coordinate in ann['coordinates']:
                coordinate.pop('z')
        assert ann in context.annotations

@behave.then(u"Item should have all gis annotation types uploaded")
def step_impl(context):
    expected_annotations_types = ['point', 'polyline', 'polygon', 'box']
    annotations_list = context.item.annotations.list()
    for annotation in annotations_list:
        assert annotation.type == 'gis'
        assert isinstance(annotation.annotation_definition, context.dl.Gis)
        expected_annotations_types.remove(annotation.coordinates['geo_type'])
    assert len(expected_annotations_types) == 0



@behave.given(u"There is an annotation description")
def step_impl(context):
    context.annotation_description = {
        "type": "box",
        "label": "car",
        "metadata": {
            "system": {
                "attributes": {"1": "Occlusion2"}
            }
        },
        "coordinates": [
            {"x": 700, "y": 200},
            {"x": 800, "y": 250},
        ],
    }


@behave.when(u"I upload annotation description to Item")
def step_impl(context):
    context.item.annotations.upload(context.annotation_description)


@behave.then(u"Item should have annotation uploaded")
def step_impl(context):
    annotation = context.item.annotations.list()[0]
    ann = {
        "type": annotation.type,
        "label": annotation.label,
        "metadata": {
            "system": {
                "attributes": annotation.attributes
            }
        },
        "coordinates": annotation.coordinates,
    }
    # remove 'z' value to match file
    for coordinate in ann['coordinates']:
        coordinate.pop('z')
    assert ann == context.annotation_description


@behave.given(u'There is an illegal annotation description')
def step_impl(context):
    context.annotation_description = {
        "type": "box",
        "label": "car",
        "attributes": ["Occlusion2"]
    }


@behave.when(u'I try to upload annotation description to Item')
def step_impl(context):
    try:
        context.item.annotations.upload(context.annotation_description)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/annotations_repo/test_note_annotation_with_messages.py
================================================
import behave


@behave.then(u'I will see the on the note annotations the following messages')
def step_impl(context):
    context.messages = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "messages":
            context.messages = eval(parameter.cells[1])

    if context.messages is not None:
        for message_index in range(len(context.messages)):
            assert context.messages[message_index] == context.annotation.messages[message_index].body


================================================
File: tests/features/steps/annotations_repo/test_rotated_box_points.py
================================================
import behave
import numpy as np
import dtlpy as dl


@behave.given(u'I create 4ptBox setting to the "{project_num}" project')
def step_impl(context, project_num):
    if project_num == "first":
        context.project_4ptBox_settings = context.project.settings.create(setting_name='4ptBox', setting_value=True,
                                                                                setting_value_type=dl.SettingsValueTypes.BOOLEAN)
    else:
        context.project_4ptBox_settings = context.second_project.settings.create(setting_name='4ptBox', setting_value=True,
                                                                                 setting_value_type=dl.SettingsValueTypes.BOOLEAN)


@behave.given(u'I set 4ptBox setting project setting to "{settings_value}"')
def step_impl(context, settings_value):
    if settings_value == "False":
        context.project.settings.update(setting_name='4ptBox', setting_value=False,
                                              setting_id=context.project_4ptBox_settings.id)
    else:
        context.second_project.settings.update(setting_name='4ptBox', setting_value=True,
                                               setting_id=context.project_4ptBox_settings.id)


@behave.when(u'I upload rotated box annotation to item "{item_num}"')
def step_impl(context, item_num):
    if item_num == "1":
        builder = context.first_item.annotations.builder()
        builder.add(annotation_definition=dl.Box(top=10, left=10, bottom=100, right=100, label='test', angle=80))
        context.first_item.annotations.upload(builder)
        context.rotated_box_geo = builder.annotations[0].geo
    else:
        builder = context.second_item.annotations.builder()
        builder.add(annotation_definition=dl.Box(top=10, left=10, bottom=100, right=100, label='test', angle=80))
        context.second_item.annotations.upload(builder)
        context.rotated_box_geo = builder.annotations[0].geo


@behave.then(u'The Geo will be of the "{format_type}" format')
def step_impl(context, format_type):
    if format_type == 'old':
        old_format = np.array([[10, 10],
                               [100, 100]])
        assert np.array_equal(context.rotated_box_geo, old_format)
    else:
        new_format = np.array([[91.0, 2.0],
                               [2.0, 18.0],
                               [18.0, 107.0],
                               [107.0, 91.0]])
        assert np.array_equal(np.floor(context.rotated_box_geo), new_format)


================================================
File: tests/features/steps/annotations_repo/text_annotations_interface.py
================================================
import behave
from time import sleep


@behave.given(u'I upload "{annotation_type}" annotation to the text item')
def step_impl(context, annotation_type):
    # Used to get item height and width from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    annotations_definitions_list = {
        "classification": context.dl.Classification(
            label="label1",
            attributes={"1": "classification1"}
        ),
        "text mark": context.dl.Text(
            text_type="block",
            start=0,
            end=5,
            label="label1",
            attributes={"1": "text1"}
        )
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type])
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


@behave.given(u'I upload "{annotation_type}" annotation with description "{annotation_description}" to the text item')
def step_impl(context, annotation_type, annotation_description):
    # Used to get item height and width from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    annotations_definitions_list = {
        "classification": context.dl.Classification(
            label="label1",
            attributes={"1": "classification1"},
            description=annotation_description
        ),
        "text mark": context.dl.Text(
            text_type="block",
            start=0,
            end=5,
            label="label1",
            attributes={"1": "text1"},
            description=annotation_description
        ),
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type])
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


================================================
File: tests/features/steps/annotations_repo/video_annotations_interface.py
================================================
import behave
from time import sleep


@behave.given(u'I upload "{annotation_type}" annotation to the video item')
def step_impl(context, annotation_type):
    # Used to get item data from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    annotations_definitions_list = {
        "classification": context.dl.Classification(
            label="label1",
            attributes={"1": "classification1"}
        ),
        "point": context.dl.Point(
            x=10,
            y=10,
            label="label1",
            attributes={"1": "point1"}
        ),
        "box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "box1"}
        ),
        "rotated box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            angle=45,
            label="label1",
            attributes={"1": "box1"}
        ),
        "cube": context.dl.Cube(
            front_bl=[50, 100],
            front_br=[100, 100],
            front_tr=[100, 50],
            front_tl=[50, 50],
            back_bl=[25, 75],
            back_br=[75, 75],
            back_tr=[75, 25],
            back_tl=[25, 25],
            label="label1",
            attributes={"1": "cube1"}
        ),
        "polygon": context.dl.Polygon(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polygon1"}
        ),
        "polyline": context.dl.Polyline(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polyline1"}
        ),
        "note": context.dl.Note(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "note1"},
            assignee=context.dl.info()['user_email'],
            creator=context.dl.info()['user_email']
        )
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type], object_id=1, frame_num=5, end_frame_num=10, fixed=False)
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


@behave.given(u'I upload "{annotation_type}" annotation with description "{annotation_description}" to the video item')
def step_impl(context, annotation_type, annotation_description):
    # Used to get item data from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    annotations_definitions_list = {
        "classification": context.dl.Classification(
            label="label1",
            attributes={"1": "classification1"},
            description=annotation_description
        ),
        "point": context.dl.Point(
            x=10,
            y=10,
            label="label1",
            attributes={"1": "point1"},
            description=annotation_description
        ),
        "box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "box1"},
            description=annotation_description
        ),
        "rotated box": context.dl.Box(
            left=50,
            top=50,
            right=100,
            bottom=100,
            angle=45,
            label="label1",
            attributes={"1": "box1"},
            description=annotation_description
        ),
        "cube": context.dl.Cube(
            front_bl=[50, 100],
            front_br=[100, 100],
            front_tr=[100, 50],
            front_tl=[50, 50],
            back_bl=[25, 75],
            back_br=[75, 75],
            back_tr=[75, 25],
            back_tl=[25, 25],
            label="label1",
            attributes={"1": "cube1"},
            description=annotation_description
        ),
        "polygon": context.dl.Polygon(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polygon1"},
            description=annotation_description
        ),
        "polyline": context.dl.Polyline(
            geo=[[25, 25], [50, 100], [70, 10]],
            label="label1",
            attributes={"1": "polyline1"},
            description=annotation_description
        ),
        "note": context.dl.Note(
            left=50,
            top=50,
            right=100,
            bottom=100,
            label="label1",
            attributes={"1": "note1"},
            assignee=context.dl.info()['user_email'],
            creator=context.dl.info()['user_email'],
            description=annotation_description
        )
    }

    builder = context.item.annotations.builder()
    builder.add(annotations_definitions_list[annotation_type], object_id=1, frame_num=5, end_frame_num=10, fixed=False)
    context.annotation = context.item.annotations.upload(annotations=builder)[0]


================================================
File: tests/features/steps/app_entity/app_with_fs_panel.py
================================================
import time
import random
from behave import given, when, then
import os
import json
import shutil
from datetime import datetime
from .. import fixtures


@given(u'I have an app with a filesystem panel in path "{filesystem_path}"')
@given(u'I have an app with a filesystem panel in path "{filesystem_path}" and remove key "{remove_key}"')
def step_impl(context, filesystem_path, remove_key=None):
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filesystem_path)
    manifest_path = os.path.join(path, 'dataloop.json')
    # Need to set the original path and backup path in the context to restore the file later
    context.original_path = manifest_path
    context.backup_path = os.path.join(path, 'dataloop_backup.json')
    # Save the original dataloop.json file to restore it later
    if not os.path.exists(context.backup_path):
        shutil.copy(manifest_path, context.backup_path)
    with open(manifest_path, 'r') as file:
        manifest = json.load(file)
    manifest['name'] = f'filesystem-panel-{datetime.now().strftime("%M%S")}'
    if remove_key:
        fixtures.remove_key_from_nested_dict(manifest, remove_key)
    manifest['components']['computeConfigs'][0]['versions']['dtlpy'] = context.dl.__version__
    with open(manifest_path, 'w') as file:
        json.dump(manifest, file)

    context.dpk = context.project.dpks.publish(manifest_filepath=os.path.join(path, 'dataloop.json'), local_path=path)
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.append(context.dpk)
    else:
        context.feature.dpks = [context.dpk]

    context.app = context.project.apps.install(dpk=context.dpk)
    if hasattr(context.feature, 'apps'):
        context.feature.apps.append(context.app)
    else:
        context.feature.apps = [context.app]


@when(u'I fetch the panel')
def step_impl(context):
    url = context.app.routes[context.dpk.components.panels[0].name]
    context.url = url.split('v1')[1]


@then(u'I should find the sdk version from the computeConfig in the panel')
def step_impl(context):
    interval = 3
    num_tries = 300
    success = False
    url = context.app.routes[context.dpk.components.panels[0].name]
    url = url.split('v1')[1]

    while num_tries > 0 and not success:
        success, response = context.dl.client_api.gen_request(req_type='GET', path=url)
        if success:
            content = response.content.decode('utf-8')
            success = 'Panel({})'.format(context.dpk.components.compute_configs[0].versions['dtlpy']) in content
        if not success:
            time.sleep(interval)
            num_tries -= 1

    assert success, 'Failed to fetch panel'


@then(u'no services deployed in the project')
def step_impl(context):
    services = context.project.services.list()
    assert len(services.items) == 0, 'Services found in project'


@given(u'I update the panel with a new sdk version')
def step_impl(context):
    valid_sdk_versions = [v for v in ['1.102.14', '1.103.2', '1.104.0'] if v != context.dl.__version__]
    new_sdk_version = random.choice(valid_sdk_versions)

    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], 'apps', 'filesystem_panel')
    manifest_path = os.path.join(path, 'dataloop.json')
    with open(manifest_path, 'r') as file:
        manifest = json.load(file)
    manifest['components']['computeConfigs'][0]['versions']['dtlpy'] = new_sdk_version
    with open(manifest_path, 'w') as file:
        json.dump(manifest, file)

    context.dpk = context.project.dpks.publish(manifest_filepath=os.path.join(path, 'dataloop.json'), local_path=path)
    context.app.dpk_version = context.dpk.version
    context.app.update()
    context.app = context.project.apps.get(app_id=context.app.id)


================================================
File: tests/features/steps/app_entity/test_app_get.py
================================================
import behave
import dictdiffer


@behave.when(u'I get the app by name')
def step_impl(context):
    context.web_app = context.project.apps.get(app_name=context.app.name)


@behave.when(u'I get the app by id')
def step_impl(context):
    context.web_app = context.project.apps.get(app_id=context.app.id)


@behave.when(u'I get the app with invalid id')
def step_impl(context):
    try:
        context.project.apps.get(app_id="Hola")
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I get the app without parameters')
def step_impl(context):
    try:
        context.project.apps.get()
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I should get identical results as the json')
def step_impl(context):
    assert context.app.to_json() == context.web_app.to_json()


@behave.when(u'I get app by name "{app_name}"')
def step_impl(context, app_name):
    context.app = context.project.apps.get(app_name=app_name)


@behave.when(u'I add app to context.feature.apps')
def step_impl(context):
    if hasattr(context.feature, "apps"):
        if context.app.id not in [app.id for app in context.feature.apps]:
            context.feature.apps.append(context.app)
    else:
        context.feature.apps = [context.app]


@behave.when(u'I validate global app by the name "{app_name}" is installed')
def step_impl(context, app_name):
    try:
        filters = context.dl.Filters(field='name',
                                     values=app_name,
                                     resource=context.dl.FiltersResource.APP,
                                     use_defaults=False)
        filters.add(field='scope', values='system')
        apps = context.dl.apps.list(filters=filters)
        if len(apps) == 0:
            raise Exception(f"App {app_name} is not installed")
        elif len(apps) > 1:
            raise Exception(f"More than one app with name {app_name} is installed")
        else:
            context.app = apps[0]
    except Exception as e:
        raise e


@behave.Then(u'I validate app.custom_installation is equal to published.dpk components')
def step_impl(context):
    if context.app.custom_installation['components'] == context.published_dpk.to_json()['components']:
        assert True
    else:
        assert False, f"TEST FAILED: {list(dictdiffer.diff(context.app.custom_installation['components'], context.published_dpk.to_json()['components']))}"


================================================
File: tests/features/steps/app_entity/test_app_install.py
================================================
import json
import os
import random
from .. import fixtures
import behave


@behave.given(u'I have an app entity from "{path}"')
@behave.when(u'I have an app entity from "{path}"')
def step_impl(context, path):
    json_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
    with open(json_path) as f:
        data = json.load(f)

    data = fixtures.update_dtlpy_version(data)

    context.dpk = context.dl.entities.Dpk.from_json(_json=data, client_api=context.project._client_api,
                                                    project=context.project)


@behave.given(u'publish the app')
def step_impl(context):
    context.dpk.name = context.dpk.name + str(random.randint(10000, 1000000))
    context.dpk = context.dpk.publish()
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.append(context.dpk)
    else:
        context.feature.dpks = [context.dpk]


@behave.when(u'I install the app with exception')
def step_impl(context):
    try:
        dpk = context.published_dpk if hasattr(context, "published_dpk") else context.dpk
        app = context.dl.entities.App.from_json({}, client_api=context.project._client_api, project=context.project)
        app = context.project.apps.install(dpk)
        if hasattr(context.feature, 'apps'):
            context.feature.apps.append(app)
        else:
            context.feature.apps = [app]
    except Exception as e:
        context.error = e


@behave.when(u'I install the app')
@behave.given(u'I install the app')
@behave.when(u'I install the app with custom_installation "{flag}"')
def step_impl(context, flag="True"):
    if hasattr(context, "custom_installation") and eval(flag):
        custom_installation = context.custom_installation
    elif eval(flag):
        custom_installation = {"components": context.dpk.to_json().get("components", {}),
                               "dependencies": context.dpk.to_json().get("dependencies", [])}
    else:
        custom_installation = None

    context.app = context.dl.entities.App.from_json(
        {},
        client_api=context.project._client_api,
        project=context.project)
    dpk = context.published_dpk if hasattr(context, "published_dpk") else context.dpk
    context.app = context.project.apps.install(dpk=dpk, custom_installation=custom_installation)
    if hasattr(context.feature, 'apps'):
        context.feature.apps.append(context.app)
    else:
        context.feature.apps = [context.app]


@behave.when(u'I try to install the app')
def step_impl(context):
    dpk = context.published_dpk if hasattr(context, "published_dpk") else context.dpk
    try:
        context.app = context.project.apps.install(dpk=dpk)
        if hasattr(context.feature, 'apps'):
            context.feature.apps.append(context.app)
        else:
            context.feature.apps = [context.app]
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I install the app without custom_installation')
def step_impl(context):
    context.app = context.dl.entities.App.from_json(
        {},
        client_api=context.project._client_api,
        project=context.project)
    dpk = context.published_dpk if hasattr(context, "published_dpk") else context.dpk
    context.app = context.project.apps.install(dpk=dpk)
    if hasattr(context.feature, 'apps'):
        context.feature.apps.append(context.app)
    else:
        context.feature.apps = [context.app]


@behave.when(u'I install the app with integration')
def step_impl(context):
    context.app = context.dl.entities.App.from_json(
        {},
        client_api=context.project._client_api,
        project=context.project)
    dpk = context.published_dpk if hasattr(context, "published_dpk") else context.dpk
    context.app = context.project.apps.install(dpk=dpk, integrations=[{
        "key": "nvidiaUser",
        "env": "nvidiaUser",
        "type": "key_value",
        "value": context.integration.id,
    }])
    if hasattr(context.feature, 'apps'):
        context.feature.apps.append(context.app)
    else:
        context.feature.apps = [context.app]


@behave.then(u'I should get the app with the same id')
def step_impl(context):
    assert context.app.id == context.project.apps.get(app_id=context.app.id).id


@behave.then(u"I should get an exception error='{error_code}'")
def step_impl(context, error_code):
    assert context.error is not None and context.error.status_code == error_code


@behave.then(u"I validate service configuration in dpk is equal to service from app")
def step_impl(context):
    context.dpk_service = None
    if not hasattr(context, "service"):
        raise AttributeError("Please make sure context has attr 'service'")
    service_runtime = context.service.runtime.to_json()
    for service in context.dpk.components.services:
        if service['name'] == context.service.name:
            context.dpk_service = service
            break
    assert context.dpk_service, "TEST FAILED: Failed to find dpk_service by field service name"
    dpk_runtime = context.dpk_service['runtime']
    if "dataloop_runner-cpu" not in dpk_runtime['runnerImage']:
        dpk_runtime['runnerImage'] = dpk_runtime['runnerImage'].split("/")[-1]
        service_runtime['runnerImage'] = service_runtime['runnerImage'].split("/")[-1]

    assert context.dpk_service["moduleName"] == context.service.module_name, f"TEST FAILED: Field moduleName"
    assert dpk_runtime == service_runtime, f"TEST FAILED: Field runtime"
    assert context.dpk_service[
               'executionTimeout'] == context.service.execution_timeout, f"TEST FAILED: Field executionTimeout"
    assert context.dpk_service['onReset'] == context.service.on_reset, f"TEST FAILED: Field onReset"
    assert context.dpk_service[
               'runExecutionAsProcess'] == context.service.run_execution_as_process, f"TEST FAILED: Field runExecutionAsProcess"
    if context.dpk_service['versions']['dtlpy'] != 'dtlpy_version':
        assert context.dpk_service['versions']['dtlpy'] == context.service.versions['dtlpy'], \
            f"TEST FAILED: Field versions.dtlpy DPK {context.dpk.components.services[0]['versions']['dtlpy']} " \
            f"Service {context.service.versions['dtlpy']}"


@behave.then(u'i can create pipeline function node from the app service')
def step_impl(context):
    comp = context.dl.compositions.get(composition_id=context.app.composition_id)
    s = context.dl.services.get(service_id=comp['spec'][0].get('state', {}).get('serviceId', None))
    func_node = context.dl.FunctionNode(service=s, name='test', function_name='run')
    assert func_node is not None
    assert func_node.service is not None


@behave.then(u'I compare service config with dpk compute configuration for the operation "{operation}"')
@behave.then(u'I compare service config with context.compute_config_item')
def step_impl(context, operation=None):
    service = context.service
    if operation:
        model_dpk = context.dpk.components.models[0]
        compute_configs = context.dpk.components.computeConfigs
        compute_config = [item for (index, item) in enumerate(compute_configs) if
                          item["name"] == model_dpk["computeConfigs"][operation]][0]
    else:
        compute_config = context.compute_config_item
    assert compute_config["runtime"]["runnerImage"] \
           == service.runtime.runner_image, f"TEST FAILED: Field runnerImage expected {compute_config['runtime']['runnerImage']} actual on service {service.runtime.runner_image}"


@behave.then(u'The dataset component has been installed successfully')
def step_impl(context):
    comp = context.dl.compositions.get(composition_id=context.app.composition_id)
    service = context.dl.services.get(service_id=comp['datasets'][0].get('state', {}).get('serviceId', None))
    dataset = context.dl.datasets.get(dataset_id=comp["datasets"][0]["datasetId"])
    execution = context.dl.executions.get(execution_id=comp["datasets"][0]["state"]["executionId"])

    assert comp["datasets"][0]["state"]["dataset"]["id"] == comp["datasets"][0]["datasetId"]
    assert service is not None
    assert dataset is not None
    assert execution is not None


================================================
File: tests/features/steps/app_entity/test_app_interface.py
================================================
import behave


@behave.when(u'I create a context.custom_installation var')
def step_impl(context):
    if hasattr(context, "dpk"):
        context.custom_installation = {"components": context.dpk.to_json().get("components", {}),
                                       "dependencies": context.dpk.to_json().get("dependencies", [])}
    else:
        raise AttributeError("'dpk' not found in 'context'")


@behave.then(u'App is not installed in the project')
def step_impl(context):
    try:
        app = context.project.apps.get(app_name=context.dpk.display_name)
        error = f"TEST FAILED: able to get app: {app.id} - {app.name}"
    except Exception:
        error = None

    assert error is None, error


================================================
File: tests/features/steps/app_entity/test_app_status.py
================================================
import behave


@behave.when(u'I resume the app')
def step_impl(context):
    try:
        app = context.project.apps.get(app_id=context.app.id)
        context.app_activated = app.resume()
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'The activation should succeed')
def step_impl(context):
    assert context.error is None, f"TEST FAILED: Expected no error, Actual got {context.error}"
    assert context.app_activated is True


@behave.when(u'I pause the app')
def step_impl(context):
    try:
        app = context.project.apps.get(app_id=context.app.id)
        context.app_deactivated = app.pause()
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'The deactivation should succeed')
def step_impl(context):
    assert context.error is None, f"TEST FAILED: Expected no error, Actual got {context.error}"
    assert context.app_deactivated is True


@behave.then(u'The service is inactive')
def step_impl(context):
    assert context.service.active is False


@behave.given(u'The service is active')
@behave.then(u'The service is active')
def step_impl(context):
    assert context.service.active is True


================================================
File: tests/features/steps/app_entity/test_app_uninstall.py
================================================
import behave


@behave.given(u'I uninstall the app')
@behave.when(u'I uninstall the app')
@behave.then(u'I uninstall the app')
def step_impl(context):
    context.project.apps.uninstall(app_id=context.app.id)
    context.feature.apps.pop(-1)


@behave.then(u"The app shouldn't be in listed")
def step_impl(context):
    try:
        context.project.apps.get(app_id=context.app.id)
        assert False
    except Exception:
        assert True


@behave.given(u'I uninstall not existed app')
def step_impl(context):
    try:
        context.project.apps.uninstall(app_id='Hello')
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/app_entity/test_app_update.py
================================================
import time

import behave


@behave.when(u'I update the app')
def step_impl(context):
    context.app_updated = context.project.apps.update(app_id=context.app.id)


@behave.then(u'The update should success')
def step_impl(context):
    assert context.app_updated is True


@behave.when(u'I update an app')
def step_impl(context):
    if hasattr(context, "custom_installation"):
        context.app.custom_installation = context.custom_installation
    context.app.update()


@behave.when(u'I increment app dpk_version')
def step_impl(context):
    version = context.app.dpk_version.split('.')
    version[2] = str(int(version[2]) + 1)
    context.app.dpk_version = '.'.join(version)


@behave.when(u'I update app auto update to "{flag}"')
def step_impl(context, flag):
    context.app.settings['autoUpdate'] = eval(flag)
    context.app.update()


@behave.when(u'I wait for app version to be updated according to dpk version')
def step_impl(context, ):
    interval = 8
    num_try = 3
    finished = False
    for i in range(num_try):
        time.sleep(interval)
        context.app = context.project.apps.get(app_id=context.app.id)
        if context.app.dpk_version == context.dpk.version:
            finished = True
            break

    if context.dpk.metadata:
        command_id = context.dpk.metadata.get('commands', {}).get('apps', None)
        if command_id is not None:
            command = context.dl.commands.get(command_id=command_id, url='api/v1/commands/faas/{}'.format(command_id))
            command.wait()
            context.dpk = context.dl.dpks.get(dpk_id=context.dpk.id)
    if context.app.metadata:
        command_id = context.app.metadata.get('system', {}).get('commands', {}).get('update', None)
        if command_id is not None:
            command = context.dl.commands.get(command_id=command_id, url='api/v1/commands/faas/{}'.format(command_id))
            command.wait()
            context.app = context.dl.apps.get(app_id=context.app.id)

    assert finished, f"TEST FAILED: App version was not updated, app current version: {context.app.dpk_version}"


@behave.when(u'I update app service SDK version to "{version}"')
def step_impl(context, version):
    services = context.project.services.list()
    context.service = [service for service in services.items if service.app['id'] == context.app.id][0]
    assert context.service.versions['dtlpy'] != version
    context.service.versions = {'dtlpy': version}
    context.service = context.service.update()


@behave.then(u'SDK version should be updated to "{version}"')
def step_impl(context, version):
    service = context.project.services.get(service_id=context.service.id)
    assert service.versions[
               'dtlpy'] == version, f"SDK version was not updated, current version: {service.versions['dtlpy']}"


@behave.then(u'App custom installation service should be updated to "{version}"')
def step_impl(context, version):
    app = context.project.apps.get(app_id=context.app.id)
    custom_installation = app.custom_installation
    components = custom_installation['components']
    services = components['services']
    service_custom_installation = \
    [service for service in services if service['name'] == context.service.app['componentName']][0]
    assert service_custom_installation['versions'][
               'dtlpy'] == version, f"SDK version was not updated in app custom installation, current version: {service_custom_installation['versions']['dtlpy']}"


================================================
File: tests/features/steps/app_entity/test_app_validate.py
================================================
import behave


@behave.then(u'I validate app.custom_installation is equal to composition')
def step_impl(context):
    context.app = context.project.apps.get(app_name=context.app.name)
    app_composition = context.project.compositions.get(context.app.composition_id)

    app_services_names = [service.get('name') for service in context.app.custom_installation.get('components').get('services')]
    comp_services_names = [comp.get('name') for comp in app_composition.get('spec')]
    app_triggers_names = [service.get('name') for service in context.app.custom_installation.get('components').get('triggers')]
    comp_triggers_names = [comp.get('name') for comp in app_composition.get('triggers')]
    assert app_services_names == comp_services_names, f"TEST FAILED: app_services_names is {app_services_names} composition_services_names is {comp_services_names}"
    assert app_triggers_names == comp_triggers_names, f"TEST FAILED: app_triggers_names is {app_triggers_names} composition_triggers_names is {comp_triggers_names}"


@behave.then(u'services should be updated')
def step_impl(context):
    services = context.project.services.list()
    for page in services:
        for service in page:
            if service.name.endswith('sdk'):
                assert service.version == '1.0.0', f"TEST FAILED: add new service"
            else:
                assert service.version == '1.0.1', f"TEST FAILED: update services, service version is {service.version}"

================================================
File: tests/features/steps/artifacts_repo/test_artifacts_delete.py
================================================
import behave


@behave.when(u'I delete artifact by "{get_method}"')
def step_impl(context, get_method):
    if get_method == 'name':
        context.artifact_get = context.project.artifacts.delete(artifact_name=context.artifact.name)
    elif get_method == 'id':
        context.artifact_get = context.project.artifacts.delete(artifact_id=context.artifact.id)
    elif get_method == 'package_name':
        context.artifact_get = context.project.artifacts.delete(package_name=context.package.name)
    elif get_method == 'execution_id':
        context.artifact_get = context.project.artifacts.delete(execution_id=context.execution.id)


@behave.then(u'Artifact does not exist "{resource}"')
def step_impl(context, resource):
    artifacts = None
    if resource == 'name':
        try:
            artifacts = context.project.artifacts.get(artifact_name=context.artifact.name)
        except Exception:
            artifacts = list()
    elif resource == 'id':
        try:
            artifacts = context.project.artifacts.get(artifact_id=context.artifact.id)
        except Exception:
            artifacts = list()
    elif resource == 'package_name':
        artifacts = context.project.artifacts.list(package_name=context.package.name)
    elif resource == 'execution_id':
        artifacts = context.project.artifacts.list(execution_id=context.execution.id)

    assert artifacts is not None

    if not isinstance(artifacts, list):
        artifacts = [artifacts]

    assert len(artifacts) == 0


================================================
File: tests/features/steps/artifacts_repo/test_artifacts_download.py
================================================
import types

import behave
import os


@behave.when(u'I download artifact by "{get_method}"')
def step_impl(context, get_method):
    if get_method == 'name':
        context.artifact_get = context.project.artifacts.download(artifact_name=context.artifact.name)
    elif get_method == 'id':
        context.artifact_get = context.project.artifacts.download(artifact_id=context.artifact.id)
    elif get_method == 'package_name':
        context.artifact_get = context.project.artifacts.download(package_name=context.package.name)
    elif get_method == 'execution_id':
        context.artifact_get = context.project.artifacts.download(execution_id=context.execution.id)


@behave.then(u'Artifact "{resource}" was downloaded successfully')
def step_impl(context, resource):
    if not isinstance(context.artifact_get, list) and not isinstance(context.artifact_get, types.GeneratorType):
        context.artifact_get = [context.artifact_get]

    for artifact_get in context.artifact_get:
        assert os.path.isfile(artifact_get)
        if resource == 'folder':
            assert os.path.basename(context.artifact_filepath) in artifact_get
    if hasattr(context, 'by_execution_id') and context.by_execution_id:
        assert len(context.artifact_get) == 1


@behave.given(u'Context has attribute execution_id = True')
def step_impl(context):
    context.by_execution_id = True


================================================
File: tests/features/steps/artifacts_repo/test_artifacts_get.py
================================================
import behave
import dictdiffer


@behave.when(u'I get artifact by "{get_method}"')
def step_impl(context, get_method):
    if get_method == 'name':
        context.artifact_get = context.project.artifacts.get(artifact_name=context.artifact.name)
    elif get_method == 'id':
        context.artifact_get = context.project.artifacts.get(artifact_id=context.artifact.id)
    elif get_method == 'package_name':
        context.artifact_get = context.project.artifacts.get(package_name=context.package.name,
                                                             artifact_name=context.artifact.name)
    elif get_method == 'execution_id':
        context.artifact_get = context.project.artifacts.get(execution_id=context.execution.id,
                                                             artifact_name=context.artifact.name)
    elif get_method.startswith('wrong'):
        try:
            if get_method == 'wrong_artifact_name':
                context.artifact_get = context.project.artifacts.get(artifact_name=get_method)
            elif get_method == 'wrong_package_name':
                context.artifact_get = context.project.artifacts.get(package_name=get_method,
                                                                     artifact_name=context.artifact.name)
            elif get_method == 'wrong_artifact_id':
                context.artifact_get = context.project.artifacts.get(artifact_id=get_method)
            elif get_method == 'wrong_execution_id':
                context.artifact_get = context.project.artifacts.get(execution_id=get_method,
                                                                     artifact_name=context.artifact.name)
        except Exception as e:
            context.error = e


@behave.then(u'I receive an Artifact entity')
def step_impl(context):
    assert isinstance(context.artifact, context.dl.entities.Artifact)


@behave.then(u'Artifact received equals to the one uploaded')
def step_impl(context):
    original_json = context.artifact.to_json()
    get_json = context.artifact_get.to_json()
    original_json['metadata'].pop('system', None)
    get_json['metadata'].pop('system', None)
    original_json.pop('annotations', None)
    get_json.pop('annotations', None)
    if original_json == get_json:
        assert True
    else:
        diffs = list(dictdiffer.diff(original_json, get_json))
        print(diffs)
        assert False


================================================
File: tests/features/steps/artifacts_repo/test_artifacts_list.py
================================================
import behave


@behave.when(u'I list Artifacts with "{param}"')
def step_impl(context, param):
    if param == 'package_name':
        context.artifacts_list = context.project.artifacts.list(package_name=context.package.name)
    elif param == 'execution_id':
        context.artifacts_list = context.project.artifacts.list(execution_id=context.execution.id)


@behave.then(u'I receive artifacts list of "{count}" items')
def step_impl(context, count):
    assert len(context.artifacts_list) == int(count)


================================================
File: tests/features/steps/artifacts_repo/test_artifacts_upload.py
================================================
import behave
import os
from PIL import Image
import io


@behave.when(u'I upload "{upload_count}" artifacts to "{resource}"')
def step_impl(context, resource, upload_count):
    if resource == 'project':
        if int(upload_count) == 1:
            context.artifact = context.project.artifacts.upload(filepath=context.artifact_filepath,
                                                                package_name=None,
                                                                package=None,
                                                                execution_id=None,
                                                                execution=None)
        else:
            image = Image.open(context.artifact_filepath)
            buffer = io.BytesIO()
            image.save(buffer, format='jpeg')
            for i in range(int(upload_count)):
                buffer.name = 'artifat_test_upload_{}'.format(i)
                context.artifact = context.project.artifacts.upload(filepath=buffer,
                                                                    package_name=None,
                                                                    package=None,
                                                                    execution_id=None,
                                                                    execution=None)
    elif resource == 'package':
        if int(upload_count) == 1:
            context.artifact = context.project.artifacts.upload(filepath=context.artifact_filepath,
                                                                package_name=None,
                                                                package=context.package,
                                                                execution_id=None,
                                                                execution=None)
        else:
            image = Image.open(context.artifact_filepath)
            buffer = io.BytesIO()
            image.save(buffer, format='jpeg')
            for i in range(int(upload_count)):
                buffer.name = 'artifat_test_upload_{}'.format(i)
                context.artifact = context.project.artifacts.upload(filepath=buffer,
                                                                    package_name=None,
                                                                    package=context.package,
                                                                    execution_id=None,
                                                                    execution=None)
    elif resource == 'package_name':
        if int(upload_count) == 1:
            context.artifact = context.project.artifacts.upload(filepath=context.artifact_filepath,
                                                                package_name=context.package.name,
                                                                package=None,
                                                                execution_id=None,
                                                                execution=None)
        else:
            image = Image.open(context.artifact_filepath)
            buffer = io.BytesIO()
            image.save(buffer, format='jpeg')
            for i in range(int(upload_count)):
                buffer.name = 'artifat_test_upload_{}'.format(i)
                context.artifact = context.project.artifacts.upload(filepath=buffer,
                                                                    package_name=context.package.name,
                                                                    package=None,
                                                                    execution_id=None,
                                                                    execution=None)
    elif resource == 'execution':
        if int(upload_count) == 1:
            context.artifact = context.project.artifacts.upload(filepath=context.artifact_filepath,
                                                                package_name=None,
                                                                package=None,
                                                                execution_id=None,
                                                                execution=context.execution)
        else:
            image = Image.open(context.artifact_filepath)
            buffer = io.BytesIO()
            image.save(buffer, format='jpeg')
            for i in range(int(upload_count)):
                buffer.name = 'artifat_test_upload_{}'.format(i)
                context.artifact = context.project.artifacts.upload(filepath=buffer,
                                                                    package_name=None,
                                                                    package=None,
                                                                    execution_id=None,
                                                                    execution=context.execution)

    elif resource == 'execution_id':
        if int(upload_count) == 1:
            context.artifact = context.project.artifacts.upload(filepath=context.artifact_filepath,
                                                                package_name=None,
                                                                package=None,
                                                                execution_id=context.execution.id,
                                                                execution=None)
        else:
            image = Image.open(context.artifact_filepath)
            buffer = io.BytesIO()
            image.save(buffer, format='jpeg')
            for i in range(int(upload_count)):
                buffer.name = 'artifat_test_upload_{}'.format(i)
                context.artifact = context.project.artifacts.upload(filepath=buffer,
                                                                    package_name=None,
                                                                    package=None,
                                                                    execution_id=context.execution.id,
                                                                    execution=None)


@behave.given(u'Context "{attribute}" is "{str_value}"')
def step_impl(context, attribute, str_value):
    if 'filepath' in attribute:
        str_value = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], str_value)
    setattr(context, attribute, str_value)


@behave.then(u'I receive an artifact object')
def step_impl(context):
    assert isinstance(context.artifact, context.dl.entities.Artifact)


================================================
File: tests/features/steps/assignment_entity/test_assignment_update.py
================================================
import behave


@behave.when(u'I update assignment name "{assignment_name}"')
def step_impl(context, assignment_name):
    context.assignment.name = assignment_name
    context.assignment.update()

================================================
File: tests/features/steps/assignments_repo/test_assignments_context.py
================================================
import behave


@behave.when(u'I get an Assignment')
def step_impl(context):
    context.assignment = context.task.assignments.list()[0]


@behave.given(u'I set Dataset to Dataset {dataset_index}')
def step_impl(context, dataset_index):
    context.dataset = context.datasets[int(dataset_index) - 1]


@behave.when(u'I append task to tasks')
def step_impl(context):
    if not hasattr(context, "tasks"):
        context.tasks = list()
    context.tasks.append(context.task)


@behave.when(u'I get the assignment from project number {project_index}')
def step_impl(context, project_index):
    context.assignment = context.projects[int(project_index) - 1].assignments.get(assignment_id=context.assignment.id)


@behave.when(u'I get the assignment from dataset number {dataset_index}')
def step_impl(context, dataset_index):
    context.assignment = context.datasets[int(dataset_index) - 1].assignments.get(assignment_id=context.assignment.id)


@behave.then(u'assignment Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.assignment.project_id == context.projects[int(project_index)-1].id


@behave.then(u'assignment Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.assignment.project.id == context.projects[int(project_index)-1].id


@behave.then(u'assignment Dataset_id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.assignment.dataset_id == context.datasets[int(dataset_index)-1].id


@behave.then(u'assignment Dataset.id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.assignment.dataset.id == context.datasets[int(dataset_index)-1].id


@behave.then(u'assignment Task_id is equal to task {task_index} id')
def step_impl(context, task_index):
    assert context.assignment.task_id == context.tasks[int(task_index)-1].id


@behave.then(u'assignment Task.id is equal to task {task_index} id')
def step_impl(context, task_index):
    assert context.assignment.task.id == context.tasks[int(task_index)-1].id


================================================
File: tests/features/steps/assignments_repo/test_assignments_create.py
================================================
import behave
from .. import fixtures


@behave.when(u'I create an Assignment from "{entity}" entity')
def step_impl(context, entity):
    context.params = {param.split('=')[0]: fixtures.get_assignment_value(params=param.split('='), context=context) for
                      param in context.table.headings if
                      fixtures.get_assignment_value(params=param.split('='), context=context) is not None}

    if entity != 'task':
        context.params['task'] = context.task

    if entity == 'task':
        context.assignment = context.task.assignments.create(**context.params)
    elif entity == 'dataset':
        context.assignment = context.dataset.assignments.create(**context.params)
    elif entity == 'project':
        context.assignment = context.project.assignments.create(**context.params)


@behave.then(u'I receive an assignment entity')
def step_impl(context):
    assert isinstance(context.task, context.dl.entities.Task)


def compare_items_list(items_a, items_b):
    equals = len(items_b) == len(items_a)
    items_c = [item if isinstance(item, str) else item.id for item in items_b]
    for item in items_a:
        equals = equals and item in items_c
    return equals


@behave.then(u'Assignment has the correct attributes')
def step_impl(context):
    for key, val in context.params.items():
        if key == 'filters':
            assignment_items = [item.id for item in context.assignment.get_items().items]
            assert compare_items_list(items_a=assignment_items, items_b=context.dataset.items.list(filters=val).items)
        elif key == 'items':
            assignment_items = [item.id for item in context.assignment.get_items().items]
            assert compare_items_list(items_a=assignment_items, items_b=[item.id for item in val])
        elif key == 'assignee_ids':
            assert context.assignment.annotator == val
        elif key == 'dataset':
            assert context.assignment.dataset_id == val.id
        elif key == 'project_id':
            assert context.assignment.project_id == val
        elif key == 'status':
            assert context.assignment.status == val
        elif key == 'metadata':
            assert context.assignment.metadata[list(val.keys())[0]] == val[list(val.keys())[0]]


================================================
File: tests/features/steps/assignments_repo/test_assignments_get.py
================================================
import behave


@behave.when(u'I get Assignment by "{get_method}"')
def step_impl(context, get_method):
    if get_method == 'id':
        context.assignment_get = context.task.assignments.get(assignment_id=context.assignment.id)
    elif get_method == 'name':
        context.assignment_get = context.task.assignments.get(assignment_name=context.assignment.name)


@behave.then(u'I get an assignment entity')
def step_impl(context):
    assert isinstance(context.assignment_get, context.dl.entities.Assignment)


@behave.then(u'Assignment received equals assignment created')
def step_impl(context):
    assert context.assignment_get.to_json() == context.assignment.to_json()


@behave.given(u'I save dataset items to context')
def step_impl(context):
    context.items_in_dataset = context.dataset.items.list().items


@behave.when(u'I get assignment from task')
def step_impl(context):
    assert hasattr(context, "task"), "TEST FAILED: Please create a task with assignments before this step"
    context.assignment = context.task.assignments.list()[0]


================================================
File: tests/features/steps/assignments_repo/test_assignments_items_operations.py
================================================
import os
import io
import behave


@behave.when(u'I get assignment items')
def step_impl(context):
    context.assignment_items = context.assignment.get_items()


@behave.then(u'I receive a list of "{count}" assignment items')
def step_impl(context, count):
    assert context.assignment_items.items_count == int(count)


@behave.then(u'I receive a list of "{count}" items for each assignment')
def step_impl(context, count):
    for assignment in context.task.assignments.list():
        assert assignment.total_items == int(count), "TEST FAILED: Assignment items expected : {} , Got : {}".format(count, assignment.total_items)






================================================
File: tests/features/steps/assignments_repo/test_assignments_list.py
================================================
import behave


@behave.when(u'I list assignments')
def step_impl(context):
    context.assignments_list = context.task.assignments.list()


@behave.then(u'I receive a list of "{count}" assignments')
def step_impl(context, count):
    assert len(context.assignments_list) == int(count)
    for ass in context.assignments_list:
        assert isinstance(ass, context.dl.entities.Assignment)


================================================
File: tests/features/steps/assignments_repo/test_assignments_reassign.py
================================================
import behave
import dictdiffer


@behave.when(u'I reassign assignment to "{new_assignee}"')
def step_impl(context, new_assignee):
    context.reassigned = context.assignment.reassign(assignee_id=new_assignee)


@behave.then(u'Assignments was reassigned to "{new_assignee}"')
def step_impl(context, new_assignee):
    assignment_json = context.reassigned.to_json()
    org_assignment_json = context.assignment.to_json()
    assignment_json.pop('annotator')
    org_assignment_json.pop('annotator')
    assignment_json.pop('url')
    org_assignment_json.pop('url')
    assignment_json.pop('id')
    org_assignment_json.pop('id')
    assignment_json.pop('name')
    org_assignment_json.pop('name')
    success = assignment_json == org_assignment_json
    if not success:
        diffs = list(dictdiffer.diff(org_assignment_json, assignment_json))
        print(diffs)
    success = success and context.reassigned.annotator == new_assignee
    assert success




================================================
File: tests/features/steps/assignments_repo/test_assignments_redistribute.py
================================================
import behave


@behave.when(u'I get the first assignment')
def step_impl(context):
    context.assignment = context.task.assignments.list()[0]


@behave.when(u'I redistribute assignment to "{new_assignees}"')
def step_impl(context, new_assignees):
    assignee_ids = new_assignees.split(',')
    workload = context.dl.Workload.generate(assignee_ids=assignee_ids)
    context.before_redistribution_item_ids = [item.id for item in context.assignment.get_items().items]
    context.redistributed = [ass for ass in context.assignment.redistribute(workload=workload) if ass.annotator in [ass_id.lower() for ass_id in assignee_ids]]


def compare_items_list(items_a, items_b):
    equals = len(items_b) == len(items_a)
    for item in items_a:
        equals = equals and item in items_b
    return equals


@behave.then(u'Assignments was redistributed to "{new_assignees}"')
def step_impl(context, new_assignees):
    assignee_ids = new_assignees.split(',')
    assert len(context.redistributed) == len(assignee_ids)
    items = list()
    for ass in context.redistributed:
        assert isinstance(ass, context.dl.entities.Assignment)
        items += ass.get_items().items
        assert ass.annotator in assignee_ids
    dest_item_ids = [item.id for item in items]

    assert compare_items_list(context.before_redistribution_item_ids, dest_item_ids)


================================================
File: tests/features/steps/billing_steps/object_creation_steps.py
================================================
import behave
import dtlpy as dl
import sys
import time
import datetime
import json
import os


@behave.given('I create "{entity}" name "{name}"')
@behave.when('I create "{entity}" name "{name}"')
def step_impl(context, entity, name):
    """
    :param context:
    :param entity: org / project / dataset
    :param name: object name
    :return:
    """

    context.num = time.time()
    context.name = f'to-delete-test-{name}_{int(context.num)}'

    if entity == "org":
        success, response = dl.client_api.gen_request(req_type='post',
                                                      path='/orgs',
                                                      json_req={"name": context.name}
                                                      )

        context.org = dl.entities.Organization.from_json(client_api=dl.client_api,
                                                         _json=response.json())

        assert isinstance(context.org, context.dl.entities.Organization)

        context.feature.dataloop_feature_org = context.org
        if not success:
            raise context.dl.exceptions.PlatformException(response)
    elif entity == "project":

        context.project = dl.projects.create(project_name=context.name)

        assert isinstance(context.project, context.dl.entities.Project)

        context.feature.dataloop_feature_project = context.project

    elif entity == "dataset":

        context.dataset = context.project.datasets.create(dataset_name=context.name)

    else:
        sys.exit("You must specify object as either 'org' 'project' 'dataset'")


@behave.given('I create custom subscription')
@behave.when('I create custom subscription')
def step_impl(context):

    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    current_utc_datetime = datetime.datetime.now(datetime.UTC)
    next_year_datetime = current_utc_datetime.replace(year=current_utc_datetime.year + 1)
    utc_formated = current_utc_datetime.strftime('%Y-%m-%dT00:00:00.000Z')
    next_year_formated = next_year_datetime.strftime('%Y-%m-%dT00:00:00.000Z')
    context.json_object['startDate'] = utc_formated
    context.json_object['endDate'] = next_year_formated
    context.json_object['scope']['entityId'] = context.org.id

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "annotation-tool-hours":
            context.json_object['plan']['resources'][0]['amount'] = float(param[1])
        if param[0] == "data-points":
            context.json_object['plan']['resources'][1]['amount'] = int(param[1])
        if param[0] == "api-calls":
            context.json_object['plan']['resources'][2]['amount'] = int(param[1])
        if param[0] == "hosted-storage":
            context.json_object['plan']['resources'][3]['amount'] = float(param[1])
        if param[0] == "system-compute":
            context.json_object['plan']['resources'][4]['amount'] = float(param[1])
        if param[0] == "compute-cpu-regular-xs":
            context.json_object['plan']['resources'][5]['amount'] = float(param[1])
        if param[0] == "compute-cpu-regular-s":
            context.json_object['plan']['resources'][6]['amount'] = float(param[1])
        if param[0] == "compute-cpu-regular-m":
            context.json_object['plan']['resources'][7]['amount'] = float(param[1])
        if param[0] == "compute-cpu-regular-l":
            context.json_object['plan']['resources'][8]['amount'] = float(param[1])
        if param[0] == "compute-cpu-highmem-xs":
            context.json_object['plan']['resources'][9]['amount'] = float(param[1])
        if param[0] == "compute-cpu-highmem-s":
            context.json_object['plan']['resources'][10]['amount'] = float(param[1])
        if param[0] == "compute-cpu-highmem-m":
            context.json_object['plan']['resources'][11]['amount'] = float(param[1])
        if param[0] == "compute-cpu-highmem-l":
            context.json_object['plan']['resources'][12]['amount'] = float(param[1])
        if param[0] == "compute-gpu-t4":
            context.json_object['plan']['resources'][13]['amount'] = float(param[1])
        if param[0] == "compute-gpu-t4-m":
            context.json_object['plan']['resources'][14]['amount'] = float(param[1])
        if param[0] == "compute-gpu-a100-s":
            context.json_object['plan']['resources'][15]['amount'] = float(param[1])
        if param[0] == "compute-gpu-a100-m":
            context.json_object['plan']['resources'][16]['amount'] = float(param[1])
        if param[0] == "account":
            context.json_object['account'] = context.org.account['id']
        if param[0] == "compute-budget":
            new_json = context.json_object
            new_resource = {
                        "description": "",
                        "icon": "icon url or alias",
                        "key": "compute-budget",
                        "name": "Compute Budget",
                        "type": "metric",
                        "price": 0,
                        "unit": "USD",
                        "overQuota": {
                          "limit": 10,
                          "price": 0
                        },
                        "amount": 0
                      }
            # Append the new resource to the resources list
            new_json["plan"]["resources"].append(new_resource)
            context.json_object = new_json
            context.json_object['plan']['resources'][17]['amount'] = float(param[1])

    success, response = dl.client_api.gen_request(req_type='post',
                                                  path=billing_api_calls['create_custom'],
                                                  json_req=context.json_object
                                                  )
    if not success:
        raise context.dl.exceptions.PlatformException(response)


@behave.given('I "{action}" enable-custom-subscription-blocking = True')
@behave.when('I "{action}" enable-custom-subscription-blocking = True')
def step_impl(context, action):

    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    if action == "create":
        context.json_object['scope']['id'] = context.org.id
        success, response = dl.client_api.gen_request(req_type='post',
                                                      path=billing_api_calls['ff'],
                                                      json_req=context.json_object
                                                      )
        context.ff_response = response.json()
        if not success:
            raise context.dl.exceptions.PlatformException(response)

    elif action == "delete":
        context.json_object['scope']['id'] = context.org.id
        success, response = dl.client_api.gen_request(req_type='delete',
                                                      path=billing_api_calls['ff'] + context.ff_response['id'],
                                                      )
        if not success:
            raise context.dl.exceptions.PlatformException(response)


================================================
File: tests/features/steps/billing_steps/test_delete_org.py
================================================
import behave
import dtlpy as dl


@behave.when('I delete the org')
def step_impl(context):
    success, response = context.dl.client_api.gen_request(path="/orgs/{}".format(context.org.id), req_type="DELETE")
    assert response.status_code == 500
    res = response.json()
    context.error = res['message']


@behave.given(u'I update the project org')
@behave.when(u'I update the project org')
def step_impl(context):
    if context.org is None:
        raise Exception('context.org is not defined')

    payload = {"org_id": context.org.id}
    success, response = context.dl.client_api.gen_request(req_type='patch', path='/projects/' + context.project.id + '/org',
                                                    data=payload)
    if not success:
        raise dl.exceptions.PlatformException(response)

    context.project = dl.projects.get(project_id=context.project.id)

================================================
File: tests/features/steps/billing_steps/test_get_plan_steps.py
================================================
import behave
import dtlpy as dl
import time
import os
import json


@behave.then('Validate plan "{field}" is "{value}"')
def step_impl(context, field, value):

    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], 'billing', 'billing_api_calls.json')
    with open(path, 'r') as file:
        context.billing_api_calls = json.load(file)

    num_try = 20
    interval = 10
    finished = False
    for i in range(num_try):

        success, response = context.dl.client_api.gen_request(req_type="get",
                                                              path=context.billing_api_calls['plg'].replace('org_id', context.org.id))
        if success == True:
            finished = True
            break
        time.sleep(interval)
    assert finished, f"TEST FAILED: Expected PLG plan and fail after {round(num_try * interval / 60, 1)} minutes"
    sub = eval(response.text.replace("true", "True"))
    plan = {"Type": sub['plan']['name'],
            # plan_answer = "Free" / "Basic / "Standard" / "Pro" / "Pro Plus"
            "Period": sub['period']
            # plan_answer = "monthly" / "annually"
            }
    assert plan[field] == value, f"TEST FAILED: Expected {value} got {plan[field]}"


================================================
File: tests/features/steps/billing_steps/test_plan_resources.py
================================================
import time
import behave
import json
import os
import dtlpy as dl
from datetime import datetime, timedelta, timezone


@behave.given('I get plans resources json')
def step_impl(context):
    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    # API request for the api_resources
    success, response = context.dl.client_api.gen_request(req_type="get", path=billing_api_calls['plans'])
    # Display the Error
    if not success:
        raise context.dl.exceptions.PlatformException(response)
    context.plans_resources = response.json()


@behave.then('I validate "{plan_type}" plan resources')
def step_impl(context, plan_type):
    plans_types = {"Free": 0,
                   "Basic": 1,
                   "Standard": 2,
                   "Pro": 3,
                   "Pro Plus": 4
                   }
    api_resources = context.plans_resources[plans_types[plan_type]]

    if plan_type == "Free":
        json_resources = context.json_object["Free"]
    if plan_type == "Basic":
        json_resources = context.json_object["Basic"]
    if plan_type == "Standard":
        json_resources = context.json_object["Standard"]
    if plan_type == "Pro":
        json_resources = context.json_object["Pro"]
    if plan_type == "Pro Plus":
        json_resources = context.json_object["Pro Plus"]

    assert json_resources == api_resources, f"TEST FAILED: Expected {json_resources} got {api_resources}"


@behave.when('I fetch "{file_name}" file from "{folder_name}"')
@behave.given('I fetch "{file_name}" file from "{folder_name}"')
def step_impl(context, folder_name, file_name):
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], folder_name, file_name)
    with open(path, 'r') as file:
        context.json_object = json.load(file)


@behave.given('I update quotas')
@behave.when('I update quotas')
def step_impl(context):
    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    # _json = '{"org":"' + context.org.id + '"}'
    org_json = {"org": context.org.id}
    success, response = dl.client_api.gen_request(req_type='post',
                                                  path=billing_api_calls['aggregation'].replace('account_id',
                                                                                                context.org.account[
                                                                                                    "id"]),
                                                  json_req=org_json
                                                  )

    if not success:
        raise context.dl.exceptions.PlatformException(response)
    response_json = response.json()
    command = context.dl.Command.from_json(_json=response_json,
                                           client_api=dl.client_api)
    command.wait(timeout=0)


@behave.when('I get usage')
def step_impl(context):
    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    # Get the current date
    current_date = datetime.now()
    # Set the startDate to the first day of the current month
    startDate = datetime(current_date.year, current_date.month, 1).strftime('%Y-%m-%d')

    # Calculate the endDate as the first day of the next month
    if current_date.month == 12:
        endDate = datetime(current_date.year + 1, 1, 1).strftime('%Y-%m-%d')
    else:
        endDate = datetime(current_date.year, current_date.month + 1, 1).strftime('%Y-%m-%d')

    success, response = dl.client_api.gen_request(req_type='get',
                                                  path=billing_api_calls['usage'].replace('account_id',
                                                                                          context.org.account["id"])
                                                       + f"?startDate={startDate}&endDate={endDate}"
                                                  )

    if not success:
        raise context.dl.exceptions.PlatformException(response)


@behave.then('I unable to upload items')
def step_impl(context):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)

    filename = 'file'

    uploaded_filename = filename + 'me.jpg'
    import io
    with open(filepath, 'rb') as f:
        buffer = io.BytesIO(f.read())
        buffer.name = uploaded_filename
    try:
        context.dataset.items.upload(
            local_path=buffer,
            remote_path=None
        )
        context.error = None
    except Exception as e:
        context.error = e


@behave.when('I delete the free subscription')
def step_impl(context):
    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    num_try = 60
    interval = 5
    for i in range(num_try):
        success, response = context.dl.client_api.gen_request(req_type="get",
                                                              path=billing_api_calls['get_free'].replace('org_id',
                                                                                                         context.org.id))

        if success:
            break
        time.sleep(interval)

    if not success:
        raise context.dl.exceptions.PlatformException(response)

    context.subscription = response.json()

    success, response = context.dl.client_api.gen_request(req_type="post",
                                                          path=billing_api_calls['cancel'].replace('subscription_id',
                                                                                                   context.subscription["id"]))

    if not success:
        raise context.dl.exceptions.PlatformException(response)


@behave.Then('I unable to activate service')
def step_impl(context):
    services = context.project.services.list().items
    service = dl.services.get(service_id=services[0].id)
    if not service.active:
        try:
            service.active = True
            service.update()
            context.error = None
            service = dl.services.get(service_id=services[0].id)
        except Exception as e:
            context.error = e
    else:
        assert False, f"TEST FAILED: service already active"


@behave.Then('I activate service')
def step_impl(context):
    services = context.project.services.list().items
    service = dl.services.get(service_id=services[0].id)
    if service.active:
        service.active = False
        service.update()
        service = dl.services.get(service_id=services[0].id)
        if not service.active:
            service.active = True
            service.update()
            service = dl.services.get(service_id=services[0].id)
            assert service.active, f"TEST FAILED: Unable to activate service"
        else:
            assert False, f"TEST FAILED: Unable to deactivate service"
    else:
        assert False, f"TEST FAILED: Service already inactive"


@behave.Then('I deactivate service named "{service_name}"')
def step_impl(context, service_name):
    services = context.project.services.list().items
    # Iterate through the services to find the one with the correct name
    for service in services:
        if service.name == service_name:
            service = dl.services.get(service_id=service.id)
            if service.active:
                service.active = False
                service.update()
                service = dl.services.get(service_id=service.id)
                assert not service.active, f"TEST FAILED: Unable to deactivate service '{service_name}'"
            else:
                assert False, f"TEST FAILED: Service '{service_name}' is already inactive"
            break
    else:
        assert False, f"TEST FAILED: Service with name '{service_name}' not found"


@behave.when(u'I update context.service')
def step_impl(context):
    num_try = 60
    interval = 7
    success = False

    for i in range(num_try):
        time.sleep(interval)
        services = context.project.services.list().items
        if services:
            service = dl.services.get(service_id=services[0].id)
            context.service = service
            success = True
            break
        context.dl.logger.debug(
            "Step is running for {:.2f}[s] and now Going to sleep {:.2f}[s]".format((i + 1) * interval,
                                                                                    interval))

    assert success, "TEST FAILED: Expected 1-service, Got {}".format(len(services))


@behave.when('I get analytics query "{pod_type}" for {sec} seconds')
def step_impl(context, pod_type, sec):


    # fetch billing api calls
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "billing_api_calls.json")
    with open(path, 'r') as file:
        billing_api_calls = json.load(file)

    # fetch analytics query json
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "billing", "analytics_query.json")
    with open(path, 'r') as file:
        analytics_query = json.load(file)

    # Get current UTC time and calculate previous and next day
    current_utc_datetime = datetime.now(timezone.utc)
    previous_day_datetime = current_utc_datetime - timedelta(days=1)
    next_day_datetime = current_utc_datetime + timedelta(days=1)

    # Convert to Unix timestamps
    previous_day_formatted = int(previous_day_datetime.timestamp() * 1000)
    next_day_formatted = int(next_day_datetime.timestamp() * 1000)

    # Edit JSON
    analytics_query["context"]["projectId"] = [context.project.id]  # Ensure projectId is an array
    analytics_query["startTime"] = previous_day_formatted
    analytics_query["endTime"] = next_day_formatted

    # Initialize or increment num_services
    if not hasattr(context, 'index_num') or context.index_num is None:
        # If 'num_services' doesn't exist in the context or is None, initialize it to 0 or 1
        context.index_num = 0  # or set to 1 if you prefer starting from 1
    else:
        # Increment 'num_services' by 1 on each run
        context.index_num += 1

    start_time = datetime.now()
    # Loop until the condition is met
    while True:

        # Check if 10 minutes have passed
        elapsed_time = (datetime.now() - start_time).total_seconds()
        if elapsed_time >= 600:  # 600 seconds = 10 minutes
            assert False, "Operation timed out after 10 minutes."

        # Make the API request to get updated data
        success, response = dl.client_api.gen_request(
            req_type='post',
            path=billing_api_calls['analytics_query'],
            json_req=analytics_query
        )

        if not success:
            raise context.dl.exceptions.PlatformException(response)
        num_seconds = 0
        response_json = response.json()

        # Check if the response contains any data
        if response_json[0].get('response', []):
            index_num = context.index_num  # Reset index_num each time a new request is made

            while index_num >= 0:
                # Check the response value for the current service
                try:
                    # Access the current response item safely
                    response_item = response_json[0].get('response', [])[index_num]
                    active_pod_type = response_item.get('podType', 0)

                    if pod_type == active_pod_type:
                        num_seconds = response_item.get('seconds', 0)
                        if num_seconds >= int(sec):
                            context.dl.logger.info(f"Condition met: num_seconds is {sec} or more.")
                            break  # Break out of the inner while loop
                        else:
                            context.dl.logger.info(f"num_seconds {num_seconds} is less than {sec}. Checking again after 30 seconds...")
                            time.sleep(30)  # Wait before checking again
                            break  # Break out of the inner while loop to make a new request
                    else:
                        context.dl.logger.info(f"Expected podType - {pod_type}, Actual podType - {active_pod_type}")
                        # Decrement index_num to check the next service
                        index_num -= 1

                except IndexError:
                    context.dl.logger.info("IndexError: No response for the current service number. Retrying...")
                    time.sleep(30)
                    index_num -= 1  # Decrement index_num and retry
                    continue

            # If the condition is met, break out of the outer loop
            if num_seconds >= int(sec):
                break  # Break out of the outer while loop

        else:
            context.dl.logger.info("No valid 'response' found in the JSON. Retrying after 30 seconds...")
            time.sleep(30)

================================================
File: tests/features/steps/bots_repo/test_bots_create.py
================================================
import behave


@behave.when(u'I try to create a bot by the name of "{bot_name}"')
def creating_a_project(context, bot_name):
    try:
        context.bot = context.project.bots.create(name=bot_name)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I create a bot by the name of "{bot_name}"')
def creating_a_project(context, bot_name):
    context.bot = context.project.bots.create(name=bot_name)
    context.bot_user = context.bot

@behave.then(u'Bot object by the name of "{bot_name}" should be created')
def project_object_should_be_created(context, bot_name):
    assert type(context.project) == context.dl.entities.Project
    assert context.project.name == context.project_name


================================================
File: tests/features/steps/bots_repo/test_bots_delete.py
================================================
import behave


@behave.when(u'I delete the created bot by "{element}"')
def step_impl(context, element):
    if element == "email":
        context.project.bots.delete(bot_email=context.bot.email)
    elif element == "id":
        context.project.bots.delete(bot_id=context.bot.id)
    else:
        assert False, "TEST FAILED: Please provide valid parameter: 'email' or 'id'"


@behave.when(u'I try to delete a bot by the name of "{bot_name}"')
def step_impl(context, bot_name):
    try:
        context.project.bots.delete(bot_email=bot_name)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I try to delete a bot by id')
def step_impl(context):
    try:
        context.project.bots.delete(bot_id=context.bot.id)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'There are no bot by the name of "{bot_name}"')
def step_impl(context, bot_name):
    try:
        bot = context.project.bots.get(bot_name=bot_name)
    except:
        assert False, 'cant get bot'


@behave.given(u'There are no bots in project')
def step_impl(context):
    try:
        for bot in context.project.bots.list():
            bot.delete()
    except:
        assert False, 'cant delete bot'


================================================
File: tests/features/steps/bots_repo/test_bots_get.py
================================================
# coding=utf-8
"""Bots repository get service testing."""

import behave


@behave.when(u'I get a bot by the name of "{bot_name}"')
def step_impl(context, bot_name):
    context.bot_get = context.project.bots.get(bot_name=bot_name)


@behave.then(u'Received bot equals created bot')
def step_impl(context):
    assert context.bot_get.to_json() == context.bot.to_json()


================================================
File: tests/features/steps/bots_repo/test_bots_list.py
================================================
# coding=utf-8
"""Bots repository list service testing."""

import behave


@behave.when(u'I list bots in project')
def creating_a_project(context):
    context.list = context.project.bots.list()


@behave.then(u'a bot with name "{bot_name}" exists in bots list')
def step_impl(context, bot_name):
    found = False
    for bot in context.list:
        if bot_name == bot.name:
            found = True
    assert found


@behave.then(u'I receive a bots list of "{list_length}"')
def step_impl(context, list_length):
    assert len(context.list) == int(list_length)


================================================
File: tests/features/steps/checkout_testing/test_checkout.py
================================================
import behave
import os


@behave.given(u'Feature: There is a package and service')
def step_impl(context):
    src_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], 'packages_checkout_create', 'package.json')
    services, context.package = context.project.packages.deploy_from_file(project=context.project,
                                                                          json_filepath=src_path)
    context.service = services[0]
    context.to_delete_packages_ids.append(context.package.id)
    context.feature.package = context.package
    context.to_delete_services_ids.append(context.service.id)
    context.feature.service = context.service


@behave.when(u'I checkout')
def step_impl(context):
    entity = context.table.headings[0]
    if entity == 'project':
        context.project.checkout()
    elif entity == 'dataset':
        context.dataset.checkout()
    elif entity == 'package':
        context.package.checkout()
    elif entity == 'service':
        context.service.checkout()
    else:
        assert False, 'Unknown entity param'


@behave.then(u'I am checked out')
def step_impl(context):
    entity = context.table.headings[0]
    if entity == 'project':
        assert context.dl.projects.get().id == context.project.id
    elif entity == 'dataset':
        assert context.project.datasets.get().id == context.dataset.id
    elif entity == 'package':
        assert context.project.packages.get().id == context.package.id
    elif entity == 'service':
        assert context.project.services.get().id == context.service.id
    else:
        assert False, 'Unknown entity param'


@behave.given(u'Feature: I pack to project directory in "{package_codebase}"')
def step_impl(context, package_codebase):
    package_codebase = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], package_codebase)
    context.codebase = context.project.codebases.pack(
        directory=package_codebase,
        name='package-codebase',
        description="some description",
    )


@behave.given(u'Feature: There is a Codebase directory with a python file in path "{code_path}"')
def step_impl(context, code_path):
    code_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], code_path)
    dirs = os.listdir(code_path)
    assert "some_code.py" in dirs
    context.feature.codebase_local_dir = code_path


@behave.given(u'Feature: I create a dataset by the name of "{dataset_name}"')
def step_impl(context, dataset_name):
    context.feature.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)


@behave.given(u'Get feature entities')
def step_impl(context):
    if hasattr(context.feature, 'done_setting') and context.feature.done_setting:
        for param in context.table.headings:
            if param == 'dataset':
                context.dataset = context.feature.dataset
            elif param == 'package':
                context.package = context.feature.package
            elif param == 'service':
                context.service = context.feature.service


@behave.given(u'Done setting')
def step_impl(context):
    context.feature.done_setting = True


================================================
File: tests/features/steps/cli_testing/cli_datasets.py
================================================
import behave
import os
import shutil


@behave.then(u'I create a dataset by the name of "{dataset_name}" in project "{project_name}"')
def step_impl(context, dataset_name, project_name):
    assert isinstance(dataset_name, str)
    project_name = project_name.replace("<random>", context.random)
    dataset_name = dataset_name.replace("<random>", context.random)
    project = context.dl.projects.get(project_name=project_name)
    project.datasets.get(dataset_name=dataset_name)


@behave.given(u'I clean folder "{dir_path}"')
def step_impl(_, dir_path):
    rel_path = os.environ['DATALOOP_TEST_ASSETS']
    dir_path = dir_path.replace("<rel_path>", rel_path)
    for item in os.listdir(dir_path):
        path = os.path.join(dir_path, item)
        if item == 'folder_keeper':
            continue
        elif os.path.isdir(path):
            shutil.rmtree(path)
        elif os.path.isfile(path):
            os.remove(path)



================================================
File: tests/features/steps/cli_testing/cli_others.py
================================================
import behave


@behave.then(u'Version is correct')
def step_impl(context):
    output = context.out.decode('utf-8')
    assert context.dl.__version__ in output, f"Version {context.dl.__version__} is not correct in output {output}"


@behave.then(u'"{msg}" in output')
def step_impl(context, msg):
    msg = msg.replace('<random>', context.random)
    output = context.out.decode('utf-8')
    assert msg.lower() in output.lower()


================================================
File: tests/features/steps/cli_testing/cli_projects.py
================================================
import behave
import random
import dtlpy as dl
import subprocess
import os
import time


@behave.given(u"I am logged in")
def step_impl(_):
    assert not dl.client_api.token_expired()


@behave.when(u"I perform command")
def step_impl(context):
    rel_path = os.environ['DATALOOP_TEST_ASSETS']

    # fix params
    for column in context.table.headings:
        index = context.table.get_column_index(column_name=column)
        column = column.replace("<random>", context.random)
        column = column.replace("<rel_path>", rel_path)
        context.table.headings[index] = column

    # set cmds
    cmds = ['dlp']
    cmds += context.table.headings

    # if list try 4 times
    num_tries = 1
    if '-p' in cmds:
        num_tries = 4

    # run command
    time.sleep(1)
    for i in range(num_tries):
        # run shell command
        p = subprocess.Popen(cmds, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        context.out, context.err = p.communicate()
        if p.returncode == 0:
            break

    # save return code
    context.return_code = p.returncode

    # debug
    if context.return_code != 0:
        if context.err is not None:
            print(context.err.decode('utf-8'))
        if context.out is not None:
            print(context.out.decode('utf-8'))


@behave.then(u'I create a project by the name of "{project_name}"')
def step_impl(context, project_name):
    assert isinstance(project_name, str)
    project_name = project_name.replace("<random>", context.random)
    context.dl.projects.get(project_name=project_name)


@behave.then(u"I succeed")
def step_impl(context):
    assert context.return_code == 0, "TEST FAILED: {}".format(context.err)


@behave.when(u"I succeed")
def step_impl(context):
    assert context.return_code == 0, "TEST FAILED: {}".format(context.err)


@behave.then(u"I dont succeed")
def step_impl(context):
    assert context.return_code != 0


@behave.given(u"I have context random number")
def step_impl(context):
    if not hasattr(context.feature, 'random'):
        context.feature.random = str(random.randrange(1000, 1000000))
    context.random = context.feature.random


@behave.given(u'I delete the project by the name of "{project_name}"')
def step_impl(context, project_name):
    assert isinstance(project_name, str)
    project_name = project_name.replace("<random>", context.random)
    project = context.dl.projects.get(project_name=project_name)
    project.delete(True, True)


@behave.then(u'I wait "{seconds}"')
@behave.when(u'I wait "{seconds}"')
def step_impl(_, seconds):
    time.sleep(float(seconds))


================================================
File: tests/features/steps/codebase_entity/test_codebase_repo_methods.py
================================================
import behave
import os


@behave.given(u'I pack from project.codebases directory by name "{codebase_name}"')
def step_impl(context, codebase_name):
    context.codebase = context.project.codebases.pack(
        directory=context.codebase_local_dir,
        name=codebase_name,
        description="some description"
    )


@behave.when(u'I unpack a code base entity by the name of "{codebase_name}" to "{unpack_path}"')
def step_impl(context, unpack_path, codebase_name):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    context.codebase.unpack(local_path=unpack_path)
    context.unpack_path = unpack_path


@behave.when(u'I list versions of code base entity "{codebase_name}"')
def step_impl(context, codebase_name):
    context.codebase_version_list = context.codebase.list_versions()


================================================
File: tests/features/steps/codebases_repo/test_codebases_get.py
================================================
import behave
import time


@behave.when(u'I get by name version "{version}" of code base "{codebase_name}"')
def step_impl(context, version, codebase_name):
    context.codebase_get = context.project.codebases.get(codebase_name=codebase_name,
                                                         codebase_id=None,
                                                         version=version)


@behave.when(u'I get a code base by name "{codebase_name}"')
def step_impl(context, codebase_name):
    context.codebase_get = context.project.codebases.get(codebase_name=codebase_name)


@behave.then(u'Codebase received equal code base packet')
def step_impl(context):
    assert context.codebase_get.to_json() == context.codebase.to_json()


@behave.when(u'I get by id version "{version}" of code base "{codebase_name}"')
def step_impl(context, version, codebase_name):
    context.codebase_get = context.project.codebases.get(
        codebase_name=None,
        codebase_id=context.codebase.item_id,
        version=version
    )


@behave.then(u"I receive a list of Codebase objects")
def step_impl(context):
    assert isinstance(context.codebase_get, list)


@behave.then(u'Codebase list have length of "{codebase_count}"')
def step_impl(context, codebase_count):
    assert len(context.codebase_get) == int(codebase_count)


@behave.then(u'I delete all project code bases')
def step_impl(context):
    dataset = context.project.datasets.get(dataset_name='Binaries')
    dataset.delete(True, True)
    time.sleep(4)
    context.project.codebases.dataset = context.project.datasets.create(dataset_name="Binaries", index_driver=context.index_driver_var)


================================================
File: tests/features/steps/codebases_repo/test_codebases_init.py
================================================
import behave


@behave.when(u"I init code bases with params: project, dataset, client_api")
def step_impl(context):
    context.codebases = context.dl.repositories.Codebases(
        project=context.project,
        dataset=context.dataset,
        client_api=context.dataset.items._client_api
    )


@behave.when(u"I init code bases with params: project, client_api")
def step_impl(context):
    context.codebases = context.dl.repositories.Codebases(
        project=context.project,
        client_api=context.dataset.items._client_api
    )


@behave.then(u'I receive a code bases repository object')
def step_impl(context):
    assert isinstance(context.codebases, context.dl.repositories.Codebases)


@behave.when(u'I try to init code bases with params: client_api')
def step_impl(context):
    try:
        context.codebases = context.dl.repositories.Codebases(
            client_api=context.dataset.items._client_api
        )
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u"I init code bases with params: dataset, client_api")
def step_impl(context):
    context.codebases = context.dl.repositories.Codebases(
        dataset=context.dataset,
        client_api=context.dataset.items._client_api
    )


@behave.then(u'Codebases project are equal')
def step_impl(context):
    assert context.project.to_json() == context.codebases.project.to_json()


@behave.then(u'Codebases dataset equal "Dataset"')
def step_impl(context):
    assert context.dataset.to_json() == context.codebases.dataset.to_json()


@behave.then(u'Codebases dataset has name "{name}"')
def step_impl(context, name):
    assert context.codebases.dataset.name == name


================================================
File: tests/features/steps/codebases_repo/test_codebases_list.py
================================================
import behave


@behave.when(u'I list all code bases')
def step_impl(context):
    context.codebase_list = context.project.codebases.list()


@behave.then(u'I receive a list of "{codebase_count}" code bases')
def step_impl(context, codebase_count):
    assert context.codebase_list.items_count == int(codebase_count), "Expected: {} , Got: {}".format(codebase_count, context.codebase_list.items_count)


@behave.given(u'There are "{codebases_num}" code bases')
def step_impl(context, codebases_num):
    context.codebase_list = context.project.codebases.list()
    assert context.codebase_list.items_count == int(codebases_num)


================================================
File: tests/features/steps/codebases_repo/test_codebases_list_versions.py
================================================
import behave


@behave.when(u'I list versions of "{codebase_name}"')
def step_impl(context, codebase_name):
    context.codebase_version_list = context.project.codebases.list_versions(codebase_name=codebase_name)


@behave.then(u'I receive a list of "{version_count}" versions')
def step_impl(context, version_count):
    assert len(context.codebase_version_list.items) == int(version_count)


================================================
File: tests/features/steps/codebases_repo/test_codebases_pack.py
================================================
import behave
import filecmp
import shutil
import os


@behave.given(u'There is a Codebase directory with a python file in path "{code_path}"')
def step_impl(context, code_path):
    code_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], code_path)
    dirs = os.listdir(code_path)
    assert "some_code.py" in dirs
    context.codebase_local_dir = code_path
    context.codebase_unpack_path = os.path.split(code_path)[-1]


@behave.given(u"I init code bases with params project, dataset, client_api")
def step_impl(context):
    context.codebases = context.dl.repositories.Codebases(
        project=context.project,
        dataset=context.dataset,
        client_api=context.dataset.items._client_api,
    )


@behave.given(u"Project has code bases repositiory")
def step_impl(context):
    context.codebases = context.project.codebases


@behave.given(u"I have a code bases repository")
def step_impl(context):
    context.codebases = context.dl.repositories.Codebases(
        project=context.project,
        dataset=context.dataset,
        client_api=context.dataset.items._client_api,
    )


@behave.given(u"I init code bases with params project, client_api")
def step_impl(context):
    context.codebases = context.dl.repositories.Codebases(
        project=context.project, client_api=context.dataset.items._client_api
    )


@behave.given(u'I pack directory by name "{codebase_name}"')
@behave.when(u'I pack directory by name "{codebase_name}"')
def step_impl(context, codebase_name):
    context.codebase = context.project.codebases.pack(
        directory=context.codebase_local_dir,
        name=codebase_name,
        description="some description"
    )


@behave.when(u"I pack directory - nameless")
def step_impl(context):
    context.codebase = context.project.codebases.pack(
        directory=context.codebase_local_dir
        # name="codebase_name",
        # description="some description"
    )


@behave.then(u"I receive a Codebase object")
def step_impl(context):
    assert isinstance(context.codebase, (context.dl.ItemCodebase,
                                         context.dl.FilesystemCodebase,
                                         context.dl.LocalCodebase,
                                         context.dl.GitCodebase,
                                         ))


@behave.then(u'Codebase in host when downloaded to "{unpack_path}" equals code base in path "{original_path}"')
def step_impl(context, original_path, unpack_path):
    original_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], original_path)
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    context.project.codebases.unpack(codebase_id=context.codebase.item_id, local_path=unpack_path)
    dirs = os.listdir(unpack_path)
    if 'folder_keeper' in dirs:
        dirs.pop(dirs.index('folder_keeper'))
    assert len(os.listdir(original_path)) == len(dirs)
    for file in os.listdir(original_path):
        origin = os.path.join(original_path, file)
        unpack = os.path.join(unpack_path, file)
        assert filecmp.cmp(origin, unpack)
    shutil.rmtree(unpack_path)


@behave.then(u'Dataset by the name of "{binaries_dataset_name}" was created')
def step_impl(context, binaries_dataset_name):
    if binaries_dataset_name == 'Binaries':
        context.dataset_binaries = context.project.datasets._get_binaries_dataset()
    else:
        context.dataset_binaries = context.project.datasets.get(
            dataset_name=binaries_dataset_name
        )


@behave.then(
    u'Codebase in host in dataset "{dataset_name}", when downloaded to "{unpack_path}" equals code base in path "{original_path}"')
def step_impl(context, original_path, dataset_name, unpack_path):
    original_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], original_path)
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    context.project.codebases.unpack(codebase_id=context.codebase.item_id, local_path=unpack_path)
    dirs = os.listdir(unpack_path)
    if 'folder_keeper' in dirs:
        dirs.pop(dirs.index('folder_keeper'))
    assert len(os.listdir(original_path)) == len(dirs)
    for file in os.listdir(original_path):
        origin = os.path.join(original_path, file)
        unpack = os.path.join(unpack_path, file)
        assert filecmp.cmp(origin, unpack)
    shutil.rmtree(unpack_path)


@behave.then(u'There should be "{version_count}" versions of the code base "{codebase_name}" in host')
def step_impl(context, version_count, codebase_name):
    codebase_name = os.path.split(os.path.split(context.codebase.item.filename)[0])[-1]
    codebase_get = context.project.codebases.get(codebase_name=codebase_name, version="all")
    if isinstance(codebase_get, list):
        count = len(codebase_get)
    else:
        # paged entity
        count = codebase_get.items_count

    assert count == int(version_count)


@behave.when(u'I modify python file - (change version) in path "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)

    with open(file_path, "a") as f:
        f.write("#some comments")
    f.close()


================================================
File: tests/features/steps/codebases_repo/test_codebases_unpack.py
================================================
import behave
import os
import filecmp
import shutil
from .. import fixtures


@behave.when(u'I unpack a code base by the name of "{codebase_name}" to "{unpack_path}"')
def step_impl(context, unpack_path, codebase_name):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    context.project.codebases.unpack(codebase_name=codebase_name, local_path=unpack_path)
    context.unpack_path = unpack_path


@behave.when(u'I unpack a code base by the id of "{codebase_name}" to "{unpack_path}"')
def step_impl(context, unpack_path, codebase_name):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    context.project.codebases.unpack(codebase_id=context.codebase.item_id, local_path=unpack_path)
    context.unpack_path = unpack_path


@behave.then(u'Unpacked code base equal to code base in "{original_path}"')
def step_impl(context, original_path):
    original_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], original_path)

    equal = fixtures.compare_dir_recursive(original_path, context.unpack_path)
    assert equal
    for file in os.listdir(original_path):
        origin = os.path.join(original_path, file)
        unpack = os.path.join(context.unpack_path, file)
        assert filecmp.cmp(origin, unpack)
    shutil.rmtree(context.unpack_path)


@behave.when(u'I try to unpack a code base by the name of "{codebase_name}" to "{unpack_path}"')
def step_impl(context, unpack_path, codebase_name):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    try:
        context.project.codebases.unpack(codebase_name=codebase_name,
                                         local_path=unpack_path)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I try to unpack a code base by the id of "{wrong_id}" to "{unpack_path}"')
def step_impl(context, unpack_path, wrong_id):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    try:
        context.project.codebases.unpack(codebase_id=wrong_id,
                                         local_path=unpack_path)
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u'I modify python file - (change version) in path "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)

    with open(file_path, "a") as f:
        f.write("#some comments")
    f.close()


@behave.when(u'I unpack a code base "{codebase_name}" version "{version}" to "{unpack_path}"')
def step_impl(context, version, codebase_name, unpack_path):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    context.project.codebases.unpack(codebase_name=codebase_name,
                                     local_path=unpack_path,
                                     version=version)
    context.unpack_path = unpack_path


@behave.then(u'I receive all versions in "{unpack_path}" and they are equal to versions in "{original_path}"')
def step_impl(context, unpack_path, original_path):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    original_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], original_path)
    dirs_unpack_path = os.listdir(unpack_path)
    if 'folder_keeper' in dirs_unpack_path:
        dirs_unpack_path.pop(dirs_unpack_path.index('folder_keeper'))
    assert len(dirs_unpack_path) == 2
    code = 'some_code.py'
    for dir in dirs_unpack_path:
        version_path = os.path.join(unpack_path, dir)
        code_path = os.path.join(version_path, code)
        if dir == 'v.0':
            with open(code_path, "a") as f:
                f.write("#some comments")
            f.close()
        assert filecmp.cmp(original_path, code_path)
        shutil.rmtree(version_path)


@behave.when(u'I try to unpack a code base "{codebase_name}" version "{version}" to "{unpack_path}"')
def step_impl(context, version, codebase_name, unpack_path):
    unpack_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], unpack_path)
    try:
        context.project.codebases.unpack(codebase_name=codebase_name,
                                         local_path=unpack_path,
                                         version=version)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/command_entity/test_command.py
================================================
import behave


@behave.when(u'Dataset is cloning')
def step_impl(context):
    context.clone_dataset = context.dataset.clone(clone_name="clone_dataset")


@behave.when(u'Dataset is cloning to existing dataset')
def step_impl(context):
    if not hasattr(context, 'filters'):
        context.filters = None
    context.clone_dataset = context.dataset.clone(dst_dataset_id=context.new_dataset.id, filters=context.filters)


@behave.then(u'Cloned dataset has "{item_count}" items')
def step_impl(context, item_count):
    pages = context.clone_dataset.items.list()
    assert pages.items_count == int(item_count), f"TEST FAILED: Expected {item_count}, Actual items in dataset {pages.items_count}"


@behave.when(u'Dataset is cloning with same name get already exist error')
def step_impl(context):
    try:
        context.clone_dataset = context.dataset.clone(clone_name=context.dataset.name)
    except context.dl.exceptions.FailedDependency as error:
        assert "Dataset with the same name already exists" in error.args[1], "TEST FAILED: Message not in error"
        assert context.dl.client_api.last_request.path_url.split('/')[-1] in error.args[
            1], "TEST FAILED: Command ID not in error"
        return
    assert False

@behave.then(u'command status is "{status}"')
def step_impl(context, status):
    assert context.command.status == status, f"TEST FAILED: Expected status {status}, Actual status {context.command.status}"

================================================
File: tests/features/steps/compute/compute.py
================================================
import datetime
import json
import os
import random
import string
import time
import dtlpy as dl
import behave
from behave import given, when, then


def random_5_chars():
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a'


@behave.given(u'I create a compute')
def step_impl(context):
    devops_output = {
        "authentication": {
            "ca": os.environ.get('COMPUTE_CLUSTER_CA'),
            "token": os.environ.get('COMPUTE_CLUSTER_TOKEN')
        },
        "config": {
            "endpoint": os.environ.get('COMPUTE_CLUSTER_ENDPOINT'),
            "kubernetesVersion": "1.29",
            "name": "rc-faas-eks",
            "nodePools": [
                {
                    "deploymentResources": {
                        "cpu": 2,
                        "gpu": 1,
                        "memory": 6
                    },
                    "dlTypes": [
                        "gpu-t4-s"
                    ],
                    "name": "gpu16-np",
                    "nodeSelector": {
                        "dl-type": "gpu-t4-snp"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "gpu-t4-snp"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "sku",
                            "operator": "Equal",
                            "value": "gpu"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 2,
                        "gpu": 1,
                        "memory": 6
                    },
                    "dlTypes": [
                        "gpu-t4-s"
                    ],
                    "name": "gpu16-p",
                    "nodeSelector": {
                        "dl-type": "gpu-t4-sp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "gpu-t4-sp"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "sku",
                            "operator": "Equal",
                            "value": "gpu"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 4,
                        "gpu": 1,
                        "memory": 16
                    },
                    "dlTypes": [
                        "gpu-t4-m"
                    ],
                    "name": "gpu32-np",
                    "nodeSelector": {
                        "dl-type": "gpu-t4-mnp"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "gpu-t4-mnp"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "sku",
                            "operator": "Equal",
                            "value": "gpu"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 4,
                        "gpu": 1,
                        "memory": 16
                    },
                    "dlTypes": [
                        "gpu-t4-m"
                    ],
                    "name": "gpu32-p",
                    "nodeSelector": {
                        "dl-type": "gpu-t4-mp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "gpu-t4-mp"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "sku",
                            "operator": "Equal",
                            "value": "gpu"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-l"
                    ],
                    "name": "highmem_l_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-l-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-l-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-l"
                    ],
                    "name": "highmem_lp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-l"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-l"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-m"
                    ],
                    "name": "highmem_m_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-m-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-m-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-m"
                    ],
                    "name": "highmem_mp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-mp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-mp"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-s"
                    ],
                    "name": "highmem_s_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-s-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-s-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-s"
                    ],
                    "name": "highmem_sp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-sp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-sp"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-xs"
                    ],
                    "name": "highmem_xs_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-xs-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-xs-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "highmem-xs"
                    ],
                    "name": "highmem_xsp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "highmem-xsp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "highmem-xsp"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 2,
                        "memory": 6
                    },
                    "dlTypes": None,
                    "name": "monitoring",
                    "nodeSelector": {
                        "component": "monitoring"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "component",
                            "operator": "Equal",
                            "value": "monitoring"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-l"
                    ],
                    "name": "regular_l_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-l-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-l-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-l"
                    ],
                    "name": "regular_lp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-lp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-lp"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-m"
                    ],
                    "name": "regular_m_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-m-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-m-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-m"
                    ],
                    "name": "regular_mp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-mp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-mp"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-s"
                    ],
                    "name": "regular_s_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-s-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-s-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-s"
                    ],
                    "name": "regular_sp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-sp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "true"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-sp"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-xs"
                    ],
                    "name": "regular_xs_np",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-xs-np"
                    },
                    "preemtible": False,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-xs-np"
                        }
                    ]
                },
                {
                    "deploymentResources": {
                        "cpu": 1,
                        "memory": 1
                    },
                    "dlTypes": [
                        "regular-xs"
                    ],
                    "name": "regular_xsp",
                    "nodeSelector": {
                        "dl-type": "no-gpu-regular",
                        "size": "regular-xsp"
                    },
                    "preemtible": True,
                    "tolerations": [
                        {
                            "effect": "NoSchedule",
                            "key": "node-pool",
                            "operator": "Equal",
                            "value": "dataloop"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "preemptible",
                            "operator": "Equal",
                            "value": "false"
                        },
                        {
                            "effect": "NoSchedule",
                            "key": "size",
                            "operator": "Equal",
                            "value": "regular-xsp"
                        }
                    ]
                }
            ],
            "provider": dl.ClusterProvider.AWS
        }
    }
    integration_name = 'cluster_integration_test_' + datetime.datetime.now().isoformat().split('.')[0].replace(':', '_')
    context.integration = context.project.integrations.create(
        integrations_type=dl.IntegrationType.KEY_VALUE,
        name=integration_name,
        options={
            'key': integration_name,
            'value': json.dumps(devops_output['authentication'])
        })
    context.feature.to_delete_integrations_ids.append(context.integration.id)
    context.feature.dataloop_feature_integration = context.integration
    compute_context = dl.ComputeContext([], context.project.org['id'], context.project.id)
    node_pools = [dl.NodePool.from_json(n) for n in devops_output['config']['nodePools']]

    cluster = dl.ComputeCluster(devops_output['config']['name'],
                                devops_output['config']['endpoint'],
                                devops_output['config']['kubernetesVersion'],
                                devops_output['config']['provider'],
                                node_pools,
                                {},
                                dl.Authentication(dl.AuthenticationIntegration(context.integration.id,
                                                                               context.integration.type)))
    context.compute = dl.computes.create(devops_output['config']['name'], compute_context, [], cluster, dl.ComputeType.KUBERNETES)
    context.feature.dataloop_feature_compute = context.compute


@behave.when(u'i set driver feaureflag')
def step_impl(context):
    settings=dl.settings.list(filters=dl.Filters(resource= dl.FiltersResource.SETTINGS, custom_filter={
        "name":"defaultFaaSDriverId", "scope.id": context.project.org['id'] })).items
    if not settings or len(settings) == 0:
        context.setting = dl.settings.create(
            setting=dl.Setting(
                value=context.compute.name,
                name='defaultFaaSDriverId',
                value_type=dl.SettingsValueTypes.STRING,
                scope=dl.SettingScope(
                    type=dl.PlatformEntityType.ORG,
                    id=context.project.org['id'],
                    role=dl.Role.ALL,
                    prevent_override=False,
                    visible=True,
                ),
                section_name=dl.SettingsSectionNames.APPLICATIONS
            )
        )
    else:
        context.setting = dl.settings.update(
            setting=dl.Setting(
                id=settings[0].id,
                value=context.compute.name,
                name='defaultFaaSDriverId',
                value_type=dl.SettingsValueTypes.STRING,
                scope=dl.SettingScope(
                    type=dl.PlatformEntityType.ORG,
                    id=context.project.org['id'],
                    role=dl.Role.ALL,
                    prevent_override=False,
                    visible=True,
                ),
                section_name=dl.SettingsSectionNames.APPLICATIONS
            )
        )


@given(u'There are 2 drivers')
def step_impl(context):
    context.default_driver = os.environ.get('DEFAULT_DRIVER', 'azure-plugins-rc')
    context.secondary_driver = os.environ.get('SECOND_DRIVER', 'plugins-rc')


@given(u'New Organization')
def step_impl(context):
    env = context.dl.client_api.environment[dl.client_api.environment.index('//')+2:context.dl.client_api.environment.index('-')]
    key = 'ORG_ID_' + env.upper()
    org_id = os.environ.get(key, '2e8370da-c559-433d-93a3-4e7922a9b524')
    context.org = context.dl.organizations.get(organization_id=org_id)
    # success, response = context.dl.client_api.gen_request(
    #     req_type='post',
    #     path='/orgs',
    #     json_req={'name': 'org_' + random_5_chars()}
    # )
    # org_id = response.json()['id']
    # context.org = context.dl.organizations.get(organization_id=org_id)


@given(u'Organization has a project')
def step_impl(context):
    success, response = context.dl.client_api.gen_request(
        req_type='patch',
        path='/projects/{}/org'.format(context.project.id),
        json_req={'org_id': context.org.id}
    )
    assert success, 'Failed to add project to organization'
    context.project = context.dl.projects.get(project_id=context.project.id)
    assert context.project.org['id'] == context.org.id, 'Project is not in the organization'


@given(u'Organization has no default driver')
def step_impl(context):
    try:
        setting_name = 'defaultFaaSDriverId'
        settings = context.dl.settings.resolve(org_id=context.org.id, user_email=context.dl.info()['user_email'])
        driver_settings = [s for s in settings if s.name == setting_name]
        for setting in driver_settings:
            setting.delete()
    except Exception as e:
        pass


@given(u'Services deployed in the project is not using the default driver')
def step_impl(context):
    def run(item):
        return item

    context.first_service = context.project.services.deploy(name='first-service-' + random_5_chars(), func=run)
    assert context.first_service.driver_id == context.default_driver, 'Service is not using the default driver'


@when(u'I set default driver')
def step_impl(context):
    context.dl.service_drivers.set_default(
        service_driver_id=context.secondary_driver,
        org_id=context.org.id,
        update_existing_services=True
    )


@then(u'Organization has a default driver user setting that uses secondary driver')
def step_impl(context):
    setting_name = 'defaultFaaSDriverId'
    settings = context.dl.settings.resolve(org_id=context.org.id, user_email=context.dl.info()['user_email'])
    setting = [s for s in settings if s.name == setting_name][0]
    assert setting.value == context.secondary_driver, 'Default driver is not set correctly'
    assert setting.scope.type == context.dl.PlatformEntityType.ORG, 'Default driver is not set correctly'
    assert setting.scope.id == context.org.id, 'Default driver is not set correctly'


@then(u'Services deployed in the project are using the secondary driver')
def step_impl(context):
    def run(item):
        return item

    context.second_service = context.project.services.deploy(service_name='second-service-' + random_5_chars(),
                                                             func=run)
    assert context.second_service.driver_id == context.secondary_driver, 'Service is not using the secondary driver'


@then(u'First service is also using the secondary driver')
def step_impl(context):
    timeout = time.time() + 60 * 2
    while time.time() < timeout:
        context.first_service = context.project.services.get(service_id=context.first_service.id)
        if context.first_service.driver_id == context.secondary_driver:
            break
        time.sleep(1)
    assert context.first_service.driver_id == context.secondary_driver, 'Service is not using the secondary driver'

================================================
File: tests/features/steps/compute/ecr_integrations.py
================================================
import os
import time
from behave import given, when, then
import random
import string

@given(u'There are no private registry integrations in the organization')
def step_impl(context):
    context.org = context.dl.organizations.get(organization_id=context.project.org['id'])
    integrations = context.org.integrations.list()
    for integration in integrations:
        if integration['type'] == context.dl.IntegrationType.PRIVATE_REGISTRY:
            integration = context.org.integrations.get(integrations_id=integration['id'])
            integration.delete(sure=True, really=True)


@given(u'A deployed service with custom docker image from "{type}" private registry')
def step_impl(context, type):
    image_name = ''
    if type == 'ECR':
        image_name = "636666312771.dkr.ecr.eu-west-1.amazonaws.com/aharon:latest"
    elif type == 'GAR':
        image_name = "europe-docker.pkg.dev/viewo-g/faas-test/zvi/from-ecr:latest"

    def run(item):
        return item

    context.service = context.project.services.deploy(
        name=f'{type}-service-' + ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a',
        func=run,
        runtime=context.dl.KubernetesRuntime(
            runner_image=image_name,
            autoscaler=None,
            num_replicas=1
        )
    )

@given(u'I execute the service')
def step_impl(context):
    context.execution = context.service.execute(execution_input={"item": context.item.id})

@then(u'I should get an ImagePullBackOff error')
def step_impl(context):
    start = time.time()
    timeout = 60 * 5
    success = False
    while time.time() - start < timeout:
        time.sleep(5)
        context.execution = context.project.executions.get(execution_id=context.execution.id)
        success = context.execution.latest_status['status'] == 'created'
        status = context.service.status()
        relevant_statuses = [s for s in status['runtimeStatus'] if s['createdAt'].split('.')[0] >= context.service.updated_at.split('.')[0]]
        has_image_pull_back_off = False
        for s in relevant_statuses:
            success = success and s['status'] == False
            has_image_pull_back_off = s['reason'] in ['ErrImagePull', 'ImagePullBackOff']

        success = success and has_image_pull_back_off
        if success:
            break

    assert success, 'ImagePullBackOff error was not received'

@when(u'I create an ECR integration')
def step_impl(context):
    context.org = context.dl.organizations.get(organization_id=context.project.org['id'])
    context.integration = context.org.integrations.create(
        integrations_type='private-registry',
        name='ecr-1',
        metadata={"provider": "aws"},
        options={
            "name": "AWS",
            "spec": {
                "accessKeyId": os.environ['ECR_ACCESS_KEY_ID'],
                "secretAccessKey": os.environ['ECR_SECRET_ACCESS_KEY'],
                "account": os.environ['ECR_ACCOUNT'],
                "region": "eu-west-1",
            }
        }
    )


@when(u'I create an GAR integration')
def step_impl(context):
    context.org = context.dl.organizations.get(organization_id=context.project.org['id'])
    options = context.org.integrations.generate_gar_options(service_account=os.environ['GAR_SERVICE_ACCOUNT_JSON'],
                                                            location=os.environ['GAR_LOCATION'])
    context.integration = context.org.integrations.create(
        integrations_type=context.dl.IntegrationType.PRIVATE_REGISTRY,
        name='gar-1',
        metadata={"provider": "gcp"},
        options=options
    )


@when(u'I pause and resume the service')
def step_impl(context):
    context.service.pause()
    context.service = context.project.services.get(service_id=context.service.id)
    context.service.resume()
    context.service = context.project.services.get(service_id=context.service.id)

@then(u'The execution should complete successfully')
def step_impl(context):
    start = time.time()
    timeout = 60 * 5
    success = False
    while time.time() - start < timeout:
        time.sleep(5)
        context.execution = context.project.executions.get(execution_id=context.execution.id)
        success = context.execution.latest_status['status'] == 'success'
        if success:
            break

    assert success, 'Execution did not complete successfully'

@when(u'I delete the context integration')
def step_impl(context):
    context.integration.delete(sure=True, really=True)

================================================
File: tests/features/steps/compute/test_compute_create.py
================================================
import behave


@behave.when(u'I try to create the compute from context.original_path')
def step_impl(context):
    try:
        org_id = context.project.org['id']
        context.compute = context.dl.computes.create_from_config_file(context.original_path, org_id)
        context.compute_id = context.compute.id
        context.to_delete_computes_ids.append(context.compute_id)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/compute/test_compute_delete.py
================================================
import behave


@behave.then(u'I able to delete compute')
def step_impl(context):
    context.compute.delete()
    context.to_delete_computes_ids.remove(context.compute_id)


================================================
File: tests/features/steps/compute/test_compute_get.py
================================================
import behave


@behave.when(u'I get compute from the compute list by the name')
@behave.when(u'I get compute from the compute list by the name with archived "{archived}"')
def step_impl(context, archived=None):
    json_req = {
        "filter": {"context": {"org": context.project.org['id']}, 'name': context.json_object['config']['name']}}
    if archived:
        json_req['filter']['archived'] = True
    success, response = context.dl.client_api.gen_request(
        req_type='post',
        path='/compute/query',
        json_req=json_req
    )

    if not success:
        assert False, f"TEST FAILED: {response.message}"

    assert response.json()[
               'totalItemsCount'] == 1, f"TEST FAILED: Expected 1 driver actual {response.json()['totalItemsCount']}"

    context.compute_id = response.json()['items'][0]['id']

    if not archived:
        context.compute = context.dl.computes.get(context.compute_id)

        if context.compute_id not in context.to_delete_computes_ids:
            context.to_delete_computes_ids.append(context.compute_id)


================================================
File: tests/features/steps/compute/test_compute_inteface.py
================================================
import behave
import os
import json
from .. import fixtures
import shutil
import base64
from datetime import datetime


@behave.given(u'I fetch the compute from "{file_name}"')
@behave.given(u'I fetch the compute from "{file_name}" file and update compute with params "{params_flag}"')
def step_impl(context, file_name, params_flag='None'):
    assert params_flag in ['None', 'True', 'False'], \
        f"params_flag should be 'None', 'True' or 'False' but got {params_flag}"

    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_name)
    with open(path, 'r') as file:
        context.json_object = json.load(file)

    # Need to set the original path and backup path in the context to restore the file later
    context.original_path = path
    context.backup_path = os.path.join(os.path.dirname(path), 'dataloop_backup.json')
    # Save the original dataloop.json file to restore it later
    if not os.path.exists(context.backup_path):
        shutil.copy(path, context.backup_path)

    context.json_object['config']['name'] = f"{context.json_object['config']['name']}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"
    if eval(params_flag):
        params = dict()
        for row in context.table:
            params[row['key']] = row['value'] if row['value'] != 'None' else None
        fixtures.update_nested_dict(context.json_object, params)

    updated_text = json.dumps(context.json_object)
    encoded_text = base64.b64encode(updated_text.encode('utf-8'))

    with open(path, 'w') as file:
        file.write(encoded_text.decode('utf-8'))



================================================
File: tests/features/steps/converter/converter.py
================================================
from behave import given, when, then
import os
import traceback
import shutil
import random
import json
import logging
import xml.etree.ElementTree as Et
from PIL import Image


########################
# to dataloop features #
########################
@given(u'There is a local "{from_format}" dataset in "{local_path}"')
def step_impl(context, from_format, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    content = os.listdir(local_path)
    if from_format != 'coco':
        assert 'labels' in content
    assert 'images' in content

    if from_format == 'yolo':
        assert 'd.names' in content
        context.local_labels_path = os.path.join(local_path, 'd.names')
        context.local_annotations_path = os.path.join(local_path, 'labels')
    elif from_format == 'coco':
        assert 'annotations.json' in content
        context.local_labels_path = None
        context.local_annotations_path = os.path.join(local_path, 'annotations.json')
    elif from_format == 'dataloop':
        pass
    else:
        assert 'classes' in content
        context.local_labels_path = os.path.join(local_path, 'classes')
        context.local_annotations_path = os.path.join(local_path, 'labels')

    context.local_dataset = local_path
    context.local_items_path = os.path.join(local_path, 'images')


@when(u'I convert local "{from_format}" dataset to "{to_format}"')
def step_impl(context, from_format, to_format):
    converter = context.dl.Converter()

    if to_format == 'dataloop':
        converter.upload_local_dataset(
            from_format=from_format,
            dataset=context.dataset,
            local_items_path=context.local_items_path,
            local_annotations_path=context.local_annotations_path,
            local_labels_path=context.local_labels_path
        )
    else:
        converter.convert_directory(
            from_format=from_format,
            to_format=to_format,
            local_path=context.local_dataset,
            dataset=context.dataset
        )


@when(u'I convert platform dataset to "{to_format}" in path "{local_path}"')
def step_impl(context, to_format, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)

    get_items_height_and_width(dataset=context.dataset, to_format=to_format)

    ann_filters = None
    if to_format in ['yolo', 'voc']:
        ann_filters = context.dl.Filters()
        ann_filters.resource = 'annotations'
        ann_filters.add(field='type', values='box')

    converter = context.dl.Converter()
    converter.convert_dataset(
        dataset=context.platform_dataset,
        to_format=to_format,
        annotation_filter=ann_filters,
        local_path=local_path
    )


@given(u'Local path in "{reverse_path}" is clean')
def step_impl(_, reverse_path):
    reverse_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], reverse_path)
    for item in os.listdir(reverse_path):
        if item != 'folder_keeper':
            path = os.path.join(reverse_path, item)
            if os.path.isfile(path):
                os.remove(path)
            elif os.path.isdir(path):
                shutil.rmtree(path)


@when(u'I reverse dataloop dataset to local "{to_format}" in "{reverse_path}"')
def step_impl(context, to_format, reverse_path):
    reverse_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], reverse_path)
    converter = context.dl.Converter()

    get_items_height_and_width(dataset=context.dataset, to_format=to_format)

    converter.convert_dataset(
        to_format=to_format,
        dataset=context.dataset,
        local_path=reverse_path
    )


@then(u'local "{annotation_format}" dataset in "{src_path}" is equal to reversed dataset in "{dest_path}"')
def step_impl(_, annotation_format, dest_path, src_path):
    dest_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], dest_path)
    src_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], src_path)

    if annotation_format == 'yolo':
        compare_yolo(src_path=src_path, dest_path=dest_path)
    elif annotation_format == 'coco':
        compare_coco(src_path=src_path, dest_path=dest_path)
    elif annotation_format == 'voc':
        compare_voc(src_path=src_path, dest_path=dest_path)
    elif annotation_format == 'dataloop':
        compare_dataloop(src_path=src_path, dest_path=dest_path)
    else:
        raise Exception('Unknown format: {}'.format(annotation_format))


#########################
# from dataloop feature #
#########################
@then(u'Converted "{annotation_format}" dataset in "{dest_path}" is equal to dataset in "{should_be_path}"')
def step_impl(_, annotation_format, dest_path, should_be_path):
    dest_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], dest_path)
    should_be_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], should_be_path)

    if annotation_format == 'yolo':
        compare_yolo(src_path=should_be_path, dest_path=dest_path)
    elif annotation_format == 'voc':
        compare_voc(src_path=should_be_path, dest_path=dest_path, reverse=False)
    elif annotation_format == 'coco':
        compare_coco(src_path=should_be_path, dest_path=dest_path, reverse=False)
    else:
        raise Exception('Unknown format: {}'.format(annotation_format))


@given(u'There is a platform dataloop dataset from "{local_path}"')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    dataset = context.platform_dataset = context.dataset = context.project.datasets.create(
        'platform_dataset_{}'.format(random.randrange(100, 100000))
        , index_driver=context.index_driver_var)
    dataset.items.upload(local_path=os.path.join(local_path, 'images'),
                         local_annotations_path=os.path.join(local_path, 'labels'))
    dataset.add_labels(['building', 'rock', 'eye', 'hat', ])


###########
# compare #
###########
def compare_yolo(src_path, dest_path):
    for root, dirs, files in os.walk(src_path):
        for file in files:
            src_file = os.path.join(root, file)
            dest_file = src_file.replace(src_path, dest_path).replace('labels', 'yolo')
            _, ext = os.path.splitext(file)
            try:
                if ext == '.txt':
                    assert os.path.isfile(dest_file)
                    assert compare_yolo_files(file_a=src_file, file_b=dest_file)
                elif ext == '.names':
                    dest_file = os.path.join(dest_path, [f for f in os.listdir(dest_path) if f.endswith('.names')][0])
                    with open(src_file, 'r') as fp:
                        src_labels = set([label.strip() for label in fp.readlines()])
                    with open(dest_file, 'r') as fp:
                        dest_labels = set([label.strip() for label in fp.readlines()])
                    assert len(dest_labels) == len(src_labels)
                    logging.error('Source Labels: {}\n Dest Labels: {}'.format(src_labels, dest_labels))
                    assert dest_labels == src_labels
                    for label in src_labels:
                        assert label in dest_labels
            except AssertionError:
                assert False, "yolo files don't match. file: {} \n {}".format(src_file, traceback.format_exc())
            except Exception:
                raise Exception("Error in comparing yolo files. file: {}".format(src_file))


def compare_coco(src_path, dest_path, reverse=True):
    if reverse:
        src_path = os.path.join(src_path, 'annotations.json')
        dest_path = os.path.join(dest_path, 'coco.json')

    with open(src_path, 'r') as f:
        src_coco = json.load(f)
    with open(dest_path, 'r') as f:
        dest_coco = json.load(f)

    assert len(src_coco['images']) == len(dest_coco['images'])
    assert len(src_coco['annotations']) == len(dest_coco['annotations'])

    if reverse:
        dest_categoties_count = 0
        for category in dest_coco['categories']:
            if 'supercategory' in category:
                dest_categoties_count += 1

        assert len(src_coco['categories']) == dest_categoties_count
    else:
        assert len(src_coco['categories']) == len(dest_coco['categories'])


def compare_voc(src_path, dest_path, reverse=True):
    if reverse:
        src_path = os.path.join(src_path, 'labels')
        dest_path = os.path.join(dest_path, 'voc')

    src_content = os.listdir(src_path)
    dest_content = os.listdir(dest_path)

    assert len(src_content) == len(dest_content), f"TEST FAILED: {len(src_content)} == {len(dest_content)}"

    for f in src_content:
        src_file = os.path.join(src_path, f)
        try:
            assert f in dest_content
            assert compare_voc_files(src_file=src_file, dest_file=os.path.join(dest_path, f))
        except:
            assert False, "voc files don't match. file: {} \n {}".format(src_file, traceback.format_exc())


def compare_dataloop(src_path, dest_path):
    src_path = os.path.join(src_path, 'labels')
    dest_path = os.path.join(dest_path, 'yolo')

    src_content = os.listdir(src_path)
    dest_content = os.listdir(dest_path)

    for f in src_content:
        src_file = os.path.join(src_path, f)
        _, ext = os.path.splitext(f)
        try:
            if ext == '.txt':
                assert f in dest_content
                assert compare_yolo_files(file_a=src_file, file_b=os.path.join(dest_path, f))
            elif ext == '.names':
                with open(src_path, 'r') as fp:
                    src_labels = [label.strip() for label in fp.readlines()]
                with open(os.path.join(dest_path, fp), 'r') as f:
                    dest_labels = [label.strip() for label in fp.readlines()]
                assert len(dest_labels) == len(src_labels)
                for label in src_labels:
                    assert label in dest_labels
        except AssertionError:
            assert False, "yolo files don't match. file: {} \n {}".format(src_file, traceback.format_exc())
        except Exception:
            raise Exception("Error in comparing yolo files. file: {}".format(src_file))


#########
# utils #
#########
def compare_attributes(attributes, obj):
    success = True
    for attr_key, attr_val in attributes.items():
        suc = False
        for elem in obj.iter():
            suc = elem.tag == attr_key and elem.text == str(attr_val)
            suc = suc or elem.text == attr_key
            if suc:
                break
        success = success and suc

    return success


def compare_xml_object(obj_a, obj_b):
    success = True

    try:
        for e in list(obj_a):
            if e.tag == 'bndbox':
                success = success and compare_xml_object(obj_a=e, obj_b=obj_b.find('bndbox'))
            elif e.tag == 'attributes':
                if e.text:
                    attributes = json.loads(e.text.replace("'", '"'))
                    success = success and compare_attributes(attributes=attributes, obj=obj_b)
            else:
                success = success and e.text == obj_b.find(e.tag).text
    except Exception as e:
        success = False

    return success


def compare_objects(src_objects, dest_objects):
    equal = True
    for src_obj in src_objects:
        found_match = False
        for des_obj in dest_objects:
            if compare_xml_object(obj_a=des_obj, obj_b=src_obj):
                found_match = True
                break
        equal = equal and found_match

    return equal


def compare_voc_files(src_file, dest_file):
    src_elem = Et.parse(src_file)
    dest_elem = Et.parse(dest_file)

    src_objects = src_elem.findall('object')
    dest_objects = dest_elem.findall('object')

    success = compare_objects(src_objects=src_objects, dest_objects=dest_objects)

    src_size = src_elem.find('size')
    dest_size = dest_elem.find('size')

    success = success and compare_xml_object(obj_a=src_size, obj_b=dest_size)
    success = success and src_elem.find('filename').text == dest_elem.find('filename').text

    return success


def includes_yolo_line(line, lines):
    includes = False
    split_line = [format(float(val), '.2f') for val in line.split()]

    for l in lines:
        split_l = [format(float(val), '.2f') for val in l.split()]
        line_equal = len(split_l) == len(split_line)
        line_equal = line_equal and \
                     split_l[0] == split_line[0] and \
                     split_l[1] == split_line[1] and \
                     split_l[2] == split_line[2] and \
                     split_l[3] == split_line[3] and \
                     split_l[4] == split_line[4]
        if line_equal:
            includes = line_equal
            break

    return includes


def compare_yolo_files(file_a, file_b):
    f_src = open(file_a, 'r')
    f_dest = open(file_b, 'r')
    src_lines = [line.strip() for line in f_src.readlines()]
    dest_lines = [line.strip() for line in f_dest.readlines()]

    success = len(dest_lines) == len(src_lines)

    if success:
        for line in src_lines:
            success = success and includes_yolo_line(line=line, lines=dest_lines)

    if not success:
        print('Error in files: {}, {}'.format(file_a, file_b))
        print('{}:\n {}'.format(file_a, src_lines))
        print('{}:\n {}'.format(file_b, dest_lines))

    return success


def get_items_height_and_width(dataset, to_format):

    items_shapes = {
        "picture3 copy.jpg": {
            "width": 1200,
            "height": 800},
        "lena copy 2.png": {
            "width": 512,
            "height": 512
        },
        "picture2 copy 11.jpg": {
            "width": 648,
            "height": 365
        }
    }

    for page in dataset.items.list():
        for item in page:
            if item.width is None or item.height is None:
                if to_format == 'coco':
                    try:
                        height = item.metadata['user']['height']
                        width = item.metadata['user']['width']
                    except Exception:
                        height = items_shapes[item.name]['height']
                        width = items_shapes[item.name]['width']

                    item.metadata['system']['height'] = height
                    item.metadata['system']['width'] = width
                elif to_format in ['voc', 'yolo']:
                    im = Image.open(item.download(save_locally=False))
                    item.metadata['system']['height'] = im.size[1]
                    item.metadata['system']['width'] = im.size[0]
                item.update(system_metadata=True)


@then(u'The converter do not overwrite the existing label')
def step_impl(context):
    dataset = context.dl.datasets.get(dataset_id=context.dataset.id)
    for label in dataset.labels:
        if label.tag == context.nested_labels[0]['label_name']:
            assert label.color == context.nested_labels[0]['color'], 'The converter overwrite the existing label'


================================================
File: tests/features/steps/converter/conveters_interface.py
================================================
import behave
import os
import json
import filecmp
import os.path
import xmltodict


@behave.given('I use "{converter_name}" converter to upload items with annotations to the dataset using the given params')
def step_impl(context, converter_name):
    context.items_path = None
    context.labels_path = None
    context.annotations_path = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "local_items_path":
            context.items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "local_labels_path":
            context.labels_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "local_annotations_path":
            context.annotations_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

    converter = context.dl.Converter()
    converters_list = {
        "COCO": context.dl.AnnotationFormat.COCO,
        "YOLO": context.dl.AnnotationFormat.YOLO,
        "VOC": context.dl.AnnotationFormat.VOC,
    }

    converter.upload_local_dataset(
        from_format=converters_list[converter_name],
        dataset=context.dataset,
        local_items_path=context.items_path,
        local_labels_path=context.labels_path,
        local_annotations_path=context.annotations_path
    )


@behave.given('I download the dataset items annotations in "{converter_name}" format using the given params')
def step_impl(context, converter_name):
    context.local_path = None

    converter = context.dl.Converter()
    converters_list = {
        "COCO": context.dl.AnnotationFormat.COCO,
        "YOLO": context.dl.AnnotationFormat.YOLO,
        "VOC": context.dl.AnnotationFormat.VOC,
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "local_path":
            context.local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

    context.downloaded_annotations = converter.convert_dataset(
        dataset=context.dataset,
        to_format=converters_list[converter_name],
        local_path=context.local_path
    )


@behave.then('I use "{converter_name}" format to compare the uploaded annotations with the downloaded annotations')
def step_impl(context, converter_name):
    converters_list = {
        "COCO": coco_annotations_compare,
        "YOLO": yolo_annotations_compare,
        "VOC": voc_annotations_compare,
    }

    converters_list[converter_name](context)


def coco_annotations_compare(context):
    with open(context.annotations_path, 'r') as annotations_file:
        context.uploaded_annotations = json.load(annotations_file)

    # Sorting images lists
    for uploaded_image in context.uploaded_annotations['images']:
        uploaded_image.pop('id')

    for downloaded_image in context.downloaded_annotations['images']:
        downloaded_image.pop('id')

    context.uploaded_annotations['images'] = sorted(context.uploaded_annotations['images'],
                                                    key=lambda img: img['file_name'])

    context.downloaded_annotations['images'] = sorted(context.downloaded_annotations['images'],
                                                      key=lambda img: img['file_name'])

    # Removing info field
    context.uploaded_annotations.pop('info')
    context.downloaded_annotations.pop('info')

    # Sorting annotations lists
    for uploaded_annotation in context.uploaded_annotations['annotations']:
        uploaded_annotation.pop('id')
        uploaded_annotation.pop('image_id')

    for downloaded_annotation in context.downloaded_annotations['annotations']:
        downloaded_annotation.pop('id')
        downloaded_annotation.pop('image_id')

    context.uploaded_annotations['annotations'] = sorted(context.uploaded_annotations['annotations'],
                                                         key=lambda img: img['area'])

    context.downloaded_annotations['annotations'] = sorted(context.downloaded_annotations['annotations'],
                                                           key=lambda img: img['area'])

    # Comparing the sorted files
    assert context.downloaded_annotations == context.downloaded_annotations


def yolo_annotations_compare(context):
    # Comparing labels
    with open(context.labels_path, 'r') as uploaded_labels_file:
        uploaded_labels = uploaded_labels_file.readlines()
        downloaded_labels_path = os.path.join(context.local_path, context.dataset.name+".names")

        with open(downloaded_labels_path, 'r') as downloaded_labels_file:
            downloaded_labels = downloaded_labels_file.readlines()
            assert uploaded_labels == downloaded_labels

    # Comparing annotations
    downloaded_annotations_path = os.path.join(context.local_path, "yolo")

    # Function to compare txt files of type yolo
    def compare_directory_files_yolo(dir1, dir2):
        dirs_cmp = filecmp.dircmp(dir1, dir2)
        # Compare the directories structure
        if len(dirs_cmp.left_only) > 0 or len(dirs_cmp.right_only) > 0 or len(dirs_cmp.funny_files) > 0:
            return False

        # Compare files data
        for item in os.listdir(dir1):
            if item.endswith('.txt'):
                file1_path = os.path.join(dir1, item)
                file2_path = os.path.join(dir2, item)

                with open(file1_path, 'r') as annotations_file1:
                    file1 = annotations_file1.readlines()

                with open(file2_path, 'r') as annotations_file2:
                    file2 = annotations_file2.readlines()

                file1.sort()
                file2.sort()

                file1_split_values = []
                file2_split_values = []

                # Floor the data for accuracy of 10 digits after the floating point
                for i in range(len(file1)):
                    file1_split_values.append(['{:.10f}'.format(float(i)) for i in file1[i].split()])
                    file2_split_values.append(['{:.10f}'.format(float(i)) for i in file2[i].split()])

                # Comparing the sorted files
                assert file1_split_values == file2_split_values, "mismatch data on file: " + item

        for common_dir in dirs_cmp.common_dirs:
            new_dir1 = os.path.join(dir1, common_dir)
            new_dir2 = os.path.join(dir2, common_dir)
            if not compare_directory_files_yolo(new_dir1, new_dir2):
                return False
        return True

    assert compare_directory_files_yolo(context.annotations_path, downloaded_annotations_path)


def voc_annotations_compare(context):
    downloaded_annotations_path = os.path.join(context.local_path, "voc")

    # Function to compare xml files of type voc
    def compare_directory_files_voc(dir1, dir2):
        # Compare the directories structure
        dirs_cmp = filecmp.dircmp(dir1, dir2)
        if len(dirs_cmp.left_only) > 0 or len(dirs_cmp.right_only) > 0 or len(dirs_cmp.funny_files) > 0:
            return False

        # Compare files data
        for item in os.listdir(dir1):
            if item.endswith('.xml'):
                file1_path = os.path.join(dir1, item)
                file2_path = os.path.join(dir2, item)

                with open(file1_path, 'r') as annotations_file1:
                    file1_json = xmltodict.parse(annotations_file1.read())

                with open(file2_path, 'r') as annotations_file2:
                    file2_json = xmltodict.parse(annotations_file2.read())

                # Sorting annotations list
                if isinstance(file1_json['annotation']['object'], list) and isinstance(file2_json['annotation']['object'], list):
                    file1_json['annotation']['object'] = sorted(
                        file1_json['annotation']['object'],
                        key=lambda obj: (obj['name'],
                                         obj['bndbox']['xmin'], obj['bndbox']['ymin'], obj['bndbox']['xmax'], obj['bndbox']['ymax'])
                    )

                    file2_json['annotation']['object'] = sorted(
                        file2_json['annotation']['object'],
                        key=lambda obj: (obj['name'],
                                         obj['bndbox']['xmin'], obj['bndbox']['ymin'], obj['bndbox']['xmax'], obj['bndbox']['ymax'])
                    )

                    for obj1_attr in file1_json['annotation']['object']:
                        obj1_attr.pop('attributes')

                    for obj2_attr in file2_json['annotation']['object']:
                        obj2_attr.pop('attributes')

                else:
                    file1_json['annotation']['object'].pop('attributes')
                    file2_json['annotation']['object'].pop('attributes')

                # Comparing the sorted files
                assert file1_json == file2_json, "mismatch data on file:" + item

        # Dive to deeper directories
        for common_dir in dirs_cmp.common_dirs:
            new_dir1 = os.path.join(dir1, common_dir)
            new_dir2 = os.path.join(dir2, common_dir)
            if not compare_directory_files_voc(new_dir1, new_dir2):
                return False
        return True

    assert compare_directory_files_voc(context.annotations_path, downloaded_annotations_path)


================================================
File: tests/features/steps/dataset_entity/dataset_entity_interface.py
================================================
import behave
import os
import json


@behave.when(u'I call dataset.download_annotations() using the given params')
def step_impl(context):
    context.local_path = None
    # context.filters - Removed to prevent overwrite
    context.annotation_options = None
    context.annotation_filters = None
    context.overwrite = False
    context.thickness = 1
    context.with_text = False
    context.remote_path = None
    context.include_annotations_in_output = True
    context.export_png_files = False
    context.filter_output_annotations = False
    context.alpha = None
    context.export_version = context.dl.ExportVersion.V1

    annotation_options_list = {
        "JSON": context.dl.VIEW_ANNOTATION_OPTIONS_JSON,
        "MASK": context.dl.VIEW_ANNOTATION_OPTIONS_MASK,
        "INSTANCE": context.dl.VIEW_ANNOTATION_OPTIONS_INSTANCE,
        "ANNOTATION_ON_IMAGE": context.dl.VIEW_ANNOTATION_OPTIONS_ANNOTATION_ON_IMAGE,
        "VTT": context.dl.VIEW_ANNOTATION_OPTIONS_VTT,
        "OBJECT_ID": context.dl.VIEW_ANNOTATION_OPTIONS_OBJECT_ID
    }

    export_versions_list = {
        "V1": context.dl.ExportVersion.V1,
        "V2": context.dl.ExportVersion.V2
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "local_path":
            context.local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "filters":
            context.filters = context.dl.Filters(custom_filter=json.loads(parameter.cells[1]))

        if parameter.cells[0] == "annotation_options":
            context.annotation_options = annotation_options_list[parameter.cells[1]]

        if parameter.cells[0] == "annotation_filters":
            context.annotation_filters = context.dl.Filters(custom_filter=json.loads(parameter.cells[1]))

        if parameter.cells[0] == "overwrite":
            context.overwrite = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "thickness":
            context.thickness = float(parameter.cells[1])

        if parameter.cells[0] == "with_text":
            context.with_text = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "remote_path":
            context.remote_path = parameter.cells[1]

        if parameter.cells[0] == "include_annotations_in_output":
            context.include_annotations_in_output = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "export_png_files":
            context.export_png_files = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "filter_output_annotations":
            context.filter_output_annotations = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "alpha":
            context.save_locally = float(parameter.cells[1])

        if parameter.cells[0] == "export_version":
            context.save_locally = export_versions_list[parameter.cells[1]]

    context.dataset.download_annotations(
        local_path=context.local_path,
        filters=context.filters,
        annotation_options=context.annotation_options,
        annotation_filters=context.annotation_filters,
        overwrite=context.overwrite,
        thickness=context.thickness,
        with_text=context.with_text,
        remote_path=context.remote_path,
        include_annotations_in_output=context.include_annotations_in_output,
        export_png_files=context.export_png_files,
        filter_output_annotations=context.filter_output_annotations,
        alpha=context.alpha,
        export_version=context.export_version
    )


================================================
File: tests/features/steps/dataset_entity/test_add_labels_methods.py
================================================
import behave
import dtlpy as dl


def compare_labels(input_labels, ontology_labels):
    for input_label in input_labels:
        label_exist = False
        nested_label = input_label['label_name'].split(".", 1)
        for ontology_label in ontology_labels:
            if nested_label[0] == ontology_label['tag']:
                label_exist = True
                if len(nested_label) == 1:
                    if 'color' in input_label:
                        assert input_label['color'] == ontology_label['color']
                    if 'display_label' in input_label:
                        assert input_label['display_label'] == ontology_label['display_label']
                    if 'attributes' in input_label:
                        assert input_label['attributes'] == ontology_label['attributes']
                    if 'children' in input_label:
                        if input_label['children']:
                            if ontology_label['children']:
                                compare_labels(input_label['children'], ontology_label['children'])
                                break
                    break
                if ontology_label['children']:
                    input_label['label_name'] = nested_label[1]
                    compare_labels([input_label], ontology_label['children'])
    return label_exist


@behave.given(u'There is no label with the same label I plan to add')
def step_impl(context):
    context.dataset.delete_labels(label_names=['aaa', 'bbb'])


@behave.when(u'I add new single label with all parameters')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'attributes': ['Name', 'Age'],
         'display_label': 'display aaa'}]

    context.dataset.add_labels(label_list=context.nested_labels)


@behave.when(u'I add single root Label')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'attributes': ['Name', 'Age'],
         'display_label': 'display aaa'}]

    context.dataset.add_labels(label_list=context.nested_labels)


@behave.when(u'I add single root Label "{name}"')
def step_impl(context, name):
    context.nested_labels = [
        {'label_name': name,
         'color': '#220605'}]

    context.dataset.add_labels(label_list=context.nested_labels)


@behave.then(u'I add single root Label')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'attributes': ['Name', 'Age'],
         'display_label': 'display aaa'}]
    try:
        context.dataset.add_labels(label_list=context.nested_labels)
    except dl.exceptions.InternalServerError:
        assert False
    assert True


@behave.then(u'Label has been added')
def step_impl(context):
    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    labels_json_from_ontology = ontology.to_json()['labels']

    assert compare_labels(context.nested_labels, labels_json_from_ontology)


@behave.when(u'I add single root Label with Label name only')
def step_impl(context):
    context.nested_labels = [{'label_name': 'aaa'}]

    context.dataset.add_labels(label_list=context.nested_labels)


@behave.when(u'I add single nested root Label with all parameters')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa.bbb',
         'color': '#220605',
         'attributes': ['Name', 'Age'],
         'display_label': 'display aaa'}]

    context.dataset.add_labels(label_list=context.nested_labels)


@behave.when(u'I add new single label with all parameters with no update_ontology parameter')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'attributes': ['Name', 'Age'],
         'display_label': 'display aaa'}]

    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    ontology.add_labels(label_list=context.nested_labels)
    ontology.update(system_metadata=True)


@behave.when(u'I add single nested Label with Label name only')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa.bbb'
         }]

    context.dataset.add_labels(label_list=context.nested_labels)


@behave.when(u'I add labels of string type')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'X.Y.Z',
         'children': [{'label_name': 'ab'},
                      {'label_name': 'bb'}]}]

    context.dataset.add_labels(label_list=["X.Y.Z.ab", "X.Y.Z.bb"])


@behave.when(u'I add labels of string type using ontology.add_labels when update_ontology is false')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'X.Y.Z',
         'children': [{'label_name': 'ab'},
                      {'label_name': 'bb'}]}]

    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    ontology.add_labels(label_list=["X", "Y"])
    ontology.update(system_metadata=True)


@behave.when(u'I add single nested label using ontology.add_label when update_ontology is true')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'X.Y.Z'}]

    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    ontology.add_label(label_name="X.Y.Z", update_ontology=True)
    ontology.update(system_metadata=True)


@behave.when(u'I add single not nested label using ontology.add_label when update_ontology is false')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'X'}]

    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    ontology.add_label(label_name="X", update_ontology=False)
    ontology.update(system_metadata=True)


@behave.then(u'I add single nested label using ontology.add_label when update_ontology is false')
def step_impl(context):
    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    try:
        ontology.add_label(label_name="X.Y.Z", update_ontology=False)
    except dl.exceptions.BadRequest:
        # to verify that Label can't be added twice
        # dtlpy.exceptions.InternalServerError: ('500', 'There is already a label with identifier "aaa"')
        return
    assert False


@behave.when(u'I add labels of ontology.label type')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'a',
         'color': '#227301',
         'children': [{'label_name': 'aa',
                       'color': '#227302',
                       'children': [{'label_name': 'aaa',
                                     'color': '#227303',
                                     },
                                    {'label_name': 'aab',
                                     'color': '#227304'}]},
                      {'label_name': 'ab',
                       'color': '#227305'}]},
        {'label_name': 'b',
         'color': '#227306',
         'children': [{'label_name': 'ba',
                       'color': '#227307'},
                      {'label_name': 'bb',
                       'color': '#227308'}]}]

    recipe = context.dataset.recipes.list()[0]
    ontology = recipe.ontologies.list()[0]

    labels = ontology.add_labels(label_list=context.nested_labels)

    ontology.labels = context.dataset.add_labels(label_list=labels)


@behave.when(u'I add many labels')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'children': [{'label_name': 'XY',
                       'color': '#227305',
                       'children': [{'label_name': 'aaa',
                                     'color': '#224705',
                                     'display_label': 'display aaa',
                                     'attributes': ['Name', 'Age'],
                                     },
                                    {'label_name': 'aab',
                                     'color': '#842367'}]},
                      {'label_name': 'XZ'}]},
        {'label_name': 'bbb',
         'color': '#287605',
         'children': [{'label_name': 'ba',
                       'color': '#298345'},
                      {'label_name': 'bb',
                       'color': '#298565'}]}]
    context.dataset.add_labels(label_list=context.nested_labels)


@behave.then(u'I update many labels')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'children': [{'label_name': 'XY',
                       'color': '#227305',
                       'children': [{'label_name': 'aaa',
                                     'color': '#224705',
                                     'display_label': 'display AAA',
                                     'attributes': ['Name1', 'Age1'],
                                     },
                                    {'label_name': 'aab',
                                     'color': '#842367'}]},
                      {'label_name': 'XZ'}]},
        {'label_name': 'bbb',
         'color': '#287605',
         'children': [{'label_name': 'ba',
                       'color': '#298345'},
                      {'label_name': 'bb',
                       'color': '#298565'}]}]
    context.dataset.update_labels(label_list=context.nested_labels)


@behave.then(u'I upsert many labels')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'aaa',
         'color': '#220605',
         'children': [{'label_name': 'XY',
                       'color': '#227305',
                       'children': [{'label_name': 'aaa',
                                     'color': '#224705',
                                     'display_label': 'display AAA',
                                     'attributes': ['Name1', 'Age2'],
                                     },
                                    {'label_name': 'aabcd',
                                     'color': '#842367'}]},
                      {'label_name': 'XZ'}]},
        {'label_name': 'bbb',
         'color': '#287605',
         'children': [{'label_name': 'ba',
                       'color': '#298345'},
                      {'label_name': 'bb',
                       'color': '#298565'}]}]
    context.dataset.update_labels(label_list=context.nested_labels, upsert=True)


@behave.when(u'I add many nested labels')
def step_impl(context):
    context.nested_labels = [
        {'label_name': 'X.Y.a',
         'color': '#220605',
         'children': [{'label_name': 'T.R.X',
                       'color': '#227305',
                       'children': [{'label_name': 'aaa',
                                     'color': '#224705',
                                     'display_label': 'display aaa',
                                     'attributes': ['Name', 'Age'],
                                     },
                                    {'label_name': 'aab',
                                     'color': '#842367'}]},
                      {'label_name': 'ab'}]},
        {'label_name': 'M.I.C',
         'color': '#287605',
         'children': [{'label_name': 'ba',
                       'color': '#298345'},
                      {'label_name': 'bb',
                       'color': '#298651'}]}
    ]
    context.dataset.add_labels(label_list=context.nested_labels)


================================================
File: tests/features/steps/dataset_entity/test_dataset_repo_methods.py
================================================
import shutil
import random
import string

import behave
import os


@behave.when(u'I download dataset entity annotations to assets')
def step_impl(context):
    local_path = os.environ['DATALOOP_TEST_ASSETS']
    if os.path.isdir(os.path.join(local_path, 'json')):
        shutil.rmtree(os.path.join(local_path, 'json'))
    context.dataset.download_annotations(local_path=local_path, overwrite=True)


@behave.when(u'I download dataset entity to "{local_path}"')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    context.dataset.download(filters=None,
                             local_path=local_path,
                             annotation_options=['mask', 'instance', 'json'],
                             thickness=3)


@behave.when(u'I delete a dataset entity')
def step_impl(context):
    context.dataset.delete(sure=True,
                           really=True)


@behave.when(u'I create a new recipe to dataset entity')
def step_impl(context):
    context.recipe = context.dataset.recipes.create()

@behave.when(u'I update dataset name to new random name')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    dataset_name = 'random_dataset_{}'.format(rand_str)
    context.dataset.name = dataset_name
    context.dataset.update(True)
    assert context.dl.datasets.get(dataset_id=context.dataset.id).name == dataset_name, 'Failed to update dataset name'


@behave.when(u'I upload to dataset entity a file in path "{item_local_path}"')
def step_impl(context, item_local_path):
    item_local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_local_path)
    context.item = context.dataset.upload_item(
        filepath=item_local_path,
        remote_path=None,
        uploaded_filename=None,
        callback=None
    )


================================================
File: tests/features/steps/dataset_entity/test_directory_tree.py
================================================
import behave
import time
import dtlpy as dl


@behave.given(u'I create a dataset named "{dataset_name}"')
def step_impl(context, dataset_name):
    assert isinstance(dataset_name, str)
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)


@behave.then(u'I get an item thumbnail response')
@behave.when(u'I get an item thumbnail response')
def step_impl(context):
    success, response = dl.client_api.gen_request(req_type='get',
                                                  path=f'/items/{context.item.id}/thumbnail')
    if not success:
        raise dl.exceptions.PlatformException(response)


@behave.then(u'dataset.directory_tree.dir_names contains "{directory_names}"')
def step_impl(context, directory_names):
    expected_names = directory_names.split(',')
    context.dataset = context.project.datasets.get(dataset_name=context.dataset.name)

    num_try = 1
    interval = 1
    finished = False

    for i in range(num_try):
        if all(name in context.dataset.directory_tree.dir_names for name in expected_names):
            finished = True
            break

        time.sleep(interval)
        context.dl.logger.warning(
            f"Step is running for {(i + 1) * 10:.2f}[s] and now Going to sleep {10:.2f}[s]")
        context.dataset = context.project.datasets.get(dataset_name=context.dataset.name)

    assert finished, f"Expected dataset to contain directories {expected_names}, Actual: {context.dataset.directory_tree.dir_names}"

================================================
File: tests/features/steps/datasets_repo/datasets_interface.py
================================================
import behave
import time
import random


@behave.given('I create a dataset by the name of "{dataset_name}" in the project')
def step_impl(context, dataset_name):
    if hasattr(context.feature, 'dataloop_feature_dataset'):
        context.dataset = context.feature.dataloop_feature_dataset
    else:
        num = random.randint(10000, 100000)
        dataset_name = 'to-delete-test-{}_{}'.format(str(num), dataset_name)
        context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)
        context.feature.dataloop_feature_dataset = context.dataset
        time.sleep(5)

    context.dataset_name = dataset_name


@behave.when('I call datasets.clone using dataset.id')
def step_imp(context):
    context.clone_dataset = context.project.datasets.clone(dataset_id=context.dataset.id, dst_dataset_id=context.new_dataset.id)


================================================
File: tests/features/steps/datasets_repo/test_dataset_context.py
================================================
import behave
import time
import random


@behave.given(u'I create projects by the name of "{projects_name}"')
def step_impl(context, projects_name):
    projects_name = list(projects_name.split(' '))
    context.projects = list()
    context.projects_name = list()

    for project_name in projects_name:
        if project_name != '':
            num = random.randint(10000, 100000)
            project_name = 'to-delete-test-{}_{}'.format(str(num), project_name)
            context.projects_name.append(project_name)
            context.projects.append(context.dl.projects.create(project_name=project_name))

    time.sleep(5)  # to sleep because authorization takes time
    context.dataset_count = 0

    if 'bot.create' in context.feature.tags:
        bot_name = 'test_bot_{}'.format(random.randrange(1000, 10000))
        context.bot = context.projects[0].bots.create(name=bot_name)
        context.feature.bot = context.bot
        context.bot_user = context.bot.email
        context.feature.bot_user = context.bot_user


@behave.given(u'I create datasets by the name of "{datasets_name}"')
def step_impl(context, datasets_name):
    datasets_name = list(datasets_name.split(" "))
    context.datasets = list()
    context.datasets_name = list()
    i = 0
    for dataset_name in datasets_name:
        if datasets_name != '':
            context.datasets_name.append(dataset_name)
            context.datasets.append(context.projects[i].datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var))
            i += 1


@behave.when(u'I checkout project number {project_index}')
def step_impl(context, project_index):
    context.projects[int(project_index) - 1].checkout()


@behave.when(u'I get a dataset number {dataset_index} from checkout project')
def step_impl(context, dataset_index):
    context.dataset = \
        context.dl.datasets.get(dataset_id=context.datasets[int(dataset_index)-1].id)


@behave.when(u'I get a dataset number {dataset_index} from project number {project_index}')
def step_impl(context, dataset_index, project_index):
    context.dataset = \
        context.projects[int(project_index)-1].datasets.get(dataset_id=context.datasets[int(dataset_index)-1].id)


@behave.then(u'dataset Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.dataset.project_id == context.projects[int(project_index)-1].id


@behave.then(u'dataset Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.dataset.project.id == context.projects[int(project_index)-1].id


================================================
File: tests/features/steps/datasets_repo/test_dataset_upload_annotations.py
================================================
import time
import behave
import os
import json


@behave.then(u'I upload annotations to dataset')
def step_impl(context):
    json_file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "upload_dataset_annotations")
    context.dataset.upload_annotations(local_path=json_file_path, clean=True)


@behave.then(u'Annotations in item equal to the annotations uploaded')
def step_impl(context):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "upload_dataset_annotations/0000000162.json")
    with open(file_path) as json_file:
        annotations_uploaded = json.load(json_file)
    annotations_uploaded = annotations_uploaded['annotations']
    item_annotations = context.item.annotations.list().annotations
    assert len(annotations_uploaded) == len(item_annotations)
    for annotation in item_annotations:
        annotation_json = annotation.to_json()
        ann = {
               "metadata": {
                   "system": {
                       "attributes": annotation.attributes
                   }
               },
               'coordinates': annotation_json['coordinates'],
               'label': annotation_json['label'],
               'type': annotation_json['type']}
        assert ann in annotations_uploaded


@behave.when(u'Item in path "{item_path}" is uploaded to "Dataset" in remote path "{remote_path}"')
def step_impl(context, item_path, remote_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.dataset.items.upload(local_path=item_path, remote_path=remote_path)

    # wait for platform attributes
    limit = 10 * 30
    stat = time.time()
    while True:
        time.sleep(3)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if "video" in context.item.mimetype:
            if context.item.fps is not None:
                break
        elif context.item.mimetype is not None:
            break
        if time.time() - stat > limit:
            raise TimeoutError("Timeout while waiting for platform attributes")

    context.item = context.dataset.items.get(item_id=context.item.id)
    if context.item.name.endswith('.mp4') and context.item.fps is None:
        context.item.fps = 25
    if context.item.name.endswith('.jpg') or context.item.name.endswith('.png'):
        if context.item.height is None:
            context.item.height = 768
        if context.item.width is None:
            context.item.width = 1536


@behave.then(u'I upload annotations to dataset in new end point "{remote_root_path}"')
def step_impl(context, remote_root_path):
    json_file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "upload_dataset_annotations")
    context.dataset.upload_annotations(local_path=json_file_path, clean=True, remote_root_path=remote_root_path)


================================================
File: tests/features/steps/datasets_repo/test_dataset_upload_csv.py
================================================
import time
import behave
import os
import json
import csv


@behave.given(u'I upload csv "{csv_file}" to dataset')
def step_impl(context, csv_file):
    csv_file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], csv_file)
    context.item = context.dataset.items.upload(local_path=csv_file_path)

    with open(csv_file_path, "r") as file:
        line_count = len(file.readlines())

    # Wait for the upload and processing to process all lines
    # CSV file has extra line for the header but also dataset has extra item for the CSV file
    # Wait for processing to complete with a 5-minute timeout
    timeout = 60
    while (timeout != 0) and (len(context.dataset.items.get_all_items()) < line_count):
        print(f"Waiting for upload and processing to complete... Running for {10 * (60 - timeout):.2f}[s]")
        time.sleep(10)
        timeout -= 1

    assert timeout != 0, "Timeout. csv load failed to complete after 10 minutes"


@behave.then(u'description in csv "{csv_file}" equal to the description uploaded')
def step_impl(context, csv_file):
    csv_file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], csv_file)
    with open(csv_file_path, newline='') as csvfile:
        csvreader = csv.reader(csvfile, delimiter=',')

        # check the header row 7th column is indeed the "item_description"
        row = next(csvreader)
        assert (row[6] == 'item_description')

        for row in csvreader:
            # search for the right item base on name
            itemname = f"*{row[2]}*"
            filters = context.dl.Filters()
            filters.add(field='type', values='file')
            filters.add(field='name', values=itemname)
            pages = context.dataset.items.list(filters=filters)
            assert len(pages.items) == 1, f"TEST FAILED: Failed to find item by name: {itemname}"
            item = pages[0][0]

            # check the item description match the text in the seventh column
            assert (row[6] == item.description)


@behave.then(u'metadata in csv "{csv_file}" equal to the metadata uploaded')
def step_impl(context, csv_file):
    csv_file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], csv_file)
    with open(csv_file_path, newline='') as csvfile:
        csvreader = csv.reader(csvfile, delimiter=',')

        # check the header row 7th column is indeed the "item_metadata"
        row = next(csvreader)
        assert (row[5] == 'item_metadata')

        for row in csvreader:
            # search for the right item base on name
            itemname = f"*{row[2]}*"
            filters = context.dl.Filters()
            filters.add(field='type', values='file')
            filters.add(field='name', values=itemname)
            pages = context.dataset.items.list(filters=filters)
            assert len(pages.items) == 1, f"TEST FAILED: Failed to find item by name: {itemname}"
            item = pages[0][0]

            # check the item description match the text in the seventh column
            json_text = row[5]
            # parse the JSON text into a Python dictionary
            json_dict = json.loads(json_text)

            # iterate over the keys and values in the dictionary
            for key, value in json_dict.items():
                # if the value is a dictionary, iterate over its keys and values
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        if not isinstance(sub_value, dict):
                            assert sub_key in item.metadata[key]
                            assert sub_value in item.metadata[key][sub_key]
                else:
                    assert value in item.metadata[key]


================================================
File: tests/features/steps/datasets_repo/test_datasets_create.py
================================================
import behave
import random
import string

from tests.features.steps import fixtures


@behave.fixture
def delete_all_datasets(context):
    for dataset in context.project.datasets.list():

        if dataset.readonly:
            dataset.set_readonly(state=False)

        context.project.datasets.delete(dataset_id=dataset.id,
                                        sure=True,
                                        really=True)


@behave.given(u'There are no datasets')
def step_impl(context):
    behave.use_fixture(delete_all_datasets, context)
    assert len(context.project.datasets.list()) == 0
    context.dataset_count = 0


@behave.when(u'I create a dataset with a random name')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    dataset_name = 'random_dataset_{}'.format(rand_str)
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)
    context.dataset_count += 1


@behave.when(u'I create another dataset with a random name')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    dataset_name = 'random_dataset_{}'.format(rand_str)
    context.new_dataset = context.project.datasets.create(dataset_name=dataset_name,
                                                          index_driver=context.index_driver_var)
    context.dataset_count += 1


@behave.then(u'I create a dataset with existing recipe')
def step_impl(context):
    context.recipe_id = context.dataset.get_recipe_ids()[0]
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    dataset_name = 'random_dataset_{}'.format(rand_str)
    context.dataset2 = context.project.datasets.create(dataset_name=dataset_name, recipe_id=context.recipe_id,
                                                       index_driver=context.index_driver_var)
    context.dataset_count += 1


@behave.then(u'dataset recipe is equal to the existing recipe')
def step_impl(context):
    assert context.dataset2.get_recipe_ids() == context.dataset.get_recipe_ids()


@behave.then(u'Dataset object with the same name should be exist')
def step_impl(context):
    assert isinstance(context.dataset, context.dl.entities.Dataset)


@behave.then(u'Dataset object with the same name should be exist in host')
def step_impl(context):
    dataset_get = context.project.datasets.get(dataset_name=context.dataset.name)
    assert dataset_get.to_json() == context.dataset.to_json()


@behave.when(u'When I try to create a dataset with a blank name')
def step_impl(context):
    try:
        context.project.datasets.create('', index_driver=context.index_driver_var)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'Dataset with same name does not exists')
def step_impl(context):
    try:
        context.project.datasets.get(dataset_name=context.dataset.name)
    except context.dl.exceptions.NotFound:
        # good results
        pass
    except:
        # dataset still exists
        assert False


@behave.given(u'I create a dataset with a random name')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    dataset_name = 'random_dataset_{}'.format(rand_str)
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)
    context.dataset_count += 1


@behave.when(u'I try to create a dataset by the same name')
def step_impl(context):
    try:
        context.project.datasets.create(dataset_name=context.dataset.name, index_driver=context.index_driver_var)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'No dataset was created')
def step_impl(context):
    assert len(context.project.datasets.list()) == context.dataset_count


@behave.given(u'Create "{amount}" datasets in project with the prefix name "{name}"')
def step_impl(context, amount, name):
    for i in range(int(amount)):
        try:
            context.project.datasets.get(dataset_name=f"{name}-{i}")
        except context.dl.exceptions.NotFound:
            context.project.datasets.create(dataset_name=f"{name}-{i}")
    assert len(context.project.datasets.list()) == int(amount)


@behave.when(u'"{action}" unsearchable paths "{paths}" to dataset')
def step_impl(context, action, paths):
    if action == 'add':
        context.dataset.schema.unsearchable_paths.add(paths=[paths])
    elif action == 'delete':
        context.dataset.schema.unsearchable_paths.remove(paths=[paths])


@behave.then(u'"{paths}" is "{action}" to dataset schema')
def step_impl(context, action, paths):
    schema = context.dataset.schema.get()
    if action == 'added':
        assert paths in schema.get('items', {}).get('unsearchablePaths', {})
    elif action == 'deleted':
        assert paths not in schema.get('items', {}).get('unsearchablePaths', {})


@behave.then(u'Dataset attribute should be as given')
# we need to test the values from the database
def step_impl(context):

    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    for key, val in context.params.items():
        if hasattr(context, 'clone_dataset'):
            assert getattr(context.clone_dataset, key) == val
        else:
            assert getattr(context.dataset, key) == val

@behave.when(u'I clone a dataset')
def step_impl(context):
    dataset_name = context.dataset.name
    context.clone_dataset = context.dataset.clone(clone_name= dataset_name + "-cloned",filters=None,
                                                  with_items_annotations=True, with_metadata=True,
                                                  with_task_annotations_status=True)


================================================
File: tests/features/steps/datasets_repo/test_datasets_delete.py
================================================
import behave


@behave.when(u'I delete the dataset that was created by name')
def step_impl(context):
    context.project.datasets.delete(dataset_name=context.dataset.name,
                                    sure=True,
                                    really=True)


@behave.when(u'I delete the dataset that was created by id')
def step_impl(context):
    context.project.datasets.delete(dataset_id=context.dataset.id,
                                    sure=True,
                                    really=True)


@behave.when(u'I try to delete a dataset by the name of "{dataset_name}"')
def step_impl(context, dataset_name):
    try:
        context.project.datasets.delete(dataset_name=dataset_name,
                                        sure=True,
                                        really=True)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'No dataset was deleted')
def step_impl(context):
    assert len(context.project.datasets.list()) == context.dataset_count


================================================
File: tests/features/steps/datasets_repo/test_datasets_download.py
================================================
import behave
import os
import glob
import shutil
import urllib.request
import base64


@behave.when(u'I download dataset to "{local_path}"')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)

    context.dataset.items.download(filters=None,
                                   local_path=local_path,
                                   file_types=None,
                                   save_locally=True,
                                   annotation_options=['mask', 'instance', 'json'],
                                   with_text=False,
                                   thickness=3)


@behave.when(u'I download dataset folder "{folder_path}" to "{local_path}"')
def step_impl(context, folder_path, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)

    context.dataset.download_folder(folder_path=folder_path,
                                    local_path=local_path,
                                    to_items_folder=False)


@behave.when(u'I download dataset to "{local_path}" without item folder')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)

    context.dataset.items.download(filters=None,
                                   local_path=local_path,
                                   file_types=None,
                                   save_locally=True,
                                   annotation_options=['mask', 'instance', 'json'],
                                   with_text=False,
                                   thickness=3,
                                   to_items_folder=False)


@behave.then(u'There is no "{log}" file in folder "{download_path}"')
def step_impl(context, log, download_path):
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)

    files = os.listdir(download_path)
    for file in files:
        assert log not in file


@behave.then(u'Dataset downloaded to "{download_path}" is equal to dataset in "{should_be_path}"')
def step_impl(context, download_path, should_be_path):
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)
    files = os.listdir(download_path)
    excepted_dirs = ['instance', 'json', 'mask']
    for file in excepted_dirs:
        assert file in files
    assert len(files) == 4


@behave.given(u'There are no folder or files in folder "{dir_path}"')
def step_impl(context, dir_path):
    dir_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], dir_path)
    dir_path = os.path.join(dir_path, "*")

    files = glob.glob(dir_path)
    for f in files:
        if os.path.isdir(f):
            shutil.rmtree(f)
        elif os.path.isfile(f):
            os.remove(f)

    assert len(glob.glob(dir_path)) == 0


def download_images(context, path):
    # imgNum - number of images
    # imgDim - images dimension
    imgNum = int(context.num_of_items)
    imgDim = [500, 500]

    if path == 'local':
        download_path = context.local_path
    else:
        download_path = context.items_path

    context.images_sizes_list = []
    # Downloading "imgNum" images to cwd with names: "stock-image-X.jpg"
    for i in range(1, int(imgNum) + 1):
        i = str(i)
        urllib.request.urlretrieve(('https://picsum.photos/' + str(imgDim[0]) + '/' + str(imgDim[1]) + '?random'),
                                   (download_path + 'stock-image-' + i + '.' + context.type_of_images))

        with open(download_path + 'stock-image-' + i + '.' + context.type_of_images, "rb") as image_file:
            context.images_sizes_list.append(base64.b64encode(image_file.read()))


@behave.given(u'I get "{num_of_items}" images of type "{type_of_images}" for the dataset')
def step_impl(context, num_of_items, type_of_images):
    context.local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Dataset_Temp", "")
    context.num_of_items = num_of_items
    context.type_of_images = type_of_images

    try:
        os.makedirs(context.local_path)
    except:
        print("Path was created")

    download_images(context, 'local')


@behave.when(u'I upload all the images for the dataset')
def step_impl(context):
    context.dataset.items.upload(local_path=context.local_path)


@behave.when(u'I download the dataset without Overwrite variable')
def step_impl(context):
    try:
        os.makedirs(context.local_path)
    except:
        print("Path was created")

    context.dataset.items.download(local_path=context.local_path)

    context.overwritten_images_sizes_list = []

    with open(context.local_path + '/items/stock-image-1.png', "rb") as image_file:
        context.overwritten_images_sizes_list.append(base64.b64encode(image_file.read()))


@behave.when(u'I download the dataset with Overwrite "{overwrite_status}"')
def step_impl(context, overwrite_status):
    if overwrite_status == "True":
        context.dataset.items.download(local_path=context.local_path, overwrite=True)
    else:
        context.dataset.items.download(local_path=context.local_path, overwrite=False)

    with open(context.local_path + '/items/stock-image-1.png', "rb") as image_file:
        context.overwritten_images_sizes_list.append(base64.b64encode(image_file.read()))


@behave.when(u'I modify the downloaded item')
def step_impl(context):
    context.items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Dataset_Temp/items", "")
    num_of_items = 1
    type_of_images = "png"

    # imgNum - number of images
    # imgDim - images dimension
    imgNum = int(num_of_items)
    imgDim = [500, 500]

    download_images(context, 'items')


@behave.then(u'The dataset item will be "{is_overwritten}"')
def step_impl(context, is_overwritten):
    if is_overwritten == "overwritten":
        assert str(context.overwritten_images_sizes_list[1]) == str(context.overwritten_images_sizes_list[0])
        assert str(context.overwritten_images_sizes_list[1]) != str(context.images_sizes_list[0])
    else:
        assert str(context.overwritten_images_sizes_list[1]) != str(context.overwritten_images_sizes_list[0])
        assert str(context.overwritten_images_sizes_list[1]) == str(context.images_sizes_list[0])

    shutil.rmtree(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Dataset_Temp"))


@behave.then(u'The folder "{download_path}" is equal to to "{should_be_path}"')
def step_impl(context, download_path, should_be_path):
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)
    should_be_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], should_be_path)
    files = os.listdir(download_path)
    excepted_dirs = os.listdir(should_be_path)
    for file in excepted_dirs:
        assert file in files


================================================
File: tests/features/steps/datasets_repo/test_datasets_download_annotations.py
================================================
import behave
import os
import json
import shutil
import time


@behave.given(u'Item in path "{item_path}" is uploaded to "Dataset"')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item_path = item_path
    context.item = context.dataset.items.upload(local_path=item_path)

    # wait for platform attributes
    limit = 10 * 30
    stat = time.time()
    while True:
        time.sleep(3)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if "video" in context.item.mimetype:
            if context.item.fps is not None:
                break
        elif context.item.mimetype is not None:
            break
        if time.time() - stat > limit:
            raise TimeoutError("Timeout while waiting for platform attributes")

    context.item = context.dataset.items.get(item_id=context.item.id)
    if context.item.name.endswith('.mp4') and context.item.fps is None:
        context.item.fps = 25
    if context.item.name.endswith('.jpg') or context.item.name.endswith('.png'):
        if context.item.height is None:
            context.item.height = 768
        if context.item.width is None:
            context.item.width = 1536

@behave.given(u'gis item in path "{item_path}" is uploaded to "Dataset"')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item_path = item_path
    gis_item = context.dl.ItemGis.from_local_file(filepath=item_path)
    context.item = context.dataset.items.upload(local_path=gis_item)


@behave.given(u'There are a few annotations in the item')
def step_impl(context):
    labels = [
        {
            "value": {
                "tag": "dog",
                "displayLabel": "Dog",
                "color": "#0305da",
                "attributes": list(),
            },
            "children": list(),
        },
        {
            "value": {
                "tag": "cat",
                "displayLabel": "Cat",
                "color": "#018f67",
                "attributes": list(),
            },
            "children": list(),
        },
    ]
    labels = [context.dl.Label.from_root(root) for root in labels]
    recipe_id = context.dataset.metadata["system"]["recipes"][0]
    recipe = context.dataset.recipes.get(recipe_id)
    ont_id = recipe.ontology_ids[0]
    ontology = recipe.ontologies.get(ont_id)
    ontology.labels = labels
    recipe.ontologies.update(ontology)
    context.project.datasets.update(dataset=context.dataset, system_metadata=True)
    annotation1 = {'metadata': {
                       'system':
                           { 'attributes': {"1": 'attr1'}}
                   },
                   'coordinates': [
                       {
                           "x": 330,
                           "y": 100,
                           "z": 0
                       },
                       {
                           "x": 460,
                           "y": 208,
                           "z": 0
                       }],
                   'label': 'dog',
                   'type': 'box'}

    annotation2 = {
                   'metadata': {
                       'system':
                           { 'attributes': {"1": 'attr2'}}
                   },
                   'coordinates': [
                       {
                           "x": 400,
                           "y": 170,
                           "z": 0
                       },
                       {
                           "x": 560,
                           "y": 270,
                           "z": 0
                       }],
                   'label': 'cat',
                   'type': 'box'}
    context.annotations = [annotation1, annotation2]
    context.item.annotations.upload(context.annotations)


@behave.when(u'I download dataset annotations')
def step_impl(context):
    if os.path.isdir(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], 'json')):
        shutil.rmtree(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], 'json'))
    context.dataset.download_annotations(local_path=os.environ['DATALOOP_TEST_ASSETS'], overwrite=True)


@behave.then(u'I get a folder named "{folder_name}" in assets folder')
def step_impl(context, folder_name):
    folder_path = os.environ['DATALOOP_TEST_ASSETS']
    dirs = [name for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]
    assert folder_name in dirs


@behave.then(u'Annotations downloaded equal to the annotations uploaded')
def step_impl(context):
    file_path = 'json/0000000162.json'
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as json_file:
        annotations_downloaded = json.load(json_file)
    annotations_downloaded = annotations_downloaded['annotations']
    assert len(annotations_downloaded) == len(context.annotations)
    for annotation in annotations_downloaded:
        ann = {
               "metadata": {
                   "system": {
                       "attributes": annotation.get('metadata', {}).get('system', {}).get('attributes')
                   }
                },
               'coordinates': annotation['coordinates'],
               'label': annotation['label'],
               'type': annotation['type']}
        assert ann in context.annotations


@behave.given(u'There is no folder by the name of "{folder_name}" in assets folder')
def step_impl(context, folder_name):
    folder_path = os.environ['DATALOOP_TEST_ASSETS']
    dirs = [name for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]
    if folder_name in dirs:
        path = os.path.join(folder_path, folder_name)
        shutil.rmtree(path)


@behave.then(u'The folder named "{folder_name}" in folder assets is empty')
def step_impl(context, folder_name):
    folder_path = os.environ['DATALOOP_TEST_ASSETS']
    assert os.listdir(os.path.join(folder_path, folder_name)) == list()


================================================
File: tests/features/steps/datasets_repo/test_datasets_get.py
================================================
import behave


@behave.when(u'I get a dataset with the created name')
def step_impl(context):
    context.dataset_get = context.project.datasets.get(dataset_name=context.dataset.name)


@behave.then(u'I get a dataset with the created name')
def step_impl(context):
    assert type(context.dataset_get) == context.dl.entities.Dataset
    assert context.dataset_get.name == context.dataset.name


@behave.then(u'The dataset I got is equal to the one created')
def step_impl(context):
    assert context.dataset.to_json() == context.dataset_get.to_json()


@behave.when(u'I get a dataset by the id of the dataset "Dataset"')
def step_impl(context):
    context.dataset_get = context.project.datasets.get(dataset_id=context.dataset.id)


@behave.when(u'I try to get a dataset by the name of "{dataset_name}"')
def step_impl(context, dataset_name):
    try:
        context.project = context.project.datasets.get(dataset_name=dataset_name)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I try to get a dataset by id')
def step_impl(context):
    try:
        context.project = context.project.datasets.get(dataset_id='some_ID')
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I get dataset by name "{dataset_name}"')
def step_impl(context, dataset_name):
    context.dataset = context.project.datasets.get(dataset_name=dataset_name)


================================================
File: tests/features/steps/datasets_repo/test_datasets_list.py
================================================
import behave
import dtlpy as dl
import fixtures


@behave.when(u'I list all datasets')
def step_impl(context):
    context.dataset_list = context.project.datasets.list()


@behave.then(u'I receive an empty datasets list')
def step_impl(context):
    assert len(context.dataset_list) == 0


@behave.given(u'I create a dataset by the name of "{dataset_name}" and count')
def step_impl(context, dataset_name):
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)
    context.dataset_count += 1


@behave.then(u'I receive a datasets list of "{dataset_count}" dataset')
def step_impl(context, dataset_count):
    assert len(context.dataset_list) == int(dataset_count)


@behave.then(u'The dataset in the list equals the dataset I created')
def step_impl(context):
    assert context.dataset.to_json() == context.dataset_list[0].to_json()


@behave.when(u'I list datasets using context.filter')
def step_impl(context):
    success, response = dl.client_api.gen_request(req_type='post', path="/datasets/query",
                                                  json_req={"context": {"projects": [context.project.id]},
                                                            "resource": "datasets",
                                                            "filter": context.filters.prepare(query_only=True)['filter']}
                                                  )
    if not success:
        raise dl.exceptions.PlatformException(response)

    context.dataset_list = response.json()['items']


@behave.when(u'I list datasets "{flag}" binaries dataset')
def step_impl(context, flag):
    if flag == "with":
        value = True

    elif flag == "without":
        value = False

    else:
        print('Wrong value - the options are "with" or "without"')

    success, response = dl.client_api.gen_request(req_type='post', path="/datasets/query",
                                                  json_req={"resource": "datasets",
                                                            "context": {"datasets": [context.project.id]},
                                                            "systemSpace": value})
    if not success:
        raise dl.exceptions.PlatformException(response)

    context.dataset_list = response.json()['items']


@behave.given(u'I Add dataset to context.datasets')
def step_impl(context):
    if not hasattr(context, "datasets"):
        context.datasets = list()
    context.datasets.append(context.dataset)


@behave.when(u'I get datasets list by params')
def step_impl(context):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    project = context.project
    context.dataset_list = project.datasets.list(**context.params)


================================================
File: tests/features/steps/datasets_repo/test_datasets_readonly.py
================================================
import behave
import os
import random


@behave.when(u'I set dataset readonly mode to "{state}"')
@behave.given(u'I set dataset readonly mode to "{state}"')
def step_impl(context, state):
    state = state == 'True'
    context.project.datasets.set_readonly(dataset=context.dataset, state=state)


def item_uploaded_successfully(context):
    try:
        item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], '0000000162.png')
        item = context.dataset.items.upload(
            local_path=item_local_path,
            remote_name='name-{}'.format(random.randrange(100, 10000))
        )
        item_uploaded = isinstance(item, context.dl.Item)
    except Exception:
        item_uploaded = False

    return item_uploaded


def dataset_updated_successfully(context):
    try:
        context.dataset.name = 'name-{}'.format(random.randrange(100, 10000))
        context.dataset.update()
        dataset_updated = True
    except Exception:
        dataset_updated = False

    return dataset_updated


@behave.then(u'Dataset is in readonly mode')
def step_impl(context):
    assert context.dataset.readonly
    assert not item_uploaded_successfully(context=context)
    assert not dataset_updated_successfully(context=context)


@behave.then(u'Dataset is not in readonly mode')
def step_impl(context):
    assert not context.dataset.readonly
    assert item_uploaded_successfully(context=context)
    assert dataset_updated_successfully(context=context)


@behave.then('I try and fail to delete the project with the readonly dataset')
def step_impl(context):
    try:
        context.project.delete(True, True)
        assert False, "Success to delete project with readonly dataset"
    except Exception as e:
        assert f"Invalid - Cannot perform operation on readonly dataset" in e.args[1]


================================================
File: tests/features/steps/datasets_repo/test_datasets_update.py
================================================
import behave


@behave.when(u'I update dataset name to "{new_dataset_name}"')
def step_impl(context, new_dataset_name):
    context.dataset.name = new_dataset_name
    context.project.datasets.update(dataset=context.dataset,
                                    system_metadata=True)


@behave.then(u'I create a dataset by the name of "{new_dataset_name}" in host')
def step_impl(context, new_dataset_name):
    context.dataset_get = context.project.datasets.get(dataset_name=new_dataset_name)
    assert context.dataset_get.name == new_dataset_name


@behave.then(u'There is no dataset by the name of "{original_dataset_name}" in host')
def step_impl(context, original_dataset_name):
    try:
        context.project.datasets.get(dataset_name=original_dataset_name)
        context.error = None
    except Exception as e:
        context.error = e
    assert context.error is not None


@behave.then(u'The dataset from host by the name of "New_Dataset_Name" is equal to the one created')
def step_impl(context):
    dataset_json = context.dataset.to_json()
    dataset_get_json = context.dataset_get.to_json()
    dataset_json.pop('updatedAt', None)
    dataset_get_json.pop('updatedAt', None)
    assert dataset_json == dataset_get_json


@behave.when(u'I try to update the "Original_Dataset_Name" name to a blank name')
def step_impl(context):
    context.dataset.name = ''
    try:
        context.project.datasets.update(dataset=context.dataset, system_metadata=True)
        context.error = None
    except Exception as e:
        context.error = e
    assert context.error is not None


@behave.when(u'I try to update the "Dataset" name to "{existing_dataset_name}"')
def step_impl(context, existing_dataset_name):
    context.dataset.name = existing_dataset_name
    try:
        context.project.datasets.update(dataset=context.dataset, system_metadata=True)
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u'I create a dataset by the name of "{original_dataset_name}"')
def step_impl(context, original_dataset_name):
    context.dataset = context.project.datasets.create(dataset_name=original_dataset_name,
                                                      index_driver=context.index_driver_var)


@behave.when(u'I update dataset metadata "{updated_metadata}"')
def step_impl(context, updated_metadata):
    metadata_parts = updated_metadata.split(".")
    metadata_category = metadata_parts[0]
    metadata_key, metadata_value = metadata_parts[1].split(":")
    metadata_value = metadata_value.strip("'")
    if metadata_category not in context.dataset.metadata:
        context.dataset.metadata[metadata_category] = {}
    context.dataset.metadata[metadata_category][metadata_key] = metadata_value
    is_system_metadata = metadata_category == "system"
    context.dataset.update(system_metadata=is_system_metadata)


@behave.then(u'I validate for "{entity}" that the updated metadata is "{updated_metadata}"')
def step_impl(context, entity, updated_metadata):
    field, expected_value = updated_metadata.split(':')
    field_path = field.split('.')
    if entity == "dataset":
        dataset = context.dataset_list[0]
        actual_value = dataset.metadata
    elif entity == "item":
        item = context.dataset.items.get(None, context.item.id)
        actual_value = item.metadata
    else:
        raise AssertionError(f"Unknown object: {entity}")
    for key in field_path:
        actual_value = actual_value.get(key)
        if actual_value is None:
            raise AssertionError(f"Field '{field}' not found in metadata")
    if isinstance(actual_value, (int, float)):
        expected_value = type(actual_value)(expected_value)
    elif isinstance(actual_value, bool):
        expected_value = expected_value.lower() == 'true'
    assert actual_value == expected_value, f"Expected {field} to be {expected_value} but got {actual_value}"

@behave.when(u'I update cloned dataset name to "{new_dataset_name}"')
def step_impl(context, new_dataset_name):
    context.clone_dataset.name = new_dataset_name
    context.project.datasets.update(dataset=context.clone_dataset,
                                    system_metadata=True)

================================================
File: tests/features/steps/datasets_repo/tets_dataset_upload_labels.py
================================================
import behave
import dtlpy as dl
import pandas as pd
import os
import json


@behave.when(u'I upload labels from csv file "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    labels_df = pd.read_csv(file_path)
    context.dataset.add_labels(label_list=labels_df["dataloop label"].to_list())


@behave.when(u'I upload labels to dataset')
def step_impl(context, ):
    context.dataset.add_labels(label_list=list(str(i) for i in range(10)))


@behave.then(u'I validate labels in recipe from file "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    labels_df = pd.read_csv(file_path)
    labels_tolist = labels_df["dataloop label"].to_list()[0].split(".")
    context.dataset = context.project.datasets.get(dataset_name=context.dataset.name)
    dataset_label_list = []
    for label in context.dataset.labels:
        dataset_label_list.append(label.display_label)
        while not label.children == []:
            labels = label.children
            for label in labels:
                dataset_label_list.append(label.display_label)

    assert dataset_label_list == labels_tolist, "TEST FAILED: \nDataset labels {}\nLabels uploaded {}".format(dataset_label_list, labels_tolist)


================================================
File: tests/features/steps/documentation_tests/test_contributor_docs.py
================================================
import behave
import os
import dtlpy as dl


@behave.when(u'List Members')
def step_impl(context):
    context.members_list = context.project.list_members(role=dl.MemberRole.OWNER)  # View all annotators in a project


@behave.given(u'Add Members "{member_email}" as "{member_role}"')
@behave.when(u'Add Members "{member_email}" as "{member_role}"')
@behave.then(u'Add Members "{member_email}" as "{member_role}"')
def step_impl(context, member_email, member_role):
    if member_email == "user":
        member_email = os.environ["TEST_USERNAME"]
    if member_email not in [member.email for member in context.project.list_members()]:
        context.project.add_member(email=member_email, role=member_role)  # role is optional - default is developer


@behave.when(u'Add Members "{member_email}" as "{member_role}" to project {project_index}')
def step_impl(context, member_email, member_role, project_index):
    if member_email not in [member.email for member in context.projects[int(project_index) - 1].list_members()]:
        context.projects[int(project_index) - 1].add_member(email=member_email, role=member_role)  # role is optional - default is developer


@behave.when(u'Add Members "{member_email}" as "{member_role}" to second_project')
def step_impl(context, member_email, member_role):
    if member_email not in [member.email for member in context.second_project.list_members()]:
        context.second_project.add_member(email=member_email, role=member_role)  # role is optional - default is developer


@behave.then(u'Update Members "{member_email}" to "{member_role}"')
def step_impl(context, member_email, member_role):
    context.project.update_member(email=member_email, role=member_role)  # Update user to annotation manager


@behave.then(u'Remove Members "{annotator_email}"')
def step_impl(context, annotator_email):
    context.project.remove_member(email=annotator_email)  # Remove contributor from project


@behave.When(u'I try to delete a member by email')
def step_impl(context):
    try:
        context.project.remove_member(email=context.bot.email)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/documentation_tests/test_dataset_docs.py
================================================
import behave
import random


@behave.given(u'Create a Dataset "{dataset_name}"')
def step_impl(context, dataset_name):
    dataset_name = dataset_name + str(random.randint(10000, 100000))
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)


@behave.when(u'Create a Dataset "{dataset_name}"')
def step_impl(context, dataset_name):
    dataset_name = dataset_name + str(random.randint(10000, 100000))
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, index_driver=context.index_driver_var)


@behave.when(u'Get Commands - Get Projects Datasets List')
def step_impl(context):
    context.datasets = context.project.datasets.list()


@behave.then(u'Get Dataset by Name')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_name=context.dataset.name)


@behave.then(u'Get a dataset by ID')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_id=context.dataset.id)


@behave.then(u'Print a Dataset')
def step_impl(context):
    context.dataset.print()


@behave.then(u'I try to get a dataset by the name of "{dataset_name}"')
def step_impl(context, dataset_name):
    try:
        context.dataset = context.project.datasets.get(dataset_name=dataset_name)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'Clone Dataset "{clone_dataset_name}"')
def step_impl(context, clone_dataset_name):
    context.clone_dataset = context.dataset.clone(clone_name=clone_dataset_name, filters=None,
                                                  with_items_annotations=True, with_metadata=True,
                                                  with_task_annotations_status=True)


@behave.when(u'I clone an item')
def step_impl(context):
    context.cloned_item = context.item.clone(with_annotations=True, with_metadata=True,
                                             with_task_annotations_status=False, allow_many=True)


@behave.then(u'I validate image pre run on cloned item')
def step_impl(context):
    executions = context.cloned_item.resource_executions.list()
    image_pre_run = False
    for page in executions:
        for execution in page:
            if execution.package_name == 'image-preprocess':
                image_pre_run = True
    assert image_pre_run is True, 'image-preprocess package did not run on cloned item'


@behave.then(u'Merge Datasets "{merge_dataset_name}"')
def step_impl(context, merge_dataset_name):
    context.dataset_ids = [context.dataset.id, context.clone_dataset.id]
    context.project_ids = [context.dataset.project.id, context.clone_dataset.project.id]
    context.datasetMerge = context.dl.datasets.merge(merge_name=merge_dataset_name, project_ids=context.project_ids,
                                                     dataset_ids=context.dataset_ids, with_items_annotations=True,
                                                     with_metadata=False,
                                                     with_task_annotations_status=False)

    project__datasets_list = context.project.datasets.list()

    assert len(project__datasets_list) == 4


================================================
File: tests/features/steps/documentation_tests/test_projects_docs.py
================================================
import behave
import time
import dtlpy as dl
import random


@behave.given(u'Create a Project "{project_name}"')
def creating_a_project(context, project_name):
    try:
        context.random_num = str(random.randint(1000, 10000))
        project_name = project_name + context.random_num
        context.project = context.dl.projects.create(project_name=project_name)
        context.to_delete_projects_ids.append(context.project.id)
    except Exception as e:
        context.error = e


@behave.when(u'Create a Project "{project_name}"')
def creating_a_project(context, project_name):
    try:
        context.random_num = str(random.randint(1000, 10000))
        project_name = project_name + context.random_num
        context.project = context.dl.projects.create(project_name=project_name)
        context.to_delete_projects_ids.append(context.project.id)
    except Exception as e:
        context.error = e


@behave.then(u'Get my projects')
def user_projects_list(context):
    assert context.dl.projects.list() != []


@behave.then(u'Get a project by name "{project_name}"')
def project_object_should_be_created(context, project_name):
    try:
        context.project = context.dl.projects.get(project_name=project_name + context.random_num)
    except Exception as e:
        context.error = e

    assert type(context.project) == context.dl.entities.Project
    assert context.project.name == project_name + context.random_num


@behave.then(u'Get a project by project ID')
def step_impl(context):
    assert context.project.id is not None

@behave.then(u'Print a Project')
def step_impl(context):
    context.project.print()

@behave.then(u'Delete project by context.project')
def step_impl(context):
    try:
        context.project.delete(True,True)
    except Exception as e:
        context.error = e



================================================
File: tests/features/steps/documentation_tests/test_recipe_docs.py
================================================
import behave
import dtlpy as dl
import os


@behave.when(u'Get Recipe from List')
def step_impl(context):
    context.recipe = context.dataset.recipes.list()[0]

@behave.then(u'Get Recipe by ID')
def step_impl(context):
    context.recipe = context.dataset.recipes.get(recipe_id=context.recipe.id)


@behave.then(u'Recipe Clone')
def step_impl(context):
    context.recipe = context.dataset.recipes.get(recipe_id=context.recipe.id)
    context.recipe2 = context.recipe.clone(shallow=False)  # dataset or dataset id, if the function is given dataset or dataset id parameters, the recipe will be linked to the given dataset

@behave.then(u'Delete Recipe')
def step_impl(context):
    context.dataset.recipes.get(recipe_id=context.recipe.id).delete(force=True)

@behave.then(u'Recipe Switch')
def step_impl(context):
    context.dataset.switch_recipe(recipe=context.recipe2)  # or use recipe Id


@behave.when(u'View Datasets Labels')
def step_impl(context):
    # as objects
    context.labels = context.dataset.labels
    # as instance map
    context.labels = context.dataset.instance_map


@behave.then(u'Add one Label "{label_name}"')
def step_impl(context,label_name):
    # Add one label
    context.dataset.add_label(label_name=label_name)


@behave.then(u'Add Multiple Labels "{label_1}", "{label_2}", "{label_3}"')
def step_impl(context,label_1,label_2,label_3):
    # Add multiple
    context.dataset.add_labels(label_list=[label_1,label_2,label_3])


@behave.then(u'Add a single label "{label_name}" with a specific color ({num1:d}, {num2:d}, {num3:d})')
def step_impl(context,label_name,num1,num2,num3):
    # Add single label with specific color
    context.dataset.add_label(label_name=label_name, color=(num1,num2,num3))


@behave.then(u'Add a single label "{label_name}" with a specific color ({num1:d}, {num2:d}, {num3:d}) and attributes ["{att1}", "{att2}"]')
def step_impl(context,label_name,num1,num2,num3,att1,att2):
    # Add single label with specific color and attributes
    context.dataset.add_label(label_name=label_name, color=(num1,num2,num3), attributes=[att1,att2])


@behave.then(u'Add a single label "{label_name}" with an image "{image}" and attributes ["{att1}","{att2}"]')
def step_impl(context,label_name,image,att1,att2):
    # Add single label with an icon and attributes
    item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], image)
    context.dataset.add_label(label_name=label_name, icon_path=item_local_path, attributes=[att1, att2])


@behave.then(u'Add Labels using Label object')
def step_impl(context):
    # Create Labels list using Label object
    context.labels = [
        dl.Label(tag='Donkey', color=(1, 1, 1)),
        dl.Label(tag='Mammoth', color=(34, 56, 7)),
        dl.Label(tag='Bird', color=(100, 14, 150))
    ]
    # Add Labels to Dataset
    context.dataset.add_labels(label_list=context.labels)
    # or you can also create a recipe from the label list
    context.temp_recipe = context.dataset.recipes.list()[0]
    context.recipe = context.dataset.recipes.create(recipe_name=context.temp_recipe.title, labels=context.labels)


@behave.then(u'Add a Label with children and attributes')
def step_impl(context):
    # Add label with children and attributes
    label = [dl.Label(
        tag='Fish',
        attributes=['carnivore', 'herbivores'],
        color=(34, 6, 231),
        children=[
            dl.Label(
                tag='Shark',
                color=(34, 6, 231),
                attributes=['baby', 'old'],
            ),
            dl.Label(
                tag='Salmon',
                color=(34, 6, 231),
                attributes=['pink', 'norwegian'],
            )
        ]
    )]
    context.dataset.add_labels(label_list=label)


@behave.then(u'Add multiple Labels with children and attributes "{recipe_name}"')
def step_impl(context,recipe_name):
    # Add multiple labels with children and attributes
    # Create Labels list
    labels = [dl.Label(
        tag='Fish',
        attributes=['carnivore', 'herbivores'],
        color=(34, 6, 231),
        children=[
            dl.Label(
                tag='Shark',
                color=(34, 6, 231),
                attributes=['baby', 'old'],
            ),
            dl.Label(
                tag='Salmon',
                color=(34, 6, 231),
                attributes=['pink', 'norwegian'],
            )
        ]
    ),
        dl.Label(
            tag='Meat',
            attributes=['carnivore', 'herbivores'],
            color=(34, 6, 231),
            children=[
                dl.Label(
                    tag='Beef',
                    color=(34, 6, 231),
                    attributes=['Rib', 'Brisket'],
                ),
                dl.Label(
                    tag='Lamb',
                    color=(34, 6, 231),
                    attributes=['baby', 'old'],
                )
            ]
        )
    ]
    # Add Labels to Dataset
    context.dataset.add_labels(label_list=labels)
    # or you can also create a recipe from the label list
    context.recipe = context.dataset.recipes.create(recipe_name=recipe_name, labels=labels)

@behave.when(u'Option A')
def step_impl(context):
    # Option A
    # add father label
    context.labels = context.dataset.add_label(label_name="animal", color=(123, 134, 64), attributes=["Farm", "Home"])
    # add child label
    context.labels = context.dataset.add_label(label_name="animal.Dog", color=(45, 34, 164), attributes=["Big", "Small"])
    # add grandchild label
    context.labels = context.dataset.add_label(label_name="animal.Dog.poodle", attributes=["Black", "White"])

@behave.when(u'Option B')
def step_impl(context):
    # Option B:only if you dont have attributes
    # parent and grandparent (animal and dog) will be generate automaticly
    context.labels = context.dataset.add_label(label_name="animal.Dog.poodle")

@behave.when(u'Option C')
def step_impl(context):
    # Option C: with the Big Dict
    nested_labels = [
        {'label_name': 'animal.Dog',
         'color': '#220605',
         'children': [{'label_name': 'poodle',
                       'color': '#298345'},
                      {'label_name': 'labrador',
                       'color': '#298651'}]},
        {'label_name': 'animal.cat',
         'color': '#287605',
         'children': [{'label_name': 'Persian',
                       'color': '#298345'},
                      {'label_name': 'Balinese',
                       'color': '#298651'}]}
    ]
    # Add Labels to the dataset:
    context.labels = context.dataset.add_labels(label_list=nested_labels)

@behave.then(u'Create a Recipe From from a Label list "{recipe_name}"')
def step_impl(context,recipe_name):
    # dont forget to create labels list using different scripts
    context.recipe = context.dataset.recipes.create(recipe_name=recipe_name, labels=context.labels)

@behave.then(u'Update Label Features')
def step_impl(context):
    context.dataset.delete_labels(label_names=['Cat', 'Dog'])

@behave.then(u'Delete Labels by Dataset')
def step_impl(context):
    context.dataset.update_label(label_name='Cat', color="#fcba03", upsert=True)  # update label, if not exist add it
    context.dataset.update_label(label_name='Cat', color="#000080")  # update existing label , if not exist fails


================================================
File: tests/features/steps/dpk_tests/dpk_json_to_object.py
================================================
import json
import os
import random

import behave
from .. import fixtures
from ..pipeline_entity import test_pipeline_interface
import random
from operator import attrgetter


@behave.when(u"I fetch the dpk from '{file_name}' file")
@behave.given(u"I fetch the dpk from '{file_name}' file")
@behave.when(u"I fetch the dpk from '{file_name}' file with fix template '{fix_template}'")
@behave.given(u"I fetch the dpk from '{file_name}' file and update dpk with params '{params_flag}'")
def step_impl(context, file_name, fix_template="True", params_flag='None'):
    assert params_flag in ['None', 'True', 'False'], f"params_flag should be 'None', 'True' or 'False' but got {params_flag}"

    fix_template = True if fix_template != "False" else False
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_name)
    with open(path, 'r') as file:
        json_object = json.load(file)

    json_object = fixtures.update_dtlpy_version(json_object)
    if "context" in json_object.keys():
        if json_object['context'].get("project", None) is not None:
            json_object['context']['project'] = context.project.id
        if json_object['context'].get("organization", None) is not None:
            json_object['context']['organization'] = context.project.org['id']

    if eval(params_flag):
        params = dict()
        for row in context.table:
            params[row['key']] = row['value']
        fixtures.update_nested_dict(json_object, params)

    if fix_template and "pipelineTemplates" in json_object.get('components', {}).keys():
        json_object['components']['pipelineTemplates'][0] = test_pipeline_interface.generate_pipeline_json(context,
                                                                                                           json_object[
                                                                                                               'components'][
                                                                                                               'pipelineTemplates'][
                                                                                                               0])

    context.dpk = context.dl.entities.Dpk.from_json(_json=json_object,
                                                    client_api=context.dl.client_api,
                                                    project=context.project
                                                    )
    context.json_object = json_object


@behave.when(u"I add codebase to dpk")
@behave.given(u"I add codebase to dpk")
def step_impl(context):
    context.dpk.codebase = context.codebase


@behave.when(u"I add integration to dpk")
def step_impl(context):
    if context.dpk.components.services:
        for service in context.dpk.components.services:
            if 'integrations' in service:
                service['integrations'][0]['value'] = context.integration.id
    if context.dpk.components.compute_configs:
        for service in context.dpk.components.compute_configs:
            if service.integrations:
                service.integrations[0]['value'] = context.integration.id
    if context.dpk.components.integrations:
        for integration in context.dpk.components.integrations:
            if 'value' in integration:
                integration['value'] = context.integration.id


@behave.then(u"I have a dpk entity")
def step_impl(context):
    assert hasattr(context, 'dpk')


@behave.then(u"I have json object to compare")
def step_impl(context):
    assert hasattr(context, 'json_object')


@behave.then(u"The dpk is filled with the same values")
def step_impl(context):
    if 'name' in context.json_object:
        assert context.dpk.name == context.json_object['name']
    if 'id' in context.json_object:
        assert context.dpk.id == context.json_object['id']
    if 'scope' in context.json_object:
        assert context.dpk.scope == context.json_object['scope']
    if 'version' in context.json_object:
        assert context.dpk.version == context.json_object['version']
    if 'creator' in context.json_object:
        assert context.dpk.creator == context.json_object['creator']
    if 'displayName' in context.json_object:
        assert context.dpk.display_name == context.json_object['displayName']
    if 'description' in context.json_object:
        assert context.dpk.description == context.json_object['description']
    if 'icon' in context.json_object:
        assert context.dpk.icon == context.json_object['icon']
    if 'attributes' in context.json_object:
        assert context.dpk.attributes == context.json_object['attributes']
    if 'components' in context.json_object:
        assert context.dpk.components.panels == \
               context.json_object['components']['panels']
    if 'createdAt' in context.json_object:
        assert context.dpk.created_at == context.json_object['createdAt']
    if 'updatedAt' in context.json_object:
        assert context.dpk.updated_at == context.json_object['updatedAt']
    if 'codebase' in context.json_object:
        assert context.dpk.codebase == context.json_object['codebase']
    if 'url' in context.json_object:
        assert context.dpk.url == context.json_object['url']
    if 'tags' in context.json_object:
        assert context.dpk.tags == context.json_object['tags']


@behave.given(u"I publish a dpk to the platform")
def step_impl(context):
    context.dpk.name = context.dpk.name + str(random.randint(10000, 1000000))
    context.dpk = context.project.dpks.publish(context.dpk)
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.append(context.dpk)
    else:
        context.feature.dpks = [context.dpk]


@behave.when(u"I update dpk dtlpy to current version for service in index {i}")
def step_impl(context, i):
    context.dpk.components.services[int(i)]['versions'] = {'dtlpy': context.dl.__version__, "verify": True}


@behave.when(u"I remove the last service from context.custom_installation")
def step_impl(context):
    if not hasattr(context, "custom_installation"):
        raise AttributeError(
            "Please make sure to add 'custom_installation' to 'context', Can use step 'When I create a context.custom_installation var'")

    context.custom_installation.get('components').get('services').pop(-1)
    context.custom_installation.get('components').get('triggers').pop(-1)


@behave.when(u"I add service to context.custom_installation")
def step_impl(context):
    if not hasattr(context, "custom_installation"):
        raise AttributeError(
            "Please make sure to add 'custom_installation' to 'context', Can use step 'When I create a context.custom_installation var'")

    service = context.custom_installation.get('components').get('services')[-1].copy()
    service['name'] = f"{service.get('name')}-sdk"
    context.custom_installation.get('components').get('services').append(service)


@behave.when(u"I add att '{value}' to dpk '{component}' in index '{index}'")
def step_impl(context, value, component, index):
    if "=" in value:
        value = value.split("=")
        if '.id' in value[1]:
            value[1] = attrgetter(value[1])(context)
        elif '[' in value[1] or '{' in value[1]:
            value[1] = eval(value[1])
    else:
        raise ValueError("Please make sure 'value' structure is 'key=val'")

    if component == 'service':
        if "cooldownPeriod" in value:
            context.dpk.components.services[int(index)]['runtime']['autoscaler'][value[0]] = eval(value[1])
    elif component == 'model':
        model = context.dpk.components.models[int(index)]
        if "metadata" in value[0]:
            if "system" in value[0]:
                model['metadata']['system'][value[0].split('.')[-1]] = value[1]
            else:
                model['metadata'][value[0].split('.')[-1]] = value[1]
        else:
            model[value[0]] = value[1]
    else:
        raise ValueError("Please provide a valid dpk component")


@behave.then(u"I validate dpk autoscaler in composition for service in index '{index}'")
def step_impl(context, index):
    context.dpk = context.project.dpks.get(dpk_id=context.dpk.id)
    app_composition = context.project.compositions.get(context.app.composition_id)

    dpk_autoscaler_items = context.dpk.components.services[int(index)]['runtime']['autoscaler'].items()
    comp_autoscaler_items = app_composition['spec'][int(index)]['runtime']['autoscaler'].items()

    assert dpk_autoscaler_items <= comp_autoscaler_items, f"TEST FAILED: dpk_autoscaler_items is {dpk_autoscaler_items} composition_autoscaler_items is {comp_autoscaler_items}"


@behave.given(u'I remove the "{entity}" from integration from the dpk in "{component}" component in index {index}')
def step_impl(context, entity, component, index):
    if component == 'integrations':
        eval(f"context.dpk.components.{component}[{index}].pop('{entity}')")
    else:
        eval(f"context.dpk.components.{component}[{index}]['integrations'][{index}].pop('{entity}')")


@behave.given(u'I add "{entity}" to integration from the dpk in "{component}" component in index {index}')
def step_impl(context, entity, component, index):
    if "=" in entity:
        entity = entity.split("=")
        if '.id' in entity[1]:
            entity[1] = attrgetter(entity[1])(context)
        elif '[' in entity[1] or '{' in entity[1]:
            entity[1] = eval(entity[1])
        elif entity[1] == "True" or entity[1] == "False":
            entity[1] = eval(entity[1])
        if component == 'integrations':
            eval(f"context.dpk.components.{component}[{index}].update({{entity[0]: entity[1]}})")
        else:
            eval(f"context.dpk.components.{component}[{index}]['integrations'][{index}].update({{entity[0]: entity[1]}})")
    else:
        raise ValueError("Please make sure 'entity' structure is 'key=val'")


================================================
File: tests/features/steps/dpk_tests/test_dpk_delete.py
================================================
import behave


@behave.when(u'I delete published_dpk')
def step_impl(context):
    context.published_dpk.delete()
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.pop()


@behave.when(u'I delete dpk with all revisions')
def step_impl(context):
    for dpk in context.dpk.revisions.items:
        dpk.delete()
        if hasattr(context.feature, 'dpks'):
            context.feature.dpks.pop()




================================================
File: tests/features/steps/dpk_tests/test_dpk_get.py
================================================
import behave
import json
import dictdiffer


@behave.when(u'I try get the "{dpk_obj}" by id')
def step_impl(context, dpk_obj):
    try:
        context.dpk = context.dl.dpks.get(dpk_id=getattr(context, dpk_obj).id)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I get the dpk by name')
def step_impl(context):
    context.dpk = context.dl.dpks.get(dpk_name=context.published_dpk.name)


@behave.when(u'i update dpk compute config "{comp_name}" runtime "{filed}" to "{val}"')
def step_impl(context, comp_name, filed, val):
    for comp in context.dpk.components.compute_configs:
        if comp.name == comp_name:
            comp.runtime[filed] = val


@behave.when(u'i update dpk attribute "{filed}" to "{val}"')
def step_impl(context, filed, val):
    context.dpk.attributes[filed] = val


@behave.then(u'I have the same dpk as the published dpk')
def step_impl(context):
    to_json = context.dpk.to_json()
    if to_json.get('dependencies', False) is None:
        to_json.pop('dependencies', None)
    if 'context' in to_json and to_json['context'] is None:
        to_json.pop('context', None)
    assert to_json == context.published_dpk.to_json(), "TEST FAILED: Different in to_json and dpk.to_json().\n{}".format(
        list(dictdiffer.diff(to_json, context.published_dpk.to_json())))


@behave.when(u'I get a dpk with invalid id')
def step_impl(context):
    try:
        context.dpk = context.dl.dpks.get(dpk_id="1")
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I should get an exception')
def step_impl(context):
    assert context.error is not None


@behave.when(u'I get global dpk by name "{dpk_name}"')
def step_impl(context, dpk_name):
    try:
        context.dpk = context.dl.dpks.get(dpk_name=dpk_name)
        context.error = None
    except Exception as e:
        raise e


@behave.when(u'I try get the "{dpk_obj}" by name')
def step_impl(context, dpk_obj):
    try:
        context.dpk = context.project.dpks.get(dpk_name=getattr(context, dpk_obj).name)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I validate attributes response in context.req')
def step_impl(context):
    if not hasattr(context, "req") or not hasattr(context, "response"):
        return False, "Context missing 'req' / 'response' attribute"

    parsed_response = json.loads(context.response.text)

    if isinstance(parsed_response, list):
        response_list = [list(att.values())[0] for att in parsed_response ]
        assert all(var in response_list for var in context.req.get("response", None)), \
            f"TEST FAILED: Expected {context.req.get('response', None)}, Received {response_list}"

    elif isinstance(parsed_response, dict):
        response_list = list(parsed_response.keys())
        assert all(var in response_list for var in context.req.get("response", [])), \
            f"TEST FAILED: Expected {context.req.get('response', None)}, Received {response_list}"

    else:
        raise TypeError("Unexpected JSON format: not a list or dictionary")


================================================
File: tests/features/steps/dpk_tests/test_dpk_interface.py
================================================
import behave
import os
import json
from .. import fixtures


@behave.when(u'I get computeConfig from path "{compute_path}" named "{compute_name}"')
def step_impl(context, compute_path, compute_name):
    context.compute_config_item = None
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], compute_path)
    with open(path, 'r') as file:
        list_object = json.load(file)
    for compute_config in list_object:
        if compute_name == compute_config['name']:
            context.compute_config_item = compute_config
            break
    assert context.compute_config_item, f"TEST FAILED: Failed to find '{compute_name}'"
    context.compute_config_item = fixtures.update_dtlpy_version(context.compute_config_item)


@behave.when(u'I add computeConfig to dpk on "{component}" component in index "{comp_index}"')
@behave.when(u'I add computeConfig to dpk on "{component}" component in index "{comp_index}" on module in index "{module_index}"')
@behave.when(u'I add computeConfig to dpk on "{component}" component in index "{comp_index}" with operation "{operation}"')
def step_impl(context, component, comp_index, operation=None, module_index=None):
    if not hasattr(context, "compute_config_item"):
        assert False, "TEST FAILED: Please implement the step: When I get computeConfig from path '{json_path}' named '{key}'"

    if component not in ["modules", "models", "pipelineNodes", "services", "functions"]:
        assert False, f"TEST FAILED: Wrong component key '{component}' please make sure to use on of : 'modules', 'models', 'pipelineNodes', 'services', 'functions'"

    context.compute_config_obj = context.dl.entities.DpkComputeConfig.from_json(_json=context.compute_config_item)
    # Check if the dpk_obj is already in compute_configs objects
    if context.dpk.components.compute_configs or context.compute_config_obj.name not in [config.name for config in context.dpk.components.compute_configs]:
        context.dpk.components.compute_configs.append(context.compute_config_obj)

    if component != "functions":
        component_obj = getattr(context.dpk.components, component)[int(comp_index)]
    else:
        component_obj = getattr(context.dpk.components.modules[int(module_index)], component)[int(comp_index)]

    if isinstance(component_obj, dict):
        if component_obj.get('computeConfigs'):
            component_obj['computeConfigs'].update({operation: context.compute_config_obj.name})
        else:
            component_obj['computeConfigs'] = {operation: context.compute_config_obj.name}
    else:
        component_obj.compute_config = context.compute_config_obj.name


@behave.given(u'I publish a pipeline node dpk from file "{file_name}" and with code path "{code_path}"')
def step_impl(context, file_name, code_path):
    context.execute_steps(f"""
    Given I fetch the dpk from '{file_name}' file
    And I create a dataset with a random name
    When I set code path "{code_path}" to context
    And I pack directory by name "{code_path}"
    And I add codebase to dpk
    And I publish a dpk to the platform
    """)
    if not hasattr(context, 'published_dpks'):
        context.published_dpks = list()
    context.published_dpks.append(context.published_dpk)


@behave.given(u'I publish a model dpk from file "{file_name}" package "{package_name}"')
@behave.given(u'I publish a model dpk from file "{file_name}" package "{package_name}" entry point "{entry_point}" model "{model_name}" status "{status}" in index "{index}"')
@behave.given(u'I publish a model dpk from file "{file_name}" package "{package_name}" with status "{status}"')
def step_impl(context, file_name, package_name, entry_point='main.py', model_name='test-model', status='created', index='0'):
    context.execute_steps(f"""
    Given I create a dataset with a random name
    And I upload an item by the name of "test_item.jpg"
    When I upload labels to dataset
    And I upload "5" box annotation to item
    Given I fetch the dpk from '{file_name}' file
    When I create a dummy model package by the name of "{package_name}" with entry point "{entry_point}"
    And I create a model from package by the name of "{model_name}" with status "{status}" in index "{index}"
    And I publish a dpk to the platform
    """)

    if not hasattr(context, 'published_dpks'):
        context.published_dpks = list()
    context.published_dpks.append(context.published_dpk)


@behave.given(u'I fetch dpk active learning pipeline template from file with params')
def step_impl(context, entry_point='main.py', model_name='test-model', status='created', index='0'):
    params = dict()
    params['entry_point'] = entry_point
    params['model_name'] = model_name
    params['status'] = status
    params['index'] = index
    for row in context.table:
        params[row['key']] = row['value']

    if not params.get('file_name'):
        raise Exception("Please make sure to add 'file_name' to the table")
    if not params.get('package_name'):
        raise Exception("Please make sure to add 'package_name' to the table")

    context.execute_steps(f"""
    Given I create a dataset named "Upload-data"
    And I Add dataset to context.datasets
    And I create a dataset named "Ground-Truth"
    And I Add dataset to context.datasets
    When I validate global app by the name "Active Learning" is installed
    Given I fetch the dpk from 'model_dpk/modelsDpks.json' file
    When I create a dummy model package by the name of "{params['package_name']}" with entry point "{params['entry_point']}"
    And I create a model from package by the name of "{params['model_name']}" with status "{params['status']}" in index "{params['index']}"
    When I get global dpk by name "active-learning"
    Given I fetch the dpk from '{params['file_name']}' file
    """)

    if not hasattr(context, 'published_dpks'):
        context.published_dpks = list()
    context.published_dpks.append(context.dpk)
    # Make sure to publish the


================================================
File: tests/features/steps/dpk_tests/test_dpk_list.py
================================================
import behave


@behave.when(u'I list the dpks')
def step_impl(context):
    context.dpk_list = context.dl.dpks.list()


@behave.then(u'I should see at least {count} dpks')
def step_impl(context, count):
    assert len(context.dpk_list) >= int(count)


================================================
File: tests/features/steps/dpk_tests/test_dpk_publish.py
================================================
import json
import os
import random
import behave


@behave.when(u'I publish a dpk to the platform')
@behave.when(u'I publish a dpk to the platform without random name "{random_name}"')
def step_impl(context, random_name=True):
    random_name = True if random_name != 'False' else False
    if random_name:
        context.dpk.name = context.dpk.name + str(random.randint(10000, 1000000))
    context.published_dpk = context.project.dpks.publish(context.dpk)
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.append(context.published_dpk)
    else:
        context.feature.dpks = [context.published_dpk]


@behave.when(u'dpk has base id')
def step_impl(context):
    assert context.dpk.base_id is not None


@behave.when(u'i save dpk base id')
def step_impl(context):
    context.dpk_base_id = context.dpk.base_id


@behave.then(u'i check the new dpk hase the same base id')
def step_impl(context):
    assert context.dpk_base_id == context.dpk.base_id, "Base id is not the same"
    assert context.dpk_base_id != context.dpk.id, "Base id is the same as id"


@behave.when(u'I try to publish a dpk to the platform')
def step_impl(context):
    try:
        context.dpk.name = context.dpk.name + str(random.randint(10000, 1000000))
        context.published_dpk = context.project.dpks.publish(context.dpk)
        if hasattr(context.feature, 'dpks'):
            context.feature.dpks.append(context.published_dpk)
        else:
            context.feature.dpks = [context.published_dpk]
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I publish without context')
def step_impl(context):
    try:
        context.dpk.name = context.dpk.name + str(random.randint(10000, 1000000))
        context.published_dpk = context.dl.dpks.publish(context.dpk)
        if hasattr(context.feature, 'dpks'):
            context.feature.dpks.append(context.published_dpk)
        else:
            context.feature.dpks = [context.published_dpk]
    except Exception as e:
        context.error = e


@behave.when(u'I add context to the dpk')
def step_impl(context):
    dpk_context = {
        "project": context.project.id
    }
    context.dpk.context = dpk_context
    context.dpk.scope = "organization"


@behave.when(u'I add pipeline template "{template_path}" to the dpk')
def step_impl(context, template_path):
    pipeline_template_path = template_path
    path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], pipeline_template_path)
    with open(path, 'r') as file:
        json_object = json.load(file)
    context.dpk.components[json_object["name"]] = {
        "type": "pipelineTemplate",
        "spec": json_object
    }


@behave.then(u'The pipeline template "{template}" should be created')
def step_impl(context, template):
    pipeline_template_name = context.dpk.components[template]["spec"]["name"]
    success, response = context.project._client_api.gen_request(req_type='POST', path='/pipelines/templates/query',
                                                                json_req={
                                                                    "org": {
                                                                        "filter": {
                                                                            "$and": [{"name": pipeline_template_name}]}
                                                                    }
                                                                })
    template = response.json()["org"]["items"][0]
    assert template["name"] == pipeline_template_name


@behave.when(u"I add the context.dataset to the dpk model")
def step_impl(context):
    for model in context.dpk.components.models:
        model['datasetId'] = context.dataset.id


@behave.then(u'The user defined properties should have the same values')
def step_impl(context):
    dpk = context.dpk
    p_dpk = context.published_dpk
    assert dpk.display_name == p_dpk.display_name
    assert dpk.version if dpk.version else "1.0.0" == p_dpk.version
    assert dpk.attributes == p_dpk.attributes
    assert dpk.icon == p_dpk.icon
    assert dpk.tags == p_dpk.tags
    assert dpk.scope == p_dpk.scope
    assert dpk.description == p_dpk.description
    assert dpk.components.to_json() == p_dpk.components.to_json()


@behave.then(u'id, name, createdAt, codebase, url and creator should have values')
def step_impl(context):
    dpk = context.published_dpk
    assert dpk.id is not None
    assert dpk.name is not None
    assert dpk.created_at is not None
    assert dpk.codebase is not None
    assert dpk.url is not None


@behave.when(u'I set the model in the context')
def step_impl(context):
    composition = context.project.compositions.get(composition_id=context.app.composition_id)
    context.model = context.project.models.get(model_id=composition["models"][0]["modelId"])


@behave.when(u'I add models list to context.models and expect to get "{total_models}" models')
@behave.then(u'I expect to get "{total_models}" models in project')
def step_impl(context, total_models):
    filters = context.dl.Filters()
    filters.resource = context.dl.FiltersResource.MODEL
    filters.sort_by(field='name', value=context.dl.FiltersOrderByDirection.ASCENDING)
    context.models = context.project.models.list(filters=filters).items
    assert len(context.models) == int(total_models), f"Expected {total_models} models, got {len(context.models)}"


@behave.when(u'I increment dpk version')
def step_impl(context):
    version = context.dpk.version.split('.')
    version[2] = str(int(version[2]) + 1)
    context.dpk.version = '.'.join(version)


@behave.when(u'I publish a dpk')
def step_impl(context):
    context.dpk = context.project.dpks.publish(context.dpk)
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.append(context.dpk)
    else:
        context.feature.dpks = [context.dpk]


================================================
File: tests/features/steps/dpk_tests/test_dpk_pull.py
================================================
import os.path

import behave


@behave.when(u'I pull the dpk')
def step_impl(context):
    context.dpk_path = context.dl.dpks.pull(dpk_id=context.dpk.id)


@behave.then(u'I should have a dpk file')
def step_impl(context):
    assert os.path.exists(context.dpk_path)


================================================
File: tests/features/steps/dpk_tests/test_dpk_update.py
================================================
import behave


@behave.when(u'I add dependency to the dpk with params')
def step_impl(context):
    """
    This step is used to add one dependency to the dpk with all relevant attributes name , version etc.
    """
    params = dict()
    for row in context.table:
        try:
            value = eval(row['value'])
        except NameError:
            value = row['value']
        params.update({row['key']: value})

    # Add dependency to the dpk
    if not isinstance(context.dpk.dependencies, list):
        context.dpk.dependencies = list()
    context.dpk.dependencies.append(params)

    # Add the dependency name to the dpks_names list
    if not hasattr(context, 'dpks_names'):
        context.dpks_names = list()
    context.dpks_names.append({"name": params['name'], "flag": False})


@behave.when(u'i update the dpk attr "{field}" with value "{val}"')
def step_impl(context, field, val):
    if field == 'attributes':
        val = eval(val)
    setattr(context.dpk, field, val)


@behave.when(u'i update the dpk pipe template preview')
def step_impl(context):
    context.last_preview = context.dpk.components.pipeline_templates[0].get('preview')
    context.dpk.components.pipeline_templates[0][
        'preview'] = "data:image/png;base64,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"


@behave.when(u'i update the dpk')
def step_impl(context, ):
    context.dpk = context.dpk.update()


@behave.when(u'I try to update the dpk')
def step_impl(context, ):
    try:
        context.dpk = context.dpk.update()
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I should see the dpk updated successfully')
def step_impl(context):
    assert context.dpk.attributes == {'status': 'updated'}, f'attributes is not updated'
    assert context.dpk.display_name == 'newDisplayName', f'display_name is not updated'
    assert context.dpk.icon == 'newIcon', f'icon is not updated'
    assert context.dpk.scope == 'project', f'scope is updated'
    assert context.dpk.components.pipeline_templates[0]['preview'] != context.last_preview, f'preview is not updated'


================================================
File: tests/features/steps/dpk_tests/test_dpk_validation.py
================================================
import behave
import dtlpy as dl


def recusive_dependencies(context, dpk, dependencies: list = None):
    if not dependencies:
        dependencies = []
    dependencies.append(dpk.name)
    if dpk.dependencies:
        for dep in dpk.dependencies:
            dpk = context.dl.dpks.get(dpk_name=dep['name'])
            recusive_dependencies(context, dpk, dependencies)
    return dependencies


@behave.then(u'I validate app dependencies are installed')
def step_impl(context):
    if not hasattr(context, "published_dpk"):
        raise AttributeError("Please make sure context has attr 'published_dpk'")
    dependencies_name = recusive_dependencies(context, context.published_dpk, [])
    filters = dl.Filters(resource='apps')
    filters.custom_filter = {"filter": {"$or": [{"projectId": context.project.id}, {"scope": "system"}]}}
    context.apps_list = context.project.apps.list(filters=filters).items
    apps_dpk_names = [app.dpk_name for app in context.apps_list]
    for app_name in dependencies_name:
        if app_name not in apps_dpk_names:
            assert False, f"App '{app_name}' was not installed"


@behave.then(u'I validate dpk dependencies have "{ref_relation}" relation refs')
@behave.then(u'I validate dpk dependencies have "{ref_relation}" relation refs "{flag}"')
def step_impl(context, ref_relation, flag=None):
    if not hasattr(context, "dpks_names"):
        raise AttributeError("Please make sure context has attr 'dpks_names'")
    for dpk_name in context.dpks_names:
        try:
            dpk = context.project.dpks.get(dpk_name=dpk_name['name'])
            for refs in dpk.metadata['system']['refs']:
                if refs['type'] == 'dpk' and refs['metadata']['relation'] == ref_relation:
                    dpk_name['flag'] = True
                elif flag:
                    # Refs has different ref_relation and flag only is on
                    assert False, f"DPK '{dpk_name['name']}' Should have only '{ref_relation}' field, Actual: {refs['metadata']['relation']}"

        except dl.exceptions.NotFound:
            assert False, f"DPK {dpk_name} was not installed"

    for flag_dpk in context.dpks_names:
        assert flag_dpk['flag'], f"App '{flag_dpk['name']}' missing '{ref_relation}' field"


@behave.then(u'I validate app dependencies have "{ref_relation}" relation refs')
@behave.then(u'I validate app dependencies have "{ref_relation}" relation refs "{flag}"')
def step_impl(context, ref_relation, flag=None):
    apps_names = [{"name": dependency['name'], "flag": False} for dependency in context.published_dpk.dependencies]
    for app_name in apps_names:
        try:
            filters = dl.Filters(resource='apps')
            filters.add(field='dpkName', values=app_name['name'])
            app = context.project.apps.list(filters=filters).items[0]
            for refs in app.metadata['system']['refs']:
                if refs['type'] == 'app' and refs['metadata']['relation'] == ref_relation:
                    app_name['flag'] = True
                elif flag:
                    # Refs has different ref_relation and flag only is on
                    assert False, f"App '{app_name['name']}' Should have only '{ref_relation}' field, Actual: {refs['metadata']['relation']}"

        except dl.exceptions.NotFound:
            assert False, f"App '{app_name}' was not installed"

    for flag_app in apps_names:
        assert flag_app['flag'], f"App {flag_app['name']} missing {ref_relation} field"


@behave.then(u'I validate app dependencies not have "{ref_relation}" relation refs')
def step_impl(context, ref_relation):
    apps_names = [{"name": dependency['name'], "flag": False} for dependency in context.published_dpk.dependencies]
    for app_name in apps_names:
        try:
            filters = dl.Filters(resource='apps')
            filters.add(field='dpkName', values=app_name['name'])
            context.app = context.project.apps.list(filters=filters).items[0]
            for refs in context.app.metadata['system']['refs']:
                if refs['type'] == 'app' and refs['id'] == context.app.id and refs['metadata']['relation'] == ref_relation:
                    assert False, f"App '{app_name['name']}' Should not have '{ref_relation}' field"

        except dl.exceptions.NotFound:
            assert False, f"App '{app_name}' was not installed"


@behave.then(u'I validate dpk dependencies not have "{ref_relation}" relation refs')
def step_impl(context, ref_relation):
    if not hasattr(context, "dpks_names"):
        raise AttributeError("Please make sure context has attr 'dpks_names'")

    for dpk_name in context.dpks_names:
        try:
            dpk = context.project.dpks.get(dpk_name=dpk_name['name'])
            for refs in dpk.metadata['system']['refs']:
                if refs['type'] == 'dpk' and refs['id'] == context.published_dpk.id and refs['metadata']['relation'] == ref_relation:
                    assert False, f"DPK '{dpk_name['name']}' Should not have '{ref_relation}' field"

        except dl.exceptions.NotFound:
            assert False, f"DPK '{dpk_name}' was not installed"


@behave.then(u'I validate app dependencies not installed')
def step_impl(context):
    apps_names = [{"name": dependency['name'], "flag": False} for dependency in context.published_dpk.dependencies]
    for app_name in apps_names:
        filters = dl.Filters(resource='apps')
        filters.add(field='dpkName', values=app_name['name'])
        if context.project.apps.list(filters=filters).items:
            assert False, f"App '{app_name['name']}' Should not be installed"


@behave.then(u'I validate the context.model has the attribute "{key}" with value "{val}"')
def step_impl(context, key, val):
    if "metadata" in key:
        last_key = key.split('.')[-1]
        if "system" in key:
            assert context.model.metadata['system'][last_key] == eval(val), f"TEST FAILED: Expected: {val} , Actual : {context.model.metadata['system'][last_key]}"
        else:
            assert context.model.metadata[last_key] == eval(val), f"TEST FAILED: Expected: {val} , Actual : {context.model.metadata[last_key]}"
    else:
        assert getattr(context.model, key) == eval(val), f"TEST FAILED: Expected: {val} , Actual : {getattr(context.model, key)}"


================================================
File: tests/features/steps/drivers_repo/test_drivers_create.py
================================================
import behave
import time


@behave.when(u'I create driver "{driver_type}" with the name "{driver_name}"')
def step_impl(context, driver_type, driver_name):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']

    assert hasattr(context, "integration"), "TEST FAILED: Context has no object integration"
    try:
        context.driver_type = driver_type

        time.sleep(5)  # Wait for integration
        context.driver = context.project.drivers.create(
            name=driver_name,
            driver_type=context.driver_type,
            integration_id=context.integration.id,
            bucket_name=params.get('bucket_name', None),
            integration_type=context.integration.type,
            project_id=context.project.id,
            allow_external_delete=params.get('allow_external_delete', True),
            region=params.get('region', None),
            storage_class=params.get('storage_class', ""),
            path=params.get('path', ""))
        context.to_delete_drivers_ids.append(context.driver.id)
        context.error = None
    except Exception as e:
        context.error = e
        assert False, e


@behave.then(u'I validate driver with the name "{driver_name}" is created')
def step_impl(context, driver_name):
    try:
        context.driver = context.project.drivers.get(driver_id=context.driver.id)
        context.error = None
    except Exception as e:
        context.error = e

    assert driver_name == context.driver.name, "TEST FAILED: Expected - {}, Got - {}".format(driver_name, context.driver.name)
    assert context.driver_type == context.driver.type, "TEST FAILED: Expected - {}, Got - {}".format(context.driver_type, context.driver.type)


@behave.when(u'I create dataset "{dataset_name}" with driver entity')
def step_impl(context, dataset_name):
    context.dataset = context.project.datasets.create(dataset_name=dataset_name, driver_id=context.driver.id, index_driver=context.index_driver_var)
    context.to_delete_datasets_ids.append(context.dataset.id)

@behave.when(u'I sync dataset in context')
@behave.when(u'I sync dataset in context with is {wait_parameter}')
def step_impl(context, wait_parameter="True"):
    wait_parameter = str(wait_parameter).lower() == "true"
    if wait_parameter:
        success = context.dataset.sync()
        assert success, "TEST FAILED: Failed to sync dataset"
    else:
        try:
            context.dataset.sync(wait=wait_parameter)
            context.error = None
        except Exception as e:
            context.error = e

@behave.then(u'I validate driver dataset has "{item_count}" items')
def step_impl(context, item_count):
    num_try = 18
    interval = 10
    finished = False
    pages = None

    for i in range(num_try):
        pages = context.dataset.items.list()
        if pages.items_count == int(item_count):
            finished = True
            break
        time.sleep(interval)

    assert finished, f"TEST FAILED: Expected dataset to have {item_count} items, Actual: {pages.items_count} after {round(num_try * interval / 60, 1)} minutes"


@behave.then(u'I stream Item by path "{item_path}"')
def step_impl(context, item_path):
    try:
        item = context.dataset.items.get(filepath=item_path)
        response = context.dl.client_api.gen_request(req_type="GET", path=item.stream.split("v1")[-1])
    except Exception as e:
        context.error = e
        assert False, "TEST FAILED: Not able to stream the item"

    assert response, f"TEST FAILED: Response is empty"


================================================
File: tests/features/steps/drivers_repo/test_drivers_delete.py
================================================
import behave


@behave.when(u'I delete driver by the name "{driver_name}"')
def step_impl(context, driver_name):
    try:
        context.project.drivers.delete(driver_name=driver_name, sure=True, really=True)
        context.to_delete_drivers_ids.pop()
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I validate driver "{driver_name}" not longer in project drivers')
def step_impl(context, driver_name):
    assert driver_name not in [driver.name for driver in context.project.drivers.list()], "TEST FAILED: Driver: {} found in project drivers list".format(driver_name)


================================================
File: tests/features/steps/execution_monitoring/test_execution_monitoring.py
================================================
import behave
import json
import time
import os


@behave.when(u'I push and deploy package with param "{on_reset}" in "{package_directory_path}"')
def step_impl(context, package_directory_path, on_reset):
    package_directory_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], package_directory_path)
    package_json_path = os.path.join(package_directory_path, 'package.json')

    with open(package_json_path, 'r') as f:
        package_json = json.load(f)

    if on_reset == 'None':
        package_json['services'][0].pop('onReset', None)
    else:
        package_json['services'][0]['onReset'] = on_reset

    with open(package_json_path, 'w') as f:
        json.dump(package_json, f)

    services, context.package = context.project.packages.deploy_from_file(project=context.project,
                                                                          json_filepath=package_json_path)

    context.service = services[0]
    context.to_delete_packages_ids.append(context.package.id)
    context.to_delete_services_ids.append(context.service.id)


@behave.when(u'I execute')
def step_impl(context):
    context.execution = context.service.execute(project_id=context.project.id)


@behave.when(u'I terminate execution')
def step_impl(context):
    context.execution.terminate()


@behave.then(u'Execution was terminated with error message "{message}"')
def step_impl(context, message):
    num_tries = 60
    interval = 7
    terminated = False

    for i in range(num_tries):
        time.sleep(interval)
        execution = context.service.executions.get(execution_id=context.execution.id)
        terminated = execution.to_terminate
        terminated = terminated and execution.latest_status['status'] == 'failed'
        terminated = terminated and message in execution.latest_status['message']
        if terminated:
            break

    assert terminated, f"TEST FAILED: after {round(num_tries * interval / 60, 1)} minutes"


@behave.then(u'Execution "{on_reset}" on timeout')
def step_impl(context, on_reset):
    time.sleep(context.service.execution_timeout + context.service.drain_time + 5)
    num_tries = 60
    interval = 7

    reset = False
    for _ in range(num_tries):
        execution = context.service.executions.get(execution_id=context.execution.id)
        for stat in execution.status:
            if on_reset == 'rerun' \
                    and stat['status'] == 'rerun' \
                    and 'Execution stopped due to runner timeout' in stat['message']:
                reset = True
            elif on_reset == 'failed' \
                    and stat['status'] == 'failed' \
                    and 'Execution stopped due to runner timeout' in stat['message']:
                reset = True
            if reset:
                break
        if reset:
            break
        time.sleep(interval)

    assert reset, f"TEST FAILED: after {round(num_tries * interval / 60, 1)} minutes"


@behave.when(u'I terminate an execution')
def step_impl(context):
    context.execution = context.service.executions.list().items[0]
    context.execution.terminate()


================================================
File: tests/features/steps/executions_repo/test_execution_rerun.py
================================================
import behave
import time
import os
import random
import json


@behave.then(u'I validate execution response params')
def step_impl(context):
    execution_json = context.dl.executions.get(execution_id=context.execution.id).to_json()
    for row in context.table:
        assert execution_json[row['key']] == row[
            'value'], f"TEST FAILED: Expected {row['value']}, Actual {execution_json[row['key']]}"


@behave.when(u'I rerun the execution')
def step_impl(context):
    context.execution = context.execution.rerun(False)


@behave.when(u'I rerun the execution with batch function')
def step_impl(context):
    filters = context.dl.Filters(field='id', values=[context.execution.id],
                                 operator=context.dl.FiltersOperations.IN,
                                 resource=context.dl.FiltersResource.EXECUTION)

    command = context.dl.executions.rerun_batch(filters=filters)
    assert command is not None, "Failed to rerun batch execution"
    assert command.status == 'success', "Failed to rerun batch execution"
    context.execution = context.dl.executions.get(execution_id=context.execution.id)


================================================
File: tests/features/steps/executions_repo/test_execution_validation.py
================================================
import behave
import time


@behave.then(u'I expect execution status progress include "{value}" in "{key}" with a frequency of "{frequency}"')
def step_impl(context, value, key, frequency):
    num_try = 30
    interval = 8
    success = False

    for i in range(num_try):
        time.sleep(interval)
        counter = 0
        execution = context.service.executions.get(execution_id=context.execution.id)
        statuses = execution.status
        for status in statuses:
            if eval(value) == status.get(key, None):
                counter += 1
                if counter == int(frequency):
                    success = True
                    break
    assert success, f"TEST FAILED: after {round(num_try * interval / 60, 1)} minutes"


================================================
File: tests/features/steps/executions_repo/test_executions_context.py
================================================
import behave


@behave.given(u'I append service to services')
def step_impl(context):
    if not hasattr(context, "services"):
        context.services = list()
    context.services.append(context.service)


@behave.when(u'I get the execution from project number {project_index}')
def step_impl(context, project_index):
    context.execution = context.projects[int(project_index) - 1].executions.get(execution_id=context.execution.id)


@behave.when(u'I get the execution from service number {project_index}')
def step_impl(context, project_index):
    context.execution = context.services[int(project_index) - 1].executions.get(execution_id=context.execution.id)


@behave.then(u'Execution Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.execution.project_id == context.projects[int(project_index)-1].id


@behave.then(u'Execution Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.execution.project.id == context.projects[int(project_index)-1].id


@behave.then(u'Execution Service_id is equal to service {service_index} id')
def step_impl(context, service_index):
    assert context.execution.service_id == context.services[int(service_index) - 1].id


@behave.then(u'Execution Service.id is equal to service {service_index} id')
def step_impl(context, service_index):
    assert context.execution.service.id == context.services[int(service_index) - 1].id


================================================
File: tests/features/steps/executions_repo/test_executions_create.py
================================================
import string
import behave
import time
import os
import random
import binascii
import io


@behave.when(u'I create an execution with "{input_type}"')
def step_impl(context, input_type):
    time.sleep(5)
    sync = None
    inputs = list()
    with_input_entity = False
    project_id = None

    if input_type != 'None':
        with_input_entity = True

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "sync":
            sync = param[1] == "True"
        if param[0] == "inputs":
            inputs = param[1].split(',')

    if with_input_entity:
        execution_inputs = list()

        for resource in inputs:
            if resource == 'Item':
                execution_inputs.append(
                    context.dl.FunctionIO(
                        type=resource,
                        value={
                            'item_id': context.item.id,
                            'dataset_id': context.dataset.id
                        },
                        name='item')
                )
            elif resource == 'Annotation':
                execution_inputs.append(
                    context.dl.FunctionIO(
                        type=resource,
                        value={
                            'item_id': context.item.id,
                            'dataset_id': context.dataset.id,
                            'annotation_id': context.annotation.id
                        },
                        name='annotation')
                )
            elif resource == 'Dataset':
                execution_inputs.append(
                    context.dl.FunctionIO(
                        type=resource,
                        value={
                            'dataset_id': context.dataset.id
                        },
                        name='dataset')
                )
            elif resource == 'Json':
                execution_inputs.append(
                    context.dl.FunctionIO(
                        type=resource,
                        value=context.execution_json_input,
                        name='config')
                )
            elif resource == 'e28':
                project_id = context.project.id
                context.item_id = "6500397081943346e28"
                sync = False
                execution_inputs.append(
                    context.dl.FunctionIO(
                        type=context.dl.PackageInputType.ITEMS,
                        value=[context.item_id],
                        name='items')
                )

        context.execution = context.service.executions.create(
            service_id=context.service.id,
            sync=sync,
            execution_input=execution_inputs,
            timeout=600,
            project_id=project_id
        )
    else:
        if inputs:
            resource = inputs[0]
        else:
            resource = 'no_input'

        if resource == 'Item':
            context.execution = context.service.executions.create(
                service_id=context.service.id,
                resource=resource,
                sync=sync,
                item_id=context.item.id,
                dataset_id=context.dataset.id,
            )
        elif resource == 'Dataset':
            context.execution = context.service.executions.create(
                service_id=context.service.id,
                resource=resource,
                sync=sync,
                dataset_id=context.dataset.id,
            )
        elif resource == 'Annotation':
            context.execution = context.service.executions.create(
                service_id=context.service.id,
                resource=resource,
                sync=sync,
                item_id=context.item.id,
                dataset_id=context.dataset.id,
                annotation_id=context.annotation.id
            )
        elif resource == 'no_input':
            context.execution = context.service.executions.create(
                service_id=context.service.id,
                project_id=context.project.id,
                sync=sync
            )

    if sync:
        assert context.execution.latest_status['status'] == 'success'


@behave.then(u"I receive an Execution entity")
def step_impl(context):
    assert isinstance(context.execution, context.dl.entities.Execution)


@behave.then(u'Execution was executed on "{resource_type}"')
def step_impl(context, resource_type):
    num_try = 60
    interval = 5
    success = False

    for i in range(num_try):
        time.sleep(interval)
        execution = context.service.executions.get(execution_id=context.execution.id)
        if execution.latest_status['status'] == 'success':
            success = True
            break
        if execution.latest_status['status'] == 'failed':
            success = False
            break
    assert success, f"TEST FAILED: after {round(num_try * interval / 60, 1)} minutes"


@behave.then(u'Execution output is "{execution_output}"')
def step_impl(context, execution_output):
    execution = context.service.executions.get(execution_id=context.execution.id)
    assert execution.output == execution_output, f"Wrong execution result Expected: {execution_output} got: {execution.output}"


@behave.then(u'Execution was executed and finished with status "{execution_status}"')
@behave.then(u'Execution was executed and finished with status "{execution_status}" and message "{message}"')
def step_impl(context, execution_status, message=None):
    success = False
    num_try = 70
    interval = 10
    for i in range(num_try):
        execution = context.service.executions.get(execution_id=context.execution.id)
        if execution.latest_status['status'] == execution_status:
            success = True
            if message:  # message is not None
                assert execution.latest_status['message'] == message
            break
        elif execution.latest_status['status'] != execution_status and execution.latest_status['status'] not in [
            'in-progress', 'inProgress', 'created', 'pending']:
            break
        time.sleep(interval)

    assert success, f"TEST FAILED: Execution status is {execution.latest_status['status']}, after {round(num_try * interval / 60, 1)} minutes"


@behave.given(u'I upload item in "{item_path}" to dataset')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.dataset.items.upload(local_path=item_path)


@behave.then(u'Execution input is a valid itemId')
def step_impl(context):
    execution = context.execution
    assert execution.input['items'] == [context.item_id]


@behave.given(u'A service that receives items input')
def step_impl(context):
    def run(items):
        return items

    context.service = context.project.services.deploy(
        service_name='items-input-{}'.format(''.join(random.choice(string.ascii_lowercase) for i in range(5))),
        func=run,
        runtime=context.dl.KubernetesRuntime(
            autoscaler=context.dl.KubernetesRabbitmqAutoscaler(
                min_replicas=0,
                max_replicas=0
            )
        )
    )
    context.to_delete_services_ids.append(context.service.id)


@behave.when(u'I upload item in "{item_path}"')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.dataset.items.upload(local_path=item_path)


@behave.when(u'I execute pipeline with input type "{input_type}"')
def step_impl(context, input_type):
    if input_type == "None":
        execution_input = None
    elif input_type == context.dl.PackageInputType.ITEM:
        execution_input = list()
        execution_input.append(context.dl.FunctionIO(
            type=context.dl.PackageInputType.ITEM,
            value={'item_id': context.item.id},
            name='item'))
    elif input_type == context.dl.PackageInputType.MODEL:
        execution_input = list()
        execution_input.append(context.dl.FunctionIO(
            type=context.dl.PackageInputType.MODEL,
            value={'model_id': context.model.id},
            name='model'))
    else:
        raise ValueError("input_type must be 'None' or 'Item'")

    context.execution = context.pipeline.execute(
        execution_input=execution_input)


@behave.when(u'I add a service to the DPK')
def step_impl(context):
    context.dpk.components.services.append({
        'name': 'test-service-{}'.format(str(random.randrange(0, 1000))),
        'moduleName': context.dpk.components.modules[0].name
    })


@behave.when(u'I run predict on the item with the model from the app')
def step_impl(context):
    models = context.project.models.list().items
    context.model = [model for model in models if model.app['id'] == context.app.id][0]
    context.model_predict_execution = context.model.predict(item_ids=[context.item.id])


@behave.when(u'I execute the app service')
def step_impl(context):
    services = context.project.services.list().items
    context.service = [service for service in services if service.app['id'] == context.app.id][0]
    module = context.dpk.components.modules[0]
    func = [f for f in module.functions if f.name == 'predict_items'][0]
    context.service_executions = context.service.execute(
        execution_input={'items': [context.item.id]},
        project_id=context.project.id,
        function_name=func.name
    )


@behave.then(u'Both executions should have app and driverId')
def step_impl(context):
    interval = 5
    max_wait = 5 * 60
    success = False
    start = time.time()
    executions = [context.model_predict_execution, context.service_executions]
    errors = []
    assertions = []

    def validate(statement, error_message):
        if not statement:
            errors.append(error_message)
        assertions.append(statement)

    while not success and time.time() - start < max_wait:
        assertions = []
        errors = []
        for execution in executions:
            execution = context.service.executions.get(execution_id=execution.id)
            validate(execution.driver_id is not None and execution.driver_id == context.service.driver_id,
                     f"driver_id is not equal to service driver_id for execution {execution.id}")
            validate(execution.app is not None and execution.app['id'] == context.app.id,
                     f"app id is not equal to context app id for execution {execution.id}")
            validate(execution.latest_status['status'] in ['success', 'failed'],
                     f"execution status is not success for execution {execution.id}")

        success = all(assertions)
        if success:
            break
        time.sleep(interval)

    assert success, f"TEST FAILED: after {max_wait} seconds. Errors: {errors}"


@behave.then(u'I use CRC to check original item in "{original_item_path}" and streamed item from new dataset are not corrupted')
def step_impl(context, original_item_path):
    original_item = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], original_item_path)
    item = context.dataset.items.get(filepath="/" + original_item_path)
    downloded_item = context.dataset.items.download(items=item.id,
                                                    save_locally=False)
    assert calculate_crc(original_item) == calculate_crc(downloded_item), f"TEST FAILED: files didn't pass crc test"


def calculate_crc(item):
    crc = 0
    if isinstance(item, io.BytesIO):
        item.seek(0)
        for chunk in iter(lambda: item.read(4096), b''):
            crc = binascii.crc32(chunk, crc)
        return crc & 0xFFFFFFFF
    if isinstance(item, str):
        try:
            with open(item, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b''):
                    crc = binascii.crc32(chunk, crc)
            return crc & 0xFFFFFFFF
        except FileNotFoundError:
            print(f"Error: File not found - {item}")
            return None
    print("Error: Unsupported input type. Provide a file path (str) or io.BytesIO object.")
    return None

================================================
File: tests/features/steps/executions_repo/test_executions_get.py
================================================
import time

import behave
import logging
import json
from operator import attrgetter


@behave.when(u"I get execution by id")
def step_impl(context):
    context.execution_get = context.service.executions.get(execution_id=context.execution.id)


@behave.then(u"I receive an Execution object")
def step_impl(context):
    assert isinstance(context.execution_get, context.dl.entities.Execution)


@behave.then(u"Execution received equals to execution created")
def step_impl(context):
    execution_get_json = context.execution_get.to_json()
    execution_json = context.execution.to_json()
    execution_get_json.pop('status')
    execution_json.pop('status')
    execution_get_json.pop('latestStatus')
    execution_json.pop('latestStatus')
    execution_get_json.pop('statusLog')
    execution_json.pop('statusLog')
    execution_get_json.pop('updatedAt')
    execution_json.pop('updatedAt')

    if execution_get_json != execution_json:
        logging.error(
            'FAILED: response json is:\n{}\n\nto_json is:\n{}'.format(json.dumps(context.execution_get.to_json(),
                                                                                 indent=2),
                                                                      json.dumps(context.execution.to_json(),
                                                                                 indent=2)))
        assert False


@behave.when(u'I get service execution by "{resource}"')
def step_impl(context, resource):
    resource_att = resource.split(".")
    filters = context.dl.Filters()
    filters.add(field="resource.type", values=resource_att[0])
    filters.add(field="resource.{}".format(resource_att[1]), values="{}".format(attrgetter(resource)(context)))
    filters.resource = context.dl.FiltersResource.EXECUTION
    for i in range(10):
        execution_list = context.service.executions.list(filters=filters)
        if execution_list.items_count != 0:
            context.execution = execution_list.items[0]
            break
        else:
            time.sleep(10)


@behave.then(u"I validate execution params")
def step_impl(context):
    for row in context.table:
        value = row['value']
        execution_val = attrgetter(row['key'])(context.execution)

        if row['value'] == "current_user":
            value = context.dl.info()['user_email']
        elif row['value'] == "piper":
            value = ["piper@dataloop.ai", "pipelines@dataloop.ai"]
        assert execution_val in value, "TEST FAILED: Expected to get {}, Actual got {}".format(value, execution_val)


@behave.then(u"I validate params in executions list")
def step_impl(context):
    for execution in context.execution_list.items:
        for row in context.table:
            if "item.id" in row['value']:
                row['value'].replace('item.id', context.item.id)
            assert attrgetter(row['key'])(execution) == eval(row['value']), \
                "TEST FAILED: Expected to get {}, Actual got {}".format(eval(row['value']),
                                                                        attrgetter(row['key'])(context.execution))


================================================
File: tests/features/steps/executions_repo/test_executions_list.py
================================================
import time
import behave
from operator import attrgetter
import dtlpy as dl

@behave.when(u"I list service executions")
def step_impl(context):
    context.execution_list = context.service.executions.list()


@behave.then(u'I receive a list of "{count}" executions')
def step_impl(context, count):
    assert len(context.execution_list.items) == int(count), f"TEST FAILED: Expected {count} , Actual {len(context.execution_list.items)}"
    if int(count) > 0:
        for page in context.execution_list:
            for execution in page:
                assert isinstance(execution, context.dl.entities.Execution)


@behave.then(u'I wait until I receive a list of "{count}" executions')
def step_impl(context, count):
    interval = 5
    num_tries = 10
    for i in range(num_tries):
        context.execution_list = context.service.executions.list()
        if len(context.execution_list.items) == int(count):
            break
        else:
            context.execution_list = None
            time.sleep(interval)
    assert len(context.execution_list.items) == int(count), f"TEST FAILED: Expected {count} , Actual {len(context.execution_list.items)}"
    if int(count) > 0:
        for page in context.execution_list:
            for execution in page:
                assert isinstance(execution, context.dl.entities.Execution)


@behave.when(u'I create FAAS query filter with params')
def step_impl(context):
    """
    Search keywords : executions list with filter
    """
    context.filters = context.dl.Filters()

    params = dict()
    for row in context.table:
        if "id" in row['key'] or "name" in row['key'] or "Id" in row['key']:
            context.filters.add(field=row['key'], values=attrgetter(row['key'])(context))
        else:
            params[row['key']] = row['value']

    if params.get("resource"):
        context.filters.resource = params['resource']
    if params.get("updatedAt.gt"):
        context.filters.add(field="updatedAt", values=attrgetter(params['updatedAt.gt'])(context), operator=dl.FiltersOperations.GREATER_THAN)
    if params.get("updatedAt.lt"):
        context.filters.add(field="updatedAt", values=attrgetter(params['updatedAt.lt'])(context), operator=dl.FiltersOperations.LESS_THAN)


@behave.when(u"I list executions with filters")
def step_impl(context):
    context.execution_list = context.project.executions.list(filters=context.filters)


@behave.then(u'I validate pipeline executions params')
def step_impl(context):
    """
    Search keywords : pipeline executions list
    """
    filters = dl.Filters()
    filters.resource = "executions"
    filters.add(field="pipeline.id", values=context.pipeline.id)
    filters.sort_by(field='createdAt', value=dl.FiltersOrderByDirection.ASCENDING)
    index = 0
    for execution in context.project.executions.list(filters=filters).items:
        if ".id" in context.table[index]['value']:
            input_str = context.table[index]['value'].replace("item.id", context.item.id)
            input_str = input_str.replace("dataset.id", context.dataset.id)
        else:
            input_str = context.table[index]['value']
        expected_input = eval(input_str)
        execution_input = attrgetter(context.table[index]['key'])(execution)
        assert execution_input == expected_input, "TEST FAILED: Expected : {}, Actual got : {}".format(expected_input, execution_input)
        index += 1


================================================
File: tests/features/steps/executions_repo/test_executions_multiple.py
================================================
import behave
import time


@behave.when(u"I create an execution for all functions")
def step_impl(context):
    context.first_module_first_function_execution = context.first_service.executions.create(
        service_id=context.first_service.id,
        sync=True,
        stream_logs=False,
        return_output=False,
        execution_input=context.dl.FunctionIO(
            type='Item',
            value={'item_id': context.item.id},
            name='item'),
        function_name='first',
        timeout=300)
    context.first_module_second_function_execution = context.first_service.executions.create(
        service_id=context.first_service.id,
        sync=True,
        stream_logs=False,
        return_output=False,
        execution_input=context.dl.FunctionIO(
            type='Item',
            value={'item_id': context.item.id},
            name='item'),
        function_name='second',
        timeout=300)
    context.second_module_first_function_execution = context.second_service.executions.create(
        service_id=context.second_service.id,
        sync=True,
        stream_logs=False,
        return_output=False,
        execution_input=context.dl.FunctionIO(
            type='Item',
            value={'item_id': context.item.id},
            name='item'),
        function_name='first',
        timeout=300)
    context.second_module_second_function_execution = context.second_service.executions.create(
        service_id=context.second_service.id,
        sync=True,
        stream_logs=False,
        return_output=False,
        execution_input=context.dl.FunctionIO(
            type='Item',
            value={'item_id': context.item.id},
            name='item'),
        function_name='second',
        timeout=300)


@behave.then(u'Execution was executed on item for all functions')
def step_impl(context):
    # get new item
    item = context.dl.items.get(item_id=context.item.id)
    try:
        success = item.metadata['user']['first_module_first_function'] and \
                  item.metadata['user']['first_module_second_function'] and \
                  item.metadata['user']['second_module_first_function'] and \
                  item.metadata['user']['second_module_second_function']
    except:
        success = False
    assert success


================================================
File: tests/features/steps/features_vectors/test_features_create.py
================================================
import behave
import random
import string


@behave.given(u'There are no feature sets')
def step_impl(context):
    try:
        for f in context.project.feature_sets.list():
            f.delete()
        assert len(context.project.feature_sets.list()) == 0
    except Exception as error:
        assert 'not found' in error.args[1]


@behave.when(u'I create a feature sets with a random name')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    feature_sets_name = 'random_{}'.format(rand_str)
    context.feature_set = context.project.feature_sets.create(name=feature_sets_name,
                                                              set_type='model',
                                                              entity_type='item',
                                                              size=3)
    context.to_delete_feature_set_ids.append(context.feature_set.id)


@behave.when(u'I create a feature')
def step_impl(context):
    context.feature = context.feature_set.features.create(value=[0, 2, 15],
                                                          feature_set_id=context.feature_set.id,
                                                          entity_id=context.item.id,
                                                          version='1.0.0',
                                                          project_id=context.project.id)
    context.to_delete_feature_ids.append(context.feature.id)


@behave.then(u'FeatureSet object should be exist')
def step_impl(context):
    assert isinstance(context.feature_set, context.dl.entities.FeatureSet)


@behave.then(u'Feature object should be exist')
def step_impl(context):
    assert isinstance(context.feature, context.dl.entities.Feature)


@behave.then(u'Feature object should be exist in host')
def step_impl(context):
    feature_get = context.project.features.get(feature_id=context.feature.id)
    assert feature_get.to_json() == context.feature.to_json()


@behave.then(u'FeatureSet object should be exist in host')
def step_impl(context):
    feature_set_get = context.project.feature_sets.get(feature_set_id=context.feature_set.id)
    assert feature_set_get.to_json() == context.feature_set.to_json()


@behave.then(u'FeatureSet list have len 1')
def step_impl(context):
    assert len(context.project.feature_sets.list()) == 1


@behave.then(u'Feature list have len 1')
def step_impl(context):
    assert len(context.feature_set.features.list()) == 1


@behave.when(u'I get feature sets')
def step_impl(context):
    context.feature_set_get = context.project.feature_sets.get(feature_set_id=context.feature_set.id)


@behave.then(u'It is equal to feature sets created')
def step_impl(context):
    assert context.feature_set.to_json() == context.feature_set_get.to_json()


@behave.when(u'I get feature')
def step_impl(context):
    context.feature_get = context.project.features.get(feature_id=context.feature.id)


@behave.then(u'It is equal to feature created')
def step_impl(context):
    assert context.feature.to_json() == context.feature_get.to_json()


================================================
File: tests/features/steps/features_vectors/test_features_delete.py
================================================
import behave
import random
import string


@behave.then(u'features set does not exists')
def step_impl(context):
    try:
        context.project.feature_sets.get(feature_set_id=context.feature_set.id)
    except context.dl.exceptions.NotFound:
        # good results
        pass
    except:
        # features still exists
        assert False


@behave.then(u'features does not exists')
def step_impl(context):
    try:
        context.feature_set.features.get(feature_id=context.feature.id)
    except context.dl.exceptions.NotFound:
        # good results
        pass
    except:
        # features still exists
        assert False


@behave.when(u'I delete the features set that was created')
def step_impl(context):
    context.project.feature_sets.delete(feature_set_id=context.feature_set.id)


@behave.when(u'I delete the features that was created')
def step_impl(context):
    context.feature_set.features.delete(feature_id=context.feature.id)


================================================
File: tests/features/steps/filters_entity/filters_interface.py
================================================
import behave
from operator import attrgetter
import dtlpy as dl


@behave.given(u'I init Filters() using the given params')
def step_impl(context):
    context.resource = context.dl.FiltersResource.ITEM

    filters_resource_list = {
        "ITEM": context.dl.FiltersResource.ITEM,
        "ANNOTATION": context.dl.FiltersResource.ANNOTATION,
        "EXECUTION": context.dl.FiltersResource.EXECUTION,
        "PACKAGE": context.dl.FiltersResource.PACKAGE,
        "DPK": context.dl.FiltersResource.DPK,
        "APP": context.dl.FiltersResource.APP,
        "SERVICE": context.dl.FiltersResource.SERVICE,
        "TRIGGER": context.dl.FiltersResource.TRIGGER,
        "MODEL": context.dl.FiltersResource.MODEL,
        "WEBHOOK": context.dl.FiltersResource.WEBHOOK,
        "RECIPE": context.dl.FiltersResource.RECIPE,
        "DATASET": context.dl.FiltersResource.DATASET,
        "ONTOLOGY": context.dl.FiltersResource.ONTOLOGY,
        "TASK": context.dl.FiltersResource.TASK,
        "PIPELINE": context.dl.FiltersResource.PIPELINE,
        "PIPELINE_EXECUTION": context.dl.FiltersResource.PIPELINE_EXECUTION,
        "COMPOSITION": context.dl.FiltersResource.COMPOSITION,
        "FEATURE": context.dl.FiltersResource.FEATURE,
        "FEATURE_SET": context.dl.FiltersResource.FEATURE_SET,
        "ORGANIZATIONS": context.dl.FiltersResource.ORGANIZATIONS,
        "DRIVERS": context.dl.FiltersResource.DRIVERS,
        "SETTINGS": context.dl.FiltersResource.SETTINGS,
        "RESOURCE_EXECUTION": context.dl.FiltersResource.RESOURCE_EXECUTION
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "resource":
            context.resource = filters_resource_list[parameter.cells[1]]

    context.filters = context.dl.Filters(resource=context.resource)


@behave.when(u'I call Filters.add() using the given params')
def step_impl(context):
    context.field = None
    context.values = None
    context.operator = None
    context.method = None

    filters_operations_list = {
        "OR": context.dl.FiltersOperations.OR,
        "AND": context.dl.FiltersOperations.AND,
        "IN": context.dl.FiltersOperations.IN,
        "NOT_EQUAL": context.dl.FiltersOperations.NOT_EQUAL,
        "EQUAL": context.dl.FiltersOperations.EQUAL,
        "GREATER_THAN": context.dl.FiltersOperations.GREATER_THAN,
        "LESS_THAN": context.dl.FiltersOperations.LESS_THAN,
        "EXISTS": context.dl.FiltersOperations.EXISTS,
        "MATCH": context.dl.FiltersOperations.EXISTS
    }

    filters_method_list = {
        "OR": context.dl.FiltersMethod.OR,
        "AND": context.dl.FiltersMethod.AND,
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "field":
            context.field = parameter.cells[1]

        if parameter.cells[0] == "values":
            if ".id" in parameter.cells[1]:
                context.values = attrgetter(parameter.cells[1])(context)
            else:
                context.values = parameter.cells[1]

        if parameter.cells[0] == "operator":
            if parameter.cells[1] == 'IN' and hasattr(context, 'values'):
                if not isinstance(context.values, list):
                    context.values = [context.values]
            context.operator = filters_operations_list[parameter.cells[1]]

        if parameter.cells[0] == "method":
            context.method = filters_method_list[parameter.cells[1]]

    context.filters.add(
        field=context.field,
        values=context.values,
        operator=context.operator,
        method=context.method,
    )


@behave.when(u'I call Filters.add_join() using the given params')
def step_impl(context):
    context.field = None
    context.values = None
    context.operator = None
    context.method = context.dl.FiltersMethod.AND

    filters_operations_list = {
        "OR": context.dl.FiltersOperations.OR,
        "AND": context.dl.FiltersOperations.AND,
        "IN": context.dl.FiltersOperations.IN,
        "NOT_EQUAL": context.dl.FiltersOperations.NOT_EQUAL,
        "EQUAL": context.dl.FiltersOperations.EQUAL,
        "GREATER_THAN": context.dl.FiltersOperations.GREATER_THAN,
        "LESS_THAN": context.dl.FiltersOperations.LESS_THAN,
        "EXISTS": context.dl.FiltersOperations.EXISTS,
        "MATCH": context.dl.FiltersOperations.EXISTS
    }

    filters_method_list = {
        "OR": context.dl.FiltersMethod.OR,
        "AND": context.dl.FiltersMethod.AND,
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "field":
            context.field = parameter.cells[1]

        if parameter.cells[0] == "values":
            if parameter.cells[1] == "True":
                context.values = True
            elif parameter.cells[1] == "False":
                context.values = False
            else:
                context.values = parameter.cells[1]

        if parameter.cells[0] == "operator":
            context.operator = filters_operations_list[parameter.cells[1]]

        if parameter.cells[0] == "method":
            context.method = filters_method_list[parameter.cells[1]]

    context.filters.add_join(
        field=context.field,
        values=context.values,
        operator=context.operator,
        method=context.method,
    )


@behave.when(u'I call Filters.pop() using the given params')
def step_impl(context):
    context.field = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "field":
            context.field = parameter.cells[1]

    context.filters.pop(
        field=context.field
    )


@behave.when(u'I call Filters.pop_join() using the given params')
def step_impl(context):
    context.field = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "field":
            context.field = parameter.cells[1]

    context.filters.pop_join(
        field=context.field
    )


@behave.when(u'I call Filters.has_field() using the given params')
def step_impl(context):
    context.field = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "field":
            context.field = parameter.cells[1]

    context.has_field_result = context.filters.has_field(
        field=context.field
    )


@behave.when(u'I call Filters.sort_by() using the given params')
def step_impl(context):
    context.field = None
    context.value = context.dl.FiltersOrderByDirection.ASCENDING

    filters_order_by_direction_list = {
        "DESCENDING": context.dl.FiltersOrderByDirection.DESCENDING,
        "ASCENDING": context.dl.FiltersOrderByDirection.ASCENDING
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "field":
            context.field = parameter.cells[1]

        if parameter.cells[0] == "value":
            context.value = filters_order_by_direction_list[parameter.cells[1]]

    context.filters.add(
        field=context.field,
        value=context.value,
    )


@behave.when(u'I call Filters.reset()')
def step_impl(context):
    context.filters.reset()


@behave.when(u'I create dataset filters by metadata - "{field}" = "{value}"')
def step_impl(context, field, value):
    context.filters = context.dl.Filters(resource=dl.FiltersResource.DATASET)
    metadata_field = f"metadata.{field}"
    context.filters.add(field=metadata_field, values=value)


================================================
File: tests/features/steps/filters_entity/test_filters.py
================================================
import behave
import json
import os
from PIL import Image
import io
import random
from multiprocessing.pool import ThreadPool
import logging
import datetime
from operator import attrgetter


@behave.given(u'There are items, path = "{item_path}"')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    dataset = context.dataset
    dl = context.dl

    # import dtlpy as dl
    # assert isinstance(dataset, dl.entities.Dataset)

    directory = dict()
    annotated_label = dict()
    annotated_type = dict()
    name_dir = dict()
    metadata = dict()

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == 'directory':
            if param[1] != 'None':
                directory = json.loads(param[1])
        elif param[0] == 'annotated_label':
            if param[1] != 'None':
                annotated_label = json.loads(param[1])
        elif param[0] == 'annotated_type':
            if param[1] != 'None':
                annotated_type = json.loads(param[1])
        elif param[0] == 'name':
            if param[1] != 'None':
                name_dir = json.loads(param[1])
        elif param[0] == 'metadata':
            if param[1] != 'None':
                metadata = json.loads(param[1])

    image = Image.open(item_path)
    buffer = io.BytesIO()
    image.save(buffer, format='JPEG')

    def upload(i_buffer, remote_path, do_next=None, i_ann_type=None, i_label=None, i_meta=None):
        item = dataset.items.upload(local_path=i_buffer, remote_path=remote_path)

        if do_next == 'directory':
            assert isinstance(item, dl.Item)
        elif do_next == 'label':
            assert isinstance(item, dl.Item)
            ann = dl.Annotation.new(annotation_definition=dl.Point(y=100, x=200, label=i_label),
                                    item=item)
            ann.upload()
        elif do_next == 'type':
            assert isinstance(item, dl.Item)

            if i_ann_type == 'box':
                bottom = random.randrange(0, 500)
                top = random.randrange(0, 500)
                right = random.randrange(0, 500)
                left = random.randrange(0, 500)
                ann = dl.Annotation.new(
                    annotation_definition=dl.Box(label="test_label", bottom=bottom, top=top, left=left, right=right),
                    item=item)

            elif i_ann_type == 'polygon':
                geo = [
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                ]
                ann = dl.Annotation.new(
                    annotation_definition=dl.Polygon(label="test_label", geo=geo),
                    item=item)
            else:
                return

            ann.upload()
        elif do_next == 'metadata':
            assert isinstance(item, dl.Item)

            # put metadata field
            i_metadata_field = i_meta.split('.')
            i_field_pointer = item.metadata
            for field in i_metadata_field:
                if field not in i_field_pointer:
                    if i_metadata_field.index(field) == len(i_metadata_field) - 1:
                        i_field_pointer[field] = True
                    else:
                        i_field_pointer[field] = dict()
                        i_field_pointer = i_field_pointer[field]
                else:
                    i_field_pointer = i_field_pointer[field]
            item.update()

    pool = ThreadPool(processes=32)

    # directory
    name = 'dir.jpg'
    for dir_path in directory:
        for i in range(int(directory[dir_path])):
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}'.format(i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_path
                             })

    # name
    name = '.jpg'
    for item_name in name_dir:
        for i in range(int(name_dir[item_name])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}{}'.format(random.randrange(0, 100), item_name, random.randrange(0, 100),
                                              name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name
                             })

    # annotated label
    name = 'label.jpg'
    for label in annotated_label:
        for i in range(int(annotated_label[label])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}'.format(label, i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name,
                                 'do_next': 'label',
                                 'i_label': label
                             })

    # annotated type
    name = 'type.jpg'
    for ann_type in annotated_type:
        for i in range(int(annotated_type[ann_type])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}'.format(ann_type, i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name,
                                 'do_next': 'type',
                                 'i_ann_type': ann_type
                             })

    # metadata
    name = 'metadata.jpg'
    for meta in metadata:
        for i in range(int(metadata[meta])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}'.format(meta.replace('.', '_'), i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name,
                                 'do_next': 'metadata',
                                 'i_meta': meta
                             })

    pool.close()
    pool.join()
    pool.terminate()


@behave.given(u'There are items for another dataset, path = "{item_path}"')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    dataset = context.second_dataset
    dl = context.dl

    # import dtlpy as dl
    # assert isinstance(dataset, dl.entities.Dataset)

    directory = dict()
    annotated_label = dict()
    annotated_type = dict()
    name_dir = dict()
    metadata = dict()

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == 'directory':
            if param[1] != 'None':
                directory = json.loads(param[1])
        elif param[0] == 'annotated_label':
            if param[1] != 'None':
                annotated_label = json.loads(param[1])
        elif param[0] == 'annotated_type':
            if param[1] != 'None':
                annotated_type = json.loads(param[1])
        elif param[0] == 'name':
            if param[1] != 'None':
                name_dir = json.loads(param[1])
        elif param[0] == 'metadata':
            if param[1] != 'None':
                metadata = json.loads(param[1])

    image = Image.open(item_path)
    buffer = io.BytesIO()
    image.save(buffer, format='JPEG')

    def upload(i_buffer, remote_path, do_next=None, i_ann_type=None, i_label=None, i_meta=None):
        item = dataset.items.upload(local_path=i_buffer, remote_path=remote_path)

        if do_next == 'directory':
            assert isinstance(item, dl.Item)
        elif do_next == 'label':
            assert isinstance(item, dl.Item)
            ann = dl.Annotation.new(annotation_definition=dl.Point(y=100, x=200, label=i_label),
                                    item=item)
            ann.upload()
        elif do_next == 'type':
            assert isinstance(item, dl.Item)

            if i_ann_type == 'box':
                bottom = random.randrange(0, 500)
                top = random.randrange(0, 500)
                right = random.randrange(0, 500)
                left = random.randrange(0, 500)
                ann = dl.Annotation.new(
                    annotation_definition=dl.Box(label="test_label", bottom=bottom, top=top, left=left, right=right),
                    item=item)

            elif i_ann_type == 'polygon':
                geo = [
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                    (random.randrange(0, 500), random.randrange(0, 500)),
                ]
                ann = dl.Annotation.new(
                    annotation_definition=dl.Polygon(label="test_label", geo=geo),
                    item=item)
            else:
                return

            ann.upload()
        elif do_next == 'metadata':
            assert isinstance(item, dl.Item)

            # put metadata field
            i_metadata_field = i_meta.split('.')
            i_field_pointer = item.metadata
            for field in i_metadata_field:
                if field not in i_field_pointer:
                    if i_metadata_field.index(field) == len(i_metadata_field) - 1:
                        i_field_pointer[field] = True
                    else:
                        i_field_pointer[field] = dict()
                        i_field_pointer = i_field_pointer[field]
            item.update()

    pool = ThreadPool(processes=32)

    # directory
    name = 'dir.jpg'
    for dir_path in directory:
        for i in range(int(directory[dir_path])):
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}'.format(i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_path
                             })

    # name
    name = '.jpg'
    for item_name in name_dir:
        for i in range(int(name_dir[item_name])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}{}'.format(random.randrange(0, 100), item_name, random.randrange(0, 100),
                                              name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name
                             })

    # annotated label
    name = 'label.jpg'
    for label in annotated_label:
        for i in range(int(annotated_label[label])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}'.format(label, i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name,
                                 'do_next': 'label',
                                 'i_label': label
                             })

    # annotated type
    name = 'type.jpg'
    for ann_type in annotated_type:
        for i in range(int(annotated_type[ann_type])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}'.format(ann_type, i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name,
                                 'do_next': 'type',
                                 'i_ann_type': ann_type
                             })

    # metadata
    name = 'metadata.jpg'
    for meta in metadata:
        for i in range(int(metadata[meta])):
            dir_name = '/'
            for j in range(random.randrange(0, 5)):
                dir_name = '{}{}/'.format(dir_name, random.randrange(0, 1000))

            buffer = io.BytesIO()
            image.save(buffer, format='JPEG')

            buffer.name = '{}_{}_{}'.format(meta.replace('.', '_'), i, name)

            pool.apply_async(func=upload,
                             kwds={
                                 'i_buffer': buffer,
                                 'remote_path': dir_name,
                                 'do_next': 'metadata',
                                 'i_meta': meta
                             })

    pool.close()
    pool.join()
    pool.terminate()


@behave.when(u'I create filters')
def step_impl(context):
    context.filters = context.dl.Filters()


@behave.when(u'I add field "{field}" with values "{values}" and operator "{operator}"')
def step_impl(context, field, values, operator):
    if operator == 'None':
        operator = None

    if values == 'True':
        values = True
    elif values == 'False':
        values = False
    elif values == "timestamp":
        if not hasattr(context, 'start_date'):
            assert False, "Need to create timestamp before"
        if operator == 'gt':
            values = context.start_date
        else:
            values = context.end_date
    elif values.startswith('{'):
        values = json.loads(values)

    if field.startswith('{'):
        field = json.loads(field)
    elif field == "metadata.system.size":
        values = int(values)

    if operator == "in":
        if "." in values:
            values = attrgetter(values)(context)
        values = values.split(",")

    context.filters.add(field=field, values=values, operator=operator)


@behave.when(u'I join field "{field}" with values "{values}" and operator "{operator}"')
def step_impl(context, field, values, operator):
    if operator == 'None':
        operator = None

    if values == 'True':
        values = True
    elif values == 'False':
        values = False
    elif values.startswith('{'):
        values = json.loads(values)

    if field.startswith('{'):
        field = json.loads(field)

    if operator == "in":
        values = values.split(",")

    context.filters.add_join(field=field, values=values, operator=operator)


@behave.when(u'I list items with filters')
def step_impl(context):
    context.items_list = context.dataset.items.list(filters=context.filters)


@behave.then(u'I receive "{count}" items')
def step_impl(context, count):
    if len(context.items_list.items) != int(count):
        logging.info('filter: {}'.format(context.filters.prepare()))
        logging.error('Filtering error. expected item count: {}. received item count:{}'.format(int(count),
                                                                                                context.items_list.items_count))
        assert False


@behave.when(u'I update items with filters, field "{field}"')
def step_impl(context, field):
    updated_value = {
        'metadata': {
            'user': {
                field: True
            }
        }
    }
    context.dataset.items.update(filters=context.filters, update_values=updated_value)


@behave.when(u'I delete items with filters')
def step_impl(context):
    context.dataset.items.delete(filters=context.filters)


@behave.when(u'I convert "{date_time}" days ago to timestamp')
def step_impl(context, date_time):
    num = int(date_time)
    context.start_date = (datetime.date.today() - datetime.timedelta(days=num)).isoformat()
    context.end_date = (datetime.date.today() + datetime.timedelta(days=3)).isoformat()


@behave.when(u'I use custom filter for Specific task and status from today')
def step_impl(context):
    if not hasattr(context, 'start_date'):
        assert False, "Need to create timestamp before"

    field = 'metadata.system.refs'
    values = {
        "id": context.task.id,
        "type": "task",
        "metadata":
            {
                "status": {'${}'.format(context.dl.FiltersOperations.EQUAL.value): "completed"},
                "timestamp": {'${}'.format(context.dl.FiltersOperations.GREATER_THAN.value): context.start_date}
            }
    }

    operator = context.dl.FiltersOperations.MATCH

    context.filters.add(field=field, values=values, operator=operator)


@behave.then(u'I add attribute to items with box annotations')
def step_impl(context):
    context.filters = context.dl.Filters()
    context.filters.add_join(field='type', values='box')
    for page in context.dataset.items.list(filters=context.dl.Filters(field='annotated', values=True)):
        for item in page:
            for annotation in item.annotations.list():
                annotation.attributes = dict()
                annotation.update(True)

    for page in context.dataset.items.list(filters=context.filters):
        for item in page:
            for annotation in item.annotations.list():
                annotation.attributes['1'] = "attr1"
                annotation.update(True)


@behave.when(u'I add "{resource}" filter with "{field}" and "{values}"')
def step_impl(context, resource, field, values):

    context.filters.resource = resource
    if "." in values:
        try:
            context.filters.add(field=field, values=attrgetter(values)(context))

        except Exception as e:
            context.error = e
    else:
        context.filters.add(field=field, values=values)


================================================
File: tests/features/steps/item_collections/test_item_collections.py
================================================
import os
import behave
import dtlpy as dl
from dtlpy import exceptions

@behave.when(u'I create a collection with the name "{collection_name}"')
def step_impl(context, collection_name):
    try:
        context.collection = context.dataset.collections.create(name=collection_name)
        assert context.collection.name == collection_name
    except exceptions.PlatformException as e:
        context.error_message = f"Failed to create collection: {str(e)}"
        context.success = False
        raise

@behave.then(u'The collection "{collection_name}" is created successfully')
def step_impl(context, collection_name):
    collections = context.dataset.collections.list_all_collections()
    assert any(c["name"] == collection_name for c in collections), f"Collection '{collection_name}' not found!"

@behave.given(u'I have an existing collection named "{collection_name}"')
def step_impl(context, collection_name):
    try:
        context.collection = context.dataset.collections.create(name=collection_name)
    except exceptions.PlatformException as e:
        if "already exists" in str(e):
            collections = context.dataset.collections.list_all_collections()
            context.collection = next((c for c in collections if c.name == collection_name), None)
            assert context.collection is not None, f"Collection '{collection_name}' could not be retrieved."

@behave.when(u'I try to create a collection with the name "{collection_name}"')
def step_impl(context, collection_name):
    try:
        context.dataset.collections.create(name=collection_name)
        context.success = True
    except ValueError as e:
        context.error_message = str(e)
        context.success = False
    except exceptions.PlatformException as e:
        context.error_message = str(e)
        context.success = False

@behave.then(u'I receive an error stating that the collection name already exists')
def step_impl(context):
    assert not context.success
    assert "already exists" in context.error_message

@behave.when(u'I update the collection name to "{new_name}"')
def step_impl(context, new_name):
    context.updated_collection = context.dataset.collections.update(
        collection_name=context.collection.name,
        new_name=new_name
    )
    assert context.updated_collection.name == new_name

@behave.then(u'The collection name is updated to "{new_name}"')
def step_impl(context, new_name):
    collections = context.dataset.collections.list_all_collections()
    assert any(c['name'] == new_name for c in collections), f"Collection '{new_name}' not found!"

@behave.when(u'I delete the collection "{collection_name}"')
def step_impl(context, collection_name):
    context.dataset.collections.delete(collection_name=collection_name)

@behave.then(u'The collection "{collection_name}" is deleted successfully')
def step_impl(context, collection_name):
    collections = context.dataset.collections.list_all_collections()
    assert not any(c['name'] == collection_name for c in collections), f"Collection '{collection_name}' still exists!"

@behave.when(u'I clone the collection "{collection_name}"')
def step_impl(context, collection_name):
    context.cloned_collection = context.dataset.collections.clone(collection_name=collection_name)

@behave.then(u'A new collection with the name "{expected_name}" is created')
def step_impl(context, expected_name):
    assert context.cloned_collection.name == expected_name

@behave.when(u'I list all collections')
def step_impl(context):
    context.collections = context.dataset.collections.list_all_collections()

@behave.then(u'I receive a list of all collections with their names and keys')
def step_impl(context):
    assert isinstance(context.collections, list), "Expected a list of collections!"
    assert all("key" in c and "name" in c for c in context.collections), "Each collection must have 'key' and 'name' keys!"

@behave.given(u'I have an item in the dataset')
def step_impl(context):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], "label1.png")
    context.item = context.dataset.items.upload(local_path=local_path)

@behave.when(u'I assign the item to the collection "{collection_name}"')
def step_impl(context, collection_name):
    collections = context.dataset.collections.list_all_collections()
    if not any(c["name"] == collection_name for c in collections):
        context.dataset.collections.create(name=collection_name)
    context.item.assign_collection(collections=[collection_name])

@behave.then(u'The item is assigned to the collection "{collection_name}"')
def step_impl(context, collection_name):
    context.item = context.dataset.items.get(item_id=context.item.id)
    item_collections = context.item.list_collections()
    print(f"Item collections: {item_collections}")  # Debug output for troubleshooting
    assert any(c["name"] == collection_name for c in item_collections), (
        f"Item is not assigned to collection '{collection_name}'!"
    )

@behave.when(u'I unassign the item from the collection "{collection_name}"')
def step_impl(context, collection_name):
    context.item.unassign_collection(collections=[collection_name])

@behave.then(u'The item is no longer assigned to the collection "{collection_name}"')
def step_impl(context, collection_name):
    item_collections = context.item.list_collections()
    assert collection_name not in item_collections

@behave.when(u'I list unassigned items')
def step_impl(context):
    context.unassigned_items = context.dataset.collections.list_all_collections_unassigned_items()

@behave.then(u'I receive a list of item IDs that are not part of any collection')
def step_impl(context):
    assert isinstance(context.unassigned_items, list)
    for item in context.unassigned_items:
        assert not item.list_collections()

@behave.given(u'I have multiple collections in the dataset')
def step_impl(context):
    collection_names = ["Justice League", "Avengers", "X-Men"]
    context.collections = []
    for name in collection_names:
        try:
            collection = context.dataset.collections.create(name=name)
            context.collections.append(collection)
        except exceptions.PlatformException as e:
            if "already exists" in str(e):
                collections = context.dataset.collections.list_all_collections()
                collection = next((c for c in collections if c.name == name), None)
                if collection:
                    context.collections.append(collection)
    assert len(context.collections) == len(collection_names)

@behave.when(u'I list all unassigned items')
def step_impl(context):
    context.unassigned_items = context.dataset.collections.list_unassigned_items()

@behave.then(u'The unassigned items list is accurate')
def step_impl(context):
    assigned_item_ids = {
        item.id
        for collection in context.dataset.collections.list_all_collections()
        for item in collection.items
    }
    unassigned_item_ids = {item.id for item in context.unassigned_items}
    all_item_ids = {item.id for item in context.dataset.items.list()}

    assert assigned_item_ids.isdisjoint(unassigned_item_ids)
    assert assigned_item_ids.union(unassigned_item_ids) == all_item_ids

================================================
File: tests/features/steps/item_entity/item_entity_interface.py
================================================
import behave
import os
import json
from time import sleep


@behave.given(u'I call Item.download() using the given params')
def step_impl(context):
    context.local_path = None
    context.file_types = None
    context.save_locally = True
    context.to_array = False
    context.annotation_options = None
    context.overwrite = False
    context.to_items_folder = True
    context.thickness = 1
    context.with_text = False
    context.annotation_filters = None
    context.alpha = 1
    context.export_version = context.dl.ExportVersion.V1

    annotation_options_list = {
        "JSON": context.dl.VIEW_ANNOTATION_OPTIONS_JSON,
        "MASK": context.dl.VIEW_ANNOTATION_OPTIONS_MASK,
        "INSTANCE": context.dl.VIEW_ANNOTATION_OPTIONS_INSTANCE,
        "ANNOTATION_ON_IMAGE": context.dl.VIEW_ANNOTATION_OPTIONS_ANNOTATION_ON_IMAGE,
        "VTT": context.dl.VIEW_ANNOTATION_OPTIONS_VTT,
        "OBJECT_ID": context.dl.VIEW_ANNOTATION_OPTIONS_OBJECT_ID
    }

    export_versions_list = {
        "V1": context.dl.ExportVersion.V1,
        "V2": context.dl.ExportVersion.V2
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "local_path":
            context.local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "file_types":
            context.file_types = parameter.cells[1]

        if parameter.cells[0] == "save_locally":
            context.save_locally = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "to_array":
            context.to_array = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "annotation_options":
            context.annotation_options = annotation_options_list[parameter.cells[1]]

        if parameter.cells[0] == "overwrite":
            context.overwrite = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "to_items_folder":
            context.to_items_folder = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "thickness":
            context.thickness = float(parameter.cells[1])

        if parameter.cells[0] == "with_text":
            context.with_text = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "annotation_filters":
            context.annotation_filters = context.dl.Filters(custom_filter=json.loads(parameter.cells[1]))

        if parameter.cells[0] == "alpha":
            context.save_locally = float(parameter.cells[1])

        if parameter.cells[0] == "export_version":
            context.save_locally = export_versions_list[parameter.cells[1]]

    context.item.download(
        local_path=context.local_path,
        file_types=context.file_types,
        save_locally=context.save_locally,
        to_array=context.to_array,
        annotation_options=context.annotation_options,
        overwrite=context.overwrite,
        to_items_folder=context.to_items_folder,
        thickness=context.thickness,
        with_text=context.with_text,
        annotation_filters=context.annotation_filters,
        alpha=context.alpha,
        export_version=context.export_version
    )


@behave.when(u'I call Item.annotations.download() using the given params')
def step_impl(context):
    # Used to get item height and width from the backend
    sleep(4)
    context.item = context.dl.items.get(item_id=context.item.id)

    context.filepath = None
    context.annotation_format = None
    context.img_filepath = None
    context.height = None
    context.width = None
    context.thickness = 1
    context.with_text = False
    context.alpha = 1

    annotation_format_list = {
        "JSON": context.dl.VIEW_ANNOTATION_OPTIONS_JSON,
        "MASK": context.dl.VIEW_ANNOTATION_OPTIONS_MASK,
        "INSTANCE": context.dl.VIEW_ANNOTATION_OPTIONS_INSTANCE,
        "ANNOTATION_ON_IMAGE": context.dl.VIEW_ANNOTATION_OPTIONS_ANNOTATION_ON_IMAGE,
        "VTT": context.dl.VIEW_ANNOTATION_OPTIONS_VTT,
        "OBJECT_ID": context.dl.VIEW_ANNOTATION_OPTIONS_OBJECT_ID
    }

    for parameter in context.table.rows:
        if parameter.cells[0] == "filepath":
            context.filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "annotation_format":
            context.annotation_format = annotation_format_list[parameter.cells[1]]

        if parameter.cells[0] == "img_filepath":
            context.img_filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "height":
            context.height = float(parameter.cells[1])

        if parameter.cells[0] == "width":
            context.width = float(parameter.cells[1])

        if parameter.cells[0] == "thickness":
            context.thickness = float(parameter.cells[1])

        if parameter.cells[0] == "with_text":
            context.with_text = True if parameter.cells[1] == "True" else False

        if parameter.cells[0] == "alpha":
            context.alpha = float(parameter.cells[1])

    if context.annotation_format is None:
        context.item.annotations.download(
            filepath=context.filepath,
            height=context.height,
            width=context.width,
            thickness=context.thickness,
            with_text=context.with_text,
            alpha=context.alpha
        )
    else:
        context.item.annotations.download(
            filepath=context.filepath,
            annotation_format=context.annotation_format,
            img_filepath=context.img_filepath,
            height=context.height,
            width=context.width,
            thickness=context.thickness,
            with_text=context.with_text,
            alpha=context.alpha
        )


@behave.when(u'I update item "{att}" with "{value}" system_metadata "{flag}"')
def step_impl(context, att, value, flag):
    value = eval(value)
    flag = eval(flag)
    if "metadata" in att:
        att_split = att.split('.')
        item_metadata = context.item.metadata
        # Traverse the dictionary using the keys
        for key in att_split[1:-1]:
            item_metadata = item_metadata[key]
        # Update the value at the last key
        item_metadata[att_split[-1]] = value
    else:
        setattr(context.item, att, value)

    context.item.update(flag)


================================================
File: tests/features/steps/item_entity/test_item_attributes.py
================================================
import behave


@behave.then(u'I validate _src_item is not None')
def step_impl(context):
    item = context.dataset.items.get(item_id=context.item.id)
    assert item._src_item, "TEST FAILED: Failed get _src_item by item.get"
    filters = context.dl.Filters(use_defaults=False, field='id', values=context.item.id)
    items = list(context.item.dataset.items.list(filters=filters).all())
    assert items[0]._src_item, "TEST FAILED: Failed get _src_item by dataset.items.list(filters=filters)"


@behave.then(u'I validate cloned item has the correct src item')
def step_impl(context):
    item = context.dataset.items.get(item_id=context.cloned_item.id)
    assert item._src_item == context.item.id, f"TEST FAILED: get by id _src_item id is {item._src_item} while expected {context.item.id}"
    filters = context.dl.Filters(use_defaults=False, field='id', values=context.cloned_item.id)
    items = list(context.item.dataset.items.list(filters=filters).all())
    assert items[
               0]._src_item == context.item.id, f"TEST FAILED: filter by id _src_item id is {items[0]._src_item} while expected {context.item.id}"


@behave.then(u'I validate item is annotated')
def step_impl(context):
    item = context.dataset.items.get(item_id=context.item.id)
    assert item.annotated, "TEST FAILED: Expected item to be annotated"


@behave.when(u'I import item "{name}" from "{path}"')
def step_impl(context, path, name):
    if path == 'root':
        success, response = context.dl.client_api.gen_request(
            req_type="post",
            path=f"/datasets/{context.dataset.id}/imports",
            json_req=[{"filename": name, "storageId": name}],
        )
    else:
        success, response = context.dl.client_api.gen_request(
            req_type="post",
            path=f"/datasets/{context.dataset.id}/imports",
            json_req=[{"filename": name, "storageId": f"{path}/{name}"}],
        )

    if not success:
        raise context.dl.exceptions.PlatformException(response)
    item = context.dataset.items.get(filepath="/" + name)
    if item.creator is None:
        assert False, f"FAILED - item creator = {item.creator}"


================================================
File: tests/features/steps/item_entity/test_item_description.py
================================================
import types

import behave


@behave.when(u'I Add description "{text}" to item')
def step_impl(context, text):
    context.item = context.item.set_description(text)


@behave.then(u'I validate item.description has "{text}" value')
def step_impl(context, text):
    context.item = context.dl.items.get(item_id=context.item.id)
    assert context.item.description == text


@behave.then(u'I remove description from the root')
def step_impl(context):
    item_json = context.item.to_json()
    item_json['metadata']['user'] = {'remove': 'description'}
    item_json.pop('description')

    def new_to_json(self):
        return item_json

    funcType = types.MethodType
    context.origin_to_json = context.item.to_json
    context.origin_from_json = context.dl.Item.from_json
    context.item.to_json = funcType(new_to_json, context.item)
    context.item = context.item.update(True)
    assert context.item.metadata['user'] == {'remove': 'description'}


@behave.then(u'Return from and to Json functions to the original implementation')
def step_impl(context):
    funcType = types.MethodType
    context.item.to_json = funcType(context.origin_to_json, context.item)
    context.dl.Item.from_json = context.origin_from_json


@behave.when(u'I add new field to the root')
def step_impl(context):
    context.origin_to_json = context.item.to_json
    context.origin_from_json = context.dl.Item.from_json
    item_json = context.item.to_json()
    item_json['newField'] = 'newField'

    def new_to_json(self):
        return item_json

    def new_from_json(client_api, _json, dataset):
        return _json

    funcType = types.MethodType
    context.item.to_json = funcType(new_to_json, context.item)
    context.dl.Item.from_json = new_from_json
    context.item_new_from_json = context.item.update(True)
    print()


@behave.when(u'new field do not added')
def step_impl(context):
    assert 'newField' not in context.item_new_from_json


@behave.then(u'I remove item.description')
def step_impl(context):
    context.item.description = None


@behave.then(u'I validate item.description is None')
def step_impl(context):
    context.item = context.dl.items.get(item_id=context.item.id)
    assert context.item.description == None


@behave.then(u'I update the metadata')
def step_impl(context):
    context.original_item_json = context.item.to_json()
    context.item.metadata["system"]["modified"] = "Update"
    context.item_update = context.dataset.items.update(item=context.item, system_metadata=True)


@behave.then(u"Item was modified metadata")
def step_impl(context):
    context.item_get = context.dataset.items.get(item_id=context.item.id)
    assert "modified" in context.item_get.metadata["system"]
    assert "Update" == context.item_get.metadata["system"]["modified"]


@behave.then(u'I remove the added metadata')
def step_impl(context):
    context.original_item_json = context.item.to_json()
    context.item.metadata["system"]["modified"] = None
    context.item_update = context.dataset.items.update(item=context.item, system_metadata=True)


@behave.then(u"metadata was deleted")
def step_impl(context):
    context.item_get = context.dataset.items.get(item_id=context.item.id)
    assert "modified" not in context.item_get.metadata["system"]


@behave.when(u'I Add description "{text}" to item with "{method}"')
def step_impl(context, text, method):
    if method == "item.description":
        context.item.description = text
    elif method == "item.set_description":
        context.item.set_description(text=text)
    elif method == "dataset.items.upload":
        context.item = context.dataset.items.upload(local_path=context.item_path, item_description=text)
    elif method == "dataset.items.upload - overwrite=False":
        context.item = context.dataset.items.upload(local_path=context.item_path, item_description=text,
                                                    overwrite=False)
    elif method == "dataset.items.upload - overwrite=True":
        context.item = context.dataset.items.upload(local_path=context.item_path, item_description=text, overwrite=True)


@behave.when(u'I delete all the dataset items')
def step_impl(context):
    context.dataset.items.delete(item_id=context.item.id)


@behave.when(u'I upload an annotation with description')
def step_impl(context):
    build = context.item.annotations.builder()
    build.add(annotation_definition=context.dl.Box(10, 10, 10, 10, 'a', description='description'))
    build.upload()


@behave.then(u'Annotation has description')
def step_impl(context):
    ann = context.item.annotations.list().annotations[0]
    assert ann.description == 'description'


================================================
File: tests/features/steps/item_entity/test_item_move.py
================================================
import behave


@behave.given(u'I have directory with the name "{directory}"')
def step_impl(context, directory):
    context.dataset.items.make_dir(directory=directory)


@behave.when(u'I move the item to "{new_path}"')
def step_impl(context, new_path):
    context.item.move(new_path=new_path)


@behave.then(u'I insure that new full name is "{filename}"')
def step_impl(context, filename):
    assert context.item.filename == filename


================================================
File: tests/features/steps/item_entity/test_item_repo_methods.py
================================================
import time
import behave
import os
import json


@behave.when(u'I download an item entity by the name of "{item_name}" to "{download_path}"')
def step_impl(context, item_name, download_path):
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)
    context.item.download(save_locally=True,
                          local_path=download_path,
                          annotation_options=None)


@behave.when(u'I delete the item')
def step_impl(context):
    context.item.delete()


@behave.when(u'I update item entity name to "{name}"')
def step_impl(context, name):
    context.item.filename = name
    context.item_update = context.item.update(system_metadata=True)


@behave.when(u'I list all item entity annotations')
def step_impl(context):
    context.annotations_list = context.item.annotations.list()


@behave.when(u'I get the item entity annotation by id')
def step_impl(context):
    context.annotation_x_get = context.item.get_annotation(context.annotation_x.id)


@behave.when(u'I upload to item entity annotations from file: "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)

    with open(file_path, "r") as f:
        context.annotations = json.load(f)["annotations"]
    context.item.annotations.upload(context.annotations)


@behave.when(u'I move item to "{new_path}"')
def step_impl(context, new_path):
    context.item_name = context.item.name
    context.moved_item = context.item.move(new_path=new_path)
    context.new_path = new_path


@behave.then(u'Item in host moved to a new directory')
def step_impl(context):
    if context.new_path.startswith('/'):
        name = context.new_path + '/' + context.item_name
    elif '.' in context.new_path:
        name = '/' + context.new_path
    else:
        name = '/' + context.new_path + '/' + context.item_name
    assert context.moved_item.filename == name


@behave.then(u'Item is annotated')
def step_impl(context):
    context.item.annotated = True
    context.item = context.item.update()


@behave.given(u'I upload item in path "{item_path}" to dataset')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.dataset.items.upload(local_path=item_path)

    # wait for platform attributes
    limit = 10 * 30
    stat = time.time()
    while True:
        time.sleep(3)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if "video" in context.item.mimetype:
            if context.item.fps is not None:
                break
        elif context.item.mimetype is not None:
            break
        if time.time() - stat > limit:
            raise TimeoutError("Timeout while waiting for platform attributes")




================================================
File: tests/features/steps/item_entity/test_item_update_status.py
================================================
import time

import behave
import random


@behave.when(u'I update items status to default task actions')
def step_impl(context):
    pages = context.task.get_items()
    try:
        for page in pages:
            for item in page:
                if item.annotated:
                    item.update_status(status=context.dl.ItemStatus.COMPLETED)
                else:
                    item.update_status(status=context.dl.ItemStatus.DISCARDED)

    except context.dl.exceptions.PlatformException as e:
        assert False, "Failed to update item_status \n".format(e)


@behave.when(u'I update items status to default task actions with task id')
def step_impl(context):
    pages = context.task.get_items()
    try:
        for page in pages:
            for item in page:
                if item.annotated:
                    item.update_status(status=context.dl.ItemStatus.COMPLETED, task_id=context.task.id)
                else:
                    # item.update_status(status=context.dl.ItemStatus.DISCARDED,task_id=context.task.id)
                    item.update_status(status='discard', task_id=context.task.id)

    except context.dl.exceptions.PlatformException as e:
        assert False, "Failed to update item_status \n".format(e)


@behave.then(u'I validate default items status in task')
def step_impl(context):
    pages = context.dataset.items.list()

    for page in pages:
        for item in page:
            if item.annotated:
                assert item.metadata['system']['taskStatusLog'][0]["status"]["status"] == "completed", "Failed, Item missing status completed"
                assert item.status(task_id=context.task.id) == "completed", "Failed, Item missing status completed"
            else:
                assert item.metadata['system']['taskStatusLog'][0]["status"]["status"] == "discard", "Failed, Item missing status discard"
                assert item.status(task_id=context.task.id) == "discard", "Failed, Item missing status discard"


@behave.when(u'I update items status to custom task actions "{action_1}" "{action_2}" "{action_3}"')
def step_impl(context, action_1, action_2, action_3):
    pages = context.task.get_items()
    try:
        for page in pages:
            for item in page:
                rand = random.randrange(0, 3)
                if rand == 0:
                    item.update_status(status=action_1)
                elif rand == 1:
                    item.update_status(status=action_2)
                else:
                    item.update_status(status=action_3)

    except context.dl.exceptions.PlatformException as e:
        assert False, "Failed to update item_status \n".format(e)


@behave.then(u'I validate items status in task')
def step_impl(context):
    pages = context.dataset.items.list()

    for page in pages:
        for item in page:
            assert len(item.metadata['system']['taskStatusLog']) == 1, "Failed to set status to item"


@behave.then(u'I update item status to "{status}" with task id')
@behave.when(u'I update item status to "{status}" with task id')
def step_impl(context, status):
    try:
        context.item.update_status(status=status, task_id=context.task.id)
        context.item_status = status
    except context.dl.exceptions.PlatformException as e:
        assert False, "Failed to update item_status \n".format(e)


@behave.then(u'I remove specific "{status}" from item with task id')
def step_impl(context, status):
    try:
        context.item.update_status(status=status, task_id=context.task.id, clear=True)
        context.item_status = status
    except context.dl.exceptions.PlatformException as e:
        assert False, "Failed to update item_status \n".format(e)


@behave.when(u'I wait for item status to be "{status_value}" with action "{action_type}"')
@behave.then(u'I wait for item status to be "{status_value}" with action "{action_type}"')
def step_impl(context, status_value, action_type):
    num_try = 60
    interval = 10
    success = False
    if status_value == "None":
        status_value = None
    for i in range(num_try):
        context.item = context.dataset.items.get(item_id=context.item.id)
        item_latest_status = context.item.metadata['system']['taskStatusLog'][-1]['status']['status']
        item_latest_action = context.item.metadata['system']['taskStatusLog'][-1]['action']
        if item_latest_status == status_value and item_latest_action == action_type:
            success = True
            break
        time.sleep(interval)

    assert success, f"TEST FAILED: Expected {status_value} | {action_type}, actual status is {item_latest_status} | {item_latest_action}"


================================================
File: tests/features/steps/items_repo/items_interface.py
================================================
import behave
import os
import time


@behave.given(u'I upload an item in the path "{item_path}" to the dataset')
def step_impl(context, item_path):
    context.item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.dataset.items.upload(local_path=context.item_path)


@behave.given(u'I upload items in the path "{items_path}" to the dataset in index "{index}"')
@behave.when(u'I upload items in the path "{items_path}" to the dataset in index "{index}"')
def step_impl(context, items_path, index):
    context.items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], items_path)
    context.datasets[int(index)].items.upload(local_path=context.items_path)


@behave.given(u'I upload an item of type "{item_type}" to the dataset')
def step_impl(context, item_type):
    items_types_list = {
        # Images
        "bmp image": os.path.join("supported_items_collection", "bmp_image_item.bmp"),
        "jfif image": os.path.join("supported_items_collection", "jfif_image_item.jfif"),
        "jpeg image": os.path.join("supported_items_collection", "jpeg_image_item.jpeg"),
        "jpg image": os.path.join("supported_items_collection", "jpg_image_item.jpg"),
        "png image": os.path.join("supported_items_collection", "png_image_item.png"),
        "tif image": os.path.join("supported_items_collection", "tif_image_item.tif"),
        "tiff image": os.path.join("supported_items_collection", "tiff_image_item.tiff"),
        "webp image": os.path.join("supported_items_collection", "webp_image_item.webp"),
        # Videos
        "3gp video": os.path.join("supported_items_collection", "3gp_video_item.3gp"),
        "avi video": os.path.join("supported_items_collection", "avi_video_item.avi"),
        "m4v video": os.path.join("supported_items_collection", "m4v_video_item.m4v"),
        "mkv video": os.path.join("supported_items_collection", "mkv_video_item.mkv"),
        "mp4 video": os.path.join("supported_items_collection", "mp4_video_item.mp4"),
        "webm video": os.path.join("supported_items_collection", "webm_video_item.webm"),
        # Audios
        "aac audio": os.path.join("supported_items_collection", "aac_audio_item.aac"),
        "flac audio": os.path.join("supported_items_collection", "flac_audio_item.flac"),
        "m4a audio": os.path.join("supported_items_collection", "m4a_audio_item.m4a"),
        "mp3 audio": os.path.join("supported_items_collection", "mp3_audio_item.mp3"),
        "ogg audio": os.path.join("supported_items_collection", "ogg_audio_item.ogg"),
        "wav audio": os.path.join("supported_items_collection", "wav_audio_item.wav"),
        # Texts
        "txt text": os.path.join("supported_items_collection", "txt_text_item.txt"),
    }

    context.item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], items_types_list[item_type])
    context.item = context.dataset.items.upload(local_path=context.item_path, overwrite=True)

    # wait for platform attributes
    limit = 10 * 30
    stat = time.time()
    while True:
        time.sleep(3)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if "video" in context.item.mimetype:
            if context.item.fps is not None:
                break
        elif "image" in context.item.mimetype and context.item.metadata['system'].get("height") is not None:
            break
        elif "image" not in context.item.mimetype and 'video' not in context.item.mimetype:
            break
        if time.time() - stat > limit:
            raise TimeoutError("Timeout while waiting for platform attributes")


@behave.given(u'I upload an item in the path "{item_path}" to "{dataset_name}"')
def step_impl(context, item_path, dataset_name):
    context.item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    dataset = context.dl.datasets.get(dataset_name=dataset_name)
    context.item = dataset.items.upload(local_path=context.item_path)


================================================
File: tests/features/steps/items_repo/test_download_and_upload_ndarray_item.py
================================================
from behave import given, then, when
from PIL import Image
import numpy
import os
import dtlpy as dl
import json
from operator import attrgetter



@given(u'I convert to Numpy.NdArray an item with the name "{item_path}" and add it to context.array')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    image = Image.open(item_path)
    context.array = numpy.array(image)


@given(u'I save item_metadata context.item_metadata')
def step_impl(context):
    context.item_metadata = {'City': 'NY', 'Country': 'USA'}


@when(u'I Upload an Numpy.NdArray (context.array) item with the name "{item_name}"')
def step_impl(context, item_name):
    try:
        context.item = context.dataset.items.upload(context.array,
                                                    remote_name=item_name,
                                                    item_metadata=context.item_metadata)
    except dl.exceptions.BadRequest:
        # For Scenario: Upload illegal Numpy.NdArray item
        # In such case item is not upload and "There are no items" scansion has to be executed
        pass


@then(u'Item is correctly uploaded to platform')
def step_impl(context):
    item_get = context.dataset.items.get(item_id=context.item.id)

    for key in context.item_metadata:
        assert key in context.item.metadata
    assert item_get.id == context.item.id


@when(u'I Download as Numpy.NdArray the uploaded item')
def step_impl(context):
    context.download_array = context.item.download(save_locally=False, to_array=True)


@then(u'Download Numpy.NdArray item and context.array size equal')
# I not check numpy.array_equal since the  numpy.ndarray to io.BytesIO can the RGB values
def step_impl(context):
    assert context.download_array.size, context.array.size


@when(u'Download as Numpy.NdArray the .mp4')
def step_impl(context):
    context.download_array = context.item_mp4.download(save_locally=False, to_array=True)


@given(u'I remove log files')
def step_impl(context):
    path = os.path.join(dl.service_defaults.DATALOOP_PATH, 'reporters')
    if os.path.exists(path):
        for f in os.listdir(path):
            if any(x in f for x in ["log", ".json"]):
                if os.path.exists(os.path.join(path, f)):
                    os.remove(os.path.join(path, f))


@then(u'Log file is exist with resource "{resource_input}"')
def step_impl(context, resource_input):
    path = os.path.join(dl.service_defaults.DATALOOP_PATH, 'reporters')
    if os.path.exists(path):
        for f in os.listdir(path):
            if any(x in f for x in ["log", ".json"]):
                with open(os.path.join(path, f), 'r') as file:
                    log_file = json.load(file)
                    if attrgetter(resource_input)(context) in log_file.keys():
                        return
    assert False


@then(u'Log file does not exist')
def step_impl(context):
    path = os.path.join(dl.service_defaults.DATALOOP_PATH, 'reporters')
    if os.path.exists(path):
        for f in os.listdir(path):
            if any(x in f for x in ["log", ".json"]):
                assert False


================================================
File: tests/features/steps/items_repo/test_items_clone.py
================================================
from behave import given, when, then
import os
import time


@given(u'There is an item "{with_or_without}" "{key}" in its metadata system')
def step_impl(context, key: str, with_or_without: str):
    local_path = "sample_video.mp4"
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)
    context.item = context.dataset.items.upload(
        local_path=local_path,
        remote_name="sample_video-{}-{}-{}.mp4".format(with_or_without, key, time.time())
    )

    start = time.time()
    interval = 5
    timeout = 100

    while key not in context.item.metadata['system']:
        time.sleep(interval)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if time.time() - start > timeout:
            raise Exception('Item was not created with {} in metadata'.format(key))

    if with_or_without == 'without':
        context.item.metadata['system'].pop(key)
        context.item = context.item.update(system_metadata=True)
        assert key not in context.item.metadata['system']


@when(u'I clone the item')
def step_impl(context):
    context.item_clone = context.item.clone(
        dst_dataset_id=context.dataset.id,
        remote_filepath='{}-cloned.jpg'.format(context.item.filename)
    )


@then(u'The cloned item should trigger video preprocess function')
def step_impl(context):
    start = time.time()
    interval = 5
    timeout = 20

    def is_triggered():
        re = context.item_clone.resource_executions.list()
        e = [e for e in re.items if e.service_name == 'video-metadata-extractor' and e.function_name == 'on_create']
        return len(e) > 0

    while not is_triggered():
        time.sleep(interval)
        if time.time() - start > timeout:
            raise Exception('Item was not created with fps in metadata')


@then(u'The cloned item should have "{key}" in its metadata')
def step_impl(context, key: str):
    start = time.time()
    interval = 5
    timeout = 60
    while key not in context.item_clone.metadata['system']:
        if time.time() - start > timeout:
            raise Exception('Item was not created with fps in metadata')
        time.sleep(interval)
        context.item_clone = context.dataset.items.get(item_id=context.item_clone.id)


================================================
File: tests/features/steps/items_repo/test_items_context.py
================================================
import behave


@behave.when(u'I get the item from project number {project_index}')
def step_impl(context, project_index):
    context.item = context.projects[int(project_index) - 1].items.get(item_id=context.item.id)


@behave.when(u'I get the item from dataset number {dataset_index}')
def step_impl(context, dataset_index):
    context.item = context.datasets[int(dataset_index) - 1].items.get(item_id=context.item.id)


@behave.then(u'item Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.item.projectId == context.projects[int(project_index)-1].id


@behave.then(u'item Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.item.project.id == context.projects[int(project_index)-1].id


@behave.then(u'item Dataset_id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.item.dataset_id == context.datasets[int(dataset_index)-1].id


@behave.then(u'item Dataset.id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.item.dataset.id == context.datasets[int(dataset_index)-1].id


================================================
File: tests/features/steps/items_repo/test_items_delete.py
================================================
import behave
import os


@behave.given(u'There are no items')
def step_impl(context):
    filters = context.dl.Filters()
    filters.add(field='type', values='file')
    assert len(context.dataset.items.list(filters=filters).items) == 0


@behave.given(u'I upload an item by the name of "{item_name}"')
def step_impl(context, item_name):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], '0000000162.jpg')

    import io
    with open(local_path, 'rb') as f:
        buffer = io.BytesIO(f.read())
        buffer.name = item_name

    context.item = context.dataset.items.upload(local_path=buffer)
    context.item = context.dataset.items.get(filepath='/' + item_name)
    context.item_count = 1


@behave.given(u'I upload item by the name of "{item_name}" to a remote path "{remote_path}"')
def step_impl(context, item_name, remote_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], '0000000162.jpg')

    import io
    with open(local_path, 'rb') as f:
        buffer = io.BytesIO(f.read())
        buffer.name = item_name

    context.item = context.dataset.items.upload(local_path=buffer, remote_path=remote_path)
    context.item = context.dataset.items.get(item_id=context.item.id)
    context.item_count = 1


@behave.when(u'I delete the item by name')
def step_impl(context):
    for context.dataset in context.project.datasets.list():
        context.dataset.items.delete(filename=context.item.filename)


@behave.then(u'There are no items')
def step_impl(context):
    filters = context.dl.Filters()
    filters.add(field='type', values='file')
    assert len(context.dataset.items.list(filters=filters).items) == 0


@behave.when(u'I delete the item by id')
def step_impl(context):
    context.dataset.items.delete(item_id=context.item.id)


@behave.when(u'I try to delete an item by the name of "{item_name}"')
def step_impl(context, item_name):
    try:
        context.dataset.items.delete(filename=item_name)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'No item was deleted')
def step_impl(context):
    filters = context.dl.Filters()
    filters.add(field='type', values='file')
    assert len(context.dataset.items.list(filters=filters).items) == context.item_count


@behave.when(u'I try to delete an item by the id of "{item_id}"')
def step_impl(context, item_id):
    try:
        context.dataset.items.delete(item_id=item_id)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/items_repo/test_items_download.py
================================================
import io
import time
import behave
import os
import shutil


@behave.given(u'Folder "{folder_path}" is empty')
def step_impl(context, folder_path):
    folder_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], folder_path)
    content = [name for name in os.listdir(folder_path)]
    if len(content) != 0:
        for item in content:
            if item != 'folder_keeper':
                path = os.path.join(folder_path, item)
                if os.path.isdir(path):
                    shutil.rmtree(path)
                else:
                    os.remove(path)


@behave.when(u'I download an item by the name of "{item_name}" to "{download_path}"')
def step_impl(context, item_name, download_path):
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)
    filters = context.dl.Filters()
    filters.add(field='filename', values=item_name)
    context.dataset.items.download(filters=filters,
                                   items=None,
                                   save_locally=True,
                                   local_path=download_path,
                                   annotation_options=None)


@behave.then(u'There are "{item_count}" files in "{local_path}"')
def step_impl(context, item_count, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    content = list()
    for root, dirs, files in os.walk(local_path):
        for file in files:
            if file.endswith(".jpg"):
                content.append(file)

    assert len(content) == int(item_count), "TEST FAILED: Expected {} got {}".format(item_count, len(content))


@behave.then(u'Item is correctly downloaded to "{downloaded_path}" (compared with "{item_to_compare}")')
def step_impl(context, downloaded_path, item_to_compare):
    import cv2
    downloaded_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], downloaded_path)
    item_to_compare = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_to_compare)

    original = cv2.imread(downloaded_path)
    downloaded = cv2.imread(item_to_compare)
    if original.shape == downloaded.shape:
        difference = cv2.subtract(original, downloaded)
        b, g, r = cv2.split(difference)
        if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:
            assert True
        else:
            assert False
    else:
        assert False


@behave.when(u'I download an item by the id to "{download_path}"')
def step_impl(context, download_path):
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)
    context.dataset.items.download(items=context.item.id,
                                   save_locally=True,
                                   local_path=download_path,
                                   annotation_options=None)


@behave.when(u'I download the item without saving and create folder')
def step_impl(context):
    context.path = os.path.join(os.getcwd(), 'items_check')
    context.item_data = context.dataset.items.download(items=context.item.id,
                                                       save_locally=False,
                                                       local_path=context.path,
                                                       annotation_options=None)


@behave.then(u'file do not created')
def step_impl(context):
    assert not os.path.exists(path=context.path)


@behave.when(u'I download without saving an item by the id of "{item_name}"')
def step_impl(context, item_name):
    context.item_data = context.dataset.items.download(items=context.item.id,
                                                       save_locally=False,
                                                       local_path=None,
                                                       annotation_options=None)


@behave.then(u'I receive item data')
def step_impl(context):
    assert type(context.item_data) == io.BytesIO


@behave.when(u'I upload item data by name of "{item_name}"')
def step_impl(context, item_name):
    context.item_data.name = item_name
    context.dataset.items.upload(local_path=context.item_data,
                                 local_annotations_path=None,
                                 remote_path=None)
    # wait for platform attributes
    limit = 10 * 30
    stat = time.time()
    while True:
        time.sleep(3)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if "video" in context.item.mimetype:
            if context.item.fps is not None:
                break
        elif context.item.mimetype is not None:
            break
        if time.time() - stat > limit:
            raise TimeoutError("Timeout while waiting for platform attributes")


@behave.then(u'Item uploaded from data equals initial item uploaded')
def step_impl(context):
    pass


================================================
File: tests/features/steps/items_repo/test_items_download_batch.py
================================================
import behave
import os


@behave.when(u"I download batched items to buffer")
def step_impl(context):
    items = context.dataset.items.get_all_items()
    context.buffer = context.dataset.items.download(
        items=items,
        save_locally=False,
        local_path=None,
        annotation_options=None,
    )


@behave.when(u'I download batched items to local path "{local_path}"')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    items = context.dataset.items.get_all_items()
    context.dataset.items.download(
        items=items,
        save_locally=True,
        local_path=local_path,
        annotation_options=None,
    )


@behave.then(u"I can upload items from buffer to host")
def step_impl(context):
    counter = 0
    for buff in context.buffer:
        uploaded_filename = "file" + str(counter) + '.jpg'
        buff.name = uploaded_filename
        counter += 1
        context.dataset.items.upload(
            buff,
            remote_path=None,
        )

    context.item_list = context.dataset.items.list()
    assert len(context.item_list.items) == counter


@behave.when(u'I delete all items from host')
def step_impl(context):
    context.item_list = context.dataset.items.list()
    for item in context.item_list.items:
        if item.type != 'dir':
            context.dataset.items.delete(item_id=item.id)
    context.item_list = context.dataset.items.list()
    assert len(context.item_list.items) == 0


@behave.when(u'I download items to local path "{local_path}"')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    items = context.dataset.items.get_all_items()
    context.buffer = context.dataset.items.download(
        items=items,
        save_locally=True,
        local_path=local_path,
        annotation_options=None,
        overwrite=True
    )


@behave.then(u'Items are saved in "{local_path}"')
def step_impl(context, local_path):
    import cv2
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)

    compare_folder_path = 'downloaded_batch/should_be'
    compare_folder_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], compare_folder_path)

    for item in os.listdir(compare_folder_path):
        file_to_compare = os.path.join(compare_folder_path, item)
        original = cv2.imread(os.path.join(local_path, item))
        downloaded = cv2.imread(file_to_compare)
        if original.shape == downloaded.shape:
            difference = cv2.subtract(original, downloaded)
            b, g, r = cv2.split(difference)
            if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:
                assert True
            else:
                assert False
        else:
            assert False


================================================
File: tests/features/steps/items_repo/test_items_get.py
================================================
import behave
import os
import time


@behave.given(u"There is an item")
def step_impl(context):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)
    context.item = context.dataset.items.upload(local_path=filepath)


@behave.when(u"I get the item by id")
def step_impl(context):
    context.item_get = context.dataset.items.get(item_id=context.item.id)


@behave.then(u"I receive an Item object")
def step_impl(context):
    assert type(context.item_get) == context.dl.Item


@behave.then(u"The item I received equals the item I uploaded")
def step_impl(context):
    item_json = context.item.to_json()
    item_get_json = context.item_get.to_json()
    item_json.pop('metadata')
    item_get_json.pop('metadata')
    assert item_json == item_get_json


@behave.when(u'I try to get item by "{some_id}"')
def step_impl(context, some_id):
    try:
        context.dataset.items.get(some_id)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I get the item by remote path "{remote_path}"')
def step_impl(context, remote_path):
    context.item_get = context.dataset.items.get(filepath=remote_path)


@behave.when(u'I try to get an item by remote path "{remote_path}"')
def step_impl(context, remote_path):
    try:
        context.dataset.items.get(filepath=remote_path)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I try to use get services with no params')
def step_impl(context):
    try:
        context.dataset.items.get()
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u'There are 2 items by the name of "{file_name}"')
def step_impl(context, file_name):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)

    context.dataset.items.upload(
        local_path=filepath,
        remote_path=None
    )

    remote_path = '/folder_name/'
    context.dataset.items.upload(
        local_path=filepath,
        remote_path=remote_path
    )


@behave.then(u'I get items by dataset Id')
def step_impl(context):
    success, response = context.dl.client_api.gen_request(
        req_type="get",
        path="/datasets/{}/items".format(context.dataset.id)
    )
    assert success, "TEST FAILED: Error message: {}".format(response.json())


@behave.when(u'I get a consensus item')
def step_impl(context):
    context.folder_name = None
    for dir_name in context.dataset.directory_tree.dir_names:
        if "/.consensus/" in dir_name:
            context.folder_name = dir_name

    assert context.folder_name, f"TEST FAILED: Folder doesn't exists in: {context.dataset.directory_tree.dir_names}"

    filters = context.dl.Filters()
    filters.add(field='dir', values=[context.folder_name], operator='in')
    filters.add(field='hidden', values=True)

    assert context.dataset.items.list(filters=filters).items_count, "TEST FAILED: No consensus items in dataset"

    context.item = context.dataset.items.list(filters=filters).items[0]


@behave.when(u'Dataset in index "{index}" have "{items_count}" items')
def step_impl(context, index, items_count):
    dataset = context.datasets[int(index)]
    num_try = 16
    interval = 10
    finished = False

    for i in range(num_try):
        if dataset.items.list().items_count == int(items_count):
            finished = True
            break
        time.sleep(interval)

    assert finished, f"TEST FAILED: Expected: {items_count} items , Actual: {dataset.items.list().items_count}"


@behave.when(u"I get item thumbnail id")
def step_impl(context):
    context.item = context.dataset.items.get(item_id=context.item.id)
    context.item_thumbnail_id = context.item.metadata['system']['thumbnailId']


@behave.then(u'I validate item thumbnail id is "{condition}" to the previous thumbnail id')
def step_impl(context, condition):
    if not hasattr(context, 'item_thumbnail_id'):
        assert False, "TEST FAILED: Please make sure to run the step - 'When I get item thumbnail id'"
    context.item = context.dataset.items.get(item_id=context.item.id)
    error_message = f"TEST FAILED: Item thumbnail is not {condition}"
    if condition == "not-equal":
        assert context.item.metadata['system']['thumbnailId'] != context.item_thumbnail_id, error_message
    elif condition == "equal":
        assert context.item.metadata['system']['thumbnailId'] == context.item_thumbnail_id, error_message
    else:
        raise ValueError("condition must be 'different' or 'equal'")


@behave.then(u'I should see a thumbnail v2 on the item')
def step_impl(context):
    """
    Make sure to run the step - 'I Show annotation thumbnail for the item' before running this step
    """
    assert context.item.metadata['system'].get('annotationQueryThumbnailIdMap', {}).get(
        'default') is not None, f"TEST FAILED: No thumbnail v2 on the item {context.item.metadata['system']}"
    thumbnail_id = context.item.metadata['system']['annotationQueryThumbnailIdMap']['default']
    try:
        context.dataset.items.get(item_id=thumbnail_id)
    except context.dl.exceptions.NotFound:
        assert False, f"TEST FAILED: Thumbnail v2 not found in dataset items with Id {thumbnail_id}"


@behave.then(u'I should not see a thumbnail v2 on the item')
def step_impl(context):
    """
    Make sure to run the step - 'I Show annotation thumbnail for the item' before running this step
    """
    assert context.item.metadata['system'].get('annotationQueryThumbnailIdMap', {}).get(
        'default') is None, f"TEST FAILED: Thumbnail v2 on the item {context.item.metadata['system']}"
    try:
        context.dataset.items.get(item_id=context.thumbnail_id)
    except context.dl.exceptions.NotFound:
        pass


@behave.then(u'I validate "{value}" not in item system metadata')
def step_impl(context, value):
    item = context.dataset.items.get(item_id=context.item.id)
    value = eval(value)
    # Check if the value key exists in item metadata system key
    if value.keys() <= item.metadata['system'].keys():
        for key in value.keys():
            assert value[key] != item.metadata['system'][
                key], f"TEST FAILED: {value} found in item system metadata, {item.metadata['system']}"



================================================
File: tests/features/steps/items_repo/test_items_get_all_items.py
================================================
import behave


@behave.when(u'I get all items')
def step_impl(context):
    context.item_list = context.dataset.items.get_all_items()


@behave.then(u'I receive a list of "{item_count}" items')
def step_impl(context, item_count):
    assert len(context.item_list) == int(item_count), "Expected: {} , Got: {}\n".format(item_count, len(context.item_list))


@behave.when(u'I get all items, page size is "{page_size}"')
def step_impl(context, page_size):
    context.item_list = context.dataset.items.get_all_items(page_size=page_size)


================================================
File: tests/features/steps/items_repo/test_items_list.py
================================================
import time

import behave
import os


@behave.when(u'I list items')
def step_impl(context):
    context.list = context.dataset.items.list(filters=None,
                                              page_offset=0,
                                              page_size=100)


@behave.then(u'I receive a PageEntity object')
def step_impl(context):
    assert type(context.list) == context.dl.entities.PagedEntities


@behave.then(u'PageEntity next page items has length of "{item_count}"')
def step_impl(context, item_count):
    context.list.next_page()
    count = 0
    for item in context.list.items:
        if item.type == 'file':
            count += 1
    assert count == int(item_count)


@behave.then(u'Item in PageEntity items equals item uploaded')
def step_impl(context):
    item_json = context.item.to_json()
    item_in_list_json = context.list.items[0].to_json()
    item_json.pop('metadata')
    item_in_list_json.pop('metadata')
    assert item_json == item_in_list_json


@behave.given(u'There are "{item_count}" items')
def step_impl(context, item_count):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)

    filename = 'file'
    counter = 0
    while counter < int(item_count):
        uploaded_filename = filename + str(counter) + '.jpg'
        import io
        with open(filepath, 'rb') as f:
            buffer = io.BytesIO(f.read())
            buffer.name = uploaded_filename
        context.dataset.items.upload(
            local_path=buffer,
            remote_path=None
        )
        counter += 1


@behave.when(u'I list items with size of "{size}"')
def step_impl(context, size):
    context.list = context.dataset.items.list(filters=None,
                                              page_offset=0,
                                              page_size=int(size))


@behave.then(u'PageEntity items has next page')
def step_impl(context):
    assert context.list.has_next_page


@behave.then(u'PageEntity items does not have next page')
def step_impl(context):
    assert not context.list.has_next_page


@behave.then(u'PageEntity items has length of "{item_count}"')
def step_impl(context, item_count):
    assert len(context.list.items) == int(item_count)

    # if the is only one item we will same it to use later
    if int(item_count) == 1:
        for item in context.list.items:
            if item.type == 'file':
                context.item_in_page = item
                break


@behave.when(u'I list items with offset of "{offset}" and size of "{size}"')
def step_impl(context, offset, size):
    context.list = context.dataset.items.list(filters=None,
                                              page_offset=int(offset),
                                              page_size=int(size))


@behave.given(u'There is one item by the name of "{item_name}"')
def step_impl(context, item_name):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)
    import io
    with open(filepath, 'rb') as f:
        buffer = io.BytesIO(f.read())
        buffer.name = item_name
    context.item = context.dataset.items.upload(
        local_path=buffer,
        remote_path=None
    )


@behave.when(u'I list items with query filename="{filename_filter}"')
def step_impl(context, filename_filter):
    filters = context.dl.Filters()
    filters.add(field='filename', values=filename_filter)
    context.list = context.dataset.items.list(filters=filters)


@behave.then(u'PageEntity item received equal to item uploaded with name "test_name"')
def step_impl(context):
    item_in_page_json = context.item_in_page.to_json()
    item_json = context.item.to_json()
    item_in_page_json.pop('metadata')
    item_json.pop('metadata')
    assert item_in_page_json == item_json


@behave.given(u'There are "{item_count}" items in remote path "{remote_path}"')
def step_impl(context, item_count, remote_path):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)

    filename = 'file'
    counter = 0
    while counter < int(item_count):
        uploaded_filename = filename + str(counter) + '.jpg'
        import io
        with open(filepath, 'rb') as f:
            buffer = io.BytesIO(f.read())
            buffer.name = uploaded_filename

        context.dataset.items.upload(
            local_path=buffer,
            remote_path=remote_path
        )
        counter += 1


@behave.then(u'PageEntity items received have "{path}" in the filename')
def step_impl(context, path):
    for item in context.list.items:
        if item.type == 'file':
            assert path in item.filename


@behave.given(u'There are "{item_count}" .jpg items')
def step_impl(context, item_count):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)

    filename = 'file'
    counter = 0
    while counter < int(item_count):
        uploaded_filename = filename + str(counter) + '.jpg'

        import io
        with open(filepath, 'rb') as f:
            buffer = io.BytesIO(f.read())
            buffer.name = uploaded_filename

        context.dataset.items.upload(
            local_path=buffer,
            remote_path=None
        )
        counter += 1


@behave.given(u'There is one .png item')
def step_impl(context):
    filepath = "0000000162.png"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)
    context.png_item = context.dataset.items.upload(
        local_path=filepath,
        remote_path=None
    )


@behave.when(u'I list items with query mimetypes="{mimetype_filters}"')
def step_impl(context, mimetype_filters):
    filters = context.dl.Filters()
    filters.add(field='type', values='file')
    filters.add(field='metadata.system.mimetype', values=mimetype_filters)
    context.list = context.dataset.items.list(filters=filters)


@behave.then(u'And PageEntity item received equal to .png item uploadede')
def step_impl(context):
    png_item_json = context.png_item.to_json()
    item_in_page_json = context.item_in_page.to_json()
    png_item_json.pop('metadata')
    item_in_page_json.pop('metadata')
    assert png_item_json == item_in_page_json


@behave.given(u'There is one .mp4 item')
def step_impl(context):
    filepath = "sample_video.mp4"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)
    context.item = context.item_mp4 = context.dataset.items.upload(
        local_path=filepath,
        remote_path=None
    )


@behave.when(u'I list items with query itemType="{value}"')
def step_impl(context, value):
    filters = context.dl.Filters()
    filters.add(field='type', values=value)
    context.list = context.dataset.items.list(filters=filters)


@behave.then(u'And PageEntity item received equal to .mp4 item uploadede')
def step_impl(context):
    mp4_item_json = context.item_mp4.to_json()
    item_in_page_json = context.item_in_page.to_json()
    mp4_item_json.pop('metadata')
    item_in_page_json.pop('metadata')
    assert mp4_item_json == item_in_page_json


@behave.when(u'I validate all items is annotated in dataset in index "{index}"')
@behave.when(u'I validate all items is annotated')
def step_impl(context, index=None):
    filters = context.dl.Filters()
    filters.add(field='annotated', values=False)
    num_try = 48
    interval = 5
    finished = False

    if index:
        dataset = context.datasets[int(index)]
    else:
        dataset = context.dataset

    for i in range(num_try):
        items_count = dataset.items.list(filters=filters).items_count
        if items_count == 0:
            finished = True
            break
        time.sleep(interval)

    assert finished, f"TEST FAILED: Not all items annotated , number left - {items_count}"


================================================
File: tests/features/steps/items_repo/test_items_set_items_entity.py
================================================
import behave


@behave.when(u'I change entity to "{entity}"')
def step_impl(context, entity):
    if entity == "Item":
        entity = context.dl.Item
    elif entity == "Artifact":
        entity = context.dl.entities.Artifact
    elif entity == "Codebase":
        entity = context.dl.Codebase

    context.dataset.items.set_items_entity(entity)


@behave.then(u'Items item entity is "{entity}"')
def step_impl(context, entity):
    if entity == "Item":
        entity = context.dl.Item
    elif entity == "Artifact":
        entity = context.dl.entities.Artifact
    elif entity == "Codebase":
        entity = context.dl.Codebase

    assert context.dataset.items.items_entity == entity


@behave.when(u'I try to change entity to "Dataset"')
def step_impl(context):
    try:
        context.dataset.items.set_items_entity(context.dl.entities.Dataset)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/items_repo/test_items_update.py
================================================
import behave
import os
import time
import logging
import json
from .. import fixtures


@behave.when(u'I update items name to "{name}"')
def step_impl(context, name):
    context.item.filename = name
    context.item_update = context.dataset.items.update(item=context.item)


@behave.then(u'I receive an Item object with name "{name}"')
def step_impl(context, name):
    assert type(context.item_update) == context.dl.Item
    assert context.item_update.filename == name


@behave.then(u"Only name attributes was changed")
def step_impl(context):
    item_get_json = context.item_get.to_json()
    original_item_json = context.item.to_json()
    item_get_json.pop("filename")
    original_item_json.pop("filename")
    item_get_json.pop("name")
    original_item_json.pop("name")
    item_get_json.pop("metadata")
    original_item_json.pop("metadata")
    item_get_json.pop("updatedAt")
    original_item_json.pop("updatedAt")
    item_get_json.pop("updatedBy")
    original_item_json.pop("updatedBy")

    assert item_get_json == original_item_json, "TEST FAILED: Expected : {}, Got: {}".format(original_item_json,
                                                                                             item_get_json)


@behave.then(u'Item in host was changed to "{name}"')
def step_impl(context, name):
    time.sleep(3)
    context.item_get = context.dataset.items.get(item_id=context.item.id)
    if context.item_get.filename != name:
        logging.error(
            "item_get name = {item_get_name}, name should be {name_should}".format(
                item_get_name=context.item_get.filename, name_should=name
            )
        )
    assert context.item_get.filename == name


@behave.given(u'And There is an item by the name of "{name}"')
def step_impl(context, name):
    local_path = "0000000162.jpg"
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)
    context.item = context.dataset.items.upload(
        local_path=local_path, remote_path=None
    )


@behave.then(u'PageEntity has directory item "{dir_name}"')
def step_impl(context, dir_name):
    filters = context.dl.Filters()
    filters.add(field="type", values="dir")
    page = context.dataset.items.list(filters=filters)
    dir_exist = False
    for item in page.items:
        if item.filename == dir_name:
            dir_exist = True
            break
    assert dir_exist is True


@behave.when(u'I update item system metadata with system_metadata="{param}"')
def step_impl(context, param):
    system_metadata = param == "True"
    context.original_item_json = context.item.to_json()
    context.item.metadata["system"]["modified"] = "True"
    context.item_update = context.dataset.items.update(item=context.item, system_metadata=system_metadata)


@behave.then(u"Then I receive an Item object")
def step_impl(context):
    assert type(context.item_update) == context.dl.Item


@behave.then(u"Item in host has modified metadata")
def step_impl(context):
    context.item_get = context.dataset.items.get(item_id=context.item.id)
    assert "modified" in context.item_get.metadata["system"]


@behave.then(u"Only metadata was changed")
def step_impl(context):
    item_get_json = context.item_get.to_json()
    item_get_json.pop("metadata")
    context.original_item_json.pop("metadata")
    item_get_json.pop("updatedAt")
    context.original_item_json.pop("updatedAt")

    assert item_get_json == context.original_item_json


@behave.then(u"Item in host was not changed")
def step_impl(context):
    context.item_get = context.dataset.items.get(item_id=context.item.id)
    assert "modified" not in context.item_get.metadata["system"]


@behave.given(u'I add folder "{folder_name}" to context.dataset')
def step_impl(context, folder_name):
    context.dataset.items.make_dir(directory="/{}".format(folder_name))


@behave.when(u'I Show annotation thumbnail for the item')
def step_impl(context):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "api", "api_assets.json")
    with open(file_path, 'r') as file:
        json_obj = json.load(file)

    context.req = json_obj.get("annotation_thumbnail")
    fixtures.update_nested_structure(context, context.req.get('json_req', None))

    context.response = fixtures.gen_request(context=context, method="post", req=context.req, num_try=1, interval=0)

    response_json = context.response.json()
    command = context.dl.Command.from_json(_json=response_json,
                                           client_api=context.dl.client_api)
    command = command.wait(timeout=0)
    assert command, f"TEST FAILED: Command failed ID {command.id}"
    context.item = context.dataset.items.get(item_id=context.item.id)
    if not hasattr(context, 'thumbnail_id'):
        context.thumbnail_id = context.item.metadata['system'].get('annotationQueryThumbnailIdMap', {}).get('default')


@behave.when(u'I try to update item with params')
def step_impl(context):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']
    try:
        context.item_update = context.dataset.items.update(item=eval(params.get('item', "context.item")),
                                                           filters=eval(params.get('filters', "None")),
                                                           update_values=eval(params.get('update_values', "None")),
                                                           system_update_values=eval(
                                                               params.get('system_update_values', "None")),
                                                           system_metadata=eval(params.get('system_metadata', "False")),
                                                           )
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/items_repo/test_items_upload.py
================================================
import behave
import os
import time
import shutil
import io


@behave.when(u'I upload a file in path "{item_local_path}"')
def step_impl(context, item_local_path):
    item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], item_local_path)
    context.item = context.dataset.items.upload(local_path=item_local_path,
                                                remote_path=None
                                                )


@behave.when(u'I upload with "{option}" a file in path "{item_local_path}"')
def step_impl(context, item_local_path, option):
    if option == 'overwrite':
        overwrite = True
    else:
        overwrite = False
    item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], item_local_path)
    context.item = context.dataset.items.upload(local_path=item_local_path,
                                                remote_path=None,
                                                overwrite=overwrite
                                                )


@behave.when(u'I upload file in path "{item_local_path}" to remote path "{remote_path}"')
def step_impl(context, item_local_path, remote_path):
    item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], item_local_path)
    context.item = context.dataset.items.upload(local_path=item_local_path,
                                                remote_path=remote_path)
    context.uploaded_item_with_trigger = context.item


@behave.then(u"Item exist in host")
def step_impl(context):
    context.item_get = context.dataset.items.get(item_id=context.item.id)


@behave.then(u"Item object from host equals item uploaded")
def step_impl(context):
    item_get = context.item_get.to_json()
    item = context.item.to_json()
    item_get.pop("metadata")
    item.pop("metadata")
    assert item == item_get


@behave.then(u'Item in host when downloaded to "{download_path}" equals item in "{item_local_path}"')
def step_impl(context, item_local_path, download_path):
    import cv2
    download_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], download_path)
    file_to_compare = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], item_local_path)

    context.dataset.items.download(
        filters=None,
        items=context.item.id,
        local_path=download_path,
        file_types=None,
        save_locally=True,
        annotation_options=None,
        to_items_folder=False,
    )
    time.sleep(2)
    original = cv2.imread(file_to_compare)
    download_path = os.path.join(download_path, context.item.filename[1:])
    downloaded = cv2.imread(download_path)
    if original.shape == downloaded.shape:
        difference = cv2.subtract(original, downloaded)
        b, g, r = cv2.split(difference)
        if (
                cv2.countNonZero(b) == 0
                and cv2.countNonZero(g) == 0
                and cv2.countNonZero(r) == 0
        ):
            assert True
        else:
            assert False
    else:
        assert False
    if len(context.item.filename.split('/')) > 2:
        shutil.rmtree(os.path.split(download_path)[0])
    else:
        os.remove(download_path)


@behave.then(u"Upload method returned an Item object")
def step_impl(context):
    assert type(context.item) == context.dl.entities.Item


@behave.then(u'Item in host is in folder "{remote_path}"')
def step_impl(context, remote_path):
    assert remote_path in context.dataset.items.get(item_id=context.item.id).filename


@behave.when(u'I upload the file in path "{local_path}", opened as a buffer, with remote name "{remote_name}"')
def step_impl(context, local_path, remote_name):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)

    with open(local_path, "rb") as f:
        buffer = io.BytesIO(f.read())

    context.item = context.dataset.items.upload(
        local_path=buffer, remote_path=None, remote_name=remote_name
    )


@behave.when(u'I upload file in path "{local_path}" with remote name "{remote_name}" set via the buffer interface')
def step_impl(context, local_path, remote_name):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)

    with open(local_path, "rb") as f:
        buffer = io.BytesIO(f.read())
        buffer.name = remote_name

    context.item = context.dataset.items.upload(
        local_path=buffer, remote_path=None
    )


@behave.when(u'I upload the file in path "{local_path}" with remote name "{remote_name}"')
def step_impl(context, local_path, remote_name):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)

    context.item = context.dataset.items.upload(
        local_path=local_path, remote_path=None, remote_name=remote_name
    )


@behave.when(u'I upload the file in path "{local_path}" with description "{description}"')
def step_impl(context, local_path, description):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)

    context.item = context.dataset.items.upload(
        local_path=local_path, remote_path=None, item_description=description
    )


@behave.when(
    u'I upload the file from path "{local_path}" with remote name "{remote_name}" to remote path "{remote_path}"')
def step_impl(context, local_path, remote_path, remote_name):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)

    context.item = context.dataset.items.upload(
        local_path=local_path, remote_path=remote_path, remote_name=remote_name
    )


@behave.then(u'Item in host has name "{remote_name}"')
def step_impl(context, remote_name):
    assert remote_name in context.dataset.items.get(item_id=context.item.id).filename


@behave.then(u"Item was merged to host")
def step_impl(context):
    assert context.item_get.id == context.item.id


@behave.then(u"Item was overwrite to host")
def step_impl(context):
    assert context.item_get.id != context.item.id


@behave.when(u'I try to upload file in path "{local_path}" to remote path "{illegal_remote_path}"')
def step_impl(context, local_path, illegal_remote_path):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)
    # count error logs before
    context.error_logs_before = sum([1 for filename in os.listdir(os.getcwd()) if filename.startswith("log_")])
    # upload
    context.item = context.dataset.items.upload(local_path=local_path,
                                                remote_path=illegal_remote_path)
    # count error logs after
    context.error_logs_after = sum([1 for filename in os.listdir(os.getcwd()) if filename.startswith("log_")])


@behave.when(u'I try to upload file in path "{illegal_local_path}"')
def step_impl(context, illegal_local_path):
    illegal_local_path = os.path.join(
        os.environ["DATALOOP_TEST_ASSETS"], illegal_local_path
    )
    try:
        context.item = context.dataset.items.upload(
            local_path=illegal_local_path, remote_path=None
        )
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u"I download items to buffer")
def step_impl(context):
    items = context.dataset.items.get_all_items()
    context.buffer = context.dataset.items.download(
        items=items,
        save_locally=False,
        local_path=None,
        annotation_options=None,
    )


@behave.given(u"I delete all items from host")
def step_impl(context):
    context.item_list = context.dataset.items.list()
    for item in context.item_list.items:
        if item.type != "dir":
            context.dataset.items.delete(item_id=item.id)
    context.item_list = context.dataset.items.list()
    assert len(context.item_list.items) == 0


@behave.when(u"I upload items from buffer to host")
def step_impl(context):
    context.items_uploaded = 0
    for buff in context.buffer:
        uploaded_filename = "file" + str(context.items_uploaded) + ".jpg"
        buff.name = uploaded_filename
        context.items_uploaded += 1
        item = context.dataset.items.upload(buff, remote_path=None)
        # wait for platform attributes
        limit = 10 * 30
        stat = time.time()
        while True:
            time.sleep(3)
            item = context.dataset.items.get(item_id=item.id)
            if "video" in item.mimetype:
                if item.fps is not None:
                    break
            elif item.mimetype is not None:
                break
            if time.time() - stat > limit:
                raise TimeoutError("Timeout while waiting for platform attributes")


@behave.then(u"Number of error files should be larger by one")
def step_impl(context):
    assert context.error_logs_before + 1 == context.error_logs_after


@behave.then(u'There are "{item_count}" items in host')
def step_impl(context, item_count):
    filters = context.dl.Filters()
    filters.add(field='type', values='file')
    context.item_list = context.dataset.items.list(filters=filters)
    assert len(context.item_list.items) == int(item_count) == context.items_uploaded


@behave.when(u'I use a buffer to upload the file in path "{local_path}" with remote name "{remote_name}"')
def step_impl(context, local_path, remote_name):
    local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], local_path)

    with open(local_path, "rb") as f:
        buffer = io.BytesIO(f.read())

    context.item = context.dataset.items.upload(local_path=buffer, remote_name=remote_name)


@behave.then(u'Item mimetype is the item type "{item_type}"')
def step_impl(context, item_type):
    assert context.item.metadata['system']['mimetype'].split('/')[1] == item_type


@behave.when(u'There are "{item_count}" items')
def step_impl(context, item_count):
    filepath = "0000000162.jpg"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)

    filename = 'file'
    counter = 0
    while counter < int(item_count):
        uploaded_filename = filename + str(counter) + '.jpg'
        import io
        with open(filepath, 'rb') as f:
            buffer = io.BytesIO(f.read())
            buffer.name = uploaded_filename
        context.dataset.items.upload(
            local_path=buffer,
            remote_path=None
        )
        counter += 1


@behave.when(u'There are "{item_count}" videos')
@behave.given(u'There are "{item_count}" videos')
def step_impl(context, item_count):
    filepath = "sample_video.mp4"
    filepath = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], filepath)
    filename = 'file'
    counter = 0
    while counter < int(item_count):
        uploaded_filename = filename + str(counter) + '.mp4'
        import io
        with open(filepath, 'rb') as f:
            buffer = io.BytesIO(f.read())
            buffer.name = uploaded_filename
        context.dataset.items.upload(
            local_path=buffer,
            remote_path=None
        )
        counter += 1


================================================
File: tests/features/steps/items_repo/test_items_upload_batch.py
================================================
import behave
import os


@behave.when(u'I upload item batch from "{local_path}"')
def step_impl(context, local_path):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], local_path)
    context.dataset.items.upload(local_path=local_path,
                                 remote_path=None)


@behave.then(u'Items in "{download_path}" should equal items in "{upload_path}"')
def step_impl(context, download_path, upload_path):
    import cv2
    download_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], download_path)
    upload_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], upload_path)

    download_list = os.listdir(download_path)
    upload_list = os.listdir(upload_path)
    assert len(download_list) == len(upload_list)

    for item in download_list:
        original = cv2.imread(os.path.join(upload_path, item))
        downloaded = cv2.imread(os.path.join(download_path, item))
        if original.shape == downloaded.shape:
            difference = cv2.subtract(original, downloaded)
            b, g, r = cv2.split(difference)
            if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:
                assert True
            else:
                assert False
        else:
            assert False


================================================
File: tests/features/steps/items_repo/test_items_upload_dataframe.py
================================================
import behave
import os


@behave.when(u'I upload item using data frame from "{upload_path}"')
def step_impl(context, upload_path):
    import pandas
    upload_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], upload_path)

    # get filepathes
    filepaths = list()
    # go over all file and run ".feature" files
    count = 0
    for path, subdirs, files in os.walk(upload_path):
        for filename in files:
            striped, ext = os.path.splitext(filename)
            if ext in ['.jpg', '.png']:
                filepaths.append({'local_path': os.path.join(path, filename),
                                  'item_metadata': {'user':
                                                        {'dummy': count}}})
                count += 1

    df = pandas.DataFrame(filepaths)
    context.dataset.items.upload(local_path=df)


@behave.then(u'Items should have metadata')
def step_impl(context):
    items = context.dataset.items.get_all_items()
    for item in items:
        try:
            item.metadata['user']['dummy']
        except Exception:
            assert False


================================================
File: tests/features/steps/items_repo/test_upload_and_download_images.py
================================================
from behave import given, then, when
import os
import urllib.request
import shutil
import base64


@given(u'I get "{num_of_items}" images of type "{type_of_images}"')
def step_impl(context, num_of_items, type_of_images):
    context.local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Temp", "")
    context.num_of_items = num_of_items
    context.type_of_images = type_of_images

    try:
        os.makedirs(context.local_path)
    except:
        print("Path was created")

    # imgNum - number of images
    # imgDim - images dimension
    imgNum = int(num_of_items)
    imgDim = [500, 500]

    context.images_sizes_list = []
    # Downloading "imgNum" images to cwd with names: "stock-image-X.jpg"
    for i in range(1, int(imgNum) + 1):
        i = str(i)
        urllib.request.urlretrieve(('https://picsum.photos/' + str(imgDim[0]) + '/' + str(imgDim[1]) + '?random'),
                                   (context.local_path + 'stock-image-' + i + '.' + type_of_images))

        with open(context.local_path + 'stock-image-' + i + '.' + type_of_images, "rb") as image_file:
            context.images_sizes_list.append(base64.b64encode(image_file.read()))


@when(u'I upload all the images')
def step_impl(context):
    context.dataset.items.upload(local_path=context.local_path)


@when(u'I download all the images')
def step_impl(context):
    context.dataset.items.download(local_path=context.local_path)


@then(u'The images werent changed')
def step_impl(context):
    items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Temp/items", "")
    context.downloaded_images_sizes_list = []

    for i in range(1, int(context.num_of_items) + 1):
        i = str(i)

        with open(items_path + 'stock-image-' + i + '.' + context.type_of_images, "rb") as image_file:
            context.downloaded_images_sizes_list.append(base64.b64encode(image_file.read()))

    shutil.rmtree(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Temp"))

    for i in range(0, int(context.num_of_items)):
        assert str(context.downloaded_images_sizes_list[int(i)]) == str(context.images_sizes_list[int(i)])


@when(u'I overwrite "{num_of_items}" images of type "{type_of_images}"')
def step_impl(context, num_of_items, type_of_images):
    context.num_of_items = num_of_items
    context.type_of_images = type_of_images

    try:
        os.makedirs(context.local_path)
    except:
        print("Path was created")

    # imgNum - number of images
    # imgDim - images dimension
    imgNum = int(num_of_items)
    imgDim = [500, 500]

    context.overwritten_images_sizes_list = []
    # Downloading "imgNum" images to cwd with names: "stock-image-X.jpg"
    for i in range(1, int(imgNum) + 1):
        i = str(i)
        urllib.request.urlretrieve(('https://picsum.photos/' + str(imgDim[0]) + '/' + str(imgDim[1]) + '?random'),
                                   (context.local_path + 'stock-image-' + i + '.' + type_of_images))

        with open(context.local_path + 'stock-image-' + i + '.' + type_of_images, "rb") as image_file:
            context.overwritten_images_sizes_list.append(base64.b64encode(image_file.read()))


@when(u'I download the item with Overwrite "{overwrite_status}"')
def step_impl(context, overwrite_status):
    items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Temp", "")
    item = context.dataset.items.get(filepath="/stock-image-1." + context.type_of_images)

    context.downloaded_images_sizes_list = []

    if overwrite_status == "True":
        context.file_path = item.download(local_path=items_path, overwrite=True, to_items_folder=False)
    else:
        item.download(local_path=items_path, overwrite=False, to_items_folder=False)

    with open(items_path + 'stock-image-1' + '.' + context.type_of_images, "rb") as image_file:
        context.downloaded_images_sizes_list.append(base64.b64encode(image_file.read()))


@when(u'I download the item with Overwrite value "{overwrite_status}" to path "{file_path}"')
def step_impl(context, overwrite_status, file_path):
    items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)

    if overwrite_status == "True":
        context.download_path = context.item.download(local_path=items_path, overwrite=True)
    else:
        context.download_path = context.item.download(local_path=items_path, overwrite=False)


@then(u'check that the new download will be with the same path "{file_path}"')
def step_impl(context, file_path):
    items_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    download_path = context.item.download(local_path=items_path, overwrite=True)
    shutil.rmtree(items_path)
    assert context.download_path == download_path, "The download path is not the same as the previous one"


@then(u'The images will be "{is_overwritten}"')
def step_impl(context, is_overwritten):
    if is_overwritten == "overwritten":
        assert str(context.downloaded_images_sizes_list[0]) == str(context.images_sizes_list[0])
        assert str(context.downloaded_images_sizes_list[0]) != str(context.overwritten_images_sizes_list[0])
    else:
        assert str(context.downloaded_images_sizes_list[0]) != str(context.images_sizes_list[0])
        assert str(context.downloaded_images_sizes_list[0]) == str(context.overwritten_images_sizes_list[0])

    shutil.rmtree(os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "Upload_Download_Temp"))


================================================
File: tests/features/steps/ml_subsets/test_ml_subsets.py
================================================
import behave
import dtlpy as dl
import random


@behave.given(u'I upload 10 items to the dataset')
def step_impl(context):
    # Assuming 'context.dataset' is already created in the background steps
    # Upload 10 dummy items (you can upload real files, or mock this logic)
    for i in range(10):
        context.dataset.items.upload(local_path='/path/to/dummy/file{}.jpg'.format(i))
    # Refresh dataset
    context.items = list(context.dataset.items.list().all())
    assert len(context.items) == 10


@behave.when(u'I split dataset items into ML subsets with default percentages')
def step_impl(context):
    # Using default percentages of 80-10-10
    filters = dl.Filters(field='type', values='file')
    context.dataset.split_ml_subsets(items_query=filters)


@behave.then(u'Items are splitted according to the default ratio')
def step_impl(context):
    # After splitting, ~80% train, ~10% val, ~10% test.
    # Let's count subsets.
    items = list(context.dataset.items.list(filters=dl.Filters(field='type', values='file')).all())
    train_count = 0
    val_count = 0
    test_count = 0
    for item in items:
        subset = item.get_current_subset()
        if subset == 'train':
            train_count += 1
        elif subset == 'validation':
            val_count += 1
        elif subset == 'test':
            test_count += 1

    assert train_count > 0
    assert val_count > 0
    assert test_count > 0
    # Simple ratio check (not exact due to rounding):
    # Just ensure distribution is as expected.
    total = len(items)
    assert abs((train_count/total)*100 - 80) < 20  # Tolerant check
    assert abs((val_count/total)*100 - 10) < 10
    assert abs((test_count/total)*100 - 10) < 10


@behave.given(u'I select 3 specific items from the dataset')
def step_impl(context):
    # Just pick first 3 items
    context.items = list(context.dataset.items.list(filters=dl.Filters(field='type', values='file')).all())
    context.selected_item_ids = [item.id for item in context.items[:3]]
    

@behave.when(u'I assign the \'train\' subset to those selected items')
def step_impl(context):
    filters = dl.Filters()
    filters.add(field='id', values=context.selected_item_ids, operator=dl.FiltersOperations.IN)
    response = context.dataset.assign_subset_to_items(items_query=filters, subset='train')
    assert response == True

@behave.then(u'Those items have train subset assigned')
def step_impl(context):
    for item_id in context.selected_item_ids:
        item = context.dataset.items.get(item_id=item_id)
        result = item.get_current_subset()
        assert result == 'train'


@behave.when(u'I remove subsets from those selected items')
def step_impl(context):
    filters = dl.Filters()
    filters.add(field='id', values=context.selected_item_ids, operator=dl.FiltersOperations.IN)
    response = context.dataset.remove_subset_from_items(items_query=filters)
    assert response == True


@behave.then(u'Those items have no ML subset assigned')
def step_impl(context):
    for item_id in context.selected_item_ids:
        item = context.dataset.items.get(item_id=item_id)
        result = item.get_current_subset()
        assert result is None


@behave.given(u'I have a single item from the dataset')
def step_impl(context):
    context.items = list(context.dataset.items.list(filters=dl.Filters(field='type', values='file')).all())
    # pick one random item
    context.single_item = context.items[0]


@behave.when(u'I assign the \'validation\' subset to this item at the item level')
def step_impl(context):
    context.single_item.assign_subset('validation')


@behave.then(u'The item has \'validation\' subset assigned')
def step_impl(context):
    result = context.single_item.get_current_subset()
    assert result == 'validation'


@behave.given(u'I have a single item with a subset assigned')
def step_impl(context):
    context.items = list(context.dataset.items.list(filters=dl.Filters(field='type', values='file')).all())
    context.single_item = context.items[1]
    context.single_item.assign_subset('test')



@behave.when(u'I remove the subset from the item')
def step_impl(context):
    context.single_item.remove_subset()


@behave.then(u'The item has no ML subset assigned')
def step_impl(context):
    context.single_item = context.dataset.items.get(item_id=context.single_item.id)
    result = context.single_item.get_current_subset()
    assert result is None


@behave.given(u'I have a single item with \'test\' subset assigned')
def step_impl(context):
    context.items = list(context.dataset.items.list(filters=dl.Filters(field='type', values='file')).all())
    context.single_item = context.items[2]
    context.single_item.assign_subset('test')
    assert context.single_item.get_current_subset() == 'test'


@behave.when(u'I get the current subset of the item')
def step_impl(context):
    context.current_subset = context.single_item.get_current_subset()


@behave.then(u'The result is \'test\'')
def step_impl(context):
    assert context.current_subset == 'test'


@behave.given(u'Some items in the dataset have subsets assigned and some do not')
def step_impl(context):
    context.items = list(context.dataset.items.list(filters=dl.Filters(field='type', values='file')).all())
    # Assign 'train' to first 2 items, leave others without subsets
    for item in context.items[:2]:
        item.assign_subset('train')
    # Others remain without subset


@behave.when(u'I get items missing ML subset')
def step_impl(context):
    context.missing_ids = context.dataset.get_items_missing_ml_subset()


@behave.then(u'I receive a list of item IDs with no ML subset assigned')
def step_impl(context):
    # The first 2 have train, others have None.
    assigned_ids = set(item.id for item in context.items[:2])
    missing_ids = set(item.id for item in context.items[2:])
    received_ids = set(context.missing_ids)
    assert received_ids == missing_ids
    assert received_ids and received_ids.isdisjoint(assigned_ids)


================================================
File: tests/features/steps/model_entity/test_model_name.py
================================================
import behave


@behave.when(u'I rename model to "{model_name}"')
def step_impl(context, model_name):
    context.model.name = model_name
    context.model.update()


@behave.then(u'model name is "{model_name}"')
def step_impl(context, model_name):
    context.model = context.dl.models.get(model_id=context.model.id)
    assert context.model.name == model_name, f"TEST FAILED: model name is different. from be: {context.model.name}, from context: {model_name}"


@behave.then(u'The model name not changed')
def step_impl(context):
    model_name = context.model.name
    model = context.project.models.get(model_id=context.model.id)
    assert model_name == model.name, f"TEST FAILED: Expected model name: '{model_name}', Actual: '{model.name}'"

================================================
File: tests/features/steps/models_repo/test_model_flow.py
================================================
import json
import time
import random

import behave

import dtlpy as dl
import os


@behave.when(u'I create a dummy model package by the name of "{package_name}" with entry point "{entry_point}"')
def step_impl(context, package_name, entry_point):
    if package_name == "ac-lr-package":
        context.codebase_local_dir = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], 'model_ac_lr')
        docker_image = None
    else:
        context.codebase_local_dir = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], 'models_flow')
        docker_image = 'jjanzic/docker-python3-opencv'
    module = dl.PackageModule.from_entry_point(
        entry_point=os.path.join(context.codebase_local_dir, entry_point))
    context.dpk.components.modules._dict[0] = module.to_json()
    context.codebase = context.project.codebases.pack(
        directory=context.codebase_local_dir,
        name=package_name,
        description="some description"
    )
    context.dpk.codebase = context.codebase
    context.dpk.components.modules[0].entry_point = entry_point
    context.dpk.name = f"to_delete_{package_name}_{str(random.randrange(0, 1000))}"
    context.dpk.display_name = f"to_delete_{package_name}_{str(random.randrange(0, 1000))}"
    if (context.dpk.components.compute_configs._dict):
        context.dpk.components.compute_configs._dict[0].update({
            "name": "default",
            "versions": {"dtlpy": dl.__version__},
            'runtime': dl.KubernetesRuntime(
                runner_image=docker_image,
                pod_type=dl.InstanceCatalog.REGULAR_XS,
                autoscaler=dl.KubernetesRabbitmqAutoscaler(
                    min_replicas=1,
                    max_replicas=5),
                concurrency=10).to_json()
        })
    else:
        context.dpk.components.compute_configs._dict[0] = {
            "name": "default",
            "versions": {"dtlpy": dl.__version__},
            'runtime': dl.KubernetesRuntime(
                runner_image=docker_image,
                pod_type=dl.InstanceCatalog.REGULAR_XS,
                autoscaler=dl.KubernetesRabbitmqAutoscaler(
                    min_replicas=1,
                    max_replicas=5),
                concurrency=10).to_json()
        }


@behave.when(u'model should be with mltype "{mltype}"')
@behave.then(u'model should be with mltype "{mltype}"')
def step_impl(context, mltype):
    assert context.model.metadata.get('system', {}).get(
        'mlType').get('subType',
                      {}) == mltype, f"TEST FAILED: model ml_type is {context.model.ml_type} and not {mltype}"


@behave.then(u'model should have a new configration')
def step_impl(context):
    context.model = dl.models.get(model_id=context.model.id)
    expected_config = {
        'system_prompt': 'test',
        'max_tokens': 100,
        'temperature': 0.5
    }
    assert context.model.configuration['system_prompt'] == expected_config[
        'system_prompt'], f"TEST FAILED: model configuration is not as expected, expected: {expected_config['system_prompt']}, got: {context.model.configuration['system_prompt']}"
    assert context.model.configuration['max_tokens'] == expected_config[
        'max_tokens'], f"TEST FAILED: model configuration is not as expected, expected: {expected_config['max_tokens']}, got: {context.model.configuration['max_tokens']}"
    assert context.model.configuration['temperature'] == expected_config[
        'temperature'], f"TEST FAILED: model configuration is not as expected, expected: {expected_config['temperature']}, got: {context.model.configuration['temperature']}"


@behave.when(u'I create a model from package by the name of "{model_name}" with status "{status}" in index "{index}"')
def step_impl(context, model_name, status, index):
    model = {
        'name': model_name,
        'description': 'model for testing',
        'datasetId': context.dataset.id,
        'moduleName': context.dpk.components.modules[0].name,
        'scope': 'project',
        'model_artifacts': [dl.LinkArtifact(
            url='https://storage.googleapis.com/model-mgmt-snapshots/ResNet50/model.pth',
            filename='model.pth').to_json()],
        'status': status,
        'configuration': {'weights_filename': 'model.pth',
                          'batch_size': 16,
                          'num_epochs': 10},
        'labels': [label.tag for label in context.dataset.labels],
        'metadata': {'system': {'subsets': {'train': dl.Filters().prepare(),
                                            'validation': dl.Filters().prepare()}}}
    }
    if int(index) == 0:
        context.dpk.components.models[int(index)].update(model)
    else:
        context.dpk.components.models.append(model)


@behave.when(u'i fetch the model by the name "{model_name}"')
def step_impl(context, model_name):
    context.model = context.project.models.get(model_name=model_name)


@behave.when(u'I upload artifact in "{item_path}"')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.model.artifacts.upload(filepath=item_path)
    context.model = dl.models.get(model_id=context.model.id)


@behave.then(u'src model and clone model should have the same artifact name and different id')
def step_impl(context):
    src_arts = dl.items.get(item_id=context.model.model_artifacts[0].id)
    clone_arts = dl.items.get(item_id=context.model_clone.model_artifacts[0].id)
    assert src_arts.name == clone_arts.name, f"TEST FAILED: artifact name is not as expected, expected: {src_arts.name}, got: {clone_arts.name}"
    assert src_arts.id != clone_arts.id, f"TEST FAILED: artifact id is not as expected, expected: {src_arts.id}, got: {clone_arts.id}"


@behave.then(u'"{obj_entity}" has app scope')
@behave.when(u'"{obj_entity}" has app scope')
def step_impl(context, obj_entity):
    if not hasattr(context, obj_entity):
        raise AttributeError(f"Please make sure context has attr '{obj_entity}'")

    dpk_base_id = context.dpk.base_id if context.dpk.base_id else context.published_dpk.base_id
    dpk_version = context.dpk.version if context.dpk.version else context.published_dpk.version
    assert getattr(context, obj_entity).app, f"TEST FAILED: Expected to have 'app' attribute in context.{obj_entity}"

    assert getattr(context, obj_entity).app[
               'id'] == context.app.id, f"TEST FAILED: app id is not as expected, expected: {context.app.id}, got: {getattr(context, obj_entity).app['id']}"
    assert getattr(context, obj_entity).app[
               'dpkId'] == dpk_base_id, f"TEST FAILED: dpk id is not as expected, expected: {dpk_base_id}, got: {getattr(context, obj_entity).app['dpkId']}"
    assert getattr(context, obj_entity).app[
               'dpkVersion'] == dpk_version, f"TEST FAILED: dpk version is not as expected, expected: {dpk_version}, got: {getattr(context, obj_entity).app['dpkVersion']}"
    assert getattr(context, obj_entity).app[
               'dpkName'] == context.dpk.name, f"TEST FAILED: dpk name is not as expected, expected: {context.dpk.name}, got: {getattr(context, obj_entity).app['dpkName']}"
    if hasattr(getattr(context, obj_entity), 'package_revision'):
        assert getattr(context,
                       obj_entity).package_revision == dpk_version, f"TEST FAILED: service package version is not as expected, expected: {dpk_version}, got: {getattr(context, obj_entity).package_revision}"


@behave.then(u'service runnerImage is "{runner_image}"')
def step_impl(context, runner_image):
    assert context.service.runtime.runner_image == runner_image, f"service runner image expected :{runner_image} got {context.service.runtime.runner_image}"


@behave.when(u'I run model embed datasets')
def step_impl(context):
    try:
        context.model = dl.models.get(model_id=context.model.id)
        all_datasets = context.project.datasets.list()
        dataset_ids = [dataset.id for dataset in all_datasets]
        context.command = context.model.embed_datasets(dataset_ids=dataset_ids, attach_trigger=True)
    except Exception as err:
        pass



@behave.then(u'command massage is in model')
def step_impl(context):
    model = dl.models.get(model_id=context.model.id)
    err_msg = model.metadata.get('system', {}).get('embedDatasets', {}).get('error', None)
    assert err_msg, f"TEST FAILED: command message is not in model metadata"


@behave.then(u'model service has "{executions_num}" executions and "{triggers_num}" triggers')
def step_impl(context, executions_num, triggers_num):
    context.model = dl.models.get(model_id=context.model.id)
    services = context.model.services.list().items[0]
    executions = services.executions.list().items
    triggers = services.triggers.list().items
    assert len(executions) == int(
        executions_num), f"TEST FAILED: Expected {executions_num}, Actual executions in service {len(executions)}"
    assert len(triggers) == int(
        triggers_num), f"TEST FAILED: Expected {triggers_num}, Actual triggers in service {len(triggers)}"

@behave.when(u'i change model status to "{status}"')
def step_impl(context, status):
    context.model = dl.models.get(model_id=context.model.id)
    context.model.status = status
    context.model = context.model.update()

@behave.when(u'I "{func}" the model')
@behave.when(u'I "{func}" the model with exception "{flag}"')
def step_impl(context, func, flag=None):
    context.model = dl.models.get(model_id=context.model.id)
    try:
        if func == 'evaluate':
            service_config = None
            if hasattr(context, "service_config"):
                service_config = context.service_config
            context.execution = context.model.evaluate(dataset_id=context.dataset.id, filters=dl.Filters(),
                                                       service_config=service_config)
            context.to_delete_services_ids.append(context.execution.service_id)
        elif func == 'deploy':
            context.service = context.model.__getattribute__(func)()
        else:
            context.execution = context.model.__getattribute__(func)()
        context.error = None
    except Exception as e:
        # If flag is None, Test should be failed and raise error
        if not flag:
            raise e
        context.error = e


@behave.when(u'i train the model with init param model none')
def step_impl(context):
    try:
        context.execution = context.model.train(service_config={"initParams": {"model_entity": None}})
    except Exception as e:
        context.error = e


@behave.when(u'i predict the model on the item id "{item_id}"')
def step_impl(context, item_id):
    try:
        context.execution = context.model.predict(item_id)
    except Exception as e:
        context.error = e


@behave.then(u'model status should be "{status}" with execution "{flag}" that has function "{func}"')
def step_impl(context, status, flag, func):
    num_try = 54
    interval = 10
    completed = False

    if eval(flag):
        if isinstance(context.execution, dl.Execution):
            for i in range(num_try):
                time.sleep(interval)
                context.execution = dl.executions.get(execution_id=context.execution.id)
                assert context.execution.function_name == func
                if context.execution.latest_status['status'] in ['success', 'failed']:
                    completed = True
                    break
    else:
        time.sleep(10)
        completed = True
    assert completed, f"TEST FAILED: execution was not completed, after {round(num_try * interval / 60, 1)} minutes"

    context.model = dl.models.get(model_id=context.model.id)
    assert context.model.status == status, f"TEST FAILED: model status is not as expected, expected: {status}, got: {context.model.status}"


@behave.then(u'model status should be "{status}"')
def step_impl(context, status):
    assert context.model_clone.status == status, f"TEST FAILED: model status is not as expected, expected: {status}, got: {context.model_clone.status}"


@behave.then(u'I clean the project')
def step_impl(context):
    i_project = dl.projects.get(project_id=context.project.id)
    for pipeline in i_project.pipelines.list().items:
        try:
            pipeline.delete()
        except Exception:
            pass
    for service in i_project.services.list().items:
        try:
            service.delete()
        except Exception as e:
            if 'Service cannot be deleted as long as it has running/pending pipeline' in str(e):
                services = service.executions.list()
                for page in services:
                    for s in page:
                        try:
                            s.terminate()
                        except Exception as e:
                            pass
            pass
    for model in i_project.models.list().items:
        try:
            model.delete()
        except Exception:
            pass


@behave.then(u'Dataset has a scores file')
def step_impl(context):
    model_csv_filename = f'{context.model.id}.csv'
    model_json_filename = f'{context.model.id}-interpolated.json'
    filters = dl.Filters(field='hidden', values=True)
    filters.add(field='name', values=[model_csv_filename, model_json_filename], operator=dl.FiltersOperations.IN)
    items = context.dataset.items.list(filters=filters)
    assert items.items_count != 0, f'Found {items.items_count} items with name {model_csv_filename}.'


@behave.when(u'i call the precision recall api')
def step_impl(context):
    payload = {
        'datasetId': context.dataset.id,
        'confThreshold': 0,
        'iouThreshold': 0.3,
        'metric': 'accuracy',
        'modelId': context.model.id,
    }
    success, response = dl.client_api.gen_request(req_type="post",
                                                  path=f"/ml/metrics/precisionRecall",
                                                  json_req=payload)
    assert success, f'Failed to call precision recall api, response: {response}'
    context.response = response.json()


@behave.then(u'i should get a json response')
def step_impl(context):
    assert isinstance(context.response, dict), f'Failed to call precision recall api, response: {context.response}'
    with open(os.path.join(os.environ["DATALOOP_TEST_ASSETS"], 'models_flow', 'precisionrecall.json'), 'r') as f:
        expected_output = json.load(f)

    assert context.response == expected_output, f'results are not as expected, expected: {expected_output}, got: {context.response}'
    assert len(context.response['recall']) == 202 and len(context.response[
                                                              'precision']) == 202, f'points are not as expected, expected: 201, got: {len(context.response["recall"])}'


@behave.given(u'I upload "{item_num}" box annotation to item')
@behave.when(u'I upload "{item_num}" box annotation to item')
def step_impl(context, item_num):
    builder = context.item.annotations.builder()
    item_num = int(item_num)
    for i in range(item_num):
        builder.add(annotation_definition=dl.Box(top=i * 10, left=i * 10, bottom=i * 20, right=i * 20, label=str(i)))
    context.item.annotations.upload(builder)


@behave.then(u'Log "{log_message}" is in model.log() with operation "{operation}"')
def step_impl(context, log_message, operation):
    num_tries = 60
    interval_time = 5
    success = False

    for i in range(num_tries):
        time.sleep(interval_time)
        for log in context.model.log(view=False, model_operation=operation):
            if log_message in log:
                success = True
                break
        if success:
            break

    assert success, f"TEST FAILED: after {round(num_tries * interval_time / 60, 1)} minutes"


@behave.then(u'service metadata has a model id and operation "{operation}"')
def step_impl(context, operation):
    context.service = dl.services.get(service_id=context.execution.service_id)
    assert context.service.metadata.get('ml', {}).get(
        'modelId') == context.model.id, f"TEST FAILED: model id is not in service metadata"
    assert context.service.metadata.get('ml', {}).get(
        'modelOperation') == operation, f"TEST FAILED: model operation is not in service metadata"


@behave.when(u'I add service_config to context from dpk model configuration')
def step_impl(context):
    context.service_config = context.dpk.components.models[0]['metadata']['system']['ml']['serviceConfig']
    context.service_config['versions'] = {"dtlpy": dl.__version__}


@behave.when(u'i add a Service config runtime')
def step_impl(context):
    context.service_config = {'runtime': {}}
    params = context.table.headings
    for row in params:
        row = row.split('=')
        context.service_config['runtime'][row[0]] = row[1]


@behave.then(u'check service runtime')
def step_impl(context):
    service = dl.services.get(service_id=context.execution.service_id).to_json()
    params = context.table.headings
    for row in params:
        row = row.split('=')
        assert service['runtime'][row[0]] == row[
            1], f"TEST FAILED: service runtime is not as expected, expected: {row[1]}, got: {service['runtime'][row[0]]}"


@behave.then(u'i have a model service')
@behave.when(u'i have a model service')
def step_impl(context):
    context.service = context.model.services.list().items[0]
    assert context.service is not None, f"TEST FAILED: service is not in model services"
    assert context.service.metadata.get('ml', {}).get(
        'modelId') == context.model.id, f"TEST FAILED: model id is not in service metadata"


@behave.when(u'I update the model variable in pipeline to reference to this model')
def step_impl(context):
    context.pipeline.variables[0].value = context.model.id
    context.pipeline = context.pipeline.update()


@behave.then(u'the model service id updated')
def step_impl(context):
    service = context.model.services.list().items[0]
    assert service is not None, f"TEST FAILED: service is not in model services"
    assert service.id == context.service.id, f"TEST FAILED: service id is not as expected, expected: {context.service.id}, got: {service.id}"
    assert service.metadata.get('ml', {}).get(
        'modelId') == context.model.id, f"TEST FAILED: model id is not in service metadata"


@behave.when(u'i add service id "{service_id}" to model metadata')
def step_impl(context, service_id):
    context.model = dl.models.get(model_id=context.model.id)
    context.model.metadata['system']['deploy']['services'].append(service_id)
    context.model = context.model.update(True)


@behave.when(u'I delete model')
@behave.then(u'I delete model')
def step_impl(context):
    context.model.delete()


@behave.then(u'model is deleted')
def step_impl(context):
    try:
        context.model = dl.models.get(model_id=context.model.id)
        assert False, "TEST FAILED: model is not deleted"
    except:
        pass


@behave.then(u'model metadata should include operation "{operation}" with filed "{field}" and length "{value}"')
def step_impl(context, operation, field, value):
    value = int(value)
    context.model = dl.models.get(model_id=context.model.id)
    assert operation in context.model.metadata['system'], f"TEST FAILED: operation {operation} is not in model metadata"
    assert field in context.model.metadata['system'][operation], f"TEST FAILED: field {field} is not in model metadata"
    assert len(context.model.metadata['system'][operation][
                   field]) == value, f"TEST FAILED: field {field} length is not as expected, expected: {value}, got: {len(context.model.metadata['system'][operation][field])}"


@behave.when(u'I clone a model')
@behave.when(u'I clone a model and set status "{model_status}"')
def step_impl(context, model_status='created'):
    context.model = context.model.clone(
        model_name='clone_model',
        project_id=context.model.project_id,
        dataset=dl.datasets.get(dataset_id=context.model.dataset_id),
        status=model_status
    )


@behave.then(u'model input should be equal "{input}", and output should be equal "{output}"')
def step_impl(context, input, output):
    assert context.model.input_type == input, f"TEST FAILED: model input is not as expected, expected: {input}, got: {context.model.input}"
    assert context.model.output_type == output, f"TEST FAILED: model output is not as expected, expected: {output}, got: {context.model.output}"


@behave.then(u'model do not have operation "{operation}"')
def step_impl(context, operation):
    assert operation not in context.model.metadata.get('system',
                                                       {}), f"TEST FAILED: operation {operation} is in model metadata"


@behave.then(u'models with the names "{models_name}" status "{model_status}"')
def step_impl(context, models_name, model_status):
    names = models_name.split(",")

    for model in context.project.models.list().items:
        if model.name in names:
            assert model.status == model_status, f"TEST FAILED: model {model.id} status is not as expected, expected: {model_status}, got: {model.status}"


@behave.when(u'I remove attributes "{values}" from dpk model in index "{index}"')
def step_impl(context, values, index=0):
    model = context.dpk.components.models[int(index)]
    values = values.split(",")
    for value in values:
        if "metadata" in value:
            if "system" in value:
                del model['metadata']['system'][value.split('.')[-1]]
            else:
                del model['metadata'][value.split('.')[-1]]
        else:
            del model[value]


================================================
File: tests/features/steps/models_repo/test_models_create.py
================================================
import behave
import random
import string
import os


@behave.when(u'I create a model with a random name')
@behave.given(u'I create a model with a random name')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    model_name = 'random-model-{}'.format(rand_str)
    # create dataset
    try:
        context.dataset = context.project.datasets.get('model_dataset')
    except context.dl.exceptions.NotFound:
        context.dataset = context.project.datasets.create('model_dataset', index_driver=context.index_driver_var)


    # create model
    context.model = context.package.models.create(model_name=model_name,
                                                  dataset_id=context.dataset.id,
                                                  ontology_id=context.dataset.ontology_ids[0],
                                                  train_filter=context.dl.Filters(),
                                                  project_id=context.project.id)
    if not hasattr(context, 'model_count'):
        context.model_count = 0
    context.model_count += 1


@behave.when(u'I create a model without dataset')
def step_impl(context):
    rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
    model_name = 'random_model-{}'.format(rand_str)
    context.model = context.package.models.create(model_name=model_name, dataset_id=None, project_id=context.project.id)
    if not hasattr(context, 'model_count'):
        context.model_count = 0
    context.model_count += 1


@behave.when(u'I create a model without filter')
def step_impl(context):
    try:
        rand_str = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))
        model_name = 'random_model-{}'.format(rand_str)
        # create dataset
        try:
            context.dataset = context.project.datasets.get('model_dataset')
        except context.dl.exceptions.NotFound:
            context.dataset = context.project.datasets.create('model_dataset', index_driver=context.index_driver_var)
        # create model
        context.model = context.package.models.create(model_name=model_name,
                                                      dataset_id=context.dataset.id,
                                                      ontology_id=context.dataset.ontology_ids[0],
                                                      project_id=context.project.id)
    except Exception as e:
        context.error = e


@behave.when(u'I update model status to "{model_status}"')
@behave.given(u'I update model status to "{model_status}"')
def step_impl(context, model_status):
    context.model.status = model_status
    context.model.update()


@behave.when(u'I deploy the model')
def step_impl(context):
    context.model.deploy()


@behave.when(u'I train the model')
def step_impl(context):
    try:
        context.model.train()
    except Exception as e:
        context.error = e


@behave.Then(u'Model filter should not be empty')
def step_impl(context):
    assert context.model.metadata['system'].get('subsets', {}).get('train',
                                                                   None) is not None, 'train filter is empty'


@behave.then(u'The project have only one bot')
def step_impl(context):
    bots = context.project.bots.list()
    assert len(bots) == 1, 'more than one bot was created'


@behave.then(u'Model object with the same name should be exist')
def step_impl(context):
    assert isinstance(context.model, context.dl.entities.Model)


@behave.then(u'Model object with the same name should be exist in host')
def step_impl(context):
    model_get = context.project.models.get(model_name=context.model.name)
    assert model_get.to_json() == context.model.to_json()


@behave.then(u'Model module_name should be "{module_name}"')
def step_impl(context, module_name):
    module_name_input = module_name.split(",")
    if len(module_name_input) > 1:
        for i in range(len(module_name_input)):
            assert context.models[i].module_name == module_name_input[i]
    else:
        assert context.model.module_name == module_name


@behave.when(u'I create a model with the same name')
def step_impl(context):
    try:
        context.package.models.create(model_name=context.model.name,
                                      dataset_id=context.dataset.id,
                                      ontology_id=context.dataset.ontology_ids[0],
                                      project_id=context.project.id)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'No model was created')
def step_impl(context):
    pages = context.project.models.list()
    assert pages.items_count == context.model_count, 'model count doesnt match. {} from server, {} from test'.format(
        pages.items_count, context.model_count)

@behave.when(u'I upload an artifact "{artifact_path}" to the model')
def step_impl(context, artifact_path):
    artifact_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], artifact_path)
    context.model.artifacts.upload(filepath=artifact_path)


@behave.when(u'I clone the model')
def step_impl(context):
    context.model_clone = context.model.clone(model_name='clone_{}'.format(context.model.name))


@behave.when(u'I delete the clone model')
def step_impl(context):
    context.model_clone.delete()


@behave.then(u'artifact is exist in the host')
def step_impl(context):
    try:
        artifact = context.dl.items.get(item_id=context.model.model_artifacts[0].id)
    except context.dl.exceptions.NotFound:
        artifact = None
    assert artifact is not None, 'artifact not found'


================================================
File: tests/features/steps/models_repo/test_models_delete.py
================================================
import behave


@behave.Given(u'There are no models in project')
def step_impl(context):
    models = context.project.models.list()
    count = 0
    for model in models.all():
        model.delete()
        count += 1
    print(f'deleted {count} models')
    if not hasattr(context, 'model_count'):
        context.model_count = 0


================================================
File: tests/features/steps/models_repo/test_models_get.py
================================================
import behave


@behave.when(u'I get last model in project')
def step_impl(context):
    context.model = context.project.models.list().items[-1]


================================================
File: tests/features/steps/models_repo/test_models_list.py
================================================
import behave


@behave.when(u'I create "{num_models}" models')
def step_impl(context, num_models):
    models = list()
    for i_model in range(int(num_models)):
        models.append(context.package.models.create(model_name='model-num-{}'.format(i_model),
                                                    dataset_id=context.dataset.id,
                                                    labels=[]))
    context.models = models


@behave.when(u'I list models with filter field "{field}" and values "{values}"')
def step_impl(context, field, values):
    filters = context.dl.Filters(resource='models',
                                 field=field,
                                 values=values)
    context.list_results = list(context.project.models.list(filters=filters).all())


@behave.then(u'I get "{models_number}" entities')
def step_impl(context, models_number):
    assert len(context.list_results) == int(models_number)


================================================
File: tests/features/steps/notifications/notifications.py
================================================
import time
import behave
import dtlpy as dl
from operator import attrgetter


@behave.given(u'Service has wrong docker image')
def step_impl(context):
    context.service.runtime.runner_image = 'randomassimagenamenoonewillneveruse19'
    context.service = context.service.update(force=True)


@behave.then(u'I receive "{error}" notification with resource "{resource_input}"')
def step_impl(context, error: str, resource_input):
    success = False
    timeout = 7 * 60
    start = time.time()
    while time.time() - start < timeout:
        messages = dl.messages._list(context={'project': context.project.id})
        if len(messages) > 0:
            ms = [m for m in messages if error in m.notification_code and m.resource_id == attrgetter(resource_input)(context)]
            if len(ms) > 0:
                success = True
                break
        time.sleep(5)

    assert success, 'No notification received'


@behave.then(u'Service is deactivated by system')
def step_impl(context):
    timeout = 10 * 60
    start = time.time()
    while time.time() - start < timeout:
        context.service = context.project.services.get(service_id=context.service.id)
        if context.service.active is False:
            break
        time.sleep(5)

    assert context.service.active is False


@behave.given(u'I delete service code base')
def step_impl(context):
    dataset = context.project.datasets._get_binaries_dataset()
    for item in dataset.items.list().all():
        item.delete()


@behave.given(u'I add bad requirements to service')
def step_impl(context):
    context.package.requirements = [
        dl.PackageRequirement(name='nofuckingwaysucharequirementexist1723', version='1.0.0')
    ]
    context.package = context.package.update()
    context.service.package_revision = context.package.version
    context.service = context.service.update()


@behave.when(u'Service minimum scale is "{scale}"')
def step_impl(context, scale: str):
    scale = int(scale)
    context.service.runtime.autoscaler.min_replicas = scale
    context.service = context.service.update()


================================================
File: tests/features/steps/ontologies_repo/test_ontologies_create.py
================================================
import time
import behave
import os
import json


@behave.given(u"dataset has recipe")
def step_impl(context):
    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    assert len(context.dataset.metadata["system"]["recipes"]) > 0


@behave.given(u"dataset has at least {count} ontology")
def step_impl(context, count):
    assert len(context.recipe.ontology_ids) >= int(count)


@behave.when(u'I create a new ontology with labels from file "{file_path}" and attributes "{attributes}"')
def step_impl(context, file_path, attributes):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)

    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    context.ontology = context.recipe.ontologies.create(labels=context.labels,
                                                        project_ids=context.dataset.project.id,
                                                        attributes=attributes)


@behave.when(u'I create a new ontology with labels from file "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)

    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    context.ontology = context.recipe.ontologies.create(labels=context.labels,
                                                        project_ids=context.dataset.project.id,
                                                        attributes=list())


@behave.when(u'I create a new ontology with no projectIds, with labels from file "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)

    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    context.ontology = context.recipe.ontologies.create(
        labels=context.labels,
        attributes=list())


@behave.when(u"I try to create a new ontology with labels '{labels}'")
def step_impl(context, labels):
    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    try:
        context.ontology = context.recipe.ontologies.create(
            labels=json.loads(labels),
            attributes=list())
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I update dataset ontology to the one created')
def step_impl(context):
    context.recipe.ontology_ids = [context.ontology.id]
    context.dataset.recipes.update(context.recipe)


@behave.when(u'I try toupdate dataset ontology to the one created')
def step_impl(context):
    context.recipe.ontology_ids = [context.ontology.id]
    try:
        context.dataset.recipes.update(context.recipe)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'Dataset ontology in host equal ontology uploaded')
def step_impl(context):
    ontology_get = context.recipe.ontologies.get(ontology_id=context.ontology.id)
    assert ontology_get.to_json() == context.ontology.to_json()
    for root in ontology_get.to_json()['roots']:
        assert root in context.labels


@behave.then(u'Dataset ontology in host have an attributes')
def step_impl(context):
    ontology_get = context.recipe.ontologies.get(ontology_id=context.ontology.id)
    assert len(ontology_get.attributes) == 2


@behave.when(u'I create a new ontology with labels and project id of "{other_project_name}" from file "{file_path}"')
def step_impl(context, other_project_name, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)

    context.other_project = context.dl.projects.get(project_name=other_project_name)

    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    context.ontology = context.recipe.ontologies.create(
        labels=context.labels,
        project_ids=context.other_project.id,
        attributes=list())


@behave.when(u'I try create a new ontology with labels and "{some_project_id}" from file "{file_path}"')
def step_impl(context, some_project_id, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)
    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata["system"]["recipes"][0])
    try:
        context.ontology = context.recipe.ontologies.create(
            labels=context.labels,
            project_ids=some_project_id,
            attributes=list())
        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u'There is another project by the name of "{other_project_name}"')
def step_impl(context, other_project_name):
    context.other_project = context.dl.projects.create(other_project_name)
    context.to_delete_projects_ids.append(context.other_project.id)
    time.sleep(5)  # to sleep because authorization takes time


================================================
File: tests/features/steps/ontologies_repo/test_ontologies_delete.py
================================================
import behave


@behave.when(u'I delete ontology by id')
def step_impl(context):
    context.recipe.ontologies.delete(ontology_id=context.ontology.id)


@behave.then(u'Ontology does not exist in dataset')
def step_impl(context):
    try:
        context.recipe.ontologies.get(ontology_id=context.ontology.id)
        assert False
    except Exception as e:
        assert "NotFound" in str(type(e))


@behave.when(u'I try to delete ontology by "{some_id}"')
def step_impl(context, some_id):
    try:
        context.recipe.ontologies.delete(ontology_id=some_id)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/ontologies_repo/test_ontologies_get.py
================================================
import behave


@behave.given(u'Dataset has ontology')
def step_impl(context):
    context.recipe = context.dataset.recipes.get(recipe_id=context.dataset.metadata['system']['recipes'][0])
    context.ontology = context.recipe.ontologies.get(ontology_id=context.recipe.ontology_ids[0])


@behave.when(u'I get a ontology by id')
def step_impl(context):
    context.ontology = context.recipe.ontologies.get(ontology_id=context.ontology.id)


@behave.then(u'I get an Ontology object')
def step_impl(context):
    assert type(context.ontology) == context.dl.Ontology


@behave.when(u'I try to get Ontology by "{some_id}"')
def step_impl(context, some_id):
    try:
        context.ontology = context.recipe.ontologies.get(ontology_id=some_id)
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/ontologies_repo/test_ontologies_update.py
================================================
import behave
import os
import json


@behave.when(u'I update ontology with labels from file "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)
    context.ontology.add_labels(context.labels)
    context.recipe.ontologies.update(context.ontology)


# for attr 2.0
# @behave.when(u'I update ontology attributes to "{attribute1}", "{attribute2}"')
# def step_impl(context, attribute1, attribute2):
#     context.ontology.update_attributes(key='1', title=str(attribute1), attribute_type=context.dl.AttributesTypes.CHECKBOX,
#                                        values=[1])
#     context.ontology.update_attributes(key='2', title=str(attribute2), attribute_type=context.dl.AttributesTypes.CHECKBOX,
#                                        values=[2])

@behave.when(u'I update ontology attributes to "{attribute1}", "{attribute2}"')
def step_impl(context, attribute1, attribute2):
    context.ontology.attributes = [attribute1, attribute2]
    context.recipe.ontologies.update(context.ontology)


@behave.when(u'I update ontology system metadata')
def step_impl(context):
    context.ontology.metadata['system']['something'] = 'value'
    context.recipe.ontologies.update(ontology=context.ontology, system_metadata=True)


================================================
File: tests/features/steps/ontology_entity/test_ontology_attributes.py
================================================
import behave, os, json
import time


@behave.when(u'I add "{input_type}" attribute to ontology')
def step_impl(context, input_type):
    if input_type == 'checkbox':
        att_type = context.dl.AttributesTypes.CHECKBOX
    elif input_type == 'radio_button':
        att_type = context.dl.AttributesTypes.RADIO_BUTTON
    elif input_type == 'slider':
        att_type = context.dl.AttributesTypes.SLIDER
    elif input_type == 'yes_no':
        att_type = context.dl.AttributesTypes.YES_NO
    elif input_type == 'free_text':
        att_type = context.dl.AttributesTypes.FREE_TEXT

    scope = optional = values = attribute_range = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "title":
            title = param[1]
        elif param[0] == "key":
            key = param[1]
        elif param[0] == "scope":
            if param[1].startswith('['):
                param[1] = eval(param[1])
            elif param[1] == 'all':
                param[1] = '*'
            scope = param[1]
        elif param[0] == "optional":
            optional = eval(param[1])
        elif param[0] == "values":
            if param[1].startswith('['):
                param[1] = eval(param[1])
            values = param[1]
        elif param[0] == "attribute_range":
            att_range = param[1].split(',')
            param[1] = context.dl.AttributesRange(min_range=int(att_range[0]), max_range=int(att_range[1]), step=int(att_range[2]))
            attribute_range = param[1]

    try:
        if getattr(context, 'ontology', None) is None:
            context.recipe_id = context.dataset.metadata['system']['recipes'][0]
            context.recipe = context.dataset.recipes.get(context.recipe_id)
            context.ontology_id = context.recipe.ontology_ids[0]
            context.ontology = context.recipe.ontologies.get(context.ontology_id)
        context.ontology.update_attributes(title=title,
                                           key=key,
                                           attribute_type=att_type,
                                           scope=scope,
                                           optional=optional,
                                           values=values,
                                           attribute_range=attribute_range)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I validate attribute "{input_type}" added to ontology')
def step_impl(context, input_type):
    context.ontology = context.recipe.ontologies.get(ontology_id=context.recipe.ontology_ids[0])

    if input_type == 'checkbox':
        att_type = 'options'
    elif input_type == 'radio_button':
        att_type = 'options'
    elif input_type == 'slider':
        att_type = context.dl.AttributesTypes.SLIDER
    elif input_type == 'yes_no':
        att_type = context.dl.AttributesTypes.YES_NO
    elif input_type == 'free_text':
        att_type = context.dl.AttributesTypes.FREE_TEXT

    assert att_type in [att['type'] for att in context.ontology.to_json()['metadata']['attributes']], "TEST FAILED: {} not in Ontology attributes".format(att_type)


@behave.then(u'I delete attributes with key "{input_type}" in ontology')
def step_impl(context, input_type):
    context.ontology.delete_attributes(keys=input_type)


@behave.then(u'I delete all attributes in ontology')
def step_impl(context):
    context.ontology = context.recipe.ontologies.get(ontology_id=context.recipe.ontology_ids[0])
    all_keys = [att['key'] for att in context.ontology.to_json()['metadata']['attributes']]
    context.ontology.delete_attributes(keys=all_keys)

    context.ontology = context.recipe.ontologies.get(ontology_id=context.recipe.ontology_ids[0])
    assert context.ontology.to_json()['metadata']['attributes'] == [], "TEST FAILED: Not all attributes are deleted.\n{}".format(context.ontology.to_json()['metadata']['attributes'])


================================================
File: tests/features/steps/ontology_entity/test_ontology_bamba_icon.py
================================================
import behave
import os
import time


@behave.when(u'I add label "{label_tag}" to ontology with "{icon_path}"')
def step_impl(context, label_tag, icon_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], icon_path)
    context.ontology.add_label(label_name="label-{}".format(label_tag), icon_path=file_path, update_ontology=True)


@behave.then(u'I validate dataset labels images are different')
def step_impl(context):
    context.dataset = context.project.datasets.get(dataset_name=context.dataset.name)
    items_filename = []
    for label in context.dataset.labels:
        item = context.project.items.get(item_id=label.display_data['displayImage']['itemId'])
        items_filename.append(item.name)

    assert len(tuple(items_filename)) == len(context.dataset.labels), "TEST FAILED: Items created with the same name" \
                                                                      "\nItems name list:{}".format(items_filename)


================================================
File: tests/features/steps/ontology_entity/test_ontology_repo_methods.py
================================================
import behave, os, json

@behave.when(u'I update ontology entity with labels from file "{file_path}"')
def step_impl(context, file_path):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], file_path)
    with open(file_path) as f:
        context.labels = json.load(f)
    context.ontology.add_labels(context.labels)
    context.ontology.update()

@behave.then(u'Dataset ontology in host has labels uploaded from "{file_path}')
def step_impl(context, file_path):
    context.ontology = context.recipe.ontologies.get(ontology_id=context.ontology.id)
    assert len(context.ontology.labels) == len(context.labels)
    for label in context.ontology.labels:
        assert label.to_root() in context.labels


@behave.when(u'I update ontology entity system metadata')
def step_impl(context):
    context.ontology.metadata['system']['something'] = 'value'
    context.ontology.update(system_metadata=True)

@behave.when(u'I delete ontology entity')
def step_impl(context):
    context.ontology.delete()

@behave.when(u'I add label to ontology')
def step_impl(context):
    context.ontology.add_label(label_name='label', color='#DC143C', children=None, attributes='attr1')
    context.label = {'name':'label', 'color':'#DC143C', 'attributes':['attr1'], 'display_label':'Label'}

@behave.when(u'I update ontology entity')
def step_impl(context):
    context.ontology.update()

@behave.then(u'Ontology in host has label')
def step_impl(context):
    context.ontology = context.recipe.ontologies.get(ontology_id=context.ontology.id)
    label = context.ontology.labels[0]
    assert label.tag == context.label['name']
    assert label.color == context.label['color']
    assert label.attributes == context.label['attributes']
    assert label.display_label == context.label['display_label']

================================================
File: tests/features/steps/packages_flow/packages_flow.py
================================================
import behave
import json
import time
import os


@behave.when(u'I copy all relevant files from "{package_assets_path}" to "{package_directory_path}"')
def step_impl(context, package_assets_path, package_directory_path):
    package_assets_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], package_assets_path)
    package_directory_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], package_directory_path)

    # main.py
    assets_main_path = os.path.join(package_assets_path, 'main.py')
    package_main_path = os.path.join(package_directory_path, 'main.py')

    with open(assets_main_path, 'r') as f:
        main_text = f.read()

    with open(package_main_path, 'w') as f:
        f.write(main_text)

    # mock
    assets_mock_path = os.path.join(package_assets_path, 'mock.json')
    package_mock_path = os.path.join(package_directory_path, 'mock.json')

    with open(assets_mock_path, 'r') as f:
        assets_mock = json.load(f)

    assets_mock['inputs'][0]['value']['dataset_id'] = context.dataset.id
    assets_mock['inputs'][0]['value']['item_id'] = context.item.id

    with open(package_mock_path, 'w') as f:
        json.dump(assets_mock, f, indent=2)

    # package.json
    assets_service_path = os.path.join(package_assets_path, 'package.json')
    package_service_path = os.path.join(package_directory_path, 'package.json')

    with open(assets_service_path, 'r') as f:
        assets_service = json.load(f)

    with open(package_service_path, 'w') as f:
        json.dump(assets_service, f, indent=2)

    dataloop_dir = os.path.join(package_directory_path, '.dataloop')
    os.mkdir(dataloop_dir)
    state_json = os.path.join(dataloop_dir, 'state.json')
    with open(state_json, 'w') as f:
        json.dump(
            {
                "project": context.project.id
            },
            f
        )


@behave.when(u'I test local package in "{package_directory_path}"')
def step_impl(context, package_directory_path):
    package_directory_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], package_directory_path)
    context.project.checkout()
    context.project.packages.test_local_package(cwd=package_directory_path)


@behave.when(u'I push and deploy package in "{package_directory_path}"')
def step_impl(context, package_directory_path):
    package_directory_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], package_directory_path, 'package.json')
    services, context.package = context.project.packages.deploy_from_file(project=context.project,
                                                                          json_filepath=package_directory_path)
    context.to_delete_packages_ids.append(context.package.id)
    for p_service in services:
        context.to_delete_services_ids.append(p_service.id)


@behave.when(u'I upload item in "{item_path}" to dataset')
def step_impl(context, item_path):
    time.sleep(10)
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.uploaded_item_with_trigger = context.dataset.items.upload(local_path=item_path)


@behave.when(u'I clone item to dataset')
def step_impl(context):
    time.sleep(10)
    context.uploaded_item_with_trigger = context.uploaded_item_with_trigger.clone(dst_dataset_id=context.dataset.id,
                                                                                  remote_filepath='/clone/')


@behave.then(u'Item "{item_num}" annotations equal annotations in "{assets_annotations_path}"')
def step_impl(context, item_num, assets_annotations_path):
    num_try = 60
    interval = 10
    assets_annotations_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], assets_annotations_path)
    if item_num == '1':
        item = context.item
    else:
        item = context.uploaded_item_with_trigger

    with open(assets_annotations_path, 'r') as f:
        should_be_annotations = json.load(f)['annotations']

    # try to get annotations
    annotations = list()
    for i in range(num_try):
        annotations = item.annotations.list()
        if len(annotations) > 0:
            break
        else:
            time.sleep(interval)

    assert len(annotations) == 1

    annotation = annotations[0]
    should_be_annotation = should_be_annotations[0]

    assert len(should_be_annotations) == 1
    assert annotation.label == should_be_annotation['label']
    assert annotation.coordinates == should_be_annotation['coordinates']
    assert annotation.type == should_be_annotation['type']
    assert annotation.attributes == should_be_annotation['attributes']


================================================
File: tests/features/steps/packages_repo/package_slot.py
================================================
import behave


@behave.when(u'I add UI slot to the package')
def step_impl(context):
    context.filters = context.dl.Filters()
    context.slot = context.dl.PackageSlot(module_name='default_module',
                                          function_name='run',
                                          display_name="ui_slot",
                                          display_icon='fas fa-play',
                                          post_action=context.dl.SlotPostAction(
                                              type=context.dl.SlotPostActionType.NO_ACTION),
                                          display_scopes=[
                                              context.dl.SlotDisplayScope(
                                                  filters={},
                                                  resource=context.dl.SlotDisplayScopeResource.ITEM,
                                                  panel=context.dl.UI_BINDING_PANEL_STUDIO)
                                          ])

    context.package.slots = []
    context.package.slots.append(context.slot)
    context.package = context.package.update()


@behave.when(u'I add UI slot to the package with all scopes')
def step_impl(context):
    context.filters = context.dl.Filters()
    context.slot = context.dl.PackageSlot(module_name='default_module',
                                          function_name='run',
                                          display_name="ui_slot",
                                          display_icon='fas fa-play',
                                          post_action=context.dl.SlotPostAction(
                                              type=context.dl.SlotPostActionType.NO_ACTION),
                                          display_scopes=[
                                              context.dl.SlotDisplayScope(
                                                  filters=context.dl.Filters(resource=context.dl.FiltersResource.ITEM),
                                                  resource=context.dl.SlotDisplayScopeResource.ITEM,
                                                  panel=context.dl.UI_BINDING_PANEL_STUDIO),
                                              context.dl.SlotDisplayScope(
                                                  filters=context.dl.Filters(
                                                      resource=context.dl.FiltersResource.ANNOTATION),
                                                  resource=context.dl.SlotDisplayScopeResource.ANNOTATION,
                                                  panel=context.dl.UI_BINDING_PANEL_STUDIO),
                                              context.dl.SlotDisplayScope(
                                                  filters=context.dl.Filters(
                                                      resource=context.dl.FiltersResource.DATASET),
                                                  resource=context.dl.SlotDisplayScopeResource.DATASET_QUERY,
                                                  panel=context.dl.UI_BINDING_PANEL_BROWSER),
                                              context.dl.SlotDisplayScope(
                                                  filters=context.dl.Filters(
                                                      resource=context.dl.FiltersResource.DATASET),
                                                  resource=context.dl.SlotDisplayScopeResource.DATASET,
                                                  panel=context.dl.UI_BINDING_PANEL_BROWSER),
                                              context.dl.SlotDisplayScope(
                                                  filters=context.dl.Filters(resource=context.dl.FiltersResource.TASK),
                                                  resource=context.dl.SlotDisplayScopeResource.TASK,
                                                  panel=context.dl.UI_BINDING_PANEL_TABLE)
                                          ])

    context.package.slots = []
    context.package.slots.append(context.slot)
    context.package = context.package.update()


@behave.then(u'I validate slot is added to the package')
def step_impl(context):
    assert context.package.slots != [], "No slots added to the package"
    assert context.package.slots[
               0].to_json() == context.slot.to_json(), "The slot in the package is not equal to uploaded slot ###package###\n {}\n###uploaded-Slot###\n{}". \
        format(context.package.slots[0].to_json(), context.slot.to_json())


@behave.when(u'I add new function to UI slot')
def step_impl(context):
    context.filters = context.dl.Filters()
    context.slot_1 = context.dl.PackageSlot(module_name='default_module',
                                            function_name='run_1',
                                            display_name="ui_slot_run_1",
                                            display_icon='fas fa-play',
                                            post_action=context.dl.SlotPostAction(
                                                type=context.dl.SlotPostActionType.NO_ACTION),
                                            display_scopes=[
                                                context.dl.SlotDisplayScope(
                                                    filters={},
                                                    resource=context.dl.SlotDisplayScopeResource.ITEM,
                                                    panel=context.dl.UI_BINDING_PANEL_STUDIO)
                                            ])

    context.package.slots.append(context.slot_1)
    context.package = context.package.update()


================================================
File: tests/features/steps/packages_repo/packages_delete.py
================================================
import behave


@behave.then(u'I delete package')
def step_impl(context):
    try:
        context.package.delete()
    except Exception as e:
        context.error = e
        assert False, "FAILED TEST: Failed to delete package {} ID : {}".format(context.package.name, context.package.id)


================================================
File: tests/features/steps/packages_repo/packages_generate.py
================================================
import behave
import os
import shutil
from .. import fixtures


@behave.given(u'Directory "{dir_path}" is empty')
def step_impl(context, dir_path):

    dir_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], dir_path)

    if not os.path.isdir(dir_path):
        os.mkdir(dir_path)

    for item in os.listdir(dir_path):
        if item == 'folder_keeper':
            continue
        elif os.path.isdir(os.path.join(dir_path, item)):
            shutil.rmtree(os.path.join(dir_path, item))
        elif os.path.isfile(os.path.join(dir_path, item)):
            os.remove(os.path.join(dir_path, item))
    content = [item for item in os.listdir(dir_path) if item != 'folder_keeper']

    assert not content


@behave.when(u'I generate package by the name of "{package_name}" to "{src_path}"')
def step_impl(context, package_name, src_path):
    if package_name == 'None':
        package_name = None

    if src_path == 'None':
        src_path = None
    else:
        src_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], src_path)

    if src_path is not None:
        if not os.path.isdir(src_path):
            os.mkdir(src_path)

    context.project.packages.generate(name=package_name, src_path=src_path)


@behave.then(u'Package local files in "{generated_path}" equal package local files in '
             u'"{to_compare_path}"')
def step_impl(context, generated_path, to_compare_path):

    generated_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], generated_path)
    to_compare_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], to_compare_path)

    assert fixtures.compare_dir_recursive(generated_path, to_compare_path)


@behave.given(u'cwd is "{cwd}"')
def step_impl(context, cwd):
    context.original_cwd = os.getcwd()
    cwd = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], cwd)
    os.chdir(cwd)


@behave.when(u'cwd goes back to original')
def step_impl(context):
    os.chdir(context.original_cwd)


================================================
File: tests/features/steps/packages_repo/packages_get.py
================================================
import behave


@behave.when(u'I get package by the name of "{package_name}"')
def step_impl(context, package_name):
    context.package_get = context.project.packages.get(package_name=package_name)


@behave.when(u'get global model package')
@behave.given(u'get global model package')
def step_impl(context):
    package_filter = context.dl.Filters(field='name', values='testmodel', use_defaults=False,
                                        resource=context.dl.FiltersResource.PACKAGE)
    context.package = context.dl.packages.list(filters=package_filter).items[0]


@behave.when(u'I get package by id')
def step_impl(context):
    context.package_get = context.project.packages.get(package_id=context.package.id)


@behave.then(u'I get a package entity')
def step_impl(context):
    assert 'Package' in str(type(context.package_get))


@behave.then(u'It is equal to package created')
def step_impl(context):
    assert context.package.to_json() == context.package_get.to_json()


================================================
File: tests/features/steps/packages_repo/packages_list.py
================================================
import behave


@behave.when(u'I list all project packages')
def step_impl(context):
    context.packages_list = context.project.packages.list()


@behave.then(u'I receive a list of "{num_packages}" packages')
def step_impl(context, num_packages):
    assert context.packages_list.items_count == int(num_packages)


================================================
File: tests/features/steps/packages_repo/packages_name_validation.py
================================================
import behave
import os


@behave.when(u'I try to push package')
def step_impl(context):
    try:
        codebase_id = None
        package_name = None
        inputs = None
        src_path = None
        outputs = None
        modules = None

        params = context.table.headings
        for param in params:
            param = param.split('=')
            if param[0] == 'package_name':
                if param[1] != 'None':
                    package_name = param[1]
            elif param[0] == 'src_path':
                if param[1] != 'None':
                    src_path = param[1]
                    src_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], src_path)
            elif param[0] == 'codebase_id':
                if param[1] != 'None':
                    codebase_id = param[1]
            elif param[0] == 'inputs':
                if param[1] != 'None':
                    inputs = param[1]
            elif param[0] == 'outputs':
                if param[1] != 'None':
                    outputs = param[1]
            elif param[0] == 'modules':
                if param[1] != 'None':
                    modules = param[1]

        if modules == 'no_input':
            func = context.dl.PackageFunction()
            modules = context.dl.PackageModule(functions=func,
                                               name=context.dl.entities.package_defaults.DEFAULT_PACKAGE_MODULE_NAME)

        codebase = None
        if codebase_id is not None:
            codebase = context.dl.entities.ItemCodebase(item_id=codebase_id)

        # module = context.dl.entities.DEFAULT_PACKAGE_MODULE
        package = context.project.packages.push(
            codebase=codebase,
            package_name=package_name,
            modules=modules,
            src_path=src_path
        )
        context.to_delete_packages_ids.append(package.id)
    except context.dl.exceptions.BadRequest as e:
        assert 'Invalid package name:' in e.message or 'Name must be at most 35 characters' in e.message
        context.name_is_valid = False
        context.error = e


@behave.when(u'I validate name "{package_name}"')
def step_impl(context, package_name):
    try:
        context.dl.packages._name_validation(package_name)
        context.name_is_valid = True
    except context.dl.exceptions.BadRequest as e:
        assert 'Invalid package name:' in e.message or 'Name must be at most 35 characters' in e.message
        context.name_is_valid = False


@behave.then(u'Name is valid')
def step_impl(context):
    return context.name_is_valid


@behave.then(u'Name is invalid')
def step_impl(context):
    return not context.name_is_valid


================================================
File: tests/features/steps/packages_repo/packages_push.py
================================================
import behave
import os
import json
import shutil
import random

from .. import fixtures


@behave.when(u'I push "{package_number}" package')
def step_impl(context, package_number):
    codebase_id = None
    package_name = None
    inputs = list()
    src_path = None
    outputs = list()
    modules = None
    package_type = 'faas'

    params = context.table.headings
    for param in params:
        param = param.split('=')
        if param[0] == 'package_name':
            if param[1] != 'None':
                package_name = param[1]
        elif param[0] == 'src_path':
            if param[1] != 'None':
                src_path = param[1]
                src_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], src_path)
        elif param[0] == 'codebase_id':
            if param[1] != 'None':
                codebase_id = param[1]
        elif param[0] == 'inputs':
            if param[1] != 'None':
                inputs = fixtures.get_package_io(params=param[1].split(','), context=context)
        elif param[0] == 'outputs':
            if param[1] != 'None':
                outputs = fixtures.get_package_io(params=param[1].split(','), context=context)
        elif param[0] == 'modules':
            if param[1] != 'None':
                modules = param[1]
        elif param[0] == 'type':
            if param[1] != 'None':
                package_type = param[1]

    modules_name = context.dl.entities.package_defaults.DEFAULT_PACKAGE_MODULE_NAME if package_type == 'faas' else 'model-adapter'
    if package_type == 'ml':
        func = [context.dl.PackageFunction(name='train_model',
                                           inputs=[context.dl.FunctionIO(name='model',
                                                                         type=context.dl.PackageInputType.MODEL),
                                                   context.dl.FunctionIO(name='cleanup',
                                                                         type=context.dl.PackageInputType.BOOLEAN)
                                                   ],
                                           outputs=[context.dl.FunctionIO(name='model',
                                                                          type=context.dl.PackageInputType.MODEL)])]
        modules = context.dl.PackageModule(functions=func,
                                           name=modules_name,
                                           init_inputs=[context.dl.FunctionIO(name='model_entity',
                                                                              type=context.dl.PackageInputType.MODEL)])
    if modules == 'no_input':
        func = [context.dl.PackageFunction()]
        modules = context.dl.PackageModule(functions=func,
                                           name=modules_name)

    elif inputs or outputs:
        func = [context.dl.PackageFunction(inputs=inputs, outputs=outputs)]
        modules = context.dl.PackageModule(functions=func,
                                           name=modules_name)
    codebase = None
    if codebase_id is not None:
        codebase = context.dl.entities.ItemCodebase(item_id=codebase_id)

    try:
        package = context.project.packages.push(
            codebase=codebase,
            package_name=package_name,
            modules=modules,
            src_path=src_path,
            package_type=package_type
        )

        context.to_delete_packages_ids.append(package.id)
        if package_number == 'first':
            context.first_package = package
            context.package = package
        else:
            context.second_package = package

        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I receive package entity')
def step_impl(context):
    assert 'Package' in str(type(context.first_package))


@behave.given(u'I create a package with secrets with entry point "{path}"')
def step_impl(context, path):
    modules = [
        context.dl.PackageModule(name='default_module',
                                 entry_point='main.py',
                                 init_inputs=[
                                     context.dl.FunctionIO(type=context.dl.PackageInputType.STRING, name="test",
                                                           value='$env(nvidiaUser)',
                                                           integration={"type": "key_value"})],
                                 functions=[
                                     context.dl.PackageFunction(
                                         inputs=[
                                             context.dl.FunctionIO(type=context.dl.PackageInputType.ITEM, name="item")],
                                         outputs=[],
                                         name='run'),
                                 ])]

    context.package = context.project.packages.push(
        package_name="secretspackage",
        src_path=os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path),
        modules=modules,
    )

    context.service = context.package.services.deploy(
        service_name=context.package.name,
        package=context.package,
        sdk_version=context.dl.__version__,
        init_input={'test': '$env(nvidiaUser)'},
        integrations=[{
            "key": "nvidiaUser",
            "env": "nvidiaUser",
            "type": "key_value",
            "value": context.integration.id
        }]
    )


@behave.then(u'Package entity equals local package in "{path_to_compare}"')
def step_impl(context, path_to_compare):
    # get package local info
    path_to_compare = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], path_to_compare)
    with open(os.path.join(path_to_compare, 'package.json')) as f:
        package_json = json.load(f)
    name = package_json['name']
    inputs = package_json['modules'][0]['functions'][0]['input']
    outputs = package_json['modules'][0]['functions'][0]['output']

    # unpack package source code
    # base_temp_dir = tempfile.mktemp()
    base_temp_dir = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], 'temp_{}'.format(str(random.randrange(0, 1000))))
    if not os.path.isdir(base_temp_dir):
        os.mkdir(base_temp_dir)

    context.project.codebases.unpack(codebase_id=context.first_package.codebase_id,
                                     local_path=base_temp_dir)

    # assertions
    assert fixtures.compare_dir_recursive(path_to_compare, base_temp_dir)
    assert name == context.first_package.name
    assert inputs == [_io.to_json() for _io in context.first_package.modules[0].functions[0].inputs]
    assert outputs == [_io.to_json() for _io in context.first_package.modules[0].functions[0].outputs]

    shutil.rmtree(base_temp_dir)


@behave.then(u'I receive another package entity')
def step_impl(context):
    assert 'Package' in str(type(context.second_package))


def compare_codebase_id(first_package, second_package):
    if first_package.codebase.item_id is not None and second_package.codebase.item_id is not None:
        return first_package.codebase.item_id != second_package.codebase.item_id
    else:
        raise Exception('Packages does not have codebase id')


@behave.then(u'1st package and 2nd package only differ in code base id')
def step_impl(context):
    assert compare_codebase_id(context.first_package, context.second_package)

    first_package_json = context.first_package.to_json()
    second_package_json = context.second_package.to_json()

    first_package_json.pop('codebase', None)
    second_package_json.pop('codebase', None)

    assert first_package_json.pop('updatedAt') != second_package_json.pop('updatedAt')
    assert int(first_package_json.pop('version').replace(".", "")) == \
           int(second_package_json.pop('version').replace(".", "")) - 1
    assert first_package_json == second_package_json


@behave.when(u'I update package')
def step_impl(context):
    context.package = context.package.update()


@behave.when(u'i update the context module from package')
def step_impl(context):
    context.module = context.package.modules[0]


@behave.then(u'I expect package version to be "{version}" and revision list size "{revision_size}"')
def step_impl(context, version, revision_size):
    package_revision_size = len(context.package.revisions)

    assert version == context.package.version, "TEST FAILED: Expect version to be {} got {}".format(version,
                                                                                                    context.package.version)
    assert int(
        revision_size) == package_revision_size, "TEST FAILED: Expect package revision size to be {} got {}".format(
        revision_size, package_revision_size)


@behave.then(u'I validate service version is "{version}"')
def step_impl(context, version):
    assert version == context.service.package_revision, "TEST FAILED: Expect version to be {} got {}".format(version,
                                                                                                             context.service.package_revision)


@behave.when(u'I add new function to package')
def step_impl(context):
    function_1 = {'name': 'run_1',
                  'description': None,
                  'input': [{'name': 'item', 'type': 'Item'}],
                  'output': [],
                  'displayIcon': ''}

    context.package.modules[0].add_function(function_1)
    context.package = context.package.update()


@behave.when(u'Add requirements "{req}" to package')
def step_impl(context, req):
    context.package.requirements = [context.dl.PackageRequirement(name=req)]
    context.package.update()


@behave.given(u'I delete dataset Binaries')
def step_impl(context):
    datasets = context.project.datasets.list()
    context.binaries_dataset_ids = list()
    for dataset in datasets:
        if dataset.name == 'Binaries':
            context.binaries_dataset_ids.append(dataset.id)
            dataset.delete(True, True)
    datasets = context.project.datasets.list()
    for dataset in datasets:
        if dataset.name == 'Binaries':
            assert False, 'Failed to delete Binaries dataset'
    context.project = context.dl.projects.get(project_id=context.project.id)


================================================
File: tests/features/steps/packages_repo/test_package_module.py
================================================
import behave
import os
from .. import fixtures
import dtlpy as dl
from operator import attrgetter


@behave.when(u'I create PackageModule with params')
def step_impl(context):
    module_name = "default_module"
    init_inputs = None
    entry_point = None
    class_name = "ServiceRunner"
    functions = [dl.PackageFunction()]

    params = context.table.headings
    for param in params:
        param = param.split('=')
        if param[0] == 'name':
            if param[1] != 'None':
                module_name = param[1]
        elif param[0] == 'init_inputs':
            if param[1] != 'None':
                init_inputs = fixtures.get_package_io(params=param[1].split(','), context=context)
        elif param[0] == 'entry_point':
            if param[1] != 'None':
                entry_point = param[1]
        elif param[0] == 'class_name':
            if param[1] != 'None':
                class_name = param[1]
        elif param[0] == 'functions':
            if param[1] != 'None':
                functions = list()
                for func in eval(param[1]):
                    function_name = func.get("function_name", "run")
                    description = func.get("description", None)
                    inputs = fixtures.get_package_io(params=func['inputs'].split(','), context=context) if func.get(
                        'inputs') else []
                    outputs = fixtures.get_package_io(params=func['outputs'].split(','), context=context) if func.get(
                        'outputs') else []
                    functions.append(dl.PackageFunction(name=function_name,
                                                        inputs=inputs,
                                                        outputs=outputs,
                                                        description=description))

    context.module = dl.PackageModule(
        name=module_name,
        entry_point=entry_point,
        init_inputs=init_inputs,
        class_name=class_name,
        functions=functions
    )


@behave.when(u'I update PackageModule function "{function_index}" input "{inputs_index}" use "{source}"')
def step_impl(context, function_index, inputs_index, source):
    f_i = int(function_index) - 1
    i_i = int(inputs_index) - 1
    assert hasattr(context, "module"), "TEST FAILED: Need to have context module - use 'When I create PackageModule'"
    if source == 'module':
        source = context.module
    elif source == 'package':
        source = context.package.modules[0]

    for row in context.table:
        setattr(source.functions[f_i].inputs[i_i], row['key'], row['value'])


@behave.then(u'I verify PackageModule params')
def step_impl(context):
    if not hasattr(context, "module"):
        if hasattr(context, "package"):
            context.module = context.package.modules[0]
        else:
            assert False, "TEST FAILED: Need to have context module - use 'When I create PackageModule'"
    for row in context.table:
        att = f"context.module.{row['key']}"
        val = f"'{row['value']}'"
        exec(f"assert {att} == {val}, 'TEST FAILED: Expected '+{val}+', Actual '+{att}")


================================================
File: tests/features/steps/packages_repo/test_packages_context.py
================================================
import behave


@behave.given(u'I set Project to Project {project_index}')
def step_impl(context, project_index):
    context.project = context.projects[int(project_index) - 1]


@behave.when(u'I get the package from project number {project_index}')
def step_impl(context, project_index):
    context.package = context.projects[int(project_index) - 1].packages.get(package_id=context.package.id)


@behave.then(u'package Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.package.project_id == context.projects[int(project_index)-1].id


@behave.then(u'package Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.package.project.id == context.projects[int(project_index)-1].id


================================================
File: tests/features/steps/pipeline_entity/pipeline_delete.py
================================================
import time

from behave import given, when, then


@when(u'I delete a pipeline by the name of "{pipeline_delete}"')
def step_impl(context, pipeline_delete):
    context.project.pipelines.delete(pipeline_name=pipeline_delete)


@when(u'I delete a pipeline by the id')
def step_impl(context):
    context.project.pipelines.delete(pipeline_id=context.pipeline.id)


@then(u'There are no pipeline by the name of "{pipeline_delete}"')
def step_impl(context, pipeline_delete):
    try:
        pipeline = None
        pipeline = context.dl.pipelines.get(pipeline_name=pipeline_delete)
    except context.dl.exceptions.NotFound:
        assert pipeline is None


@when(u'I try to delete a pipeline by the name of "{pipeline_name}"')
def step_impl(context, pipeline_name):
    try:
        context.project.pipelines.delete(pipeline_name=pipeline_name,)
        context.error = None
    except Exception as e:
        context.error = e


@when(u'I delete all nodes')
def step_impl(context):
    """
    Remove all nodes from pipeline
    """

    context.pipeline = context.project.pipelines.get(pipeline_name=context.pipeline_name)

    for node in context.pipeline.nodes:
        assert context.pipeline.nodes.remove(node.name), "TEST FAILED: Failed to delete node {}".format(node.name)

    context.pipeline = context.pipeline.update()


================================================
File: tests/features/steps/pipeline_entity/pipeline_execute.py
================================================
import random
import string
import time

import behave

import dtlpy as dl


@behave.when(u'I add a code node to the pipeline')
def step_impl(context):
    def run(item, string):
        item.metadata['user'] = {'userInput': string}
        item.update()
        return item

    context.new_node = dl.CodeNode(
        name='codeNode',
        position=(4, 4),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name
    )

    context.pipeline.nodes.add(node=context.new_node)
    context.pipeline.update()


@behave.when(u'I add a predict node to the pipeline')
def step_impl(context):
    context.new_node = dl.entities.pipeline.PipelineNode.from_json({
        "id": "d4de0a46-7597-43bb-a80a-a77cb41ab62d",
        "inputs": [
            {
                "portId": "1dec3c79-680d-4ac7-b380-f51f9e578de6",
                "nodeId": "1dec3c79-680d-4ac7-b380-f51f9e578de6",
                "type": "Item",
                "name": "item",
                "displayName": "item",
                "io": "input"
            }
        ],
        "outputs": [
            {
                "portId": "5285f12a-ddbe-4e64-b8dd-acf1c05626a7",
                "nodeId": "5285f12a-ddbe-4e64-b8dd-acf1c05626a7",
                "type": "Item",
                "name": "item",
                "displayName": "item",
                "io": "output"
            },
            {
                "portId": "bc16fe31-8e42-47bc-912d-13e6b135f98c",
                "nodeId": "bc16fe31-8e42-47bc-912d-13e6b135f98c",
                "type": "Annotation[]",
                "name": "annotations",
                "displayName": "annotations",
                "io": "output"
            }
        ],
        "metadata": {
            "position": {
                "x": 1490,
                "y": 640,
                "z": 0
            },
            "modelName": context.model.name,
            "modelId": context.model.id,
            "serviceConfig": {

            }
        },
        "name": context.model.name,
        "type": "ml",
        "namespace": {
            "functionName": "predict",
            "projectName": context.project.name,
            "serviceName": "model-mgmt-app-predict",
            "moduleName": context.model.module_name,
            "packageName": "model-mgmt-app"
        },
        "projectId": context.project.id
    })

    context.pipeline.nodes.add(context.new_node)
    context.pipeline.update()


@behave.when(u'i update runnerImage "{image}" to pipeline node with type "{node_type}"')
def step_impl(context, image, node_type):
    context.pipeline = context.project.pipelines.get(pipeline_id=context.pipeline.id)
    for node in context.pipeline.nodes:
        if node.node_type == node_type:
            if 'serviceConfig' not in node.metadata:
                node.metadata['serviceConfig'] = {}
            if 'runtime' not in node.metadata['serviceConfig']:
                node.metadata['serviceConfig']['runtime'] = {}
            node.metadata['serviceConfig']['runtime']['runnerImage'] = image
            context.pipeline = context.pipeline.update()
            break

@behave.when(u'i get the service for the pipeline node with type "{node_type}"')
def step_impl(context, node_type):
    for node in context.pipeline.nodes:
        if node.node_type == node_type:
            context.service = dl.services.get(service_name=node.namespace.service_name)
            break

@behave.when(u'I execute the pipeline batch items')
def step_impl(context):
    context.command = context.pipeline.execute_batch(
        execution_inputs=dl.FunctionIO(type=dl.PackageInputType.STRING, value='test', name='string'),
        filters=dl.Filters(field='dir', values='/test', context={'datasets': [context.dataset.id]}))


@behave.when(u'I get the pipeline service')
def step_impl(context):
    service_name = context.pipeline.nodes[0].namespace.service_name
    context.service = dl.services.get(service_name=service_name)


@behave.when(u'I execute the service batch items')
def step_impl(context):
    context.command = context.service.execute_batch(
        execution_inputs=dl.FunctionIO(type=dl.PackageInputType.STRING, value='test', name='string'),
        filters=dl.Filters(field='dir', values='/test', context={'datasets': [context.dataset.id]}))


@behave.then(u'pipeline execution are success in "{items_count}" items')
def step_impl(context, items_count):
    assert context.command.status == dl.ExecutionStatus.SUCCESS
    assert len(context.command.spec['inputs']) == eval(items_count)
    assert context.pipeline.pipeline_executions.list().items_count == eval(items_count)


@behave.then(u'service execution are success in "{items_count}" items')
def step_impl(context, items_count):
    assert context.command.status == dl.ExecutionStatus.SUCCESS
    assert len(context.command.spec['inputs']) == eval(items_count)
    assert context.service.executions.list().items_count == eval(items_count)


@behave.given(u'I install a pipeline with 2 dataset nodes')
def step_impl(context):
    pipeline_json = {
        "nodes": [
            {
                "id": "64d7bf6f-e42e-4693-9319-a389cea84680",
                "inputs": [
                    {
                        "portId": "286ebf65-7c96-4a25-826a-2901c978013b",
                        "nodeId": "286ebf65-7c96-4a25-826a-2901c978013b",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "a2916fdb-199f-420d-b9c7-c81cbbdff7fb",
                        "nodeId": "a2916fdb-199f-420d-b9c7-c81cbbdff7fb",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10217,
                        "y": 10127,
                        "z": 0
                    },
                    "datasetId": context.dataset.id,
                    "componentGroupName": "data",
                    "repeatable": True
                },
                "name": context.dataset.name,
                "type": "storage",
                "namespace": {
                    "functionName": "clone_item",
                    "projectName": "DataloopTasks",
                    "serviceName": "pipeline-utils",
                    "moduleName": "default_module",
                    "packageName": "pipeline-utils"
                },
                "projectId": context.project.id
            },
            {
                "id": "d65c4651-28aa-4fed-a5f4-71e4fcfd2e15",
                "inputs": [
                    {
                        "portId": "5aed83a8-d3b6-45ed-b323-697379d0fe2e",
                        "nodeId": "5aed83a8-d3b6-45ed-b323-697379d0fe2e",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "00e0fe41-c57a-45a2-939a-6650f9e19868",
                        "nodeId": "00e0fe41-c57a-45a2-939a-6650f9e19868",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10644,
                        "y": 10176,
                        "z": 0
                    },
                    "datasetId": context.dataset.id,
                    "componentGroupName": "data",
                    "repeatable": True
                },
                "name": context.dataset.name,
                "type": "storage",
                "namespace": {
                    "functionName": "clone_item",
                    "projectName": "DataloopTasks",
                    "serviceName": "pipeline-utils",
                    "moduleName": "default_module",
                    "packageName": "pipeline-utils"
                },
                "projectId": context.project.id
            }
        ],
        "connections": [
            {
                "src": {
                    "nodeId": "64d7bf6f-e42e-4693-9319-a389cea84680",
                    "portId": "a2916fdb-199f-420d-b9c7-c81cbbdff7fb"
                },
                "tgt": {
                    "nodeId": "d65c4651-28aa-4fed-a5f4-71e4fcfd2e15",
                    "portId": "5aed83a8-d3b6-45ed-b323-697379d0fe2e"
                },
                "condition": "{}"
            }
        ],
        "startNodes": [
            {
                "nodeId": "64d7bf6f-e42e-4693-9319-a389cea84680",
                "type": "root",
                "id": "90ae8a0d-287b-4e19-821d-e4b03edaf5a4"
            }
        ]
    }
    context.pipeline = context.project.pipelines.create(
        pipeline_json=pipeline_json,
        name="test-execute-specific-node-{}".format(
            ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a'
        )
    )
    context.pipeline.install()


@behave.when(u'I execute the second node which is not the root node')
def step_impl(context):
    context.root_node_id = context.pipeline.start_nodes[0]['nodeId']
    context.execution_node_id = [n for n in context.pipeline.nodes if n.node_id != context.root_node_id][0].node_id
    context.cycle = context.pipeline.execute(node_id=context.execution_node_id,
                                             execution_input={'item': context.item.id})


@behave.then(u'Then pipeline should start from the requested node')
def step_impl(context):
    timeout = 60 * 1
    start_time = time.time()
    success = False
    while time.time() - start_time < timeout:
        time.sleep(1)
        cycle = context.pipeline.pipeline_executions.get(pipeline_execution_id=context.cycle.id)
        if cycle.status == 'success':
            root_node = [n for n in cycle.nodes if n.node_id == context.root_node_id][0]
            execution_node = [n for n in cycle.nodes if n.node_id == context.execution_node_id][0]
            success = root_node.status == 'pending'
            success = success and execution_node.status == 'success'
            if success:
                break

    assert success, 'Pipeline did not start from the requested node'


================================================
File: tests/features/steps/pipeline_entity/pipeline_flow.py
================================================
import os
import time
import uuid
import random
import dtlpy as dl
import behave
import json


@behave.when(u'I create a package and service to pipeline')
@behave.given(u'I create a package and service to pipeline')
def step_impl(context):
    module = dl.PackageModule(
        entry_point='main.py',
        class_name='ServiceRunner',
        functions=[
            dl.PackageFunction(
                name='automate',
                inputs=[
                    dl.FunctionIO(type="Item", name="item")
                ],
                outputs=[
                    dl.FunctionIO(type="Item", name="item")
                ],
                description=''
            )
        ])

    project = dl.projects.get(project_id=context.project.id)
    context.package = project.packages.push(
        package_name='test-pipeline',
        modules=[module],
        src_path=os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "pipeline_flow")
    )

    context.service = context.package.services.deploy(service_name=context.package.name,
                                                      package=context.package,
                                                      runtime={"gpu": False, "numReplicas": 1, 'concurrency': 1,
                                                               'autoscaler': {
                                                                   'type': dl.KubernetesAutuscalerType.RABBITMQ,
                                                                   'minReplicas': 1,
                                                                   'maxReplicas': 5,
                                                                   'queueLength': 10}}
                                                      )
    context.to_delete_services_ids.append(context.service.id)


@behave.when(u'I create a pipeline from sdk')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='sdk-pipeline-test', project_id=context.project.id)

    function_node = dl.FunctionNode(
        name='automate',
        position=(1, 1),
        service=dl.services.get(service_id=context.service.id),
        function_name='automate'
    )

    task_node = dl.TaskNode(
        name='My Task',
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=100)],
        position=(2, 2),
        project_id=context.project.id,
        dataset_id=context.dataset.id,
    )

    dataset_node = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(3, 3)
    )

    context.pipeline.nodes.add(node=function_node).connect(node=task_node).connect(node=dataset_node)
    function_node.add_trigger()
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'I add a node and connect it to the start node')
def step_impl(context):
    context.pipeline.pause()

    def run(item):
        item.metadata['user'] = {'Hello': 'World'}
        item.update()
        return item

    context.new_node = dl.CodeNode(
        name='My Function',
        position=(4, 4),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name
    )

    context.pipeline.nodes.add(node=context.new_node).connect(node=context.pipeline.nodes[0])
    context.pipeline.update()


@behave.then(u'New node is the start node')
def step_impl(context):
    assert context.new_node.is_root()


@behave.when(u'I create a pipeline from sdk with pipeline trigger')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='sdk-pipeline-test', project_id=context.project.id)

    dataset_node_1 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(1, 1)
    )

    dataset_node_2 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(2, 2)
    )

    context.pipeline.nodes.add(node=dataset_node_1).connect(node=dataset_node_2)
    context.pipeline.update()
    context.pipeline.triggers.create(actions=dl.TriggerAction.CREATED, pipeline_node_id=dataset_node_1.node_id,
                                     project_id=context.project.id)
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'I update pipeline trigger action')
def step_impl(context):
    context.trigger = context.pipeline.triggers.list()[0][0]
    context.trigger.actions = [dl.TriggerAction.UPDATED]
    context.trigger = context.trigger.update()


@behave.then(u'valid trigger updated')
def step_impl(context):
    trigger = context.pipeline.triggers.get(trigger_id=context.trigger.id)
    original_trigger_json = context.trigger.to_json()
    updated_trigger_json = trigger.to_json()
    original_trigger_json.get('spec', {}).pop('actions', None)
    assert updated_trigger_json.get('spec', {}).pop('actions', None) == [dl.TriggerAction.UPDATED]
    assert updated_trigger_json.get('spec') == original_trigger_json.get('spec')


@behave.when(
    u'I add trigger to the node and check installed with param keep_triggers_active equal to "{keep_triggers_active}"')
def step_impl(context, keep_triggers_active: str):
    keep_triggers_active = eval(keep_triggers_active)
    context.pipeline.pause(keep_triggers_active=keep_triggers_active)
    node_id = context.pipeline.nodes[1].node_id
    context.pipeline.triggers.create(pipeline_node_id=node_id)
    triggers = context.pipeline.triggers.list().items
    for trigger in triggers:
        assert trigger.active == keep_triggers_active
    context.pipeline.install()
    triggers = context.pipeline.triggers.list().items
    assert len(triggers) == 2
    for trigger in triggers:
        assert trigger.active


@behave.when(u'I create a pipeline from json')
def step_impl(context):
    pipeline_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "pipeline_flow/pipeline_flow.json")

    with open(pipeline_path, 'r') as f:
        pipeline_json = json.load(f)

    pipeline_json['projectId'] = context.project.id

    pipeline_json['nodes'][0]['namespace']['serviceName'] = context.service.name
    pipeline_json['nodes'][0]['namespace']['packageName'] = context.package.name
    pipeline_json['nodes'][0]['namespace']['projectName'] = context.project.name
    pipeline_json['nodes'][0]['projectId'] = context.project.id

    pipeline_json['nodes'][1]['metadata']['recipeTitle'] = context.recipe.title
    pipeline_json['nodes'][1]['metadata']['recipeId'] = context.recipe.id
    pipeline_json['nodes'][1]['metadata']['datasetId'] = context.dataset.id
    pipeline_json['nodes'][1]['metadata']["workload"] = [
        {
            "assigneeId": dl.info()['user_email'],
            "load": 100
        }
    ]

    pipeline_json['nodes'][2]['namespace']['projectName'] = context.project.name

    context.pipeline = context.project.pipelines.create(pipeline_json=pipeline_json, project_id=context.project.id)
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.given(u'I have a custom "{name}" pipeline from json')
def step_impl(context, name: str):
    pipeline_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "pipeline_flow/{}.json".format(name))

    with open(pipeline_path, 'r') as f:
        pipeline_json = json.load(f)

    pipeline_json['projectId'] = context.project.id

    for node in pipeline_json['nodes']:
        if node['type'] in ['code', 'function']:
            node['namespace']['projectName'] = context.project.name
            node['projectId'] = context.project.id
        elif node['type'] == 'task':
            node['metadata']['recipeTitle'] = context.recipe.title
            node['metadata']['recipeId'] = context.recipe.id
            node['metadata']['datasetId'] = context.dataset.id
            node['metadata']["taskOwner"] = dl.info()['user_email']
            node['metadata']["workload"] = [
                {
                    "assigneeId": dl.info()['user_email'],
                    "load": 100
                }
            ]

    context.pipeline = context.project.pipelines.create(pipeline_json=pipeline_json, project_id=context.project.id)
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)

    tasks = context.pipeline.project.tasks.list()

    if len(tasks) == 1:
        context.task = tasks[0]
    elif len(tasks) > 1:
        context.tasks = tasks


@behave.when(u'I upload item in "{item_path}" to pipe dataset')
def step_impl(context, item_path):
    time.sleep(5)
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.item = context.dataset.items.upload(local_path=item_path)


@behave.then(u'verify pipeline flow result')
def step_impl(context):
    interval = 10
    num_tries = 30
    fromPipe = False
    for i in range(num_tries):
        time.sleep(interval)
        context.item = context.dataset.items.get(item_id=context.item.id)
        if context.item.metadata['system'].get('fromPipe', False):
            fromPipe = True
            break
    assert fromPipe, f"TEST FAILED: item.metadata['system'] missing fromPipe: True, after {round(num_tries * interval / 60, 1)} minutes"

    time.sleep(20)
    context.item = context.dataset.items.get(item_id=context.item.id)
    ass_id = None
    for ref in context.item.metadata['system']['refs']:
        if ref['type'] == 'assignment':
            ass_id = ref['id']
    context.item.update_status(status='complete', assignment_id=ass_id, clear=False)


@behave.when(u'I remove node by the name "{node_name}" from pipeline')
def step_impl(context, node_name):
    context.pipeline.pause()
    context.pipeline = context.pipeline.pipelines.get(context.pipeline.name)
    context.pipeline.nodes.remove(node_name)
    context.pipeline.update()


@behave.when(u'check pipeline nodes')
def step_impl(context):
    assert len(context.pipeline.nodes) == 2
    assert len(context.pipeline.connections) == 1


@behave.then(u'verify pipeline sanity result')
def step_impl(context):
    assert context.dataset_finish.items.list().item_count != 0, "No items in dataset finished - Pipeline failed"

    for task in context.project.tasks.list():
        task.item_status


@behave.when(u'I create a pipeline from pipeline-sanity')
def step_impl(context):
    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)

    context.dataset_finish = context.project.datasets.create(dataset_name='dataset-' + current_time + "-finish",
                                                             index_driver=context.index_driver_var)
    context.pipeline = context.project.pipelines.create(pipeline_name='sdk-pipeline-sanity',
                                                        project_id=context.project.id)
    context.to_delete_pipelines_ids.append(context.pipeline.id)

    task_node_1 = dl.TaskNode(
        name='My Task-fix-label' + current_time,
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=50),
                  dl.WorkloadUnit(assignee_id="annotator1@dataloop.ai", load=50)],
        position=(1, 1),
        project_id=context.project_id,
        dataset_id=context.dataset.id,
        actions=('complete', 'discard', 'fix-label')
    )

    task_node_2 = dl.TaskNode(
        name='My Task-completed' + current_time,
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=100)],
        position=(1, 2),
        project_id=context.project_id,
        dataset_id=context.dataset.id
    )

    task_node_3 = dl.TaskNode(
        name='My QA Task' + current_time,
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=100)],
        position=(1, 3),
        project_id=context.project_id,
        dataset_id=context.dataset.id,
        task_type='qa'
    )

    def run(item: dl.Item):
        item.metadata['user'] = {'Hello': 'World'}
        item.update()
        return item

    code_node = dl.CodeNode(
        name='My Function',
        position=(2, 2),
        project_id=context.project_id,
        method=run,
        project_name=context.project.name
    )

    dataset_node = dl.DatasetNode(
        name=context.dataset_finish.name,
        project_id=context.project_id,
        dataset_id=context.dataset_finish.id,
        position=(3, 3)
    )

    function_node = dl.FunctionNode(
        name='automate',
        position=(4, 4),
        service=dl.services.get(service_id=context.service.id),
        function_name='automate'
    )

    context.pipeline.nodes.add(task_node_1).connect(node=task_node_2, source_port=task_node_1.outputs[2],
                                                    target_port=task_node_2.inputs[0]).connect(node=task_node_3,
                                                                                               source_port=
                                                                                               task_node_2.outputs[0]) \
        .connect(node=code_node, source_port=task_node_3.outputs[0]).connect(node=dataset_node).connect(
        node=function_node)

    filters = dl.Filters(field='datasetId', values=context.dataset.id)
    task_node_1.add_trigger(filters=filters)

    context.pipeline.update()
    context.pipeline.install()


@behave.when(u'I create a pipeline with dataset resources')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(
        name='sdk-pipeline-sanity-{}'.format(random.randrange(1000, 10000)), project_id=context.project.id)

    def run_1(item: dl.Item):
        dataset = item.dataset
        return dataset

    code_node_1 = dl.CodeNode(
        name='My Function',
        position=(0, 0),
        project_id=context.project.id,
        method=run_1,
        project_name=context.project.name,
        outputs=[dl.PipelineNodeIO(port_id=str(uuid.uuid4()),
                                   input_type=dl.PackageInputType.DATASET,
                                   name='dataset',
                                   display_name='dataset')]
    )

    def run_2(dataset: dl.Dataset):
        for page in dataset.items.list():
            for item in page:
                break
        return item

    code_node_2 = dl.CodeNode(
        name='My Function',
        position=(2, 0),
        project_id=context.project.id,
        method=run_2,
        project_name=context.project.name,
        inputs=[dl.PipelineNodeIO(port_id=str(uuid.uuid4()),
                                  input_type=context.dl.PackageInputType.DATASET,
                                  name='dataset',
                                  display_name='dataset')]
    )

    context.pipeline.nodes.add(code_node_1).connect(node=code_node_2, filters=context.filters)

    code_node_1.add_trigger()
    context.pipeline.update()
    context.pipeline.install()


@behave.when(u'I create a pipeline dataset, task "{type}" and dataset nodes - repeatable "{flag}"')
def step_impl(context, type, flag):
    flag = eval(flag)

    num = random.randrange(10000, 100000)
    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)
    pipeline_name = f'pipeline-sdk-{num}'

    context.pipeline = context.project.pipelines.create(name=pipeline_name, project_id=context.project.id)

    dataset_node_1 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(1, 1)
    )

    task_name = 'My Task-completed' + current_time
    task_node = dl.TaskNode(
        name=task_name,
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=100)],
        position=(2, 2),
        task_type=type,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        repeatable=flag
    )

    dataset_node_2 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(3, 3)
    )

    context.pipeline.nodes.add(dataset_node_1).connect(node=task_node).connect(node=dataset_node_2,
                                                                               source_port=task_node.outputs[0])
    dataset_node_1.add_trigger()

    context.pipeline.update()
    pipeline = context.project.pipelines.get(pipeline_name=pipeline_name)
    pipeline.install()

    time.sleep(5)
    try:
        context.task = context.project.tasks.get(task_name=task_name + " (" + pipeline_name + ")")
    except Exception as e:
        assert False, "Failed to get task with the name: {}\n{}".format(task_name + " (" + pipeline_name + ")", e)


@behave.when(u'I create a pipeline with task node and new recipe')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='sdk-pipeline-test', project_id=context.project.id)

    task_node = dl.TaskNode(
        name='My Task',
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=100)],
        position=(2, 2),
        project_id=context.project.id,
        dataset_id=context.dataset.id,
    )

    context.pipeline.nodes.add(node=task_node)
    context.pipeline.update()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.given(u'a pipeline with same item enters task twice')
def step_impl(context):
    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)
    context.task_name = 'My Task-completed' + current_time

    context.task_node = dl.TaskNode(
        name=context.task_name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=context.dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=context.dl.info()['user_email'], load=100)],
        position=(3, 5),
        task_type='annotation',
        priority=dl.entities.TaskPriority.LOW
    )

    context.dataset_node_1 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(1, 5)
    )

    context.dataset_node_2 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(2, 3)
    )

    context.dataset_node_3 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(2, 7)
    )

    context.pipeline_name = 'pipeline-{}'.format(current_time)
    context.pipeline = context.project.pipelines.create(name='pipeline-sdk-test', project_id=context.project.id)
    context.dataset_node_1 = context.pipeline.nodes.add(context.dataset_node_1)
    context.dataset_node_1.connect(node=context.dataset_node_2, source_port=context.dataset_node_1.outputs[0],
                                   target_port=context.dataset_node_2.inputs[0])
    context.dataset_node_1.connect(node=context.dataset_node_3, source_port=context.dataset_node_1.outputs[0],
                                   target_port=context.dataset_node_3.inputs[0])
    context.dataset_node_2.connect(node=context.task_node, source_port=context.dataset_node_2.outputs[0],
                                   target_port=context.task_node.inputs[0])
    context.dataset_node_3.connect(node=context.task_node, source_port=context.dataset_node_3.outputs[0],
                                   target_port=context.task_node.inputs[0])
    context.pipeline.update()
    context.pipeline.install()
    context.task = context.project.tasks.list()[0]
    context.pipeline = context.dl.pipelines.get(pipeline_id=context.pipeline.id)
    is_installed = context.pipeline.status == 'Installed'
    assert is_installed, "Pipeline was not installed"


@behave.when(u'I execute pipeline on item')
def step_impl(context):
    context.pipeline: dl.Pipeline
    context.cycle = context.pipeline.execute(execution_input={'item': context.item.id})


@behave.when(u'I wait for item to enter task')
def step_impl(context):
    time.sleep(2)

    num_try = 10
    interval = 10
    entered = False

    for i in range(num_try):
        time.sleep(interval)
        context.item = context.dl.items.get(item_id=context.item.id)
        refs = context.item.metadata.get('system', {}).get('refs', [])
        if len(refs) > 0:
            entered = True
            break

    assert entered, f"TEST FAILED: Item was not move to task, after {round(num_try * interval / 60, 1)} minutes"


@behave.then(u'Cycle should be completed')
def step_impl(context):
    time.sleep(2)

    num_try = 10
    interval = 10
    completed = False

    for i in range(num_try):
        time.sleep(interval)
        pipeline: dl.Pipeline = context.pipeline
        context.cycle: dl.PipelineExecution = pipeline.pipeline_executions.list().items[0]
        if context.cycle.status == 'success':
            completed = True
            break

    assert completed, "TEST FAILED: cycle was not completed"


@behave.then(u'Context should have all required properties')
def step_impl(context):
    timeout = 60 * 10
    interval = 5
    start_time = time.time()
    execution = None
    success = False
    while time.time() - start_time < timeout:
        cycles = context.pipeline.pipeline_executions.list().items
        success = [False for i in cycles]
        for i in range(len(cycles)):
            if cycles[i].status == 'success':
                success[i] = True
        if all(success):
            success = True
            break
        time.sleep(interval)

    if success:
        services = context.project.services.list().all()
        for service in services:
            executions = service.executions.list()
            assert executions.items_count == 1
            execution = executions.items[0]
            assert execution is not None, "TEST FAILED: execution was not found"
            assert execution.output['taskName'] == context.task.name
            assert execution.output['pipelineName'] == context.pipeline.name
            assert execution.output['nodeName'] == \
                   [n for n in context.pipeline.nodes if n.namespace.service_name == service.name][0].name
            assert execution.output['assignmentName'] == context.task.assignments.list()[0].name
            assert execution.output['itemStatus'] == context.item_status


@behave.given(u'pipeline with 2 nodes')
def step_impl(context):
    context.pipeline_json = {
        "nodes": [
            {
                "id": "54a10414-0bbc-4b83-8454-175f905822cf",
                "inputs": [

                ],
                "outputs": [
                    {
                        "portId": "8c983ca9-ab14-4a27-be46-60a902cf3a62",
                        "nodeId": "8c983ca9-ab14-4a27-be46-60a902cf3a62",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10327.469348772935,
                        "y": 9879.491191781051,
                        "z": 0
                    },
                    "taskType": "annotation",
                    "componentGroupName": "automation",
                    "codeApplicationName": "root",
                    "repeatable": True
                },
                "name": "code",
                "type": "code",
                "namespace": {
                    "functionName": "run",
                    "projectName": context.project.name,
                    "serviceName": "root",
                    "moduleName": "code_module",
                    "packageName": "root"
                },
                "projectId": context.project.id,
                "config": {
                    "package": {
                        "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self):\n        return '67051534634d4937e4fedb16'\n",
                        "name": "run",
                        "type": "code",
                        "codebase": {
                            "type": "item"
                        }
                    }
                }
            },
            {
                "id": "162131f4-b157-41ad-b7bc-acf6eb03e9d2",
                "inputs": [

                ],
                "outputs": [
                    {
                        "portId": "685cb42f-4027-4fb5-8264-0f65995fce4a",
                        "nodeId": "685cb42f-4027-4fb5-8264-0f65995fce4a",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10342.000785813001,
                        "y": 10080.827982708255,
                        "z": 0
                    },
                    "taskType": "annotation",
                    "componentGroupName": "automation",
                    "codeApplicationName": "cron",
                    "repeatable": True
                },
                "name": "code",
                "type": "code",
                "namespace": {
                    "functionName": "run",
                    "projectName": context.project.name,
                    "serviceName": "cron",
                    "moduleName": "code_module",
                    "packageName": "cron"
                },
                "projectId": context.project.id,
                "config": {
                    "package": {
                        "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self):\n        return '67051534634d4937e4fedb16'",
                        "name": "run",
                        "type": "code",
                        "codebase": {
                            "type": "item"
                        }
                    }
                }
            }
        ],
        "connections": [

        ],
        "startNodes": [
            {
                "nodeId": "54a10414-0bbc-4b83-8454-175f905822cf",
                "type": "root",
                "id": "812249c5-79a5-4491-b3d6-0c9308bfa2b3"
            },
            {
                "nodeId": "162131f4-b157-41ad-b7bc-acf6eb03e9d2",
                "type": "trigger",
                "trigger": {
                    "type": "Cron",
                    "spec": {
                        "cron": "*/30 * * * * *"
                    },
                    "name": "code-6514635596702936"
                },
                "id": "e29d850b-8830-46f0-945a-14c84f1ef088"
            }
        ],
    }

    start_node = [node for node in context.pipeline_json['startNodes'] if node['type'] == 'root'][0]
    cron_trigger_node = [
        node for node in context.pipeline_json['startNodes'] if
        node['type'] == 'trigger' and node['trigger']['type'] == 'Cron'
    ][0]
    context.start_node_id = start_node['nodeId']
    context.cron_trigger_node_id = cron_trigger_node['nodeId']


def random_5_chars():
    return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=5))


@behave.given(u'the node which is not the start node has a cron trigger')
def step_impl(context):
    assert context.start_node_id is not None
    assert context.cron_trigger_node_id is not None

    context.pipeline = context.project.pipelines.create(
        pipeline_json=context.pipeline_json,
        name="test-cron-trigger-{}".format(random_5_chars())
    )


@behave.when(u'installing the pipeline')
def step_impl(context):
    context.pipeline.install()


@behave.then(u'the relevant node should be executed')
def step_impl(context):
    timeout = 60 * 5
    start_time = time.time()
    success = False
    while time.time() - start_time < timeout:
        time.sleep(1)
        cycles = context.pipeline.pipeline_executions.list().items
        if cycles:
            root_service = context.project.services.get(service_name='root')
            cron_service = context.project.services.get(service_name='cron')
            root_executions = root_service.executions.list().items
            cron_executions = cron_service.executions.list().items
            assert len(root_executions) == 0, 'Root node should not be executed'
            assert len(cron_executions) == 1, 'Cron node should be executed'
            success = True
            break
    assert success, "TEST FAILED: Cron node was not executed"


================================================
File: tests/features/steps/pipeline_entity/pipeline_get.py
================================================
import time

import behave
import re


@behave.when(u'I create a pipeline with name "{test_pipeline}"')
def step_impl(context, test_pipeline):
    payload = {'name': test_pipeline,
               'projectId': context.project.id,
               'nodes': [],
               'connections': []}

    context.pipeline = context.project.pipelines.create(pipeline_json=payload, project_id=context.project.id)
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'I get pipeline by the name of "{test_pipeline}"')
def step_impl(context, test_pipeline):
    context.pipeline_get = context.project.pipelines.get(pipeline_name=test_pipeline)


def is_valid_object_id(oid):
    return bool(re.fullmatch(r"[0-9a-fA-F]{24}", oid))


@behave.when(u'create pipeline from app template')
def step_impl(context):
    success, response = context.dl.client_api.gen_request(req_type='post',
                                                          path=f'/apps/{context.app.id}/resolvePipelineTemplate')
    assert success, f'Failed to resolve pipeline template: {response}'
    context.pipeline = context.dl.Pipeline.from_json(
        client_api=context.dl.client_api,
        _json=response.json(),
        project=context.project
    )


@behave.then(u'pipeline has ids instead of vars')
def step_impl(context):
    nodes = context.pipeline.nodes
    variables = context.pipeline.variables
    for node in nodes:
        if node.node_type in ['task', 'storage']:
            assert is_valid_object_id(node.metadata['datasetId']), f"Invalid datasetId: {node.metadata['datasetId']}"
            if node.node_type in ['task']:
                assert is_valid_object_id(node.metadata['recipeId']), f"Invalid recipeId: {node.metadata['recipeId']}"
        if node.node_type == 'model':
            assert is_valid_object_id(node.metadata['modelId']), f"Invalid modelId: {node.metadata['modelId']}"
    for var in variables:
        assert is_valid_object_id(var.value), f"Invalid value: {var.value}"

@behave.then(u'I get a pipeline entity')
def step_impl(context):
    assert 'Pipeline' in str(type(context.pipeline))


@behave.then(u'It is equal to pipeline created')
def step_impl(context):
    pipeline_json = context.pipeline.to_json()
    get_pipeline_json = context.pipeline_get.to_json()
    assert pipeline_json == get_pipeline_json


@behave.when(u'i list a project pipelines i get "{list_len}"')
def step_impl(context, list_len):
    assert context.project.pipelines.list().items_count == int(list_len)


@behave.when(u'I get pipeline execution in index "{index}"')
def step_impl(context, index):
    context.pipeline_execution = context.pipeline.pipeline_executions.list().items[int(index)]


@behave.then(u'Pipeline has "{total}" cycle executions')
def step_impl(context, total):
    num_try = 10
    interval = 6
    validate = 0
    success = False

    for i in range(num_try):
        time.sleep(interval)
        total_pipeline_executions = context.pipeline.pipeline_executions.list().items_count
        if total_pipeline_executions == int(total):
            validate += 1
            if validate == 2:
                success = True
                break

    assert success, "TEST FAILED: Expected to get {}, Actual got {}".format(total, total_pipeline_executions)


================================================
File: tests/features/steps/pipeline_entity/pipeline_ml.py
================================================
import os
import json
from behave import given, when, then
import random


@given(
    'Pipeline which have a model variable and predict ml node that reference to this model variable file "{file_name}"')
def step_impl(context, file_name):
    pipeline_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "pipeline_flow", file_name)

    with open(pipeline_path, 'r') as f:
        pipeline_json = json.load(f)
    try:
        context.pipeline = context.project.pipelines.get(pipeline_name='ml-pipeline')
    except Exception as e:
        pipeline_json['projectId'] = context.project.id
        if not hasattr(context, "model"):
            context.model = context.project.models.list()[0][0]
        context.pipeline = context.project.pipelines.create(name=f'ml-pipeline-{random.randrange(100, 1000)}',
                                                            pipeline_json=pipeline_json, project_id=context.project.id)
        var = context.dl.Variable({'name': 'model_entity', 'value': context.model.id, 'type': 'Model'})
        context.pipeline.variables.append(var)
        context.pipeline.nodes[0].metadata['variableModel'] = 'model_entity'
        context.pipeline.nodes[0].project_id = context.project.id
        context.pipeline.nodes[0].namespace.project_name = context.project.name
        context.pipeline = context.pipeline.update()
        context.to_delete_pipelines_ids.append(context.pipeline.id)


@when('I update the model variable and the pipeline is still installed')
def step_impl(context):
    models = context.project.models.list()[0]
    for model in models:
        if model.id != context.model.id:
            context.model = model
            break
    context.pipeline.variables[0].value = context.model.id
    context.pipeline.pipelines.__update_variables(pipeline=context.pipeline)
    context.pipeline = context.project.pipelines.get(pipeline_id=context.pipeline.id)


@then('The pipeline installed successfully and model id placed correctly in the service initInputs')
def step_impl(context):
    if context.pipeline.status != 'Installed':
        assert False, 'pipeline status is not installed'
    service = context.project.services.list()[0][0]
    assert service.init_input['model_entity'] == context.pipeline.variables[0].value, 'model id is not correct'


================================================
File: tests/features/steps/pipeline_entity/pipeline_node_input_handling.py
================================================
import behave
import dtlpy as dl


@behave.when(u'I build a pipeline where the second node handles missing input')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='test_pipeline', project_id=context.project.id)

    def run(string):
        if string != 'return_none':
            return string

    context.root = dl.CodeNode(
        name='root',
        position=(2, 2),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name,
        inputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )],
        outputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )]
    )

    def run1(string):
        if string is None:
            raise ValueError("required input is missing")
        return string

    context.n1 = dl.CodeNode(
        name='n1',
        position=(3, 2),
        project_id=context.project.id,
        method=run1,
        project_name=context.project.name,
        inputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string",
            default_value="default_string"
        )],
        outputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )]
    )

    context.pipeline.nodes.add(node=context.root).connect(node=context.n1)
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'I build a pipeline where the second node does not handle missing input')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='test_pipeline', project_id=context.project.id)

    def run(string):
        if string != 'return_none':
            return string

    context.root = dl.CodeNode(
        name='root',
        position=(2, 2),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name,
        inputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )],
        outputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )]
    )

    def run1(string):
        if string is None:
            raise ValueError("required input is missing")
        return string

    context.n1 = dl.CodeNode(
        name='n1',
        position=(3, 2),
        project_id=context.project.id,
        method=run1,
        project_name=context.project.name,
        inputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )],
        outputs=[dl.PipelineNodeIO(
            input_type=dl.PackageInputType.STRING,
            name="string",
            display_name="string"
        )]
    )

    context.pipeline.nodes.add(node=context.root).connect(node=context.n1)
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'I execute pipeline with input type: "{input_type}" and input value: "{input_value}"')
def step_impl(context, input_type, input_value):
    value = input_value
    if input_value == "None":
        value = None

    execution_input = list()
    if input_type == dl.PackageInputType.ITEM:
        execution_input.append(context.dl.FunctionIO(
            type=dl.PackageInputType.ITEM,
            value={'item_id': value},
            name='item'))
    elif input_type == dl.PackageInputType.STRING:
        execution_input.append(context.dl.FunctionIO(
            type=dl.PackageInputType.STRING,
            value=value,
            name='string'))
    context.execution = context.pipeline.execute(
        execution_input=execution_input)


@behave.when(u'I execute pipeline without input')
def step_impl(context):
    context.execution = context.pipeline.execute(
        execution_input={})


@behave.then(u'Cycle "{cycle_number}" node "{node_number}" execution "{execution_number}" single output is: "{expected_output_value}"')
def step_impl(context, cycle_number, node_number, execution_number, expected_output_value):
    cycle_index = int(cycle_number) - 1
    node_index = int(node_number) - 1
    execution_index = int(execution_number) - 1
    cycle = context.cycles[cycle_index]
    node_id = cycle.nodes[node_index].node_id
    execution_id = cycle.executions[node_id][execution_index]['_id']
    execution = dl.executions.get(execution_id=execution_id)
    output_value = execution.output
    assert expected_output_value == output_value, "Execution output was {} where {} was expected".format(output_value, expected_output_value)


@behave.then(u'Cycle "{cycle_number}" status is "{expected_cycle_status}"')
def step_impl(context, cycle_number, expected_cycle_status ):
    cycle_index = int(cycle_number) - 1
    cycle = context.cycles[cycle_index]
    assert cycle.status == expected_cycle_status, "Cycle status is {} where {} was expected".format(cycle.status, expected_cycle_status)

================================================
File: tests/features/steps/pipeline_entity/pipeline_output_list.py
================================================
import dtlpy as dl
import behave
import time


@behave.when(u'I create a pipeline with code and task node')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='sdk-pipeline-test', project_id=context.project.id)
    context.code_node_name = 'My Function'

    def run(item):
        dataset = item.dataset
        items = []
        for page in dataset.items.list():
            for item in page:
                items.append(item.id)

        return items

    code_node = dl.CodeNode(
        name=context.code_node_name,
        position=(1, 1),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name,
        outputs=[dl.PipelineNodeIO(input_type=dl.PackageInputType.ITEMS,
                                   name='items',
                                   display_name='items')]
    )

    task_node = dl.TaskNode(
        name='My Task',
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=dl.info()['user_email'], load=100)],
        position=(2, 3),
        project_id=context.project.id,
        dataset_id=context.dataset.id,
    )

    context.pipeline.nodes.add(node=code_node).connect(node=task_node, source_port=code_node.outputs[0])
    filters = dl.Filters(field='datasetId', values=context.dataset.id)
    code_node.add_trigger(filters=filters)
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.then(u'verify pipeline output result of "{total_items}" items')
def step_impl(context, total_items):
    task = context.project.tasks.list()[0]
    num_try = 60
    interval = 15
    finished = False

    for i in range(num_try):
        time.sleep(interval)
        print("Waited Sec : {}".format(interval * (i + 1)))
        if task.get_items().items_count == int(total_items):
            print("{} is ready".format(task.name))
            finished = True
            break

    assert finished

    time.sleep(10)
    for items in task.get_items():
        for item in items:
            item.update_status(task_id=task.id, status='complete')

    time.sleep(10)
    for pipe_execution in context.pipeline.pipeline_executions.list().items:
        assert pipe_execution.status == "success", "TEST FAILED: Pipeline execution {} is - {}".format(
            pipe_execution.id, pipe_execution.status)


@behave.then(u'I validate pipeline code-node service is with the correct version "{version}"')
def step_impl(context, version):
    service_name = context.code_node_name.lower().replace(' ', '-')
    for page in context.project.services.list():
        for service in page:
            if service_name in service.name:
                context.service = service
                break

    if not hasattr(context, 'service'):
        return False, "TEST FAILED: Failed to find service"

    assert version == context.service.package_revision, "TEST FAILED: Expect version to be {} got {}".format(version, context.service.package_revision)


@behave.when(u'I update pipeline code node')
def step_impl(context):
    context.pipeline = context.project.pipelines.get(pipeline_id=context.pipeline.id)
    code = context.pipeline.nodes.get('My Function').config.get('package').get('code')
    context.pipeline.nodes.get('My Function').config['package']['code'] = "# Adding comment to code node\n{}".format(code)
    context.pipeline = context.pipeline.update()


@behave.when(u'I create a pipeline with code node')
def step_impl(context):
    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)
    context.pipeline_name = 'pipeline-{}'.format(current_time)
    context.pipeline = context.project.pipelines.create(name=context.pipeline_name, project_id=context.project.id)
    context.code_node_name = 'My Function'

    def run(item):
        dataset = item.dataset
        items = []
        for page in dataset.items.list():
            for item in page:
                items.append(item.id)

        return items

    code_node = dl.CodeNode(
        name=context.code_node_name,
        position=(1, 1),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name,
        outputs=[dl.PipelineNodeIO(input_type=dl.PackageInputType.ITEMS,
                                   name='items',
                                   display_name='items')]
    )

    context.pipeline.nodes.add(node=code_node)
    filters = dl.Filters(field='datasetId', values=context.dataset.id)
    code_node.add_trigger(filters=filters)
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


================================================
File: tests/features/steps/pipeline_entity/pipeline_rerun_cycles.py
================================================
import time
import behave
import dtlpy as dl


@behave.when(u'I build a pipeline with dynamic node status')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='test_pipeline', project_id=context.project.id)

    def run(item):
        item.metadata['user'] = {'userInput': 'test'}
        item = item.update()
        return item

    context.root = dl.CodeNode(
        name='root',
        position=(2, 2),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name
    )

    def run1(item):
        if item.metadata.get('user', {}).get('userInput', None):
            item.metadata['user']['userInput'] = None
            item = item.update()
            raise ValueError("err")
        return item

    context.n1 = dl.CodeNode(
        name='n1',
        position=(3, 2),
        project_id=context.project.id,
        method=run1,
        project_name=context.project.name
    )

    context.n2 = dl.DatasetNode(
        name='n2',
        position=(4, 2),
        project_id=context.project.id,
        dataset_id=context.dataset.id
    )

    context.t1 = dl.DatasetNode(
        name='t1',
        position=(2, 3),
        project_id=context.project.id,
        dataset_id=context.dataset.id
    )

    context.t2 = dl.DatasetNode(
        name='t2',
        position=(2, 4),
        project_id=context.project.id,
        dataset_id=context.dataset.id
    )

    context.pipeline.nodes.add(node=context.root).connect(node=context.n1).connect(node=context.n2)
    context.pipeline.nodes.add(node=context.t1).connect(node=context.n1)
    context.pipeline.nodes.add(node=context.t2).connect(node=context.n1)
    context.t1.add_trigger()
    context.t2.add_trigger(actions=dl.TriggerAction.CREATED,
                           filters=dl.Filters(field='metadata.user.userInput', values=['test']))
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'rerun the cycle from the beginning')
def step_impl(context):
    dl.pipeline_executions.rerun(
        pipeline_id=context.pipeline.id,
        method=dl.CycleRerunMethod.START_FROM_BEGINNING
    )


@behave.when(u'rerun the cycle from the execution')
def step_impl(context):
    dl.pipeline_executions.rerun(
        pipeline_id=context.pipeline.id,
        method=dl.CycleRerunMethod.START_FROM_BEGINNING,
        filters=dl.Filters(resource=dl.FiltersResource.EXECUTION),
    )


@behave.when(u'rerun the cycle from the "{node_pos}" node')
def step_impl(context, node_pos):
    dl.pipeline_executions.rerun(
        pipeline_id=context.pipeline.id,
        method=dl.CycleRerunMethod.START_FROM_NODES,
        start_nodes_ids=[context.pipeline.nodes[int(node_pos) - 1].node_id]
    )


@behave.then(u'Cycle completed with save "{save}"')
def step_impl(context, save):
    time.sleep(2)

    num_try = 60
    interval = 10
    completed = 0
    cycles = []

    for i in range(num_try):
        completed = 0
        time.sleep(interval)
        pipeline: dl.Pipeline = context.pipeline
        cycles = pipeline.pipeline_executions.list().items
        if eval(save):
            context.cycles = cycles
        for cycle in cycles:
            if cycle.status == 'success' or cycle.status == 'failed':
                completed += 1
        if completed == len(cycles):
            break
        if i + 1 % 5 == 0:
            # Print the cycle URL every 5 intervals
            context.dl.logger.info(f"Cycle URL : {pipeline.url}/executions/{cycle.id}")
        context.dl.logger.info("Step is running for {:.2f}[s] and now Going to sleep {:.2f}[s]".format((i + 1) * interval, interval))

    assert completed > 0 and len(cycles), "TEST FAILED: cycle was not completed"


@behave.when(u'rerun the cycle from the failed node')
def step_impl(context):
    dl.pipeline_executions.rerun(
        pipeline_id=context.pipeline.id,
        method=dl.CycleRerunMethod.START_FROM_FAILED_EXECUTIONS,
    )


@behave.then(u'the pipeline cycle should be rerun')
def step_impl(context):
    pipeline = dl.pipelines.get(pipeline_id=context.pipeline.id)
    pipeline_cycle = pipeline.pipeline_executions.list().items
    assert len(pipeline_cycle) == len(context.cycles), 'pipeline cycle amount should be equal to the amount of cycles'
    for cycle in pipeline_cycle:
        for old_cycle in context.cycles:
            if cycle.id == old_cycle.id:
                if cycle.executions != old_cycle.executions:
                    return True
    context.cycles = pipeline_cycle
    assert False, 'pipeline cycle should be rerun'


@behave.then(u'the pipeline cycle should not change')
def step_impl(context):
    pipeline = dl.pipelines.get(pipeline_id=context.pipeline.id)
    pipeline_cycle = pipeline.pipeline_executions.list().items
    assert len(pipeline_cycle) == len(context.cycles), 'pipeline cycle amount should be equal to the amount of cycles'
    for cycle in pipeline_cycle:
        for old_cycle in context.cycles:
            if cycle.id == old_cycle.id:
                if cycle.executions != old_cycle.executions:
                    assert False, 'pipeline cycle should not be rerun'
    return True


@behave.when(u'pipeline many to one')
def step_impl(context):
    context.pipeline = context.project.pipelines.create(name='test_pipeline-many-to-one', project_id=context.project.id)

    def run(item):
        item.metadata['user'] = {'userInput': 'test'}
        item = item.update()
        return item

    context.root = dl.CodeNode(
        name='root',
        position=(2, 2),
        project_id=context.project.id,
        method=run,
        project_name=context.project.name
    )

    def run1(item):
        if item.metadata.get('user', {}).get('userInput', None):
            item.metadata['user']['userInput'] = None
            item = item.update()
            raise ValueError("err")
        return item

    context.n1 = dl.CodeNode(
        name='n1',
        position=(3, 2),
        project_id=context.project.id,
        method=run1,
        project_name=context.project.name
    )

    context.n2 = dl.DatasetNode(
        name='n2',
        position=(4, 2),
        project_id=context.project.id,
        dataset_id=context.dataset.id
    )

    context.t1 = dl.DatasetNode(
        name='t1',
        position=(2, 3),
        project_id=context.project.id,
        dataset_id=context.dataset.id
    )

    context.t2 = dl.DatasetNode(
        name='t2',
        position=(2, 4),
        project_id=context.project.id,
        dataset_id=context.dataset.id
    )

    context.pipeline.nodes.add(node=context.root).connect(node=context.n1).connect(node=context.n2)
    context.pipeline.nodes.add(node=context.t1).connect(node=context.n1)
    context.pipeline.nodes.add(node=context.t2).connect(node=context.n1)
    context.t1.add_trigger()
    context.t2.add_trigger(actions=dl.TriggerAction.CREATED,
                           filters=dl.Filters(field='metadata.user.userInput', values=['test']))
    context.pipeline.update()
    context.pipeline.install()
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.when(u'Cycle status should be "{status}"')
def step_impl(context, status):
    time.sleep(20)
    cycle = context.pipeline.pipeline_executions.list().items[0]
    assert cycle.status == status, f'Cycle status should be {status}'


================================================
File: tests/features/steps/pipeline_entity/pipeline_reset.py
================================================
import behave
from time import sleep


@behave.then(u'I try to reset statistics with stop_if_running "{flag_status}"')
def step_impl(context, flag_status):
    if flag_status == "True":
        context.pipeline.reset(stop_if_running=True)
        sleep(2)
        # Checking that all statistics got rest
        for i in range(3):
            if context.pipeline.stats().node_averages[i].averages.avg_execution_per_day != 0:
                assert False
            if context.pipeline.stats().node_averages[i].averages.avg_time_per_execution != 0:
                assert False

        if context.pipeline.stats().pipeline_averages.avg_execution_per_day != 0:
            assert False
        if context.pipeline.stats().pipeline_averages.avg_time_per_execution != 0:
            assert False
        assert True
    else:
        try:
            context.pipeline.reset(stop_if_running=False)
            assert False
        except Exception as e:
            assert "Pipeline with installed status couldn't be updated" in e.args[1]
            assert True


================================================
File: tests/features/steps/pipeline_entity/pipeline_update.py
================================================
from behave import when, then, given


@when(u'I update pipeline description')
def step_impl(context):
    context.pipeline = context.project.pipelines.get(pipeline_name=context.pipeline.name)
    context.pipeline.description = "up"
    context.update_pipeline = context.pipeline.update()


@when(u'i update pipeline model node in index "{node_index}" configration')
def step_impl(context, node_index):
    context.pipeline = context.project.pipelines.get(pipeline_name=context.pipeline.name)
    context.pipeline.nodes[int(node_index)].metadata['modelConfig'] = {
        'system_prompt': 'test',
        'max_tokens': 100,
        'temperature': 0.5
    }
    context.pipeline = context.pipeline.update()


@then(u'Pipeline received equals Pipeline changed except for "description"')
def step_impl(context):
    assert context.pipeline.description == "up"


@then(u'I pause pipeline in context')
@when(u'I pause pipeline in context')
def step_impl(context):
    context.pipeline.pause()


@then(u'I install pipeline in context')
@when(u'I install pipeline in context')
@given(u'I install pipeline in context')
def step_impl(context):
    try:
        context.pipeline = context.pipeline.install()
    except Exception as e:
        raise e


@when(u'I update ml node "{node_name}" to variable "{variable_name}"')
def step_impl(context, node_name, variable_name):
    for node in context.pipeline.nodes:
        if node_name == node.name:
            node.metadata['variableModel'] = variable_name
            context.pipeline.update()


@when(u'I update node name to "{node_name}"')
def step_impl(context, node_name):
    context.node = context.pipeline.nodes[0]
    context.node.name = node_name
    context.pipeline.update()


@then(u'I validate task name changed')
def step_impl(context):
    try:
        pipeline_task_name = "{} ({})".format(context.node.name, context.pipeline.name)
        context.task = context.project.tasks.get(task_name=pipeline_task_name)
    except Exception as e:
        assert False, "Failed to get task with the name: {}\n{}".format(pipeline_task_name, e)


@when(u'I add integration to pipeline secrets and update pipeline')
def step_impl(context):
    if not hasattr(context, "integration"):
        raise AttributeError("Please make sure context has attr 'integration'")

    import dtlpy as dl

    pipeline_json = context.pipeline.to_json()
    pipeline_json.update({'secrets': [context.integration.id]})
    pipeline_json.pop('id')
    success, response = dl.client_api.gen_request(
        req_type='patch',
        path='/pipelines/{}'.format(context.pipeline.id),
        json_req=pipeline_json
    )
    if not success:
        raise dl.exceptions.PlatformException(response)


================================================
File: tests/features/steps/pipeline_entity/pipeline_validation.py
================================================
import behave


@behave.when(u'I update node in index "{index}" to start node')
def step_impl(context, index):
    context.pipeline.start_nodes.append({"nodeId": context.pipeline.nodes[int(index)].node_id,
                                         "type": "root"})

    try:
        context.pipeline = context.pipeline.update()
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I try to install pipeline in context')
def step_impl(context):
    try:
        context.pipeline = context.pipeline.install()
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_actions.py
================================================
from behave import when, then, given
import time
import dtlpy as dl


@when(u'I create custom pipeline for code node with progress.update(action="{action_input}")')
def step_impl(context, action_input):
    """
    Pipeline contain 3 code nodes
    Code-node#1 contain progress.update(action='') and has 2 node outputs with action "first-output" and action "second-output"
    Dataset-node#1 connect to output[0] - action "first-output"
    Dataset-node#2 connect to output[1] - action "second-output"
    """
    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)
    context.pipeline_name = 'pipeline-{}'.format(current_time)

    context.pipeline = context.project.pipelines.create(name=context.pipeline_name, project_id=context.project.id)

    if "first" in action_input:
        def run_1(item, progress):
            progress.update(action="first-output")
            import time
            time.sleep(1)
            return item
    elif "second" in action_input:
        def run_1(item, progress):
            progress.update(action="second-output")
            import time
            time.sleep(1)
            return item
    else:
        assert False, "action_input Should be 'first-output' OR 'second-output'"

    code_node_1 = dl.CodeNode(
        name="code-1",
        position=(1, 2),
        project_id=context.project.id,
        method=run_1,
        project_name=context.project.name,
        outputs=[dl.PipelineNodeIO(input_type=dl.PackageInputType.ITEM,
                                   name='item',
                                   display_name='item',
                                   actions=['first-output', 'second-output'])
                 ]
    )

    dataset_node_1 = dl.DatasetNode(
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        name="dataset-1",
        position=(2, 1)
    )

    dataset_node_2 = dl.DatasetNode(
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        name="dataset-2",
        position=(2, 3)
    )

    filters = context.dl.Filters()
    filters.add(field='datasetId', values=context.dataset.id)
    context.pipeline.nodes.add(code_node_1).connect(node=dataset_node_1, source_port=code_node_1.outputs[0],
                                                    action="first-output")
    context.pipeline.nodes[0].connect(node=dataset_node_2, source_port=code_node_1.outputs[0], action="second-output")
    code_node_1.add_trigger(filters=filters)

    context.pipeline = context.pipeline.update()
    context.pipeline.install()

    time.sleep(5)


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_connectors.py
================================================
from behave import when
import time
import dtlpy as dl
import random


@when(u'I create a pipeline with code nodes with 2 outputs and code node with 2 inputs')
def step_impl(context):
    """
    Pipeline contain 2 code nodes with 2 outpus and 2 inputs
    Code-node#1 code node have 2 outputs - return item and dataset resources
    Code-node#2 code node have 2 inputs - get item and dataset resources
    """
    num = random.randrange(10000, 100000)
    context.pipeline_name = f'pipeline-sdk-{num}'

    context.pipeline = context.project.pipelines.create(name=context.pipeline_name, project_id=context.project.id)

    def run_1(item: dl.Item):
        dataset = item.dataset
        return item, dataset

    def run_2(item: dl.Item, dataset: dl.Dataset):
        assert item.dataset.id == dataset.id
        return item

    code_node_1 = dl.CodeNode(
        name="code-1",
        position=(1, 1),
        project_id=context.project.id,
        method=run_1,
        project_name=context.project.name,
        outputs=[dl.PipelineNodeIO(input_type=dl.PackageInputType.ITEM,
                                   name='item',
                                   display_name='item'),
                 dl.PipelineNodeIO(input_type=dl.PackageInputType.DATASET,
                                   name='dataset',
                                   display_name='dataset')
                 ]
    )

    code_node_2 = dl.CodeNode(
        name="code-2",
        position=(2, 2),
        project_id=context.project.id,
        method=run_2,
        project_name=context.project.name,
        inputs=[dl.PipelineNodeIO(input_type=dl.PackageInputType.ITEM,
                                  name='item',
                                  display_name='item'),
                dl.PipelineNodeIO(input_type=dl.PackageInputType.DATASET,
                                  name='dataset',
                                  display_name='dataset')
                ]
    )

    filters = context.dl.Filters()
    filters.add(field='datasetId', values=context.dataset.id)
    context.pipeline.nodes.add(code_node_1)
    context.pipeline.nodes[0].connect(node=code_node_2, target_port=code_node_2.inputs[0], source_port=code_node_1.outputs[0])
    context.pipeline.nodes[0].connect(node=code_node_2, target_port=code_node_2.inputs[1], source_port=code_node_1.outputs[1])
    code_node_1.add_trigger(filters=filters)

    context.pipeline = context.pipeline.update()
    context.pipeline.install()

    time.sleep(5)


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_dataset_node_trigger_to_pipeline.py
================================================
import random
import string
import time

from behave import given, when, then


def find_dataset_node(context):
    nodes = context.pipeline.nodes
    return next((node for node in nodes if node.node_type == 'storage'), None)


def get_item_executions(context):
    return context.item.resource_executions.list().items_count


@then(u'The dataset node is marked as triggered')
def step_impl(context):
    node = find_dataset_node(context)
    assert node is not None, "TEST FAILED: Dataset node not found"
    trigger_to_pipeline = node.metadata.get('triggerToPipeline')
    assert trigger_to_pipeline is not None, "TEST FAILED: Dataset node does not have triggerToPipeline in the metadata"
    assert trigger_to_pipeline.get('active'), "TEST FAILED: Dataset node is not marked as active"
    assert trigger_to_pipeline.get('triggered'), "TEST FAILED: Dataset node is not marked as triggered"


@then(u'The dataset node is assigned with a command id')
def step_impl(context):
    node = find_dataset_node(context)
    assert node is not None, "TEST FAILED: Dataset node not found"
    trigger_to_pipeline = node.metadata.get('triggerToPipeline')
    assert trigger_to_pipeline is not None, "TEST FAILED: Dataset node does not have triggerToPipeline in the metadata"
    assert trigger_to_pipeline.get('commandId') is not None, "TEST FAILED: Dataset node is not assigned with a " \
                                                             "command id "


@then(u'The dataset node has data_filters')
def step_impl(context):
    node = find_dataset_node(context)
    assert node is not None, "TEST FAILED: Dataset node not found"
    trigger_to_pipeline = node.metadata.get('triggerToPipeline')
    assert trigger_to_pipeline is not None, "TEST FAILED: Dataset node does not have triggerToPipeline in the metadata"
    assert trigger_to_pipeline.get('filter') is not None, "TEST FAILED: Dataset node is not assigned with a filter"
    assert node.data_filters is not None, "TEST FAILED: Dataset node is not assigned with a filter"


@then(u'The uploaded item has "{num}" executions')
def step_impl(context, num):
    time.sleep(2)
    executions = get_item_executions(context)
    assert executions == int(num), f"TEST FAILED: Item has {executions} executions"


@given(u'I have a pipeline with 2 dataset nodes with variables and triggerToPipeline=true')
def step_impl(context):
    pipeline_json = {
        "nodes": [
            {
                "id": "13898b44-7a61-4684-a172-90196b2c9fbd",
                "inputs": [
                    {
                        "portId": "70c69fbc-e705-4892-abba-04325c3bba4e",
                        "nodeId": "70c69fbc-e705-4892-abba-04325c3bba4e",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "92529eee-31c9-4b61-82e4-722d9862f22b",
                        "nodeId": "92529eee-31c9-4b61-82e4-722d9862f22b",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10280,
                        "y": 10043,
                        "z": 0
                    },
                    "componentGroupName": "data",
                    "repeatable": True,
                    "variableDataset": "dataset",
                    "triggerToPipeline": {
                        "active": True
                    }
                },
                "name": "dataset",
                "type": "storage",
                "namespace": {
                    "functionName": "clone_item",
                    "projectName": "DataloopTasks",
                    "serviceName": "pipeline-utils",
                    "moduleName": "default_module",
                    "packageName": "pipeline-utils"
                },
                "projectId": context.project.id
            },
            {
                "id": "057af07a-5455-462e-a942-5f3f015e7eb7",
                "inputs": [
                    {
                        "portId": "edb87533-59da-40b7-b8a8-28cb95769372",
                        "nodeId": "edb87533-59da-40b7-b8a8-28cb95769372",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "3df23f62-4581-452e-9595-31ef50ed5c40",
                        "nodeId": "3df23f62-4581-452e-9595-31ef50ed5c40",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10599,
                        "y": 10098,
                        "z": 0
                    },
                    "componentGroupName": "data",
                    "repeatable": True,
                    "variableDataset": "dataset"
                },
                "name": "dataset",
                "type": "storage",
                "namespace": {
                    "functionName": "clone_item",
                    "projectName": "DataloopTasks",
                    "serviceName": "pipeline-utils",
                    "moduleName": "default_module",
                    "packageName": "pipeline-utils"
                },
                "projectId": context.project.id
            }
        ],
        "connections": [
            {
                "src": {
                    "nodeId": "13898b44-7a61-4684-a172-90196b2c9fbd",
                    "portId": "92529eee-31c9-4b61-82e4-722d9862f22b"
                },
                "tgt": {
                    "nodeId": "057af07a-5455-462e-a942-5f3f015e7eb7",
                    "portId": "edb87533-59da-40b7-b8a8-28cb95769372"
                },
                "condition": "{}"
            }
        ],
        "startNodes": [
            {
                "nodeId": "13898b44-7a61-4684-a172-90196b2c9fbd",
                "type": "root",
                "id": "ee209df9-7007-4e33-8433-225656093c15"
            }
        ],
        "variables": [
            {
                "name": "dataset",
                "type": "Dataset",
                "value": context.dataset.id
            }
        ]
    }

    context.pipeline = context.project.pipelines.create(
        pipeline_json=pipeline_json,
        name="triggerToPipeline-var{}".format(
            ''.join(random.choices(string.ascii_lowercase + string.digits, k=4)) + 'a'
        )
    )

@then(u'All dataset items should run')
def step_impl(context):
    start = time.time()
    timeout = 60
    success = False
    item_count = context.dataset.items.list().items_count

    while time.time() - start < timeout:
        cycle_count = context.pipeline.pipeline_executions.list().items_count
        if cycle_count == item_count:
            success = True
            break

    assert success, 'Pipeline did not run all items'

================================================
File: tests/features/steps/pipeline_entity/test_pipeline_execution.py
================================================
import behave
from operator import attrgetter
import time


@behave.then(u'I validate pipeline execution params include node executions "{node_exec_flag}"')
def step_impl(context, node_exec_flag):
    executions_val = list()
    for row in context.table:
        test_value = row['value']
        #  Add the pipeline cycle_execution creator
        executions_val.append(attrgetter(row['key'])(context.execution))

        if eval(node_exec_flag):
            time.sleep(7)
            context.execution = context.pipeline.pipeline_executions.get(context.execution.id)
            for i in range(0, 10):
                if context.execution.status == "pending":
                    time.sleep(7)
                    context.execution = context.pipeline.pipeline_executions.get(context.execution.id)
                else:
                    break
            assert context.execution.executions, "TEST FAILED: Executions empty.{}".format(context.execution.executions)
            for execution in context.execution.executions.values():
                assert isinstance(execution, list), "TEST FAILED: execution Should be a list"
                #  Add the pipeline cycle connected executions creator
                execution_id = execution[0]['_id']
                executions = context.dl.executions.get(execution_id=execution_id)
                executions_val.append(attrgetter(row['key'])(executions))

        if row['value'] == "current_user":
            test_value = context.dl.info()['user_email']
        elif row['value'] == "piper":
            test_value = ["piper@dataloop.ai", "pipelines@dataloop.ai"]
        for val in executions_val:
            assert isinstance(val, str), "TEST FAILED: val must be a string"
            assert val in test_value, "TEST FAILED: Expected to get {}, Actual got {}".format(test_value, val)


@behave.then(u'I validate pipeline execution params')
def step_impl(context, node_exec_flag):
    executions_val = list()
    for row in context.table:
        test_value = row['value']
        executions_val.append(attrgetter(row['key'])(context.execution))

        if eval(node_exec_flag):
            time.sleep(5)
            context.execution = context.pipeline.pipeline_executions.get(context.execution.id)
            assert context.execution.executions, "TEST FAILED: Executions empty.{}".format(context.execution.executions)
            for execution in context.execution.executions.values():
                assert isinstance(execution, list), "TEST FAILED: execution Should be a list"
                execution_id = execution[0]['_id']
                executions = context.dl.executions.get(execution_id=execution_id)
                executions_val.append(attrgetter(row['key'])(executions))

        if row['value'] == "current_user":
            test_value = context.dl.info()['user_email']
        for val in executions_val:
            assert val == test_value, "TEST FAILED: Expected to get {}, Actual got {}".format(test_value, val)


@behave.then(u'I wait for item to enter task')
def step_impl(context):
    timeout = 60 * 5 * 1000
    start_time = time.time()
    success = False
    while time.time() < start_time + timeout:
        context.item = context.dataset.items.get(item_id=context.item.id)
        if len(context.item.system['refs']) > 1:
            success = True
            break
    assert success, "TEST FAILED: Item did not enter task"


@behave.then(u'I expect that pipeline execution has "{execution_number}" success executions')
def step_impl(context, execution_number):
    time.sleep(.500)
    assert context.pipeline.pipeline_executions.list().items_count != 0, "Pipeline not executed found 0 executions"
    context.pipeline = context.project.pipelines.get(context.pipeline.name)

    num_try = 72
    interval = 8
    executed = False
    execution_count = 0
    pipeline_executions = None
    for i in range(num_try):
        time.sleep(interval)
        context.cycles = context.pipeline.pipeline_executions.list().items
        cycle = context.cycles[0]
        execution_list = cycle.executions
        execution_count = 0
        for ex in execution_list.values():
            execution_count = execution_count + len(ex)
        if execution_count == int(execution_number):
            # validate no other executions were created
            filters = context.dl.Filters(resource=context.dl.FiltersResource.EXECUTION)
            filters.add(field='pipeline.executionId', values=cycle.id)
            filters.add(field='pipeline.id', values=context.pipeline.id)
            pipeline_executions = context.dl.executions.list(filters=filters)
            execution_count = pipeline_executions.items_count
            executed = execution_count == int(execution_number)
            break


    if pipeline_executions and pipeline_executions.items[-1].latest_status['status'] == 'failed':
        assert False, f"TEST FAILED: Last execution failed with message: {pipeline_executions.items[-1].latest_status['message']}"
    assert executed, "TEST FAILED: Pipeline has {} executions instead of {}".format(execution_count, execution_number)
    return executed


@behave.then(u'The pipeline has been executed "{num}" times')
def step_impl(context, num):
    time.sleep(2)
    executions = context.pipeline.pipeline_executions.list().items_count
    assert executions == int(num), f"TEST FAILED: Pipeline has been executed {executions} times"


@behave.when(u'I get pipeline cycle execution in index "{num}"')
def step_impl(context, num):
    filters = context.dl.Filters()
    filters.resource = context.dl.FiltersResource.PIPELINE_EXECUTION
    context.execution = context.pipeline.pipeline_executions.list(filters=filters).items[eval(num)]


@behave.then(u'I validate Cycle execution status is "{status}"')
def step_impl(context, status):
    num_try = 60
    interval = 10
    executed = False
    context.pipeline_execution_id = context.pipeline_execution.id

    for i in range(num_try):
        time.sleep(interval)
        context.pipeline_execution = context.pipeline.pipeline_executions.get(pipeline_execution_id=context.pipeline_execution_id)
        if context.pipeline_execution.status == status:
            executed = True
            break

    assert executed, "TEST FAILED: Pipeline cycle status is {} Expected to get {}".format(context.pipeline_execution.status, status)
    return executed


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_install.py
================================================
from behave import when, then
import time
import dtlpy as dl


@when(u'I delete current nodes and add dataset nodes to pipeline')
def step_impl(context):
    """
    Remove all nodes from pipeline
    Add dataset node#1 > dataset node#2 > connect them > Start pipeline
    Code-node#1
    """

    context.pipeline = context.project.pipelines.get(pipeline_name=context.pipeline_name)

    for node in context.pipeline.nodes:
        assert context.pipeline.nodes.remove(node.name), "TEST FAILED: Failed to delete node {}".format(node.name)

    dataset_node_1 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(2, 2)
    )

    dataset_node_2 = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(3, 3)
    )

    context.pipeline.nodes.add(dataset_node_1).connect(dataset_node_2)
    context.pipeline = context.pipeline.update()
    context.pipeline.install()

    time.sleep(5)


@then(u'Pipeline status is "{status}"')
def step_impl(context, status):
    try:
        context.pipeline = context.project.pipelines.get(pipeline_name=context.pipeline.name)
        assert context.pipeline.status == status
    except:
        print("\nTEST FAILED: Pipeline status expected: {} , Got: {}\n".format(status, context.pipeline.status))
        success, response = context.dl.client_api.gen_request(
            req_type="get",
            path="/compositions/{composition_id}".format(
                composition_id=context.pipeline.composition_id)
        )
        assert False, "Error message: {}\n".format(response.json()['errorText']['message'])


@when(u'I update node input output to infinite loop')
def step_impl(context):
    try:
        context.pipeline.nodes.add(context.nodes[0])
        context.nodes.pop(0)

        context.pipeline.nodes[0].connect(context.pipeline.nodes[0])
        context.pipeline.update()
        context.error = None
    except Exception as e:
        context.error = e



================================================
File: tests/features/steps/pipeline_entity/test_pipeline_interface.py
================================================
import time
import uuid
import dtlpy as dl
import behave
import random
import os
import json

from dtlpy.exceptions import NotFound


@behave.Given(u'I create pipeline with the name "{pipeline_name}"')
def step_impl(context, pipeline_name):
    context.pipeline_name = f'{pipeline_name}-{random.randrange(1000, 10000)}'
    context.pipeline = context.project.pipelines.create(name=context.pipeline_name, project_id=context.project.id)
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.given(u'I create "{node_type}" node with params')
def step_impl(context, node_type):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']

    if node_type == "dataset":
        data_filters = params.get('data_filters', None)
        if data_filters:
            data_filters = context.dl.Filters(custom_filter=json.loads(data_filters))

        context.node = dl.DatasetNode(
            name=params.get('name', context.dataset.name),
            project_id=context.project.id,
            dataset_id=params.get('dataset_id', context.dataset.id),
            dataset_folder=params.get('folder', None),
            position=eval(params.get('position', "(1, 1)")),
            load_existing_data=params.get('load_existing_data', None),
            data_filters=data_filters,
        )

    elif node_type == 'code':
        def run(item):
            return item

        context.node = dl.CodeNode(
            name=params.get('name', "codenode"),
            position=eval(params.get('position', "(1, 1)")),
            project_id=context.project.id,
            method=run,
            project_name=context.project.name,
            inputs=[dl.PipelineNodeIO(port_id=str(uuid.uuid4()),
                                      input_type=params.get('input_type', "Item"),
                                      name=params.get('input_name', "item"),
                                      color=None,
                                      display_name=params.get('input_display_name', "Item"),
                                      actions=None if not params.get('input_actions') else params.get(
                                          'input_actions').split(','))],
            outputs=[dl.PipelineNodeIO(port_id=str(uuid.uuid4()),
                                       input_type=params.get('output_type', "Item"),
                                       name=params.get('output_name', "item"),
                                       color=None,
                                       display_name=params.get('output_display_name', "Item"),
                                       actions=None if not params.get('output_actions') else params.get(
                                           'output_actions').split(','))]

        )
    elif node_type == 'task':
        context.task_name = params.get('name', "My Task")
        context.node = dl.TaskNode(
            name=context.task_name,
            recipe_id=context.recipe.id,
            recipe_title=context.recipe.title,
            task_owner=params.get('taskOwner', dl.info()['user_email']),
            workload=[dl.WorkloadUnit(assignee_id=params.get('assigneeId', dl.info()['user_email']), load=100)],
            position=eval(params.get('position', "(1, 1)")),
            task_type=params.get('type', "annotation"),
            project_id=context.project.id,
            dataset_id=context.dataset.id,
            repeatable=eval(params.get("repeatable", "True"))
        )
    elif node_type == 'faas':
        if not hasattr(context, 'service'):
            assert False, "TEST FAILED: Service is not defined , Please make sure to run step 'I deploy a service'"
        context.node = dl.FunctionNode(
            name=params.get(context.service.name, "My Function"),
            service=context.service,
            function_name=params.get('function_name', "run"),
            project_id=context.project.id,
            project_name=context.project.name,
            position=eval(params.get('position', "(1, 1)"))
        )

    context.nodes.append(context.node)


@behave.when(u'I add and connect all nodes in list to pipeline entity')
def step_impl(context):
    assert context.nodes, "TEST FAILED: Nodes list is empty.{}".format(context.nodes)

    context.pipeline.nodes.add(context.nodes[0])
    context.nodes.pop(0)

    for i in range(len(context.nodes)):
        context.pipeline.nodes[i].connect(context.nodes[i])

    try:
        context.pipeline = context.pipeline.update()
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I add trigger to first node with params')
def step_impl(context):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']

    context.filters = context.dl.Filters()
    for key, val in params.items():
        context.filters.add(field=key, values=eval(val))

    context.pipeline.nodes[0].add_trigger(filters=context.filters)
    context.pipeline = context.pipeline.update()


@behave.when(u'I update pipeline attributes with params')
def step_impl(context):
    context.pipeline = context.pipeline.pipelines.get(pipeline_id=context.pipeline.id)
    for row in context.table:
        att = f"context.pipeline.{row['key']}"
        val = f"'{row['value']}'"
        exec(f"{att} = {val}")

    context.pipeline = context.pipeline.update()


@behave.when(u'I get pipeline in context by id')
def step_impl(context):
    context.pipeline = context.pipeline.pipelines.get(pipeline_id=context.pipeline.id)


@behave.then(u'I validate pipeline attributes with params')
def step_impl(context):
    for row in context.table:
        att = f"context.pipeline.{row['key']}"
        val = f"'{row['value']}'"
        exec(f"assert {att} == {val}, 'TEST FAILED: Expected '+{val}+', Actual '+{att}")


@behave.when(u'I add action "{action}" to connection in index "{num}"')
def step_impl(context, action, num):
    context.pipeline.connections[int(num)].action = action

    try:
        context.pipeline = context.pipeline.update()
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I add all nodes in list to pipeline entity')
def step_impl(context):
    assert context.nodes, "TEST FAILED: Nodes list is empty.{}".format(context.nodes)

    for i in range(len(context.nodes)):
        context.pipeline.nodes.add(context.nodes[i])

    try:
        context.pipeline = context.pipeline.update()
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I update pipeline context.node "{att}" with "{val}"')
def step_imp(context, att, val):
    setattr(context.node, att, eval(val))
    context.pipeline.update()


def check_no_connection_for_input(pipeline_json, node_input):
    connections = [connection['tgt']['portId'] for connection in pipeline_json['connections']]
    # If portId in connection return False
    if node_input['portId'] in connections:
        return False
    return True


def generate_pipeline_json(context, pipeline_json):
    pipeline_json['name'] = 'json-pipe-{}'.format(random.randrange(10000, 100000))
    pipeline_json['creator'] = context.dl.info()['user_email']
    pipeline_json['projectId'] = context.project.id
    pipeline_json['orgId'] = context.project.org['id']

    for node in pipeline_json.get('nodes', []):
        node['projectId'] = context.project.id

    datasets_node = [node for node in pipeline_json.get('nodes', []) if node['type'] == 'storage']
    if not hasattr(context, "datasets"):
        for node in datasets_node:
            node['name'] = context.dataset.name
            node['metadata']["datasetId"] = context.dataset.id
    else:
        for index in range(len(datasets_node)):
            datasets_node[index]['name'] = context.datasets[index].name
            datasets_node[index]['metadata']["datasetId"] = context.datasets[index].id

    task_nodes = [node for node in pipeline_json.get('nodes', []) if node['type'] == 'task']
    for node in task_nodes:
        node['projectId'] = context.project.id
        if not hasattr(context, "datasets"):
            node['metadata']["recipeTitle"] = context.dataset.recipes.list()[0].title
            node['metadata']["recipeId"] = context.dataset.recipes.list()[0].id
            node['metadata']["datasetId"] = context.dataset.id
        else:
            node['metadata']["recipeTitle"] = context.datasets[0].recipes.list()[0].title
            node['metadata']["recipeId"] = context.datasets[0].recipes.list()[0].id
            node['metadata']["datasetId"] = context.datasets[0].id
        node['metadata']["taskOwner"] = context.dl.info()['user_email']
        node['metadata']["workload"] = [
            {
                "assigneeId": context.dl.info()['user_email'],
                "load": 100
            }
        ]

    function_nodes = [node for node in pipeline_json.get('nodes', []) if node['type'] == 'function']
    for node in function_nodes:
        node['namespace']['serviceName'] = context.service.name
        node['namespace']['packageName'] = context.package.name
        node['namespace']['projectName'] = context.project.name

    ml_nodes = [node for node in pipeline_json.get('nodes', []) if node['type'] == 'ml']
    for node in ml_nodes:
        node['namespace']['projectName'] = context.project.name
        if not hasattr(context, "model"):
            break
        elif node['namespace']['functionName'] == "train":
            node['metadata']['aiLibraryId'] = context.model.package.id
            if check_no_connection_for_input(pipeline_json, node['inputs'][0]):
                node['inputs'][0]['defaultValue'] = context.model.id
        elif node['namespace']['functionName'] == "predict":
            node['name'] = context.model.name
            node['metadata']["modelId"] = context.model.id
            node['metadata']["modelName"] = context.model.name
        elif node['namespace']['functionName'] == "evaluate":
            node['metadata']['aiLibraryId'] = context.model.package.id
            for node_input in node['inputs']:
                if node_input['type'] == 'Model' and check_no_connection_for_input(pipeline_json, node_input):
                    node_input['defaultValue'] = context.model.id
                elif node_input['type'] == 'Dataset' and check_no_connection_for_input(pipeline_json, node_input):
                    node_input['defaultValue'] = context.dataset.id

    custom_nodes = [node for node in pipeline_json.get('nodes', []) if node['type'] == 'custom']
    for node in custom_nodes:
        node['namespace']['projectName'] = context.project.name
        node['namespace']['packageName'] = context.dpk.name
        node['projectId'] = context.project.id
        node['dpkName'] = context.dpk.name
        node['appName'] = context.dpk.display_name

    code_nodes = [node for node in pipeline_json.get('nodes', []) if node['type'] == 'code']
    for node in code_nodes:
        if node.get('metadata'):
            if node['metadata'].get('serviceConfig'):
                if node['metadata']['serviceConfig'].get('versions'):
                    node['metadata']['serviceConfig']['versions']['dtlpy'] = context.dl.__version__

    variables = pipeline_json.get('variables', []) if pipeline_json.get('variables', []) else []
    for variable in variables:
        if variable['type'] == "Model" and hasattr(context, "model"):
            variable['value'] = context.model.id
        elif variable['type'] == "Dataset":
            variable['value'] = context.dataset.id

    return pipeline_json


@behave.given(u'I create pipeline from json in path "{path}"')
def step_impl(context, path):
    test_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
    with open(test_path, 'r') as pipeline_path:
        pipeline_json = json.load(pipeline_path)

    pipeline_payload = generate_pipeline_json(
        context=context,
        pipeline_json=pipeline_json
    )

    try:
        context.pipeline = context.project.pipelines.create(pipeline_json=pipeline_payload,
                                                            project_id=context.project.id)
        context.to_delete_pipelines_ids.append(context.pipeline.id)
        for node in pipeline_json['nodes']:
            if node['type'] == 'task':
                context.task_name = node['name']
        context.error = None
    except Exception as e:
        context.error = e


@behave.Then(u'I dont get Pipeline Task by name')
def step_impl(context):
    flag = False
    for i in range(10):
        time.sleep(1)
        try:
            context.task = context.project.tasks.get(task_name=(context.task_name + " (" + context.pipeline_name + ")"))
        except NotFound:
            flag = True
            break
    assert flag, f"TEST FAILED: Task still exist"


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_model_compute_configs.py
================================================
from behave import given, when, then
import random
import string


def random_5_chars():
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a'


@given(u'a dpk with models and compute configs')
def step_impl(context):
    dpk_json = {
        "displayName": "custom nodes test",
        "name": "compute-configs-{}".format(random_5_chars()),
        "scope": "project",
        "version": "1.0.18",
        "codebase": {
            "type": "git",
            "gitUrl": 'fddfvd',
            "gitTag": "master"
        },
        "attributes": {
            "Provider": "Dataloop",
            "Category": "Pipeline",
            "Pipeline Type": "Node",
            "Node Type": "Other"
        },
        "components": {
            "modules": [
                {
                    "name": "model-adapter-module-config",
                    "entryPoint": "main.py",
                    "className": "ModelAdapter",
                    "computeConfig": "module",
                    "initInputs": [
                        {
                            "type": "Model",
                            "name": "model_entity"
                        }
                    ],
                    "functions": [
                        {
                            "name": "evaluate_model",
                            "input": [
                                {
                                    "type": "Model",
                                    "name": "model"
                                },
                                {
                                    "type": "Dataset",
                                    "name": "dataset"
                                },
                                {
                                    "type": "Json",
                                    "name": "filters"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Dataset",
                                    "name": "dataset"
                                },
                                {
                                    "type": "Model",
                                    "name": "model"
                                }
                            ],
                            "displayName": "Evaluate a Model",
                            "displayIcon": ""
                        },
                        {
                            "name": "predict_dataset",
                            "input": [
                                {
                                    "type": "Dataset",
                                    "name": "dataset"
                                },
                                {
                                    "type": "Json",
                                    "name": "filters"
                                }
                            ],
                            "output": [],
                            "displayName": "Predict Dataset with DQL",
                            "displayIcon": ""
                        },
                        {
                            "name": "predict_items",
                            "input": [
                                {
                                    "type": "Item[]",
                                    "name": "items"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Item[]",
                                    "name": "items"
                                },
                                {
                                    "type": "Annotation[]",
                                    "name": "annotations"
                                }
                            ],
                            "displayName": "Predict Items",
                            "displayIcon": ""
                        },
                        {
                            "name": "train_model",
                            "input": [
                                {
                                    "type": "Model",
                                    "name": "model"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Model",
                                    "name": "model"
                                }
                            ],
                            "displayName": "Train a Model",
                            "displayIcon": ""
                        }
                    ]
                },
                {
                    "name": "model-adapter-function-config",
                    "entryPoint": "main.py",
                    "className": "ModelAdapter",
                    "computeConfig": "module",
                    "initInputs": [
                        {
                            "type": "Model",
                            "name": "model_entity"
                        }
                    ],
                    "functions": [
                        {
                            "name": "evaluate_model",
                            "computeConfig": "function",
                            "input": [
                                {
                                    "type": "Model",
                                    "name": "model"
                                },
                                {
                                    "type": "Dataset",
                                    "name": "dataset"
                                },
                                {
                                    "type": "Json",
                                    "name": "filters"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Dataset",
                                    "name": "dataset"
                                },
                                {
                                    "type": "Model",
                                    "name": "model"
                                }
                            ],
                            "displayName": "Evaluate a Model",
                            "displayIcon": ""
                        },
                        {
                            "name": "predict_dataset",
                            "input": [
                                {
                                    "type": "Dataset",
                                    "name": "dataset"
                                },
                                {
                                    "type": "Json",
                                    "name": "filters"
                                }
                            ],
                            "output": [],
                            "displayName": "Predict Dataset with DQL",
                            "displayIcon": ""
                        },
                        {
                            "name": "predict_items",
                            "input": [
                                {
                                    "type": "Item[]",
                                    "name": "items"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Item[]",
                                    "name": "items"
                                },
                                {
                                    "type": "Annotation[]",
                                    "name": "annotations"
                                }
                            ],
                            "displayName": "Predict Items",
                            "displayIcon": ""
                        },
                        {
                            "name": "train_model",
                            "computeConfig": "function",
                            "input": [
                                {
                                    "type": "Model",
                                    "name": "model"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Model",
                                    "name": "model"
                                }
                            ],
                            "displayName": "Train a Model",
                            "displayIcon": ""
                        }
                    ]
                }
            ],
            "services": [
            ],
            "models": [
                {
                    "computeConfigs": {
                        "deploy": "deploy",
                        "train": "train",
                        "default": "default"
                    },
                    "name": "model-with-compute-configs",
                    "moduleName": "model-adapter-module-config",
                    "description": "test-model-dpk",
                    "scope": "project",
                    "datasetId": context.dataset.id,
                    "tags": [],
                    "status": "created",
                    "labels": ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
                    "configuration": {
                        "weights_filename": "model.pth",
                        "batch_size": 16,
                        "num_epochs": 10
                    },
                    "inputType": "image",
                    "outputType": "box",
                    "artifacts": [
                        {
                            "type": "link",
                            "url": "https://storage.googleapis.com/model-mgmt-snapshots/ResNet50/model.pth"
                        }
                    ],
                    "metadata": {
                        "system": {
                            "ml": {
                                "defaultConfiguration": {
                                    "weights_filename": "model.pth",
                                    "input_size": 256
                                },
                                "outputType": "box",
                                "inputType": "image"
                            },
                            "subsets": {
                                "train": {
                                    "filter": {
                                        "$and": [
                                            {
                                                "dir": "/train"
                                            }
                                        ]
                                    },
                                    "page": 0,
                                    "pageSize": 1000,
                                    "resource": "items"
                                },
                                "validation": {}
                            }
                        }
                    }
                },
                {
                    "name": "model-with-config-in-module",
                    "moduleName": "model-adapter-module-config",
                    "description": "test-model-dpk",
                    "scope": "project",
                    "datasetId": context.dataset.id,
                    "tags": [],
                    "status": "created",
                    "labels": ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
                    "configuration": {
                        "weights_filename": "model.pth",
                        "batch_size": 16,
                        "num_epochs": 10
                    },
                    "inputType": "image",
                    "outputType": "box",
                    "artifacts": [
                        {
                            "type": "link",
                            "url": "https://storage.googleapis.com/model-mgmt-snapshots/ResNet50/model.pth"
                        }
                    ],
                    "metadata": {
                        "system": {
                            "ml": {
                                "defaultConfiguration": {
                                    "weights_filename": "model.pth",
                                    "input_size": 256
                                },
                                "outputType": "box",
                                "inputType": "image"
                            },
                            "subsets": {
                                "train": {
                                    "filter": {
                                        "$and": [
                                            {
                                                "dir": "/train"
                                            }
                                        ]
                                    },
                                    "page": 0,
                                    "pageSize": 1000,
                                    "resource": "items"
                                },
                                "validation": {}
                            }
                        }
                    }
                },
                {
                    "name": "model-with-config-in-function",
                    "moduleName": "model-adapter-function-config",
                    "description": "test-model-dpk",
                    "scope": "project",
                    "datasetId": context.dataset.id,
                    "tags": [],
                    "status": "created",
                    "labels": ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
                    "configuration": {},
                    "inputType": "image",
                    "outputType": "box",
                    "artifacts": [
                        {
                            "type": "link",
                            "url": "https://storage.googleapis.com/model-mgmt-snapshots/ResNet50/model.pth"
                        }
                    ],
                    "metadata": {
                        "system": {
                            "ml": {
                                "defaultConfiguration": {
                                    "weights_filename": "model.pth",
                                    "input_size": 256
                                },
                                "outputType": "box",
                                "inputType": "image"
                            },
                            "subsets": {
                                "train": {
                                    "filter": {
                                        "$and": [
                                            {
                                                "dir": "/train"
                                            }
                                        ]
                                    },
                                    "page": 0,
                                    "pageSize": 1000,
                                    "resource": "items"
                                },
                                "validation": {}
                            }
                        }
                    }
                }
            ],
            "computeConfigs": [
                {
                    "name": "deploy",
                    "runtime": {
                        "podType": "regular-xs",
                        "concurrency": 1,
                        "singleAgent": False,
                        "autoscaler": {
                            "type": "rabbitmq",
                            "minReplicas": 0,
                            "maxReplicas": 0,
                            "queueLength": 1000
                        },
                        "runnerImage": "test-compute-deploy"
                    }
                },
                {
                    "name": "train",
                    "runtime": {
                        "podType": "regular-xs",
                        "concurrency": 1,
                        "singleAgent": False,
                        "autoscaler": {
                            "type": "rabbitmq",
                            "minReplicas": 0,
                            "maxReplicas": 0,
                            "queueLength": 1000
                        },
                        "runnerImage": "test-compute-train"
                    }
                },
                {
                    "name": "default",
                    "runtime": {
                        "podType": "regular-xs",
                        "concurrency": 1,
                        "singleAgent": False,
                        "autoscaler": {
                            "type": "rabbitmq",
                            "minReplicas": 0,
                            "maxReplicas": 0,
                            "queueLength": 1000
                        },
                        "runnerImage": "test-compute-default"
                    }
                },
                {
                    "name": "module",
                    "runtime": {
                        "podType": "regular-xs",
                        "concurrency": 1,
                        "singleAgent": False,
                        "autoscaler": {
                            "type": "rabbitmq",
                            "minReplicas": 0,
                            "maxReplicas": 0,
                            "queueLength": 1000
                        },
                        "runnerImage": "test-compute-from-module"
                    }
                },
                {
                    "name": "function",
                    "runtime": {
                        "podType": "regular-xs",
                        "concurrency": 1,
                        "singleAgent": False,
                        "autoscaler": {
                            "type": "rabbitmq",
                            "minReplicas": 0,
                            "maxReplicas": 0,
                            "queueLength": 1000
                        },
                        "runnerImage": "test-compute-from-function"
                    }
                }
            ]
        }
    }
    dpk = context.dl.Dpk.from_json(_json=dpk_json)
    context.dpk = context.project.dpks.publish(dpk=dpk)
    if hasattr(context.feature, 'dpks'):
        context.feature.dpks.append(context.dpk)
    else:
        context.feature.dpks = [context.dpk]


@given(u'pipeline with train model')
def step_impl(context):
    pipeline_json = {
        "name": "test-pipeline-{}".format(random_5_chars()),
        "projectId": context.project.id,
        "nodes": [
            {
                "id": "6bdab0b3-07ed-4402-8984-961021368b1d",
                "inputs": [
                    {
                        "portId": "2ed2fb72-08ea-456f-880f-4b8208e6d174",
                        "nodeId": "2ed2fb72-08ea-456f-880f-4b8208e6d174",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "ef7a6583-a6ad-47dd-8e9e-9573014e3be7",
                        "nodeId": "ef7a6583-a6ad-47dd-8e9e-9573014e3be7",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 9905.91568182221,
                        "y": 9852.332327637567,
                        "z": 0
                    },
                    "repeatable": True
                },
                "name": "Train Model",
                "type": "ml",
                "namespace": {
                    "functionName": "train",
                    "projectName": "aaron-test-integrations",
                    "serviceName": "model-mgmt-app-train",
                    "moduleName": "model-mgmt-app-train",
                    "packageName": "model-mgmt-app"
                },
                "projectId": "0811a1e6-2023-4fbc-91f2-3ee263544924"
            }
        ],
        "connections": [
        ],
        "startNodes": [
            {
                "nodeId": "6bdab0b3-07ed-4402-8984-961021368b1d",
                "type": "root",
                "id": "b0d148dc-6c3c-4edb-bd40-7dd7b89b3d08"
            }
        ],
    }
    context.pipeline = context.project.pipelines.create(
        pipeline_json=pipeline_json,
        name='test-pipeline-{}'.format(random_5_chars()),
        project_id=context.project.id
    )
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@given(u'models are set in context')
def step_impl(context):
    models = context.project.models.list()
    for model in models.items:
        if model.name == 'model-with-compute-configs':
            context.model_with_compute_configs = model
        if model.name == 'model-with-config-in-module':
            context.model_with_config_in_module = model
        if model.name == 'model-with-config-in-function':
            context.model_with_config_in_function = model


@when(u'I execute pipeline on model with compute configs')
def step_impl(context):
    pipe_execution = context.pipeline.execute({"model": context.model_with_compute_configs.id})
    assert pipe_execution.status != 'failed', "TEST FAILED: Execution status is 'failed', Please check if pipeline cycle execution has transitionErrors"


@then(u'service should use model train compute config')
def step_impl(context):
    filters = context.dl.Filters(resource=context.dl.FiltersResource.SERVICE)
    filters.add(field='metadata.ml.modelId', values=context.model_with_compute_configs.id)
    s = context.project.services.list(filters=filters).items[0]
    assert s.runtime.runner_image == 'test-compute-train'


@when(u'I execute pipeline on model with config in module')
def step_impl(context):
    pipe_execution = context.pipeline.execute({"model": context.model_with_config_in_module.id})
    assert pipe_execution.status != 'failed', "TEST FAILED: Execution status is 'failed', Please check if pipeline cycle execution has transitionErrors"


@then(u'service should use model module compute config')
def step_impl(context):
    filters = context.dl.Filters(resource=context.dl.FiltersResource.SERVICE)
    filters.add(field='metadata.ml.modelId', values=context.model_with_config_in_module.id)
    s = context.project.services.list(filters=filters).items[0]
    assert s.runtime.runner_image == 'test-compute-from-module'


@when(u'I execute pipeline on model with config in function')
def step_impl(context):
    pipe_execution = context.pipeline.execute({"model": context.model_with_config_in_function.id})
    assert pipe_execution.status != 'failed', "TEST FAILED: Execution status is 'failed', Please check if pipeline cycle execution has transitionErrors"


@then(u'service should use model function compute config')
def step_impl(context):
    filters = context.dl.Filters(resource=context.dl.FiltersResource.SERVICE)
    filters.add(field='metadata.ml.modelId', values=context.model_with_config_in_function.id)
    s = context.project.services.list(filters=filters).items[0]
    assert s.runtime.runner_image == 'test-compute-from-function'


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_pulling_task.py
================================================
import time
import dtlpy as dl
import behave


@behave.when(u'I create pipeline with pulling task with type "{task_type}" node and dataset node')
def step_impl(context, task_type):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']

    if params['priority'] == "LOW":
        params['priority'] = dl.TaskPriority.LOW
    elif params['priority'] == "MEDIUM":
        params['priority'] = dl.TaskPriority.MEDIUM
    elif params['priority'] == "HIGH":
        params['priority'] = dl.TaskPriority.HIGH
    else:
        "TEST FAILED: Please provide valid priority value : LOW / MEDIUM / HIGH"

    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)
    context.pipeline_name = 'pipeline-{}'.format(current_time)

    context.pipeline = context.project.pipelines.create(name=context.pipeline_name, project_id=context.project.id)

    context.task_name = 'My Task-completed' + current_time
    context.task_node = dl.TaskNode(
        name=context.task_name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        recipe_id=context.recipe.id,
        recipe_title=context.recipe.title,
        task_owner=context.dl.info()['user_email'],
        workload=[dl.WorkloadUnit(assignee_id=context.dl.info()['user_email'], load=100)],
        position=(1, 1),
        task_type=task_type,
        batch_size=int(params['batch_size']),
        max_batch_workload=int(params['max_batch_workload']),
        priority=params['priority']
    )

    dataset_node = dl.DatasetNode(
        name=context.dataset.name,
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(2, 2)
    )

    filters = context.dl.Filters()
    filters.add(field='datasetId', values=context.dataset.id)
    context.pipeline.nodes.add(context.task_node).connect(node=dataset_node, source_port=context.task_node.outputs[0])
    context.task_node.add_trigger(filters=filters)

    try:
        context.pipeline.update()
        pipeline = context.project.pipelines.get(pipeline_name=context.pipeline_name)
        pipeline.install()
    except Exception as e:
        context.error = e

    time.sleep(5)


@behave.when(u'I get task by pipeline task node')
@behave.then(u'I get task by pipeline task node')
def step_impl(context):
    try:
        pipeline_task_name = "{} ({})".format(context.task_name, context.pipeline.name)
        context.task = context.project.tasks.get(task_name=pipeline_task_name)
    except Exception as e:
        assert False, "Failed to get task with the name: {}\n{}".format(pipeline_task_name, e)


@behave.then(u'I validate pulling task created equal to pipeline task node')
def step_impl(context):
    assert context.task_node.task_type == context.task.spec['type'], "TEST FAILED: task_type Expected - {}, Got - {}".format(context.task_node.task_type, context.task.spec['type'])

    assert context.task_node.batch_size == context.task.metadata['system']['batchSize'], "TEST FAILED: batch_size Expected - {}, Got - {}".format(context.task_node.batch_size,
                                                                                                                                                  context.task.metadata['system']['batchSize'])

    assert context.task_node.max_batch_workload == context.task.metadata['system']['maxBatchWorkload'], "TEST FAILED: max_batch_workload Expected - {}, Got - {}".format(
        context.task_node.max_batch_workload, context.task.metadata['system']['maxBatchWorkload'])

    assert context.task_node.priority == context.task.priority, "TEST FAILED: priority Expected - {}, Got - {}".format(context.task_node.priority, context.task.priority)


@behave.then(u'I expect pipeline error to be "{pipeline_error}"')
def step_impl(context, pipeline_error):

    filters = context.dl.Filters(resource=context.dl.FiltersResource.COMPOSITION)
    filters.add(field='id', values=[context.pipeline.composition_id], operator=context.dl.FILTERS_OPERATIONS_IN)
    composition_response = context.dl.pipelines._list(filters=filters)
    assert composition_response['totalItemsCount'] == 1, "TEST FAILED: Composition expected : {} , Got : {}".format(1, composition_response['totalItemsCount'])
    assert pipeline_error in composition_response['items'][0]['errorText']['message'], "TEST FAILED: Wrong error message.\n{}".format(composition_response['items'][0]['errorText']['message'])


    # ToDo : After DAT-31746 fixed - need to update test to use this assertion
    # assert context.install_response['status'] == 'Failure', "TEST FAILED: Status expected: Failure, Got: {}".format(context.install_response['status'])
    # assert pipeline_error in context.install_response['errorText']['message'], "TEST FAILED: Wrong error message.\n{}".format(context.install_response['errorText']['message'])
    # assert type in context.install_response['errorText']['type'], "TEST FAILED: Node type expected: {}, Got: {}".format(type, context.install_response['errorText']['type'])


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_refs.py
================================================
from behave import given, when, then
import random
import string
import time


def random_5_chars():
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a'


@given(u'a service')
def step_impl(context):
    def run(item):
        print('hello world')

    context.service = context.project.services.deploy(
        service_name='test-service-{}'.format(random_5_chars()),
        func=run,
    )
    context.package = context.service.package


@given(u'model is trained')
def step_impl(context):
    context.model.status = 'trained'
    context.model = context.model.update()
    assert context.model.status == 'trained'


@given(u'a dpk with custom node')
def step_impl(context):
    dpk_json = {
        "name": "test-dpk-with-custom-node-{}".format(
            ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))),
        "version": "0.0.21",
        "displayName": "Test",
        "codebase": {
            "type": "git",
            "gitUrl": "https://github.com/dataloop-ai-apps/data-split.git",
            "gitTag": "v0.0.21"
        },
        "scope": "project",
        "latest": True,
        "components": {
            "pipelineNodes": [
                {
                    "panel": "dataSplit",
                    "name": "dataSplit",
                    "invoke": {
                        "type": "function",
                        "namespace": "data-split.data_split.data_split"
                    },
                    "categories": [
                        "data"
                    ],
                    "scope": "project"
                }
            ],
            "modules": [
                {
                    "name": "data_split",
                    "entryPoint": "modules/data_split.py",
                    "className": "ServiceRunner",
                    "initInputs": [

                    ],
                    "functions": [
                        {
                            "name": "data_split",
                            "description": "The 'Data Split' node is a data processing tool that allows you to dynamically split your data into multiple groups at runtime. Whether you need to sample items for QA tasks or divide the items into multiple datasets, the Data Split node makes it easy.\n \n Simply define your groups, set their distribution, and tag each item with its assigned group using a metadata field. Use the Data Split node at any point in your pipeline to customize your data processing.",
                            "input": [
                                {
                                    "type": "Item",
                                    "name": "item"
                                }
                            ],
                            "output": [
                                {
                                    "type": "Item",
                                    "name": "item"
                                }
                            ],
                            "displayIcon": "qa-sampling",
                            "displayName": "Data Split"
                        }
                    ]
                }
            ]
        }
    }
    dpk = context.dl.Dpk.from_json(_json=dpk_json)
    context.dpk = context.project.dpks.publish(dpk=dpk)


@given(u'an app')
def step_impl(context):
    context.app = context.project.apps.install(
        dpk=context.dpk,
        app_name='test-app-{}'.format(random_5_chars()),
    )


@given(u'pipeline with model, service, code and custom nodes')
def step_impl(context):
    pipeline_json = {
        "name": "test-pipeline-{}".format(random_5_chars()),
        "projectId": context.project.id,
        "nodes": [
            {
                "id": "5e01d6b5-34be-499c-b0d8-e4facddb02f9",
                "inputs": [
                    {
                        "portId": "fda6a964-1458-4214-bc8f-1bb2a844a745",
                        "nodeId": "fda6a964-1458-4214-bc8f-1bb2a844a745",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "59958af9-7592-4c86-a785-fd6bc5da6720",
                        "nodeId": "59958af9-7592-4c86-a785-fd6bc5da6720",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 65,
                        "y": 115,
                        "z": 0
                    },
                    "repeatable": True
                },
                "name": "FaaS Node",
                "type": "function",
                "namespace": {
                    "functionName": context.package.modules[0].functions[0].name,
                    "projectName": context.project.name,
                    "serviceName": context.service.name,
                    "moduleName": context.package.modules[0].name,
                    "packageName": context.package.name
                },
                "projectId": context.project.id,
            },
            {
                "id": "6062556d-b52b-4d2c-80cc-4cde0ad8c658",
                "inputs": [
                    {
                        "portId": "d9b23659-b30d-4106-9994-2fe0e86c0d1d",
                        "nodeId": "d9b23659-b30d-4106-9994-2fe0e86c0d1d",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "3cdb543b-0dec-4c9f-b856-296a78b2ae75",
                        "nodeId": "3cdb543b-0dec-4c9f-b856-296a78b2ae75",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 366,
                        "y": 179,
                        "z": 0
                    },
                    "codeApplicationName": "run",
                    "repeatable": True
                },
                "name": "code",
                "type": "code",
                "namespace": {
                    "functionName": "run",
                    "projectName": context.project.name,
                    "serviceName": "run",
                    "packageName": "run"
                },
                "projectId": context.project.id,
                "config": {
                    "package": {
                        "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self, item):\n        return item\n",
                        "name": "run",
                        "type": "code",
                        "codebase": {
                            "type": "item"
                        }
                    }
                }
            },
            {
                "id": "43c05110-1741-4526-b78d-5bc548704825",
                "inputs": [
                    {
                        "portId": "f1f4c910-cd19-40a6-ba6c-1b91478175bc",
                        "nodeId": "f1f4c910-cd19-40a6-ba6c-1b91478175bc",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "1f7cd844-3785-4e0b-b533-7d4ce15c1642",
                        "nodeId": "1f7cd844-3785-4e0b-b533-7d4ce15c1642",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 674,
                        "y": 246,
                        "z": 0
                    },
                    "componentGroupName": "data",
                    "repeatable": True,
                    "pipelineNodeName": context.dpk.components.pipeline_nodes[0]['name'],
                },
                "name": "Data Split",
                "type": "custom",
                "namespace": {
                    "functionName": context.dpk.components.modules[0].functions[0].name,
                    "projectName": context.project.name,
                    "serviceName": "data-split",
                    "moduleName": context.dpk.components.modules[0].name,
                    "packageName": context.dpk.name
                },
                "projectId": context.project.id,
                "appName": context.app.name,
                "dpkName": context.dpk.name,
            },
            {
                "id": "c4d1230b-d5e1-4bdf-9b2a-0f98aaaa28d3",
                "inputs": [
                    {
                        "portId": "4cf7e27d-c4a4-4bde-b734-6e00b46cd63d",
                        "nodeId": "4cf7e27d-c4a4-4bde-b734-6e00b46cd63d",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "849e50a4-e771-4969-86fd-394847302e23",
                        "nodeId": "849e50a4-e771-4969-86fd-394847302e23",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    },
                    {
                        "portId": "50e3f7f6-c22f-43ad-aed0-ff283fee1636",
                        "nodeId": "50e3f7f6-c22f-43ad-aed0-ff283fee1636",
                        "type": "Annotation[]",
                        "name": "annotations",
                        "displayName": "annotations",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 968,
                        "y": 304,
                        "z": 0
                    },
                    "modelName": context.model.name,
                    "modelId": context.model.id,
                    "repeatable": True
                },
                "name": context.model.name,
                "type": "ml",
                "namespace": {
                    "functionName": "predict",
                    "projectName": context.project.name,
                    "serviceName": "m",
                    "moduleName": context.model.module_name,
                    "packageName": context.model.package.name
                },
                "projectId": context.project.id,
            }
        ],
        "connections": [
            {
                "src": {
                    "nodeId": "5e01d6b5-34be-499c-b0d8-e4facddb02f9",
                    "portId": "59958af9-7592-4c86-a785-fd6bc5da6720"
                },
                "tgt": {
                    "nodeId": "6062556d-b52b-4d2c-80cc-4cde0ad8c658",
                    "portId": "d9b23659-b30d-4106-9994-2fe0e86c0d1d"
                },
                "condition": "{}"
            },
            {
                "src": {
                    "nodeId": "6062556d-b52b-4d2c-80cc-4cde0ad8c658",
                    "portId": "3cdb543b-0dec-4c9f-b856-296a78b2ae75"
                },
                "tgt": {
                    "nodeId": "43c05110-1741-4526-b78d-5bc548704825",
                    "portId": "f1f4c910-cd19-40a6-ba6c-1b91478175bc"
                },
                "condition": "{}"
            },
            {
                "src": {
                    "nodeId": "43c05110-1741-4526-b78d-5bc548704825",
                    "portId": "1f7cd844-3785-4e0b-b533-7d4ce15c1642"
                },
                "tgt": {
                    "nodeId": "c4d1230b-d5e1-4bdf-9b2a-0f98aaaa28d3",
                    "portId": "4cf7e27d-c4a4-4bde-b734-6e00b46cd63d"
                },
                "condition": "{}"
            }
        ],
        "startNodes": [
            {
                "nodeId": "5e01d6b5-34be-499c-b0d8-e4facddb02f9",
                "type": "root",
                "id": "2e49364d-637e-4d17-9ccc-0498a6c1cf4f"
            }
        ],
    }
    context.pipeline = context.project.pipelines.create(
        pipeline_json=pipeline_json,
        name='test-pipeline-{}'.format(random_5_chars()),
        project_id=context.project.id
    )


@when(u'I install pipeline')
def step_impl(context):
    context.pipeline = context.pipeline.install()
    assert context.pipeline.status == 'Installed'


@then(u'service should have pipeline refs and cannot be uninstall')
def step_impl(context):
    context.service = context.project.services.get(service_id=context.service.id)
    refs = context.service.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 1
    assert pipeline_ref[0]['metadata']['relation'] == 'usedBy'
    try:
        context.service.delete()
    except context.dl.exceptions.FailedDependency as e:
        assert e.status_code == '424', f"TEST FAILED: Expected: '{424}' Actual: {e.status_code}"
        assert f"Unable to delete service '{context.service.name}'" in e.message, f"TEST FAILED: Expected: Unable to delete service '{context.service.name}'\nActual: {e.message}"


@then(u'model should have pipeline refs and cannot be deleted')
def step_impl(context):
    context.model = context.project.models.get(model_id=context.model.id)
    refs = context.model.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 1
    assert pipeline_ref[0]['metadata']['relation'] == 'usedBy'
    try:
        context.model.delete()
    except context.dl.exceptions.FailedDependency as e:
        assert e.status_code == '424', f"TEST FAILED: Expected: '{424}' Actual: {e.status_code}"
        assert f"Unable to delete model '{context.model.name}'" in e.message, f"TEST FAILED: Expected: Unable to delete model '{context.model.name}'\nActual: {e.message}"


@then(u'code node package should have pipeline refs')
def step_impl(context):
    code_node: context.dl.CodeNode = [node for node in context.pipeline.nodes if node.node_type == 'code'][0]
    package_name = code_node.namespace.package_name
    package = context.project.packages.get(package_name=package_name)
    refs = package.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 1
    assert pipeline_ref[0]['metadata']['relation'] == 'createdBy'


@then(u'code node service should have pipeline refs and cannot be uninstall')
def step_impl(context):
    code_node: context.dl.CodeNode = [node for node in context.pipeline.nodes if node.node_type == 'code'][0]
    service_name = code_node.namespace.service_name
    service = context.project.services.get(service_name=service_name)
    refs = service.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 2
    assert pipeline_ref[0]['metadata']['relation'] == 'createdBy' or pipeline_ref[1]['metadata'][
        'relation'] == 'createdBy'
    assert pipeline_ref[0]['metadata']['relation'] == 'usedBy' or pipeline_ref[1]['metadata']['relation'] == 'usedBy'
    try:
        context.service.delete()
    except context.dl.exceptions.FailedDependency as e:
        assert e.status_code == '424', f"TEST FAILED: Expected: '{424}' Actual: {e.status_code}"
        assert f"Unable to delete service '{context.service.name}'" in e.message, f"TEST FAILED: Expected: Unable to delete service '{context.service.name}'\nActual: {e.message}"


@then(u'app should have pipeline refs and cannot be uninstall')
def step_impl(context):
    app = context.project.apps.get(app_name=context.app.name)
    refs = app.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 1
    assert pipeline_ref[0]['metadata']['relation'] == 'usedBy'
    try:
        app.uninstall()
    except context.dl.exceptions.FailedDependency as e:
        assert e.status_code == '424', f"TEST FAILED: Expected: '{424}' Actual: {e.status_code}"
        assert f"Unable to delete app '{app.name}'" in e.message, f"TEST FAILED: Expected: Unable to delete app '{app.name}'\nActual: {e.message}"


@then(u'dpk should have pipeline refs and cannot be deleted')
def step_impl(context):
    dpk = context.project.dpks.get(dpk_name=context.dpk.name)
    refs = dpk.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 1
    assert pipeline_ref[0]['metadata']['relation'] == 'usedBy'
    try:
        dpk.delete()
    except context.dl.exceptions.FailedDependency as e:
        assert e.status_code == '424', f"TEST FAILED: Expected: '{424}' Actual: {e.status_code}"
        assert f"Unable to delete dpk '{dpk.display_name}'" in e.message, f"TEST FAILED: Expected: Unable to delete dpk '{dpk.display_name}'\nActual: {e.message}"


@when(u'I delete pipeline')
def step_impl(context):
    context.pipeline = context.pipeline.delete()


@then(u'service should not have pipeline refs and uninstall service "{flag}"')
def step_impl(context, flag):
    context.service = context.project.services.get(service_id=context.service.id)
    refs = context.service.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 0
    if flag == 'True':
        try:
            context.service.delete()
        except Exception as e:
            assert False, f"TEST FAILED: {e}"


@then(u'model should not have pipeline refs and delete model "{flag}"')
def step_impl(context, flag):
    context.model = context.project.models.get(model_id=context.model.id)
    refs = context.model.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 0
    if flag == 'True':
        try:
            context.model.delete()
            context.model_count -= 1
        except Exception as e:
            assert False, f"TEST FAILED: {e}"


@then(u'app should not have pipeline refs and uninstall app "{flag}"')
def step_impl(context, flag):
    app = context.project.apps.get(app_name=context.app.name)
    refs = app.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 0
    if flag == 'True':
        try:
            app.uninstall()
        except Exception as e:
            assert False, f"TEST FAILED: {e}"


@then(u'dpk should not have pipeline refs and delete dpk "{flag}"')
def step_impl(context, flag):
    dpk = context.project.dpks.get(dpk_name=context.dpk.name)
    refs = dpk.metadata.get('system', {}).get('refs', [])
    pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
    assert len(pipeline_ref) == 0
    if flag == 'True':
        try:
            dpk.delete()
        except Exception as e:
            assert False, f"TEST FAILED: {e}"


@then(u'I Should be able to uninstall service')
def step_impl(context):
    context.service = context.project.services.get(service_id=context.service.id)
    try:
        context.service.delete()
    except Exception as e:
        assert False, f"TEST FAILED: {e}"


@then(u'I Should be able to delete model')
def step_impl(context):
    context.model = context.project.models.get(model_id=context.model.id)
    try:
        context.model.delete()
        context.model_count -= 1
    except Exception as e:
        assert False, f"TEST FAILED: {e}"


@then(u'I Should be able to uninstall app')
def step_impl(context):
    app = context.project.apps.get(app_name=context.app.name)
    try:
        app.uninstall()
    except Exception as e:
        assert False, f"TEST FAILED: {e}"


@then(u'I Should be able to delete dpk')
def step_impl(context):
    dpk = context.project.dpks.get(dpk_name=context.dpk.name)
    try:
        dpk.delete()
    except Exception as e:
        assert False, f"TEST FAILED: {e}"


@when(u'I create a pipeline with gen ai model')
def step_impl(context):
    pipeline_json = {
        "name": "genmodel-pipeline-{}".format(random_5_chars()),
        "projectId": context.project.id,
        "nodes": [
            {
                "id": "aad87d60-fe52-45c9-b4a4-b1b0dcfb9212",
                "inputs": [
                    {
                        "portId": "7a1204fe-f49a-4e83-b5ce-3c78d04dbaff",
                        "nodeId": "7a1204fe-f49a-4e83-b5ce-3c78d04dbaff",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "091b20c7-8aed-4374-bdca-f3e6bbebc04b",
                        "nodeId": "091b20c7-8aed-4374-bdca-f3e6bbebc04b",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    },
                    {
                        "portId": "95876159-5dcd-4135-a1e5-5febe3aca526",
                        "nodeId": "95876159-5dcd-4135-a1e5-5febe3aca526",
                        "type": "Annotation[]",
                        "name": "annotations",
                        "displayName": "annotations",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10173.91015625,
                        "y": 9740,
                        "z": 0
                    },
                    "componentGroupName": "models",
                    "repeatable": True,
                    "mlType": "genAi",
                    "modelId": context.model.id,
                },
                "name": "Generate",
                "type": "ml",
                "namespace": {
                    "functionName": "predict",
                    "projectName": "mohamed",
                    "serviceName": "model-mgmt-app-predict",
                    "moduleName": "model-mgmt-app-predict",
                    "packageName": "model-mgmt-app"
                },
                "projectId": context.project.id,
            }
        ],
        "connections": [],
        "startNodes": [
            {
                "nodeId": "aad87d60-fe52-45c9-b4a4-b1b0dcfb9212",
                "type": "root",
                "id": "36562b38-895f-4bd5-aeb8-60f2f0b1f826"
            }
        ]
    }

    context.pipeline = context.project.pipelines.create(
        name=pipeline_json['name'],
        pipeline_json=pipeline_json,
        project_id=context.project.id
    )
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@given(u'I have a pipeline with train node and evaluate node')
def step_impl(context):
    models = context.project.models.list().items
    context.model_for_train = [m for m in models if m.status == context.dl.ModelStatus.CREATED][0]
    context.model_for_evaluate = \
        [m for m in models if m.status in [context.dl.ModelStatus.TRAINED, context.dl.ModelStatus.PRE_TRAINED]][0]
    pipeline_name = 'pipeline-{}'.format(random_5_chars())
    pipeline_json = {
        "name": pipeline_name,
        "projectId": context.project.id,
        "nodes": [
            {
                "id": "0d1575d6-c222-4916-9743-2f8e3e9d543c",
                "inputs": [
                    {
                        "portId": "794cbcc4-d0bd-4c58-a3be-ce0348e3bc25",
                        "nodeId": "794cbcc4-d0bd-4c58-a3be-ce0348e3bc25",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "a14297d7-75ee-42ff-ba79-26e85e313f3d",
                        "nodeId": "a14297d7-75ee-42ff-ba79-26e85e313f3d",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10172.3515625,
                        "y": 9719,
                        "z": 0
                    },
                    "repeatable": True
                },
                "name": "Train Model",
                "type": "ml",
                "namespace": {
                    "functionName": "train",
                    "projectName": "Test-Project1",
                    "serviceName": "model-mgmt-app-train",
                    "moduleName": "model-mgmt-app-train",
                    "packageName": "model-mgmt-app"
                },
                "projectId": context.project.id
            },
            {
                "id": "c78f7330-5894-460d-802f-28412f621aed",
                "inputs": [
                    {
                        "portId": "d2a0a760-fde2-4c7a-91c7-27d098bbc207",
                        "nodeId": "d2a0a760-fde2-4c7a-91c7-27d098bbc207",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "input",
                        "defaultValue": context.model_for_evaluate.id
                    },
                    {
                        "portId": "fc462fe9-95ac-4ed4-9428-5835c344984c",
                        "nodeId": "fc462fe9-95ac-4ed4-9428-5835c344984c",
                        "type": "Dataset",
                        "name": "dataset",
                        "displayName": "dataset",
                        "io": "input"
                    },
                    {
                        "portId": "8f3d8f25-3037-468a-918c-49be14287182",
                        "nodeId": "8f3d8f25-3037-468a-918c-49be14287182",
                        "type": "Json",
                        "name": "filters",
                        "displayName": "filters",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "c47079e9-220e-4f7f-b91e-9818dc7c7805",
                        "nodeId": "c47079e9-220e-4f7f-b91e-9818dc7c7805",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "output"
                    },
                    {
                        "portId": "502caf71-9c93-4214-92ae-8f34e69dc88f",
                        "nodeId": "502caf71-9c93-4214-92ae-8f34e69dc88f",
                        "type": "Dataset",
                        "name": "dataset",
                        "displayName": "dataset",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10172.3515625,
                        "y": 9896,
                        "z": 0
                    }
                },
                "name": "Evaluate Model",
                "type": "ml",
                "namespace": {
                    "functionName": "evaluate",
                    "projectName": "Test-Project1",
                    "serviceName": "model-mgmt-app-evaluate",
                    "moduleName": "model-mgmt-app-evaluate",
                    "packageName": "model-mgmt-app"
                },
                "projectId": context.project.id
            },
            {
                "id": "d4fe0be0-243a-49ab-bfbf-678248234729",
                "inputs": [

                ],
                "outputs": [
                    {
                        "portId": "3134969b-12c8-4e1c-b7e4-42afd61114cc",
                        "nodeId": "3134969b-12c8-4e1c-b7e4-42afd61114cc",
                        "type": "Model",
                        "name": "model",
                        "displayName": "model",
                        "io": "output"
                    },
                    {
                        "portId": "8cd0d65d-9398-4ce3-8189-3e9f46d69e03",
                        "nodeId": "8cd0d65d-9398-4ce3-8189-3e9f46d69e03",
                        "type": "Dataset",
                        "name": "dataset",
                        "displayName": "dataset",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 9825.3515625,
                        "y": 9802,
                        "z": 0
                    },
                    "codeApplicationName": "first-{}".format(random_5_chars())
                },
                "name": "code",
                "type": "code",
                "namespace": {
                    "functionName": "run",
                    "projectName": "Test-Project1",
                    "serviceName": "first",
                    "moduleName": "code_module",
                    "packageName": "first"
                },
                "projectId": context.project.id,
                "config": {
                    "package": {
                        "code": "import dtlpy as dl\n\n\nclass ServiceRunner:\n\n    def run(self, context: dl.Context):\n        dataset = context.project.datasets.list()[0]\n        model = [m for m in context.project.models.list().items if m.status == 'created'][0]\n        return [model, dataset]",
                        "name": "run",
                        "type": "code",
                        "codebase": {
                            "type": "item"
                        }
                    }
                }
            }
        ],
        "connections": [
            {
                "src": {
                    "nodeId": "d4fe0be0-243a-49ab-bfbf-678248234729",
                    "portId": "3134969b-12c8-4e1c-b7e4-42afd61114cc"
                },
                "tgt": {
                    "nodeId": "0d1575d6-c222-4916-9743-2f8e3e9d543c",
                    "portId": "794cbcc4-d0bd-4c58-a3be-ce0348e3bc25"
                },
                "condition": "{}"
            },
            {
                "src": {
                    "nodeId": "d4fe0be0-243a-49ab-bfbf-678248234729",
                    "portId": "8cd0d65d-9398-4ce3-8189-3e9f46d69e03"
                },
                "tgt": {
                    "nodeId": "c78f7330-5894-460d-802f-28412f621aed",
                    "portId": "fc462fe9-95ac-4ed4-9428-5835c344984c"
                },
                "condition": "{}"
            }
        ],
        "startNodes": [
            {
                "nodeId": "d4fe0be0-243a-49ab-bfbf-678248234729",
                "type": "root",
                "id": "68bfab61-7677-4854-a6a6-cb40228a939a"
            }
        ]
    }
    context.pipeline = context.project.pipelines.create(
        name=pipeline_name,
        pipeline_json=pipeline_json,
        project_id=context.project.id
    )
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@given(u'I pause pipeline when executions are created')
def step_impl(context):
    interval = 5
    timeout_seconds = 60 * 5
    start = time.time()
    while time.time() - start < timeout_seconds:
        filters = context.dl.Filters(resource=context.dl.FiltersResource.EXECUTION)
        filters.add(field='pipeline.id', values=context.pipeline.id)
        executions = context.project.executions.list(filters=filters)
        if len(executions.items) == 3:
            context.pipeline.pause()
            context.pipeline = context.project.pipelines.get(pipeline_id=context.pipeline.id)
            break
        time.sleep(interval)
    assert context.pipeline.status == context.dl.CompositionStatus.UNINSTALLED


@then(u'model services should still have refs')
def step_impl(context):
    model_services = [s for s in context.project.services.list().items if 'ml' in s.metadata]
    for model_service in model_services:
        refs = model_service.metadata.get('system', {}).get('refs', [])
        pipeline_ref = [ref for ref in refs if ref['id'] == context.pipeline.id]
        assert pipeline_ref.__len__() == 2, 'model service {} should have 2 refs but has {}'.format(
            model_service.name, pipeline_ref.__len__())
        used_by_ref = [ref for ref in pipeline_ref if ref['metadata']['relation'] == 'usedBy']
        assert used_by_ref.__len__() == 1, 'model service {} should have 1 usedBy ref but has {}'.format(
            model_service.name, used_by_ref.__len__())
        created_by_ref = [ref for ref in pipeline_ref if ref['metadata']['relation'] == 'createdBy']
        assert created_by_ref.__len__() == 1, 'model service {} should have 1 createdBy ref but has {}'.format(
            model_service.name, created_by_ref.__len__())


================================================
File: tests/features/steps/pipeline_entity/test_pipeline_task_node.py
================================================
from behave import given, when, then
import random
import string
import time


def random_5_chars():
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a'


@given(u'a pipeline with task node that receives many to one input')
def step_impl(context):
    pipe_json = {
        "name": "Test Task Many to One - {}".format(random_5_chars()),
        "projectId": context.project.id,
        "nodes": [
            {
                "id": "e6a4e030-3edf-4ec2-a348-cf048970fd55",
                "inputs": [
                    {
                        "portId": "47ce56b1-96e7-4d52-b6bf-4786f957575e",
                        "nodeId": "47ce56b1-96e7-4d52-b6bf-4786f957575e",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "f3f22654-cb08-4da6-aa8e-e9e71bc7db38",
                        "nodeId": "f3f22654-cb08-4da6-aa8e-e9e71bc7db38",
                        "type": "Item[]",
                        "name": "items",
                        "displayName": "items",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 9872.3515625,
                        "y": 9824,
                        "z": 0
                    },
                    "componentGroupName": "automation",
                    "codeApplicationName": "split",
                    "repeatable": True
                },
                "name": "split",
                "type": "code",
                "namespace": {
                    "functionName": "run",
                    "projectName": context.project.name,
                    "serviceName": "split",
                    "moduleName": "code_module",
                    "packageName": "split"
                },
                "projectId": context.project.id,
                "config": {
                    "package": {
                        "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self, item):\n        return item.dataset.items.list().items\n",
                        "name": "run",
                        "type": "code",
                        "codebase": {
                            "type": "item"
                        }
                    }
                }
            },
            {
                "id": "325ca877-578c-48e5-b94c-d3cc39853e5b",
                "inputs": [
                    {
                        "portId": "63de9478-ccd2-4840-808a-1ac550c43da3",
                        "nodeId": "63de9478-ccd2-4840-808a-1ac550c43da3",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "028d2311-1421-47ff-bcfe-e2e0dacb76d9",
                        "nodeId": "028d2311-1421-47ff-bcfe-e2e0dacb76d9",
                        "type": "Item",
                        "name": "item",
                        "displayName": "Complete",
                        "actions": [
                            "completed",
                            "discard"
                        ],
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10222.3515625,
                        "y": 9858,
                        "z": 0
                    },
                    "recipeTitle": context.dataset.name + " - Default recipe",
                    "recipeId": context.dataset.get_recipe_ids()[0],
                    "taskType": "annotation",
                    "datasetId": context.dataset.id,
                    "taskOwner": context.dataset.creator,
                    "workload": [
                        {
                            "assigneeId": context.dataset.creator,
                            "load": 100
                        }
                    ],
                    "maxBatchWorkload": 7,
                    "batchSize": 5,
                    "priority": 2,
                    "componentGroupName": "workflow",
                    "repeatable": True,
                    "dueDate": 0,
                    "groups": [

                    ]
                },
                "name": "Labeling",
                "type": "task",
                "namespace": {
                    "functionName": "move_to_task",
                    "projectName": "DataloopTasks",
                    "serviceName": "pipeline-utils",
                    "moduleName": "default_module",
                    "packageName": "pipeline-utils"
                }
            },
            {
                "id": "585fc85b-69a1-4dcd-9f63-d2b4f303fded",
                "inputs": [
                    {
                        "portId": "2058bd88-dc64-481f-9943-a1d809db2144",
                        "nodeId": "2058bd88-dc64-481f-9943-a1d809db2144",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "input"
                    }
                ],
                "outputs": [
                    {
                        "portId": "a60b1a10-9dc3-4442-a575-0a2007d68bd9",
                        "nodeId": "a60b1a10-9dc3-4442-a575-0a2007d68bd9",
                        "type": "Item",
                        "name": "item",
                        "displayName": "item",
                        "io": "output"
                    }
                ],
                "metadata": {
                    "position": {
                        "x": 10632.3515625,
                        "y": 9890,
                        "z": 0
                    },
                    "componentGroupName": "automation",
                    "codeApplicationName": "done",
                    "repeatable": True
                },
                "name": "done",
                "type": "code",
                "namespace": {
                    "functionName": "run",
                    "projectName": context.project.name,
                    "serviceName": "done",
                    "moduleName": "code_module",
                    "packageName": "done"
                },
                "projectId": context.project.id,
                "config": {
                    "package": {
                        "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self, item):\n        return item\n",
                        "name": "run",
                        "type": "code",
                        "codebase": {
                            "type": "item"
                        }
                    }
                }
            }
        ],
        "connections": [
            {
                "src": {
                    "nodeId": "325ca877-578c-48e5-b94c-d3cc39853e5b",
                    "portId": "028d2311-1421-47ff-bcfe-e2e0dacb76d9"
                },
                "tgt": {
                    "nodeId": "585fc85b-69a1-4dcd-9f63-d2b4f303fded",
                    "portId": "2058bd88-dc64-481f-9943-a1d809db2144"
                },
                "condition": "{}",
                "action": "completed"
            },
            {
                "src": {
                    "nodeId": "e6a4e030-3edf-4ec2-a348-cf048970fd55",
                    "portId": "f3f22654-cb08-4da6-aa8e-e9e71bc7db38"
                },
                "tgt": {
                    "nodeId": "325ca877-578c-48e5-b94c-d3cc39853e5b",
                    "portId": "63de9478-ccd2-4840-808a-1ac550c43da3"
                },
                "condition": "{}"
            }
        ],
        "startNodes": [
            {
                "nodeId": "e6a4e030-3edf-4ec2-a348-cf048970fd55",
                "type": "root",
                "id": "d6771d5a-b457-42f3-b881-944226676110"
            }
        ]
    }
    context.pipeline = context.project.pipelines.create(
        pipeline_json=pipe_json,
        name=pipe_json['name'],
        project_id=context.project.id
    )


@given(u'I execute the pipeline on item')
def step_impl(context):
    context.execution = context.pipeline.execute(
        execution_input={'item': context.item.id}
    )


@when(u'I set status on some of the input items')
def step_impl(context):
    composition_id = context.pipeline.composition_id
    context.composition = context.project.compositions.get(composition_id=composition_id)
    composition_task = context.composition['tasks'][0]
    context.task = context.project.tasks.get(task_id=composition_task['state']['taskId'])
    dataset_items = context.dataset.items.list().items
    task_items = context.task.get_items().items
    while len(task_items) < len(dataset_items):
        task_items = context.task.get_items().items
        time.sleep(1)

    for item in task_items[:2]:
        item.update_status(status=context.dl.ItemStatus.COMPLETED, task_id=context.task.id)
    cycle: context.dl.PipelineExecution = context.pipeline.pipeline_executions.list().items[0]
    last_node = cycle.nodes[-1]
    max_wait = 60 * 3
    start = time.time()
    while last_node.status == 'pending':
        if time.time() - start > max_wait:
            break
        cycle: context.dl.PipelineExecution = context.pipeline.pipeline_executions.list().items[0]
        last_node = cycle.nodes[-1]
        time.sleep(1)


@then(u'cycle should be inProgress and task node should be inProgress')
def step_impl(context):
    cycle: context.dl.PipelineExecution = context.pipeline.pipeline_executions.list().items[0]
    task_node = [n for n in cycle.nodes if n.node_type == 'task'][0]
    last_node = cycle.nodes[-1]
    assert cycle.status == 'in-progress'
    assert task_node.status == 'in-progress'
    assert last_node.status != 'pending'


@when(u'I set status on all input items')
def step_impl(context):
    dataset_items = context.dataset.items.list().items
    for item in dataset_items:
        exist_status = item.status(task_id=context.task.id)
        if exist_status != context.dl.ItemStatus.COMPLETED:
            item.update_status(status=context.dl.ItemStatus.COMPLETED, task_id=context.task.id)


@then(u'cycle should be completed and task node should be completed')
def step_impl(context):
    cycle_success = False
    task_node_success = False
    max_wait = 60 * 3
    start = time.time()
    while not cycle_success or not task_node_success:
        if time.time() - start > max_wait:
            break
        cycle: context.dl.PipelineExecution = context.pipeline.pipeline_executions.list().items[0]
        task_node = [n for n in cycle.nodes if n.node_type == 'task'][0]
        if cycle.status == 'success':
            cycle_success = True
        if task_node.status == 'success':
            task_node_success = True
        time.sleep(1)
    assert cycle_success, 'Cycle did not transition to success'
    assert task_node_success, 'Task node did not transition to success'



================================================
File: tests/features/steps/pipeline_resume/pipeline_resume.py
================================================
import copy
import random
import time

import behave
import dtlpy as dl

pipeline_json = {
    "nodes": [
        {
            "id": "021f90cf-65f8-4f61-a1a2-4775e3bff685",
            "inputs": [
                {
                    "portId": "dd83b5f7-ffac-446e-b65a-8201e0328086",
                    "type": "Item",
                    "name": "item",
                    "displayName": "item"
                }
            ],
            "outputs": [
                {
                    "portId": "572b01df-e336-4f91-b4e3-458d1e79f532",
                    "type": "Item",
                    "name": "item",
                    "displayName": "item"
                }
            ],
            "metadata": {
                "position": {
                    "x": 51.90174865722656,
                    "y": 91.95087432861328,
                    "z": 0
                },
                "repeatable": True
            },
            "name": "random_dataset_685QD",
            "type": "storage",
            "namespace": {
                "functionName": "dataset_handler",
                "serviceName": "pipeline-utils",
                "projectName": "DataloopTasks"
            },
            "projectId": "99d634a1-89bf-4d69-b201-807a29140c76",
        },
        {
            "id": "4a8bb1f9-7eb5-484a-8603-773d097f222b",
            "inputs": [
                {
                    "portId": "0a6b4481-efee-4cfa-b0cd-ffd42312a49f",
                    "type": "Item",
                    "name": "item",
                    "displayName": "item"
                }
            ],
            "outputs": [
                {
                    "portId": "c7bdac4e-0a3c-4bf0-9e45-311e23b3dd5a",
                    "type": "Task",
                    "name": "completed",
                    "displayName": "completed",
                    "color": "#14a182",
                    "actions": ["completed"]
                },
                {
                    "portId": "fc09ca5d-9a13-438b-993a-5518dc67b045",
                    "type": "Assignment",
                    "name": "completed",
                    "displayName": "done",
                    "color": "#69f4ef",
                    "actions": ["done"]
                },
                {
                    "portId": "ee8bbdf2-0e33-4688-af5c-800d88c583d0",
                    "type": "Item",
                    "name": "item",
                    "displayName": "completed",
                    "color": "#d004e4",
                    "actions": ["completed", "discard"]
                }
            ],
            "metadata": {
                "position": {
                    "x": 344,
                    "y": 87,
                    "z": 0
                },
                "recipeTitle": "Text Default Recipe",
                "recipeId": "628e3fa8b7bca61b1fa9cd3b",
                "taskType": "annotation",
                "datasetId": "628e3fa8e2a1bf337dee0c41",
                "taskOwner": "datalooptester123@gmail.com",
                "workload": [
                    {
                        "assigneeId": "matan305@bot.ai",
                        "load": 100
                    }
                ],
                "priority": 2,
                "repeatable": True,
                "dueDate": 1664744400000
            },
            "name": "First Task",
            "type": "task",
            "namespace": {
                "functionName": "move_to_task",
                "projectName": "DataloopTasks",
                "serviceName": "pipeline-utils",
                "moduleName": "default_module",
                "packageName": "pipeline-utils"
            },
            "projectId": "f8a4b8ce-5ff3-4386-84dc-1bda3a5bc92a"
        },
        {
            "id": "039c747d-6ec3-4fbb-8090-6134ecc78ae6",
            "inputs": [
                {
                    "portId": "0b357d4b-7e60-47a2-922e-7a22a4d7d6fc",
                    "type": "Item",
                    "name": "item",
                    "displayName": "item"
                }
            ],
            "outputs": [
                {
                    "portId": "a6a3115e-7678-4f6a-b39b-2dd9f54d554f",
                    "type": "Item",
                    "name": "item",
                    "displayName": "item"
                }
            ],
            "metadata": {
                "position": {
                    "x": 700,
                    "y": 89,
                    "z": 0
                },
                "repeatable": True
            },
            "name": "run",
            "type": "function",
            "namespace": {
                "functionName": "run",
                "projectName": "SoS",
                "serviceName": "custom-webm-converter-service",
                "moduleName": "webm_module",
                "packageName": "custom-webm-converter"
            },
            "projectId": "99d634a1-89bf-4d69-b201-807a29140c76"
        },
        {
            "id": "488d660b-9c81-4f12-ad38-7a57bcd3346c",
            "inputs": [
                {
                    "portId": "5a389b74-077f-4cde-a702-11775398bdb1",
                    "type": "Task",
                    "name": "task",
                    "displayName": "task"
                }
            ],
            "outputs": [],
            "metadata": {
                "position": {
                    "x": 698,
                    "y": 209,
                    "z": 0
                },
                "repeatable": True
            },
            "name": "Task Completed",
            "type": "code",
            "namespace": {
                "functionName": "run",
                "projectName": "SoS",
                "serviceName": "task-completed-hhos7jqgjf",
                "moduleName": "code_module",
                "packageName": "task-completed-7d6o2dygao"
            },
            "projectId": "99d634a1-89bf-4d69-b201-807a29140c76",
            "config": {
                "package": {
                    "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self, task):\n        return task\n",
                    "name": "run",
                    "type": "code"
                }
            }
        },
        {
            "id": "967edc32-1eb3-4cfb-b127-8c3be9005726",
            "inputs": [
                {
                    "portId": "82dd52bc-7bc4-4baa-8f2c-8654022625e5",
                    "type": "Assignment",
                    "name": "assignment",
                    "displayName": "assignment"
                }
            ],
            "outputs": [],
            "metadata": {
                "position": {
                    "x": 698,
                    "y": 323,
                    "z": 0
                },
                "repeatable": True
            },
            "name": "Assignment Completed",
            "type": "code",
            "namespace": {
                "functionName": "run",
                "projectName": "SoS",
                "serviceName": "assignment-completed-d7gd40q8xgf",
                "moduleName": "code_module",
                "packageName": "assignment-completed-te50uh5d78"
            },
            "projectId": "99d634a1-89bf-4d69-b201-807a29140c76",
            "config": {
                "package": {
                    "code": "import dtlpy as dl\n\nclass ServiceRunner:\n\n    def run(self, assignment):\n        return assignment\n",
                    "name": "run",
                    "type": "code"
                }
            }
        },
        {
            "id": "0d811e02-aeea-475e-9514-9b497e215cab",
            "inputs": [
                {
                    "portId": "f7032c2a-f568-4971-ab7b-36c89a5d6e00",
                    "type": "Item",
                    "name": "item",
                    "displayName": "item"
                }
            ],
            "outputs": [
                {
                    "portId": "05177336-3e3b-4b5d-be42-f9d08925f52a",
                    "type": "Item",
                    "name": "item",
                    "displayName": "Complete",
                    "actions": ["completed", "discard"]

                }
            ],
            "metadata": {
                "position": {
                    "x": 1017,
                    "y": 89,
                    "z": 0
                },
                "recipeTitle": "Analytics2.0 Default Recipe",
                "recipeId": "622096f30e37e67e34caeac9",
                "taskType": "annotation",
                "datasetId": "622096f38c7c3254e470e9a0",
                "taskOwner": "datalooptester123@gmail.com",
                "workload": [
                    {
                        "assigneeId": "luhupabo@ryteto.me",
                        "load": 100
                    }
                ],
                "priority": 2,
                "repeatable": True,
                "dueDate": 1664744400000
            },
            "name": "Second Task",
            "type": "task",
            "namespace": {
                "functionName": "move_to_task",
                "projectName": "DataloopTasks",
                "serviceName": "pipeline-utils",
                "moduleName": "default_module",
                "packageName": "pipeline-utils"
            },
            "projectId": "f8a4b8ce-5ff3-4386-84dc-1bda3a5bc92a"
        }
    ],
    "connections": [
        {
            "src": {
                "nodeId": "021f90cf-65f8-4f61-a1a2-4775e3bff685",
                "portId": "572b01df-e336-4f91-b4e3-458d1e79f532"
            },
            "tgt": {
                "nodeId": "4a8bb1f9-7eb5-484a-8603-773d097f222b",
                "portId": "0a6b4481-efee-4cfa-b0cd-ffd42312a49f"
            },
            "condition": "{}"
        },
        {
            "src": {
                "nodeId": "039c747d-6ec3-4fbb-8090-6134ecc78ae6",
                "portId": "a6a3115e-7678-4f6a-b39b-2dd9f54d554f"
            },
            "tgt": {
                "nodeId": "0d811e02-aeea-475e-9514-9b497e215cab",
                "portId": "f7032c2a-f568-4971-ab7b-36c89a5d6e00"
            },
            "condition": "{}"
        },
        {
            "src": {
                "nodeId": "4a8bb1f9-7eb5-484a-8603-773d097f222b",
                "portId": "ee8bbdf2-0e33-4688-af5c-800d88c583d0"
            },
            "tgt": {
                "nodeId": "039c747d-6ec3-4fbb-8090-6134ecc78ae6",
                "portId": "0b357d4b-7e60-47a2-922e-7a22a4d7d6fc"
            },
            "condition": "{}",
            "action": "completed"
        },
        {
            "src": {
                "nodeId": "4a8bb1f9-7eb5-484a-8603-773d097f222b",
                "portId": "c7bdac4e-0a3c-4bf0-9e45-311e23b3dd5a"
            },
            "tgt": {
                "nodeId": "488d660b-9c81-4f12-ad38-7a57bcd3346c",
                "portId": "5a389b74-077f-4cde-a702-11775398bdb1"
            },
            "condition": "{}",
            "action": "completed"
        },
        {
            "src": {
                "nodeId": "4a8bb1f9-7eb5-484a-8603-773d097f222b",
                "portId": "fc09ca5d-9a13-438b-993a-5518dc67b045"
            },
            "tgt": {
                "nodeId": "967edc32-1eb3-4cfb-b127-8c3be9005726",
                "portId": "82dd52bc-7bc4-4baa-8f2c-8654022625e5"
            },
            "condition": "{}",
            "action": "done"
        }
    ],
    "startNodes": [
        {
            "nodeId": "021f90cf-65f8-4f61-a1a2-4775e3bff685",
            "type": "root",
            "trigger": {
                "type": "Event",
                "spec": {
                    "actions": [
                        "Created"
                    ],
                    "resource": "Item",
                    "executionMode": "Once",
                    "filter": {}
                }
            },
            "id": "cc0d7b84-1339-429a-8fcf-c1ef28c8aae5"
        }
    ]
}


def generate_pipeline_json(project: dl.Project, dataloop_project_id: str, dataset: dl.Dataset, service: dl.Service):
    pipeline = copy.deepcopy(pipeline_json)
    recipe = dataset.recipes.list()[0]
    pipeline['name'] = 'test-pipe-resume-{}'.format(random.randrange(10000, 100000))

    for node in pipeline['nodes']:
        node['projectId'] = project.id

    datasets_node = [node for node in pipeline['nodes'] if node['type'] == 'storage']
    for node in datasets_node:
        node['name'] = dataset.name
        node['metadata']["datasetId"] = dataset.id

    task_nodes = [node for node in pipeline['nodes'] if node['type'] == 'task']
    for node in task_nodes:
        node['projectId'] = dataloop_project_id
        node['metadata']["recipeTitle"] = recipe.title
        node['metadata']["recipeId"] = recipe.id
        node['metadata']["datasetId"] = dataset.id
        node['metadata']["taskOwner"] = dataset.creator
        node['metadata']["workload"] = [
            {
                "assigneeId": dataset.creator,
                "load": 100
            }
        ]

    faas_node = [node for node in pipeline['nodes'] if node['type'] == 'function'][0]
    module = [m for m in service.package.modules if m.name == service.module_name][0]
    func = module.functions[0]
    faas_node['namespace'] = {
        "functionName": func.name,
        "projectName": project.name,
        "serviceName": service.name,
        "moduleName": service.module_name,
        "packageName": service.package.name
    }

    return pipeline


@behave.given(u'I have a resumable pipeline')
def step_impl(context):
    pipeline_utils_faas = dl.services.get(service_name='pipeline-utils')
    pipeline_payload = generate_pipeline_json(
        project=context.project,
        dataloop_project_id=pipeline_utils_faas.project_id,
        dataset=context.dataset,
        service=context.service,
    )
    context.pipeline = context.project.pipelines.create(pipeline_json=pipeline_payload, project_id=context.project.id)
    context.to_delete_pipelines_ids.append(context.pipeline.id)


@behave.given(u'Faas node service is paused')
@behave.when(u'I pause service in context')
def step_impl(context):
    context.service.pause()
    context.service = dl.services.get(service_id=context.service.id)


@behave.when(u'Faas node service is resumed')
def step_impl(context):
    context.service.resume()
    context.service = dl.services.get(service_id=context.service.id)


def get_filters(context, for_cycle=True):
    next_nodes = [
        n for n in context.pipeline.nodes if
        n.node_type == 'function' or n.name in [
            'Task Completed',
            'Assignment Completed'
        ]
    ]
    filters = dl.Filters(resource=dl.FiltersResource.EXECUTION)
    filters.add('pipeline.id', context.pipeline.id)
    if for_cycle:
        cycles_page = context.pipeline.pipeline_executions.list()
        cycle = cycles_page.items[0]
        assert cycles_page.items_count == 1
        filters.add('pipeline.executionId', cycle.id)
    filters.add(
        field='pipeline.nodeId',
        values=[n.node_id for n in next_nodes],
        operator=dl.FiltersOperations.IN
    )
    return filters


@behave.then(u'Next nodes should be executed')
def step_impl(context):
    interval = 3
    max_attempts = 15
    attempt = 0
    success = False
    filters = get_filters(context=context, for_cycle=False)

    while not success and attempt < max_attempts:
        executions_page = context.project.executions.list(filters=filters)
        success = executions_page.items_count == 3
        if success:
            break
        time.sleep(interval)
        attempt = attempt + 1

    assert success


@behave.then(u'Next nodes should not be executed')
def step_impl(context):
    filters = get_filters(context=context)
    executions_page = context.project.executions.list(filters=filters)
    assert executions_page.items_count == 0


@behave.given(u'Faas node execution is in queue')
def step_impl(context):
    executions = context.service.executions.list()
    assert executions.items_count == 1


@behave.when(u'Faas node service has completed')
def step_impl(context):
    executions = context.service.executions.list()
    assert executions.items_count == 1
    execution = executions.items[0]
    interval = 5
    max_attempts = 15
    attempt = 0
    while execution.latest_status['status'] != 'success' and attempt < max_attempts:
        time.sleep(interval)
        attempt = attempt + 1
        executions = context.service.executions.list()
        assert executions.items_count == 1
        execution = executions.items[0]

    assert execution.latest_status['status'] == 'success'


@behave.when(u'I complete item in task')
def step_impl(context):
    context.item.update_status(status=context.dl.ItemStatus.COMPLETED)
    interval = 1
    max_attempts = 10
    ready = False
    for _ in range(max_attempts):
        item = dl.items.get(item_id=context.item.id)
        task_refs = [ref for ref in item.system.get('refs') if ref['type'] == 'task']
        assert len(task_refs) == 1
        task_ref = task_refs[0]
        if task_ref.get('metadata', {}).get('status', None) == context.dl.ItemStatus.COMPLETED:
            ready = True
            break
        time.sleep(interval)

    assert ready
    time.sleep(5)  # allow extra time for events to get handled


@behave.given(u'Item reached task node')
def step_impl(context):
    interval = 3
    max_attempts = 15
    attempt = 0
    ready = False
    item = None
    while not ready and attempt < max_attempts:
        cycles_page = context.pipeline.pipeline_executions.list()
        if cycles_page.items_count == 1:
            cycle = cycles_page.items[0]
            filters = dl.Filters(resource=dl.FiltersResource.EXECUTION)
            filters.add('pipeline.id', context.pipeline.id)
            filters.add('pipeline.executionId', cycle.id)
            executions_page = context.project.executions.list(filters=filters)
            task_executions = [e for e in executions_page.items if e.function_name == 'move_to_task']
            if len(task_executions) == 1:
                task_execution = task_executions[0]
                if task_execution.latest_status['status'] == 'success':
                    item = dl.items.get(**task_execution.input['item'])
                    ready = len(item.system.get('refs', list())) == 2
                    break
        time.sleep(interval)
        attempt = attempt + 1
    assert ready
    context.item = item


@behave.then(u'Item proceeded to next node')
def step_impl(context):
    faas_node = [n for n in context.pipeline.nodes if n.node_type == 'function'][0]
    connection: dl.PipelineConnection = [
        con for con in context.pipeline.connections if con.source.node_id == faas_node.node_id
    ][0]
    next_node = [n for n in context.pipeline.nodes if n.node_type == 'task' and n.node_id == connection.target.node_id][0]
    interval = 5
    max_attempts = 15
    attempt = 0
    success = False
    while not success and attempt < max_attempts:
        filters = context.dl.Filters(
            resource=context.dl.FiltersResource.PIPELINE_EXECUTION,
            field='status',
            values='in-progress'
        )
        cycles_page = context.pipeline.pipeline_executions.list(filters=filters)
        if cycles_page.items_count == 1:
            cycle = cycles_page.items[0]
            filters = dl.Filters(resource=dl.FiltersResource.EXECUTION)
            filters.add('pipeline.id', context.pipeline.id)
            filters.add('pipeline.executionId', cycle.id)
            filters.add('pipeline.nodeId', next_node.node_id)
            executions_page = context.project.executions.list(filters=filters)
            success = executions_page.items_count == 1
            if success:
                break
        time.sleep(interval)
        attempt = attempt + 1
    assert success


@behave.when(u'I resume pipeline with resume option "{resume_option}"')
def step_impl(context, resume_option):
    context.pipeline.install(resume_option=resume_option)


@behave.when(u'I pause service')
def step_impl(context):
    context.service = context.project.services.list().items[0]
    assert context.service is not None, f"TEST FAILED: Missing service"
    context.service.pause()
    context.service = dl.services.get(service_id=context.service.id)


================================================
File: tests/features/steps/platform_urls/test_platform_urls.py
================================================
import behave
import requests


@behave.then(u'I validate the platform url "{platform_url}" works')
def step_impl(context, platform_url):
    urls_list = {
        "project.platform_url": context.dl.projects.get(project_id=context.project.id),
        "dataset.platform_url": context.dl.datasets.get(dataset_id=context.dataset.id),
        "project.dataset.platform_url": context.project.datasets.get(dataset_id=context.dataset.id),
        "item.platform_url": context.dl.items.get(item_id=context.item.id),
        "dataset.item.platform_url": context.dataset.items.get(item_id=context.item.id),
        "project.dataset.item.platform_url": context.project.datasets.get(dataset_id=context.dataset.id).items.get(item_id=context.item.id),
    }

    output_url = urls_list[platform_url].platform_url
    request_code = requests.get(url=output_url).status_code
    assert request_code == 200


================================================
File: tests/features/steps/project_entity/test_project_repo_methods.py
================================================
import behave
import os
import shutil
import filecmp
import random


@behave.when(u'I list project entity datasets')
def step_impl(context):
    context.dataset_list = context.project.list_datasets()


@behave.when(u'I get project entity dataset by the name of "{dataset_name}"')
def step_impl(context, dataset_name):
    context.dataset_get = context.project.get_dataset(dataset_name=dataset_name)


@behave.when(u'I create by project entity a dataset by the name of "{dataset_name}"')
def step_impl(context, dataset_name):
    context.dataset = context.project.create_dataset(dataset_name)
    context.dataset_count += 1


@behave.when(u'I delete a project entity')
def step_impl(context):
    context.project.delete(sure=True,
                           really=True)


@behave.when(u'I change project name to "{new_project_name}"')
def step_impl(context, new_project_name):
    new_project_name = new_project_name + str(random.randint(10000, 100000))
    context.project.name = new_project_name
    context.project.update()
    context.new_project_name = new_project_name


@behave.then(u'Project in host has name "{new_project_name}"')
def step_impl(context, new_project_name):
    project_get = context.dl.projects.get(project_id=context.project.id)
    assert project_get.name == context.new_project_name


@behave.when(u'I use project entity to pack directory by name "{codebase_name}"')
def step_impl(context, codebase_name):
    context.codebase = context.project.pack_codebase(
        directory=context.codebase_local_dir,
        name=codebase_name,
        description="some description",
    )


@behave.then(u'Project entity was deleted')
def step_impl(context):
    try:
        context.dl.projects.get(project_id=context.project.id)
        assert False
    except Exception as e:
        assert type(e) == context.dl.exceptions.NotFound
        context.project = context.dl.projects.create(context.project.name)
        context.to_delete_projects_ids.append(context.project.id)
        context.feature.dataloop_feature_project = context.project


@behave.when(u'I reclaim project')
def step_impl(context):
    context.project = context.dl.projects.get(project_id=context.project.id)


@behave.when(u'I checkout project')
def step_impl(context):
    context.project.checkout()


================================================
File: tests/features/steps/projects_repo/projects_interface.py
================================================
import behave
import time
import random


@behave.given(u'I create a project by the name of "{project_name}"')
def step_impl(context, project_name):
    if hasattr(context.feature, 'dataloop_feature_project'):
        context.project = context.feature.dataloop_feature_project
    else:
        num = random.randint(10000, 100000)
        project_name = 'to-delete-test-{}_{}'.format(str(num), project_name)
        context.project = context.dl.projects.create(project_name=project_name)
        context.to_delete_projects_ids.append(context.project.id)
        context.feature.dataloop_feature_project = context.project
        start = time.time()
        timeout = 60
        while (time.time() - start) < timeout:
            filters = context.dl.Filters(resource=context.dl.FiltersResource.DATASET)
            filters.add(field='name', values='Binaries')
            filters.system_space = True
            datasets = context.project.datasets.list(filters=filters)
            if len(datasets) > 0:
                break
            time.sleep(.200)

    context.project_name = context.project.name
    context.dataset_count = 0

    if 'bot.create' in context.feature.tags:
        if hasattr(context.feature, 'bot_user'):
            context.bot_user = context.feature.bot_user
        else:
            bot_name = 'test_bot_{}'.format(random.randrange(1000, 10000))
            context.bot = context.project.bots.create(name=bot_name)
            context.feature.bot = context.bot
            context.bot_user = context.bot.email
            context.feature.bot_user = context.bot_user


================================================
File: tests/features/steps/projects_repo/test_project_integrations.py
================================================
import os
import time

import behave
import random
import re


@behave.when(u'I create "{integration_type}" integration with name "{integration_name}" and metadata "{metadata}"')
@behave.given(u'I create "{integration_type}" integration with name "{integration_name}"')
def step_impl(context, integration_type, integration_name, metadata=None):
    integration_options = {}
    if metadata is not None:
        metadata = eval(metadata)

    if integration_type == 's3':
        integration_options["s3"] = eval(os.environ.get("aws"))
    elif integration_type == 'gcs':
        integration_options["gcs"] = {
            'key': None, 'secret': None, 'content': os.environ.get('gcs')
        }
    elif integration_type == 'azureblob':
        integration_options["azureblob"] = eval(os.environ.get("azureblob"))
    elif integration_type == 'aws-sts':
        integration_options["aws-sts"] = eval(os.environ.get("aws_sts"))
    elif integration_type == 'azuregen2':
        integration_options["azuregen2"] = eval(os.environ.get("azuregen2"))
    elif integration_type == 'key_value':
        integration_options["key_value"] = {
            'key': os.environ.get('key_value_key', 'default_key'),
            'value': os.environ.get('key_value_value', 'default_value')
        }
    elif integration_type == 'gcp-cross':
        integration_options["gcp-cross"] = {}

    assert integration_type in integration_options, "TEST FAILED: Wrong integration type: {}".format(integration_type)
    try:
        context.integration_type = integration_type

        if hasattr(context.feature, 'dataloop_feature_integration'):
            if context.feature.dataloop_feature_integration.type == integration_type and integration_name in context.feature.dataloop_feature_integration.name:
                context.integration = context.feature.dataloop_feature_integration
                return

        # Handle AzureDatalakeGen2 integration
        context.integration_type = context.integration_type.replace('azuregen2', 'azureblob')
        num = random.randint(1000, 10000)
        context.integration_name = f"{integration_name}-{num}"
        context.integration = context.project.integrations.create(
            integrations_type=context.integration_type,
            name=context.integration_name,
            options=integration_options[integration_type],
            metadata=metadata
        )
        context.feature.to_delete_integrations_ids.append(context.integration.id)
        context.feature.dataloop_feature_integration = context.integration
        if metadata is not None:
            for key, value in metadata.items():
                assert any(item['name'] == key for item in context.integration.metadata), \
                    f"TEST FAILED: Metadata key {key} not found in integration metadata"
                assert any(item['value'] == value for item in context.integration.metadata), \
                    f"TEST FAILED: Metadata value {value} not found in integration metadata"

        context.error = None
    except Exception as e:
        context.error = e


@behave.given(u'I create key value integration with key "{key}" value "{value}"')
def step_impl(context, key, value):
    integration_options = {
        'key': key, 'value': value
    }
    try:
        num = random.randint(1000, 10000)
        context.integration_name = f"{key}-{num}"
        context.integration = context.project.integrations.create(
            integrations_type=context.dl.IntegrationType.KEY_VALUE,
            name=context.integration_name,
            options=integration_options)
        context.feature.to_delete_integrations_ids.append(context.integration.id)
        context.feature.dataloop_feature_integration = context.integration
        context.error = None
    except Exception as e:
        context.error = e
        assert False, e


@behave.then(u'I validate integration with the name "{integration_name}" is created')
def step_impl(context, integration_name):
    try:
        context.integration = context.project.integrations.get(integrations_id=context.integration.id)
        context.error = None
    except Exception as e:
        context.error = e

    assert integration_name in context.integration.name, "TEST FAILED: Expected - {}, Got - {}".format(integration_name,
                                                                                                       context.integration.name)
    assert context.integration_type == context.integration.type, "TEST FAILED: Expected - {}, Got - {}".format(
        context.integration_type, context.integration.type)


@behave.when(u'I delete integration in context')
def step_impl(context):
    try:
        context.integration.delete(True, True)
        if context.feature.to_delete_integrations_ids:
            context.feature.to_delete_integrations_ids.pop(-1)
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'i got the dpk required integration')
def step_impl(context):
    success, resp = context.dl.client_api.gen_request(req_type='get',
                                                      path=f'/app-registry/{context.published_dpk.id}/requirements')
    assert success, f"TEST FAILED: Failed to get dpk requirements: {resp}"
    dpk_integrations = resp.json().get(context.published_dpk.id, {}).get('integrations', {})
    assert dpk_integrations, f"TEST FAILED: No integrations found in dpk requirements"


@behave.then(u'I validate integration not in integrations list by context.integration_name')
def step_impl(context):
    assert context.integration_name not in [integration['name'] for integration in context.project.integrations.list()], \
        "TEST FAILED: Failed to delete integration: {}".format(context.integration_name)


@behave.then(u'I validate gcp-cross-project has correct service account pattern')
def step_impl(context):
    pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    time.sleep(5)
    context.integration = context.project.integrations.get(context.integration.id)

    dataloop_service_account = False
    for metadata_obj in context.integration.metadata:
        if metadata_obj['name'] == "email":
            dataloop_service_account = bool(re.match(pattern, metadata_obj['value']))
            break

    assert dataloop_service_account, f"TEST FAILED: Not found service account , \nIntegration metadata: {context.integration.metadata}"


================================================
File: tests/features/steps/projects_repo/test_projects_create.py
================================================
import behave
import time
import random


@behave.when(u'I try to create a project by the name of "{project_name}"')
def creating_a_project(context, project_name):
    try:
        context.project = context.dl.projects.create(project_name=project_name)
        context.to_delete_projects_ids.append(context.project.id)
        time.sleep(5)  # to sleep because authorization takes time
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u'I create a project by the name of "{project_name}"')
def creating_a_project(context, project_name):
    if not project_name.startswith('to-delete-test-'):
        project_name = 'to-delete-test-' + project_name
    project_name = project_name + str(random.randint(10000, 100000))
    context.project = context.dl.projects.create(project_name=project_name)
    context.to_delete_projects_ids.append(context.project.id)
    context.project_name = project_name


@behave.then(u'Project object by the name of "{project_name}" should be created')
def project_object_should_be_created(context, project_name):
    assert type(context.project) == context.dl.entities.Project
    assert context.project.name == context.project_name


@behave.then(u'Project should exist in host by the name of "{project_name}"')
def project_should_exist_in_host(context, project_name):
    project_get = context.dl.projects.get(project_id=context.project.id)

    list_json = project_get.to_json()
    project_json = context.project.to_json()
    list_json.pop('role')
    project_json.pop('role')

    assert list_json == project_json


@behave.when(u'When I try to create a project with a blank name')
def step_impl(context):
    try:
        context.project = context.dl.projects.create(project_name='')
        context.to_delete_projects_ids.append(context.project.id)
        time.sleep(5)  # to sleep because authorization takes time
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'I receive error with status code "{status_code}"')
def step_impl(context, status_code):
    if context.error is None:
        assert False, f"Expected an error with status code {status_code}, but no error was raised."

    expected_code = int(status_code)
    actual_code = int(context.error.status_code)

    assert actual_code == expected_code, f"Expected {expected_code}, but got {actual_code}"

@behave.then(u'I should not receive an error with status code "{status_code}"')
def step_impl(context, status_code):
    if context.error.status_code != status_code:
        return
    assert False, f"Expected {status_code}, Actual Got {context.error.status_code}"



@behave.then(u'"{error}" exception should be raised')
def step_impl(context, error):
    assert error in str(type(context.error)), "TEST FAILED: Expected to get {}, Actual got {}".format(error, type(
        context.error))


@behave.then(u'"{error_msg}" in error message')
@behave.then(u'"{error_msg}" in error message with status code "{status_code}"')
def step_impl(context, error_msg, status_code=None):
    if status_code:
        assert status_code == context.error.status_code, f"TEST FAILED: Expected status_code: {status_code}\nActual Got {context.error.message}"
    assert error_msg in context.error.message, f"TEST FAILED: Actual msg: {context.error.message}"


@behave.then(u'Error message includes "{error_text}"')
def step_impl(context, error_text):
    assert error_text in str(context.error)


================================================
File: tests/features/steps/projects_repo/test_projects_delete.py
================================================
import behave


@behave.when(u'I delete a project by the name of "{project_name}"')
def step_impl(context, project_name):
    for i in range(7):
        try:
            context.dl.projects.delete(project_name=context.project_name,
                                        sure=True,
                                        really=True)
            break
        except context.dl.exceptions.InternalServerError:
            if i == 6:
                raise


@behave.when(u'I delete a project by the id of "{project_name}"')
def step_impl(context, project_name):
    context.dl.projects.delete(project_id=context.project.id,
                                sure=True,
                                really=True)


@behave.when(u'I try to delete a project by the name of "{project_name}"')
def step_impl(context, project_name):
    try:
        for i in range(7):
            try:
                context.dl.projects.delete(project_name=project_name,
                                            sure=True,
                                            really=True)
                context.error = None
                break
            except context.dl.exceptions.InternalServerError:
                if i >= 6:
                    raise
            except context.dl.exceptions.NotFound:
                raise
    except Exception as e:
        context.error = e


@behave.then(u'There are no projects by the name of "{project_name}"')
def step_impl(context, project_name):
    try:
        project = None
        project = context.dl.get(project_name=context.project_name)
    except:
        assert project is None

================================================
File: tests/features/steps/projects_repo/test_projects_get.py
================================================
# coding=utf-8
"""Projects repository get service testing."""

import behave


@behave.when(u'I get a project by the name of "{project_name}"')
def step_impl(context, project_name):
    for i in range(7):
        try:
            context.project_get = context.dl.projects.get(project_name=context.project_name)
            break
        except context.dl.exceptions.InternalServerError:
            if i >= 6:
                raise


@behave.then(u'I get a project by the name of "{project_name}"')
def step_impl(context, project_name):
    assert context.project_get.name == context.project_name


@behave.then(u'The project I got is equal to the one created')
def step_impl(context):
    project_json = context.project.to_json()
    if 'role' in project_json:
        project_json.pop('role')

    get_json = context.project.to_json()
    if 'role' in get_json:
        get_json.pop('role')

    assert project_json == get_json


@behave.when(u'I get a project by the id of Project')
def step_impl(context):
    context.project_get = context.dl.projects.get(project_id=context.project.id)


@behave.when(u'I try to get a project by the name of "{project_name}"')
def step_impl(context, project_name):
    try:
        for i in range(7):
            try:
                context.project_get = context.dl.projects.get(project_name=project_name)
                context.error = None
                break
            except context.dl.exceptions.InternalServerError:
                if i >= 6:
                    raise
            except context.dl.exceptions.NotFound:
                raise
    except Exception as e:
        context.error = e


@behave.when(u'I try to get a project by id')
def step_impl(context):
    try:
        context.project = context.dl.projects.get(project_id='some_id')
        context.error = None
    except Exception as e:
        context.error = e


================================================
File: tests/features/steps/projects_repo/test_projects_list.py
================================================
# coding=utf-8
"""Projects repository list service testing."""

import behave


@behave.when(u'I list all projects')
def step_impl(context):
    for i in range(7):
        try:
            context.list = context.dl.projects.list()
            break
        except context.dl.exceptions.InternalServerError:
            if i >= 6:
                raise
        


@behave.then(u'I receive an empty list')
def step_impl(context):
    assert len(context.list) == 0


@behave.then(u'I receive a projects list of "{list_length}" project')
def step_impl(context, list_length):
    assert len(context.list) == int(list_length)


@behave.then(u'The project in the projects list equals the project I created')
def step_impl(context):
    found = False
    for project in context.list:
        if project.name == context.project.name:
            found = True
            list_json = project.to_json()
            project_json = context.project.to_json()
            list_json.pop('role', None)
            project_json.pop('role', None)
            list_json.pop('isBlocked', None)
            project_json.pop('isBlocked', None)
            assert list_json == project_json
    assert found is True


================================================
File: tests/features/steps/recipes_repo/test_recipes_clone.py
================================================
import behave


@behave.given(u'I set dataset recipe and ontology to context')
def step_impl(context):
    context.recipe = context.dataset.recipes.list()[0]
    context.ontology = context.dataset.ontologies.list()[0]


@behave.when(u'I add new label "{label_name}" to dataset {dataset_index}')
def step_impl(context, label_name, dataset_index):
    context.datasets[int(dataset_index) - 1].add_label(label_name=label_name)


@behave.when(u'I clone recipe from  dataset {dataset1_index} to dataset {dataset2_index} with ontology')
def step_impl(context, dataset1_index, dataset2_index):
    context.recipe_clone = context.datasets[int(dataset1_index) - 1].recipes.list()[0].clone()
    context.datasets[int(dataset2_index) - 1].switch_recipe(recipe=context.recipe_clone)


@behave.when(u'I clone recipe from  dataset {dataset1_index} to dataset {dataset2_index} without ontology')
def step_impl(context, dataset1_index, dataset2_index):
    context.recipe_clone = context.datasets[int(dataset1_index) - 1].recipes.list()[0].clone(shallow=True)
    context.datasets[int(dataset2_index) - 1].switch_recipe(recipe=context.recipe_clone)


@behave.then(u'I verify that Dataset {dataset_index} has {labels_len} labels')
def step_impl(context, dataset_index, labels_len):
    context.datasets[int(dataset_index) - 1] = context.dl.datasets.get(
        dataset_id=context.datasets[int(dataset_index) - 1].id)

    assert len(context.datasets[int(dataset_index) - 1].labels) == int(labels_len)



================================================
File: tests/features/steps/recipes_repo/test_recipes_create.py
================================================
import behave
import os
import json


@behave.when(u'I create a new plain recipe')
def step_impl(context):
    context.recipe = context.dataset.recipes.create()


@behave.when(u'I create a new plain recipe with name "{recipe_name}"')
def step_impl(context, recipe_name):
    context.recipe = context.dataset.recipes.create(recipe_name=recipe_name)


@behave.when(u'I create a new project recipe')
def step_impl(context):
    context.recipe = context.dl.recipes.create(recipe_name="test-checkout")


@behave.When(u'I query recipes by "{field}" "{value}"')
def step_impl(context, field, value):
    recipe_filters = context.dl.Filters(resource=context.dl.FiltersResource.RECIPE)
    recipe_filters.add(field=field, values=value)
    context.recipies = context.project.recipes.list(filters=recipe_filters)


@behave.When(u'I have "{num}" recipies')
def step_impl(context, num):
    assert context.recipies.items_count == int(num) , f"Expected {num} recipes, got {context.recipies.items_count}"


@behave.then(u'recipe in host is exist')
def step_impl(context):
    recipes = context.project.recipes.list()
    for recipe in recipes.items:
        if recipe.id == context.recipe.id:
            assert True
            return
    assert False, "recipe not found"


@behave.when(u'I update dataset recipe to the new recipe')
def step_impl(context):
    context.dataset.metadata['system']['recipes'] = [context.recipe.id]
    context.project.datasets.update(dataset=context.dataset,
                                    system_metadata=True)


@behave.then(u'Dataset recipe in host equals the one created')
def step_impl(context):
    dataset_get = context.project.datasets.get(dataset_id=context.dataset.id)
    recipe_ids = dataset_get.metadata['system']['recipes']
    assert len(recipe_ids) == 1
    context.recipe_get = context.dataset.recipes.get(recipe_ids[0])
    assert context.recipe.to_json() == context.recipe_get.to_json()


@behave.when(
    u'I create a new recipe with param labels from "{class_file}" and attributes: "{attribute1}", "{attribute2}"')
def step_impl(context, class_file, attribute1, attribute2):
    class_file = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], class_file)
    with open(class_file) as f:
        context.labels = json.load(f)
    context.recipe = context.dataset.recipes.create(attributes=[attribute1, attribute2],
                                                    labels=context.labels)


@behave.then(u'Dataset ontology in host has labels from "{class_file}" and attributes: "{attribute1}", "{attribute2}"')
def step_impl(context, class_file, attribute1, attribute2):
    ontology_get = context.recipe.ontologies.get(ontology_id=context.recipe.ontology_ids[0])
    for root in ontology_get.to_json()['roots']:
        assert root in context.labels
    assert ontology_get.attributes == [attribute1, attribute2]


@behave.when(u'I create a new plain recipe with existing ontology id')
def step_impl(context):
    recipe = context.dataset.recipes.get(context.dataset.metadata['system']['recipes'][0])
    context.ontology = recipe.ontologies.get(recipe.ontology_ids[0])
    context.recipe = context.dataset.recipes.create(ontology_ids=context.ontology.id)


================================================
File: tests/features/steps/recipes_repo/test_recipes_delete.py
================================================
import os

import behave


@behave.given(u'Dataset has Recipes')
def step_impl(context):
    context.recipe = context.dataset.recipes.list()[0]
    context.second_recipe = context.dataset.recipes.create(recipe_name="second_recipe")


@behave.then(u'Add instruction "{path}" to Recipe')
def step_impl(context, path):
    try:
        annotation_instruction_file = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)
        context.recipe.add_instruction(annotation_instruction_file=annotation_instruction_file)
    except Exception as e:
        context.error = e


@behave.then(u'instruction are exist')
def step_impl(context):
    recipe = context.dataset.recipes.get(context.recipe.id)
    assert 'instructionDocument' in recipe.metadata['system']


@behave.when(u'I delete recipe')
def step_impl(context):
    context.recipe.delete(force=True)


@behave.then(u'Dataset has no recipes')
def step_impl(context):
    recipe_list = context.dataset.recipes.list()
    try:
        context.dataset.recipes.get(context.recipe.id)
        assert False
    except Exception as e:
        assert True
    assert context.dataset.recipes.get(context.second_recipe.id).title == context.second_recipe.title


================================================
File: tests/features/steps/recipes_repo/test_recipes_update.py
================================================
import behave


@behave.when(u'I update recipe')
def step_impl(context):
    context.recipe = context.dataset.recipes.list()[0]
    context.recipe.title = "new_name"
    context.recipe.update(system_metadata=True)


@behave.then(u'Recipe in host equals recipe edited')
def step_impl(context):
    context.recipe_get = context.dataset.recipes.get(recipe_id=context.recipe.id)
    assert context.recipe_get.title == context.recipe.title


================================================
File: tests/features/steps/service_driver_repo/test_service_driver_get.py
================================================
import behave


@behave.then(u'I validate compute service driver is "{status}"')
def step_impl(context, status='created'):
    if status not in ['created', 'archived']:
        assert False, f"TEST FAILED: status should be 'created' or 'archived' but got {status}"

    json_req = {"filter": {"context": {"org": context.project.org['id']}, 'computeId': context.compute_id}}
    if status == 'archived':
        json_req['filter']['archived'] = True
    success, response = context.dl.client_api.gen_request(
        req_type='post',
        path='/serviceDrivers/query',
        json_req=json_req
    )
    if not success:
        assert False, f"TEST FAILED: {response.message}"
    assert response.json()[
               'totalItemsCount'] == 1, f"TEST FAILED: Expected 1 driver actual {response.json()['totalItemsCount']}"
    context.service_driver_id = response.json()['items'][0]['id']
    if status == 'created':
        assert context.dl.service_drivers.get(context.service_driver_id)

    if context.service_driver_id not in context.to_delete_service_drivers_ids and status == 'created':
        context.to_delete_service_drivers_ids.append(context.service_driver_id)


@behave.then(u'I get archived service driver')
def step_impl(context):
    success, response = context.dl.client_api.gen_request(
        req_type='get',
        path=f'/serviceDrivers/{context.service_driver_id}?archived=true',
    )

    if not success and response.status_code != 404:
        assert False, f"TEST FAILED: {response.message}"


================================================
File: tests/features/steps/service_entity/test_service_debug_mode.py
================================================
import behave
import time


@behave.then(u'I validate service has "{instance_number}" instance up')
@behave.then(u'I validate service has "{instance_number}" instance up and replicaId include service name "{flag}"')
@behave.then(u'I validate service has "{instance_number}" instance up and replicaId include service name "{flag}" num_try {num_try_input:d}')
def step_impl(context, instance_number, flag="None", num_try_input=60):
    """ Get service instances / pods / replicas status """
    num_try = num_try_input
    interval = 10
    success = False

    for i in range(num_try):
        time.sleep(interval)
        status = context.service.status()
        context.service_instances = status['runtimeStatus']
        if len(context.service_instances) == int(instance_number):
            success = True
            if eval(flag):
                for instance in context.service_instances:
                    assert context.service.name in instance['replicaId'], "TEST FAILED: Expected {} in {}, Got {}".format(
                        context.service.name, instance['replicaId'], instance)
            break
        context.dl.logger.debug(
            "Step is running for {:.2f}[s] and now Going to sleep {:.2f}[s]".format((i + 1) * interval,
                                                                                    interval))

    assert success, "TEST FAILED: Expected {}, Got {}".format(instance_number, len(context.service_instances))


================================================
File: tests/features/steps/services_repo/test_service_validation.py
================================================
import behave


@behave.then(u'I validate service response params')
def step_impl(context):
    service_json = context.project.services.get(service_id=context.service.id).to_json()
    for row in context.table:
        assert service_json[row['key']] == row[
            'value'], f"TEST FAILED: Expected {row['value']}, Actual {service_json[row['key']]}"


@behave.when(u'set context.published_dpk to context.dpk')
def step_impl(context):
    context.dpk = context.published_dpk


@behave.then(u'service has app scope')
def step_impl(context):
    assert context.service.app[
               'id'] == context.app.id, f"TEST FAILED: app id is not as expected, expected: {context.app.id}, got: {context.service.app['id']}"
    assert context.service.app[
               'dpkId'] == context.dpk.base_id, f"TEST FAILED: dpk id is not as expected, expected: {context.dpk.base_id}, got: {context.service.app['dpkId']}"
    assert context.service.app[
               'dpkVersion'] == context.dpk.version, f"TEST FAILED: dpk version is not as expected, expected: {context.dpk.version}, got: {context.service.app['dpkVersion']}"
    assert context.service.app[
               'dpkName'] == context.dpk.name, f"TEST FAILED: dpk name is not as expected, expected: {context.dpk.name}, got: {context.service.app['dpkName']}"


================================================
File: tests/features/steps/services_repo/test_services_context.py
================================================
import os
import random

import behave
import dtlpy as dl
import time


@behave.given(u'I append package to packages')
def step_impl(context):
    if not hasattr(context, "packages"):
        context.packages = list()
    context.packages.append(context.package)


@behave.when(u'I get the service from project number {project_index}')
def step_impl(context, project_index):
    context.service = context.projects[int(project_index) - 1].services.get(service_id=context.service.id)


@behave.when(u'I get the service from package number {project_index}')
def step_impl(context, project_index):
    context.service = context.packages[int(project_index) - 1].services.get(service_id=context.service.id)


@behave.then(u'Service Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.service.project_id == context.projects[int(project_index) - 1].id


@behave.then(u'Service Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.service.project.id == context.projects[int(project_index) - 1].id


@behave.then(u'Service Package_id is equal to package {project_index} id')
def step_impl(context, project_index):
    assert context.service.package_id == context.packages[int(project_index) - 1].id


@behave.then(u'Service Package.id is equal to package {project_index} id')
def step_impl(context, project_index):
    assert context.service.package.id == context.packages[int(project_index) - 1].id


@behave.when(u'I deploy a service with init prams')
def step_impl(context):
    context.service = context.project.services.deploy(
        service_name=context.package.name,
        package=context.package,
        module_name=context.package.modules[0].name,
        init_input={
            'item': context.item.id,
            'ds': context.dataset.id,
            'string': 'test'
        }
    )

    context.to_delete_services_ids.append(context.service.id)


@behave.then(u'service has integrations')
def step_impl(context):
    assert len(context.service.integrations) > 0


@behave.then(u'I execute the service')
def step_impl(context):
    context.execution: dl.Execution = context.service.execute(item_id=context.item.id,
                                                              function_name='run',
                                                              project_id=context.project.id,
                                                              sync=True,
                                                              return_output=False,
                                                              stream_logs=False
                                                              )


@behave.then(u'The execution success with the right output')
def step_impl(context):
    assert context.execution.output == {
        'item_id': context.item.id,
        'string': 'test',
        'dataset': context.dataset.id
    }


@behave.when(u'I execute service on "{input}" with type "{input_type}" with name "{name_input}"')
def step_impl(context, input, input_type, name_input):
    if input.isdigit():
        input = eval(input)
    try:
        context.execution: dl.Execution = context.service.execute(project_id=context.project.id,
                                                                  execution_input=dl.FunctionIO(type=input_type,
                                                                                                value=input,
                                                                                                name=name_input)
                                                                  )
        context.error = None
    except Exception as e:
        context.error = e
        return True

    num_try = 40
    interval = 10
    finished = False

    for i in range(num_try):
        time.sleep(interval)
        status = context.execution.get_latest_status()
        if status['status'] in ['success', 'failed']:
            if "error" in status.keys():
                context.error = dl.exceptions.PlatformException
                context.error.message = status['error']
            finished = True
            break

    assert finished, f"TEST FAILED: Execution status - {status}"


================================================
File: tests/features/steps/services_repo/test_services_crashloop.py
================================================
import time
import behave
import os
import tempfile
import random
import string


@behave.given(u'Service that restart once in init')
def step_impl(context):
    module_class_name = 'ServiceRunner'
    entry_point = 'main.py'
    name = 'test-single-crash'

    code_base_string = """
import dtlpy as dl


class {module_class_name}:
    def __init__(self):
        self.project = dl.projects.get(project_id='{project_id}')
        try:
            self.project.datasets.get(dataset_name='test-dataset')
        except:
            self.project.datasets.create(dataset_name='test-dataset')
            raise Exception('Only one InitError')

    def run(self, item):
        return item

    """.format(
        module_class_name=module_class_name,
        project_id=context.project.id
    )

    def get_randon_string(length=5):
        return ''.join(random.choice(string.ascii_lowercase) for i in range(length))

    codebase_dir = os.path.join(tempfile.gettempdir(), 'codebase', get_randon_string())
    os.makedirs(codebase_dir, exist_ok=True)
    code_base_filepath = os.path.join(codebase_dir, entry_point)

    with open(code_base_filepath, 'w') as f:
        f.write(code_base_string)

    context.package = context.project.packages.push(
        package_name='package-{}'.format(name),
        src_path=codebase_dir,
        modules=[
            context.dl.PackageModule(
                class_name=module_class_name,
                entry_point=entry_point,
                functions=[
                    context.dl.PackageFunction(
                        function_name='run',
                        inputs=[
                            context.dl.FunctionIO(name='item', type=context.dl.PackageInputType.ITEM)
                        ],
                        outputs=[
                            context.dl.FunctionIO(name='item', type=context.dl.PackageInputType.ITEM)
                        ]
                    )
                ]
            )
        ]
    )


@behave.when(u'service is deployed with num replicas > 0')
def step_impl(context):
    context.service = context.package.deploy(
        service_name='service-{}'.format(context.package.name.replace('package', 'service')),
        runtime=context.dl.KubernetesRuntime(
            autoscaler=context.dl.KubernetesRabbitmqAutoscaler(
                min_replicas=1,
                max_replicas=1
            )
        )
    )


@behave.then(u'service should stay active')
def step_impl(context):
    service = context.project.services.get(service_id=context.service.id)
    assert service.active is True
    context.execution = context.service.execute(item_id=context.item.id, project_id=context.project.id)

    timeout = 60 * 10
    interval = 5
    start = time.time()

    while time.time() - start < timeout:
        context.execution = context.project.executions.get(execution_id=context.execution.id)
        if context.execution.latest_status['status'] == context.dl.ExecutionStatus.SUCCESS:
            break
        time.sleep(interval)

    assert context.execution.latest_status['status'] == context.dl.ExecutionStatus.SUCCESS


================================================
File: tests/features/steps/services_repo/test_services_create.py
================================================
import os
import dtlpy as dl
import behave
import json
import time

from dtlpy import FunctionIO, PackageInputType, KubernetesAutoscalerType



@behave.when(u"I create a service")
def step_impl(context):
    service_name = None
    package = None
    revision = None
    config = None
    runtime = None
    execution_timeout = None
    max_attempts = None
    pod_type = None
    bot_user = context.bot_user if hasattr(context, "bot_user") else None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "service_name":
            if param[1] != "None":
                service_name = param[1]
        elif param[0] == "package":
            if param[1] != "None":
                package = param[1]
        elif param[0] == "revision":
            if param[1] != "None":
                revision = param[1]
        elif param[0] == "config":
            if param[1] != "None":
                config = json.loads(param[1])
        elif param[0] == "runtime":
            if param[1] != "None":
                runtime = json.loads(param[1])
            else:
                runtime = {"numReplicas": 1, 'concurrency': 1}
        elif param[0] == "execution_timeout":
            if param[1] != "None":
                execution_timeout = int(param[1])
        elif param[0] == "max_attempts":
            if param[1] != "None":
                max_attempts = int(param[1])
        elif param[0] == "pod_type":
            if param[1] != "None":
                pod_type = param[1]
        elif param[0] == "bot_user":
            if param[1] != "None":
                bot_user = context.project.bots.get(bot_name=param[1])
            else:
                bot_user = context.bot_user


    context.service = context.package.services._create(
        bot=bot_user,
        service_name=service_name,
        package=context.package,
        revision=revision,
        init_input=config,
        runtime=runtime,
        execution_timeout=execution_timeout,
        max_attempts=max_attempts,
        pod_type=pod_type
    )
    context.to_delete_services_ids.append(context.service.id)
    if hasattr(context, "first_service"):
        context.second_service = context.service
    else:
        context.first_service = context.service


@behave.when(u"I create a service with autoscaler")
def step_impl(context):
    service_name = None
    package = None
    revision = None
    config = None
    runtime = None
    pod_type = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "service_name":
            if param[1] != "None":
                service_name = param[1]
        elif param[0] == "package":
            if param[1] != "None":
                package = param[1]
        elif param[0] == "revision":
            if param[1] != "None":
                revision = param[1]
        elif param[0] == "config":
            if param[1] != "None":
                config = json.loads(param[1])
        elif param[0] == "pod_type":
            if param[1] != "None":
                pod_type = param[1]
        elif param[0] == "runtime":
            if param[1] != "None":
                runtime = json.loads(param[1])
            else:
                runtime = {"gpu": False, "numReplicas": 1, 'concurrency': 1,
                           'autoscaler': {
                               'type': KubernetesAutoscalerType.RABBITMQ, 'minReplicas': 1,
                               'maxReplicas': 5,
                               'queueLength': 10}}

    context.service = context.package.services._create(
        bot=context.bot_user,
        service_name=service_name,
        package=context.package,
        revision=revision,
        init_input=config,
        runtime=runtime,
        pod_type=pod_type
    )
    context.to_delete_services_ids.append(context.service.id)
    if hasattr(context, "first_service"):
        context.second_service = context.service
    else:
        context.first_service = context.service


@behave.then(u'I upload an item by the name of "{item_name}"')
def step_impl(context, item_name):
    local_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], '0000000162.jpg')
    context.item = context.dataset.items.upload(local_path=local_path)


@behave.then(u"I run a service execute for the item")
def step_impl(context):
    context.ex = context.service.execute(
        execution_input=FunctionIO(name='item', value=context.item.id, type=PackageInputType.ITEM),
        function_name='run',
        project_id=context.package.project_id)


@behave.then(u"I receive a Service entity")
def step_impl(context):
    assert isinstance(context.service, context.dl.entities.Service)


@behave.then(u'Log "{log_message}" is in service.log()')
def step_impl(context, log_message):
    num_tries = 60
    interval_time = 5
    success = False

    for i in range(num_tries):
        time.sleep(interval_time)
        for log in context.service.log(view=False):
            if log_message in log:
                success = True
                break
        if success:
            break

    assert success, f"TEST FAILED: After {round(num_tries * interval_time / 60, 1)} minutes"


@behave.when(u'I deploy a service from function "{service_name}"')
def step_impl(context, service_name):
    try:
        def run(self, item=None, progress=None):
            """
            Write your main package function here

            :param progress: Use this to update the progress of your package
            :return:
            """
            # these lines can be removed
            assert isinstance(progress, dl.Progress)
            progress.update(status='inProgress', progress=0)

        context.service = context.project.services.deploy(func=run,
                                                          service_name=service_name)

    except Exception as e:
        context.error = e


@behave.then(u"I call service.execute() on items in dataset")
def step_impl(context):
    items_list = [item.id for item in context.dataset.items.list().items]
    context.execution = context.service.execute(
        execution_input=FunctionIO(name='items', value=items_list, type=PackageInputType.ITEMS),
        function_name='run',
        project_id=context.package.project_id)



================================================
File: tests/features/steps/services_repo/test_services_delete.py
================================================
import behave


@behave.when(u'I delete service by "{deletion_format}"')
def step_impl(context, deletion_format):
    if deletion_format == 'id':
        assert context.project.services.delete(service_id=context.service.id)
    elif deletion_format == 'name':
        assert context.project.services.delete(service_name=context.service.name)
    else:
        assert context.service.delete()


@behave.then(u"There are no services")
def step_impl(context):
    assert context.package.services.list().items_count == 0


@behave.when(u'I try to uninstall service')
def step_impl(context):
        try:
            context.service = context.project.services.list().items[0]
            assert context.service is not None, f"TEST FAILED: Missing service"
            context.service.delete()
            context.error = None
        except Exception as e:
            context.error = e


@behave.when(u'I uninstall service')
def step_impl(context):
    context.service = context.project.services.list().items[0]
    assert context.service is not None, f"TEST FAILED: Missing service"
    r = context.service.delete()
    assert r


================================================
File: tests/features/steps/services_repo/test_services_deploy.py
================================================
import behave
import json


@behave.given(u'There are no services')
def step_impl(context):
    assert context.package.services.list().items_count == 0


@behave.when(u'I deploy a service')
def step_impl(context):
    service_name = None
    package = None
    revision = None
    config = None
    runtime = None

    params = context.table.headings
    for param in params:
        param = param.split('=')
        if param[0] == 'service_name':
            if param[1] != 'None':
                service_name = param[1]
        elif param[0] == 'package':
            if param[1] != 'None':
                package = param[1]
        elif param[0] == 'revision':
            if param[1] != 'None':
                revision = (param[1])
        elif param[0] == 'config':
            if param[1] != 'None':
                config = json.loads(param[1])
        elif param[0] == 'runtime':
            if param[1] != 'None':
                runtime = json.loads(param[1])

    context.service = context.package.services.deploy(
        bot=context.bot_user,
        service_name=service_name,
        package=context.package,
        revision=revision,
        init_input=config,
        runtime=runtime
    )
    context.to_delete_services_ids.append(context.service.id)


@behave.then(u'There is only one service')
def step_impl(context):
    services_list = context.package.services.list()
    assert services_list.items_count == 1
    assert services_list.items[0].to_json() == context.service.to_json()


================================================
File: tests/features/steps/services_repo/test_services_get.py
================================================
import behave
import time


@behave.when(u'I get service by id')
def step_impl(context):
    context.service_get = context.project.services.get(service_id=context.service.id)


@behave.when(u'I set service in context')
def step_impl(context):
    context.service = context.project.services.get(service_id=context.service.id)


@behave.then(u'Service is archived')
def step_impl(context):
    for x in range(5):
        context.service_get = context.project.services.get(service_id=context.service_get.id)
        if context.service_get.archive:
            break
        time.sleep(1)
    assert context.service_get.archive is True, "TEST FAILED: Expected archived to be True , Got {}".format(
        context.service_get.archive)


@behave.when(u'I get service by name')
def step_impl(context):
    context.service_get = context.project.services.get(service_name=context.service.name)


@behave.given(u'I get service by name "{input_name}"')
@behave.when(u'I get service by name "{input_name}"')
def step_impl(context, input_name):
    context.service = context.project.services.get(service_name=input_name)


@behave.then(u'Service received equals to service created')
def step_impl(context):
    assert context.service.to_json() == context.service_get.to_json()


@behave.then(u'I expect preemptible value to be "{value}"')
def step_impl(context, value):
    assert context.service.runtime.preemptible == eval(
        value), "TEST FAILED: Expected preemptible to be {} , Got {}".format(value, context.service.execution_timeout)


@behave.when(u'I get service in index "{service_index}"')
def step_impl(context, service_index):
    context.service = context.project.services.list().items[int(service_index)]


@behave.when(u'I get service from context.execution')
def step_impl(context):
    context.service = context.project.services.get(service_id=context.execution.service_id)


@behave.then(u'Integration display on service secrets')
def step_impl(context):
    if not hasattr(context, "integration"):
        raise AttributeError("Please make sure context has attr 'integration'")

    assert context.integration.id in context.service.secrets, \
        f"TEST FAILED: Expected integration to be in service secrets, Actual: {context.service.secrets}"


================================================
File: tests/features/steps/services_repo/test_services_list.py
================================================
import behave


@behave.when(u"I list services")
def step_impl(context):
    context.service_list = context.package.services.list()


@behave.when(u"I list services in project")
def step_impl(context):
    context.service_list = context.project.services.list()


@behave.then(u'I receive a Service list of "{count}" objects')
def step_impl(context, count):
    assert context.service_list.items_count == int(count), f"TEST FAILED: Expected {count} , Actual {context.service_list.items_count}"
    if int(count) > 0:
        for page in context.service_list:
            for service in page:
                assert isinstance(service, context.dl.entities.Service)


================================================
File: tests/features/steps/services_repo/test_services_slot.py
================================================
import behave
import dictdiffer


@behave.when(u"I get package entity from service")
def step_impl(context):
    context.package = context.service.package


@behave.when(u"I activate UI slot in service")
def step_impl(context):
    context.service.activate_slots(project_id=context.project.id)


@behave.when(u'I update UI slot display_name to "{display_name}"')
def step_impl(context, display_name):
    assert len(context.package.slots) == 1, "TEST FAILED: Expected to get 1 UI slot , Actual got : {}".format(len(context.package.slots))

    context.package.slots[0].display_name = display_name
    context.package = context.package.update()


@behave.then(u'I validate service UI slot is equal to settings')
def step_impl(context):
    dict_1 = context.package.slots[0].to_json()
    dict_2 = context.setting_get.metadata['slots'][0]

    assert [] == list(dictdiffer.diff(dict_1, dict_2)), "TEST FAILED: Different in service slot and settings.\n{}".format(list(dictdiffer.diff(dict_1, dict_2)))



================================================
File: tests/features/steps/services_repo/test_services_update.py
================================================
import string
import time
import random
from multiprocessing.pool import ThreadPool

import behave
import json


@behave.then(u"Service attributes are modified")
def step_impl(context):
    revision = None
    config = None
    runtime = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "revision":
            if param[1] != "None":
                revision = int(param[1])
        elif param[0] == "config":
            if param[1] != "None":
                config = json.loads(param[1])
        elif param[0] == "runtime":
            if param[1] != "None":
                runtime = json.loads(param[1])

    assert context.trigger_update.revision == revision
    assert context.trigger_update.config == config
    assert context.trigger_update.runtime == runtime


@behave.then(u"I receive an updated Service object")
def step_impl(context):
    assert isinstance(context.service_update, context.dl.entities.Service)


@behave.when(u'I change service "{attribute}" to "{value}"')
def step_impl(context, attribute, value):
    if attribute in ['gpu', 'image']:
        context.service.runtime[attribute] = value
    elif attribute in ['numReplicas', 'concurrency']:
        setattr(context.service.runtime, attribute, int(value))
    elif attribute in ['config']:
        context.service.config = json.loads(value)
    elif attribute in ['packageRevision']:
        setattr(context.service, attribute, int(value))
    else:
        if value in ['True', 'False']:
            value = eval(value)
        setattr(context.service, attribute, value)


@behave.when(u'I update service')
def step_impl(context):
    try:
        context.service_update = context.service.update()
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'Service received equals service changed except for "{updated_attribute}"')
def step_impl(context, updated_attribute):
    if not hasattr(context, "service_update"):
        raise context.error
    updated_to_json = context.service_update.to_json()
    origin_to_json = context.service.to_json()
    if 'runtime' in updated_attribute:
        attribute = updated_attribute.split('.')[-1]
        updated_to_json['runtime'].pop(attribute, None)
        origin_to_json['runtime'].pop(attribute, None)
    else:
        updated_to_json.pop(updated_attribute, None)
        origin_to_json.pop(updated_attribute, None)

    updated_to_json.pop('updatedAt', None)
    origin_to_json.pop('updatedAt', None)

    updated_to_json.pop('updatedBy', None)
    origin_to_json.pop('updatedBy', None)

    updated_to_json.pop('version', None)
    origin_to_json.pop('version', None)

    updated_to_json.pop('revisions', None)
    origin_to_json.pop('revisions', None)

    assert len(context.service_update.revisions) == len(context.service_revisions) + 1
    assert updated_to_json == origin_to_json


@behave.given(u'I execute service')
def step_impl(context):
    context.execution = context.service.execute(project_id=context.service.project_id)


@behave.given(u'Service has max_attempts of "{max_attempts}"')
def step_impl(context, max_attempts):
    max_attempts = int(max_attempts)
    assert context.service.max_attempts == max_attempts


@behave.given(u'Execution is running')
def step_impl(context):
    interval = 10
    num_tries = 30
    success = False
    for i in range(num_tries):
        time.sleep(interval)
        if success:
            break
        e = context.execution = context.service.executions.get(execution_id=context.execution.id)
        success = e.latest_status['status'] in ['in-progress', 'inProgress']
    assert success, f"TEST FAILED: latest status - {e.latest_status['status']}, After {round(num_tries * interval / 60, 1)} minutes"


@behave.when(u'I update service with force="{force}"')
def step_impl(context, force: str):
    force = bool(force)
    context.service = context.service.update(force=force)


@behave.then(u'Execution stopped immediately')
def step_impl(context):
    interval = 10
    num_tries = 24
    success = False
    for i in range(num_tries):
        time.sleep(interval)
        if success:
            break
        e = context.execution = context.service.executions.get(execution_id=context.execution.id)
        success = e.latest_status['status'] == 'failed'
    assert success, f"TEST FAILED: After {round(num_tries * interval / 60, 1)} minutes"


@behave.when(u'I get service revisions')
def step_impl(context):
    context.service_revisions = context.service.revisions


@behave.when(u'I update service to latest package revision')
def step_impl(context):
    context.package = context.project.packages.get(package_id=context.service.package_id)
    context.service.package_revision = context.package.version
    context.service = context.service.update(True)


@behave.then(u'"{resource}" has updatedBy field')
def step_impl(context, resource: str):
    resource = getattr(context, resource)
    assert resource.updated_by is not None
    assert resource.updated_by == context.dl.info()['user_email']


def random_5_chars():
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=5)) + 'a'


long_timeout = 60 * 60 * 5


@behave.given(u'I have a paused "{service_type}" service with concurrency "{concurrency}"')
def step_impl(context, service_type: str, concurrency: str):
    concurrency = int(concurrency)

    service_name = f'{service_type}-service-{random_5_chars()}'

    def run(item):
        import time
        time.sleep(.5)
        return item

    context.service = context.project.services.deploy(
        func=run,
        service_name=service_name,
        execution_timeout=900 if service_type == 'short-term' else long_timeout,
        runtime=context.dl.KubernetesRuntime(
            concurrency=concurrency,
            autoscaler=context.dl.KubernetesRabbitmqAutoscaler(
                min_replicas=0,
                max_replicas=1
            )
        )
    )
    context.service.pause()
    context.service = context.dl.services.get(service_id=context.service.id)


@behave.given(u'I run "{num_executions}" executions and activate the service')
def step_impl(context, num_executions: str):
    num_executions = int(num_executions)
    item_id = context.dataset.items.list().items[0].id
    pool = ThreadPool(processes=10)
    for _ in range(num_executions):
        pool.apply_async(
            context.service.execute,
            kwds={'item_id': item_id, 'project_id': context.project.id}
        )

    pool.close()
    pool.join()
    context.service.resume()
    context.service = context.dl.services.get(service_id=context.service.id)


def create_filter(c):
    f = c.dl.Filters(resource=c.dl.FiltersResource.EXECUTION)
    f.add(field='serviceId', values=c.service.id)
    return f


@behave.when(u'I update the service back and forth "{num_times}" times from long-term to short-term')
def step_impl(context, num_times: str):
    num_times = int(num_times)
    context.machine_count = num_times + 1
    last_count = 0
    interval = 1

    def is_long_term(service: context.dl.Service):
        return service.execution_timeout == long_timeout

    def wait_for_some_executions_to_run(last_running_executions_count):
        max_wait = 60 * 3
        start_time = time.time()
        f = create_filter(c=context)
        f.add(
            field='latestStatus.status', values=context.dl.ExecutionStatus.CREATED,
            operator=context.dl.FiltersOperations.NOT_EQUAL
        )
        es = context.service.executions.list(filters=f)
        while es.items_count <= last_running_executions_count and time.time() - start_time < max_wait:
            es = context.service.executions.list(filters=f)
            time.sleep(interval)

    for _ in range(num_times):
        wait_for_some_executions_to_run(last_running_executions_count=last_count)
        if is_long_term(context.service):
            context.service.execution_timeout = timeout = 900
        else:
            context.service.execution_timeout = timeout = long_timeout
        context.service = context.service.update()
        context.service = context.dl.services.get(service_id=context.service.id)
        assert context.service.execution_timeout == timeout
        time.sleep(interval)
        filters = create_filter(c=context)
        filters.add(
            field='latestStatus.status', values=context.dl.ExecutionStatus.CREATED,
            operator=context.dl.FiltersOperations.NOT_EQUAL
        )
        executions = context.service.executions.list(filters=filters)
        last_count = executions.items_count


@behave.when(u'I expect all executions to be successful and no execution should have ran twice')
def step_impl(context):
    # all success
    max_wait = 60 * 5
    start_time = time.time()
    interval = 3

    while time.time() - start_time < max_wait:
        filters = create_filter(c=context)
        filters.add(
            field='latestStatus.status', values=context.dl.ExecutionStatus.SUCCESS,
            operator=context.dl.FiltersOperations.NOT_EQUAL
        )
        executions = context.service.executions.list(filters=filters)
        if len(executions.items) == 0:
            break
        else:
            print(f'Waiting for {executions.items_count} executions to finish')
        time.sleep(interval)

    # all ran once
    machines = set()
    filters = create_filter(c=context)
    pages = context.service.executions.list(filters=filters)
    statuses = [
        context.dl.ExecutionStatus.CREATED,
        'in-progress',
        context.dl.ExecutionStatus.SUCCESS
    ]
    statuses.sort()
    pattern = '-'.join(statuses)
    for page in pages:
        for execution in page:
            replica_id = execution.latest_status.get('replicaId', None)
            latest_status = execution.latest_status.get('status', None)
            if replica_id is not None:
                machines.add(replica_id)
            else:
                assert False, 'Execution status is missing replicaId'
            assert latest_status == context.dl.ExecutionStatus.SUCCESS, f"TEST FAILED: Expected {context.dl.ExecutionStatus.SUCCESS}, Actual {latest_status}"
            e_status_list = [s['status'] for s in execution.status]
            e_status_list.sort()
            assert '-'.join(e_status_list) == pattern, f"TEST FAILED: Expected {pattern}, Actual {'-'.join(e_status_list)}"

    # make sure executions ran on different machines
    assert len(machines) == context.machine_count, f"TEST FAILED: Expected {context.machine_count} , Actual {len(machines)}"



================================================
File: tests/features/steps/settings_context/test_settings_context.py
================================================
import os
import random
import time
import behave


@behave.when(u'I create two project and datasets by the name of "{f_project_name}" "{s_project_name}"')
def creating_a_project(context, f_project_name, s_project_name):
    f_project_name = "{}_{}".format(f_project_name, str(random.randint(10000, 100000)))
    s_project_name = "{}_{}".format(s_project_name, str(random.randint(10000, 100000)))
    context.project = context.dl.projects.create(project_name="to-delete-test-{}".format(f_project_name))
    context.to_delete_projects_ids.append(context.project.id)
    context.second_project = context.dl.projects.create(project_name="to-delete-test-{}".format(s_project_name))
    context.to_delete_projects_ids.append(context.second_project.id)
    time.sleep(5)  # to sleep because authorization takes time
    context.first_dataset = context.project.datasets.create(dataset_name='f_project_name', index_driver=context.index_driver_var)
    context.second_dataset = context.second_project.datasets.create(dataset_name='s_project_name', index_driver=context.index_driver_var)


@behave.when(u'I upload item in "{item_path}" to both datasets')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    context.first_item = context.first_dataset.items.upload(local_path=item_path)
    context.second_item = context.second_dataset.items.upload(local_path=item_path)


@behave.when(u'i upload annotations to both items')
def step_impl(context):
    builder = context.first_item.annotations.builder()
    builder.add(annotation_definition=context.dl.Box(top=100, left=100, right=200, bottom=50, label='aa'))
    context.f_annotation = context.first_item.annotations.upload(builder)
    builder2 = context.second_item.annotations.builder()
    builder2.add(annotation_definition=context.dl.Box(top=100, left=100, right=200, bottom=50, label='aa'))
    context.s_annotation = context.second_item.annotations.upload(builder2)


@behave.when(u'add settings to the project')
def step_impl(context):
    context.setting = context.project.settings.create(setting_name='4ptBox', setting_value=True,
                                                      setting_value_type=context.dl.SettingsValueTypes.BOOLEAN)


@behave.then(u'check if geo in the first item and in the second are difference')
def step_impl(context):
    first_item_geo = context.f_annotation.annotations[0].geo
    second_item_geo = context.s_annotation.annotations[0].geo
    assert len(first_item_geo) == 4
    assert len(second_item_geo) == 2


@behave.when(u'I get setting by "{identifier}"')
def step_impl(context, identifier):
    if identifier == 'id':
        context.setting_get = context.project.settings.get(setting_id=context.setting.id)
    else:
        context.setting_get = context.project.settings.get(setting_name=context.setting.name)


@behave.then(u'I check setting got is equal to the one created')
def step_impl(context):
    assert context.setting_get.to_json() == context.setting.to_json()


@behave.then(u'I get setting for context service')
def step_impl(context):

    context.settings_list = context.project.settings.list()
    for setting in context.settings_list.items:
        if context.service.name in setting.name:
            # Found expected setting
            context.setting_get = setting
            break

    if not hasattr(context, "setting_get"):
        return False, "No setting found with the name: {}".format(context.service.name)


@behave.when(u'I add settings to the project with wrong "{value}" type')
def step_impl(context, value):
    try:
        context.project.settings.create(setting_name="data-pipeline-features",
                                        setting_value=eval(value),
                                        setting_value_type=context.dl.SettingsValueTypes.BOOLEAN
                                        )
    except Exception as e:
        context.value = value
        context.err_message = e.message
        context.err_status_code = e.status_code


@behave.then(u'I expect the correct exception to be thrown')
def step_impl(context):
    value = context.value
    if type(eval(value)) == str:
        assert f'Invalid input specified, Incorrect value type passed - Passed boolean Expected string' \
               in context.err_message
    elif type(eval(value)) == int:
        assert f'Invalid input specified, Incorrect value type passed - Passed boolean Expected number' \
               in context.err_message
    assert context.err_status_code == '400'


================================================
File: tests/features/steps/tasks_repo/test_task_context.py
================================================
import os

import behave
from .. import fixtures


@behave.given(u'I create task belong to dataset {dataset_index}')
def step_impl(context, dataset_index):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    context.task = context.datasets[int(dataset_index) - 1].tasks.create(**context.params)


@behave.given(u'I upload items in "{item_path}" to datasets')
def step_impl(context, item_path):
    item_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], item_path)
    for dataset in context.datasets:
        dataset.items.upload(local_path=item_path)


@behave.when(u'I get the task from project number {project_index}')
def step_impl(context, project_index):
    context.task = context.projects[int(project_index) - 1].tasks.get(task_id=context.task.id)


@behave.when(u'I get the task from dataset number {dataset_index}')
def step_impl(context, dataset_index):
    context.task = context.datasets[int(dataset_index) - 1].tasks.get(task_id=context.task.id)


@behave.then(u'task Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.task.project_id == context.projects[int(project_index) - 1].id


@behave.then(u'task Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.task.project.id == context.projects[int(project_index) - 1].id


@behave.then(u'task Dataset_id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.task.dataset_id == context.datasets[int(dataset_index) - 1].id


@behave.then(u'task Dataset.id is equal to dataset {dataset_index} id')
def step_impl(context, dataset_index):
    assert context.task.dataset.id == context.datasets[int(dataset_index) - 1].id


================================================
File: tests/features/steps/tasks_repo/test_task_priority.py
================================================
import behave


@behave.when(u'I create Task with priority "{priority}"')
def step_impl(context, priority):
    priorities = {
        "TaskPriority.LOW": context.dl.TaskPriority.LOW,
        "TASK_PRIORITY_LOW": context.dl.TASK_PRIORITY_LOW,
        "TaskPriority.MEDIUM": context.dl.TaskPriority.MEDIUM,
        "TASK_PRIORITY_MEDIUM": context.dl.TASK_PRIORITY_MEDIUM,
        "TaskPriority.HIGH": context.dl.TaskPriority.HIGH,
        "TASK_PRIORITY_HIGH": context.dl.TASK_PRIORITY_HIGH
    }

    context.task = context.dataset.tasks.create(task_name="{}_priority_task".format(priority),
                                                priority=priorities[priority])


@behave.then(u'Task with priority "{priority}" got created')
def step_impl(context, priority):
    priorities = {"LOW": 1, "MEDIUM": 2, "HIGH": 3}
    assert context.task.priority == priorities[priority]


================================================
File: tests/features/steps/tasks_repo/test_task_update.py
================================================
import behave
import dtlpy as dl


@behave.when(u'I update task name "{task_name}"')
def step_impl(context, task_name):
    context.task.name = task_name
    context.task.update()



================================================
File: tests/features/steps/tasks_repo/test_tasks_add_and_get_items.py
================================================
import behave
from .. import fixtures
import dtlpy as dl


@behave.when(u'I get Task items by "{method}"')
def step_impl(context, method):
    if method == 'name':
        context.task_items = context.dataset.tasks.get_items(task_name=context.task.name)
    elif method == 'id':
        context.task_items = context.dataset.tasks.get_items(task_id=context.task.id)


@behave.then(u'I receive task items list of "{count}" items')
def step_impl(context, count):
    success = context.task_items.items_count == int(count)
    for page in context.task_items:
        for item in page:
            success = success and isinstance(item, context.dl.Item)

    if not success:
        print('items received: {}'.format(context.task_items.items_count))

    assert success


@behave.when(u'I add items to task')
def step_impl(context):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    context.dataset.tasks.add_items(**context.params)


@behave.when(u'I remove items from task')
def step_impl(context):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    context.dataset.tasks.remove_items(**context.params)


@behave.then(u'Task has "{assignment_count}" assignments')
def step_impl(context, assignment_count):
    assignments = dl.tasks.assignments.list(project_ids=context.project.id, task_id=context.task.id)
    count = len(assignments)
    for assignment in assignments:
        if assignment.metadata["system"]["markForDeletion"]:
            count -= 1
    assert count == int(assignment_count)


================================================
File: tests/features/steps/tasks_repo/test_tasks_create.py
================================================
import behave
import json
from .. import fixtures


@behave.when(u'I create Task')
def step_impl(context):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    context.task = context.dataset.tasks.create(**context.params)


@behave.when(u'I create Task in second project')
def step_impl(context):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    context.task = context.second_dataset.tasks.create(**context.params)


@behave.then(u'I receive a task entity')
def step_impl(context):
    assert isinstance(context.task, context.dl.entities.Task)


def compare_items_list(items_a, items_b):
    equals = len(items_b) == len(items_a)
    for item in items_a:
        equals = equals and item in items_b
    return equals


@behave.then(u'Task has the correct attributes for type "{task_type}"')
def step_impl(context, task_type):
    if task_type == "annotation":
        context.task = context.task.tasks.get(task_id=context.task.id)
    elif task_type == "qa":
        context.task = context.task.tasks.get(task_id=context.qa_task.id)
    for key, val in context.params.items():
        if key == 'filters':
            task_query = json.loads(context.task.query)
            task_query.pop('sort')
            assert task_query == val.prepare()
        elif key == 'items':
            task_items = [item.id for item in context.task.get_items().items]
            assert compare_items_list(items_a=task_items, items_b=[item.id for item in val])
        elif key == 'assignee_ids':
            task_assignments = [assignment.annotator for assignment in context.task.assignments.list()]
            assert compare_items_list(items_a=task_assignments, items_b=[v.lower() for v in val])
        elif key == 'workload':
            task_assignments = [assignment.annotator for assignment in context.task.assignments.list()]
            assert compare_items_list(items_a=task_assignments, items_b=[w_l.assignee_id for w_l in val])
        elif key == 'due_date':
            assert context.task.due_date == val * 1000
        elif key == 'task_name':
            assert val in context.task.name
        elif key == 'dataset':
            assert context.task.dataset_id == val.id
        elif key == 'project_id':
            assert context.task.project_id == val
        elif key == 'recipe_id':
            assert context.task.recipe_id == val
        elif key == 'metadata':
            if "system" in val.keys():
                assert val['system'].items() <= context.task.metadata['system'].items(), "TEST FAILED: Expected: {}, Got: {}".format(val['system'].items(),
                                                                                                                                     context.task.metadata['system'].items())
            else:
                assert context.task.metadata[list(val.keys())[0]] == val[list(val.keys())[0]], "TEST FAILED: Expected: {}, Got: {}".format(val[list(val.keys())[0]],
                                                                                                                                           context.task.metadata[list(val.keys())[0]])
        elif key == 'batch_size':
            task_assignments = [assignment.total_items for assignment in context.task.assignments.list()]
            assert all([int(val) == total for total in task_assignments])
        elif key == 'allowed_assignees':
            task_assignments = [assignment.annotator for assignment in context.task.assignments.list()]
            assert compare_items_list(items_a=task_assignments, items_b=val)


================================================
File: tests/features/steps/tasks_repo/test_tasks_delete.py
================================================
import behave


@behave.when(u'I delete task by "{delete_method}"')
def step_impl(context, delete_method):
    if delete_method == 'name':
        context.task_get = context.project.tasks.delete(task_name=context.task.name)
    elif delete_method == 'id':
        context.task_get = context.project.tasks.delete(task_id=context.task.id)
    elif delete_method == 'object':
        context.task_get = context.project.tasks.delete(task=context.task)


@behave.then(u'Task has been deleted')
def step_impl(context):
    try:
        context.project.tasks.get(taks_id=context.task.id)
        assert False
    except Exception:
        assert True


================================================
File: tests/features/steps/tasks_repo/test_tasks_get.py
================================================
import behave
import time


@behave.when(u'I get Task by "{get_method}"')
def step_impl(context, get_method):
    if get_method == 'name':
        context.task_get = context.project.tasks.get(task_name=context.task.name)
    elif get_method == 'id':
        context.task_get = context.project.tasks.get(task_id=context.task.id)


@behave.when(u'I get Task by wrong "{get_method}"')
def step_impl(context, get_method):
    try:
        if get_method == 'name':
            context.task_get = context.project.tasks.get(task_name='randomName')
        elif get_method == 'id':
            context.task_get = context.project.tasks.get(task_id='111111111111')
        context.error = None
    except Exception as e:
        context.error = e


@behave.then(u'Task received equals task created')
def step_impl(context):
    assert context.task.to_json() == context.task_get.to_json()


@behave.then(u'I expect Task created with "{total_items}" items')
def step_impl(context, total_items):
    num_try = 15
    interval = 10
    success = False

    for i in range(num_try):
        time.sleep(interval)
        if int(total_items) == context.task.get_items(get_consensus_items=True).items_count:
            success = True
            break

    assert success, "TEST FAILED: Task total expected : {} , Actual : {}".format(total_items, context.task.get_items(
        get_consensus_items=True).items_count)


================================================
File: tests/features/steps/tasks_repo/test_tasks_list.py
================================================
import behave
import datetime
import random


@behave.when(u'I list Tasks by param "{param}" value "{value}"')
def step_impl(context, param, value):
    kwargs = dict()
    if param == 'project_ids':
        project_ids = list()
        if value == 'current_project':
            project_ids.append(context.project.id)
        elif value == 'second_project':
            project_ids.append(context.second_project.id)
        elif value == 'both':
            project_ids.append(context.project.id)
            project_ids.append(context.second_project.id)
        kwargs['project_ids'] = project_ids
    elif param == 'recipe':
        project_ids = list()
        project_ids.append(context.project.id)
        project_ids.append(context.second_project.id)
        kwargs['project_ids'] = project_ids
        if value == 'current_project':
            for dataset in context.project.datasets.list():
                if dataset.name != 'Binaries':
                    recipe = dataset.get_recipe_ids()[0]
        elif value == 'second_project':
            for dataset in context.second_project.datasets.list():
                if dataset.name != 'Binaries':
                    recipe = dataset.get_recipe_ids()[0]
        else:
            raise Exception('Unknown type of recipe value')
        kwargs['recipe'] = recipe
    elif param == 'creator':
        project_ids = list()
        project_ids.append(context.project.id)
        project_ids.append(context.second_project.id)
        kwargs['project_ids'] = project_ids
        kwargs['creator'] = context.dl.info()['user_email']
    elif param in ['min_date', 'max_date']:
        project_ids = list()
        project_ids.append(context.project.id)
        project_ids.append(context.second_project.id)
        kwargs['project_ids'] = project_ids
        if value == '2_days_from_now':
            value = (datetime.datetime.today().timestamp() + (2 * 24 * 60 * 60)) * 1000
        elif value == '2_weeks_from_now':
            value = (datetime.datetime.today().timestamp() + (2 * 7 * 24 * 60 * 60)) * 1000
        elif value == 'today':
            value = (datetime.datetime.today().timestamp() + (60 * 60)) * 1000
        kwargs[param] = value
    else:
        kwargs[param] = value

    context.tasks_list = context.project.tasks.list(**kwargs)
    print(kwargs)


@behave.then(u'I receive a list of "{count}" tasks')
def step_impl(context, count):
    success = len(context.tasks_list) == int(count)
    for task in context.tasks_list:
        success = success and isinstance(task, context.dl.entities.Task)
    if not success:
        print(context.tasks_list)
        print(context.project.tasks.list())
    assert success, 'Tasks list is not as expected, expect: {} got: {}'.format(count, context.tasks_list)


@behave.given(u'There is a second project and dataset')
def step_impl(context):
    context.second_project = context.dl.projects.create(
        '{}-second-project-tasks-list'.format(random.randrange(100, 1000000)))
    context.second_dataset = context.second_project.datasets.create(
        'second_dataset_{}'.format(random.randrange(100, 1000000))
        , index_driver=context.index_driver_var)


================================================
File: tests/features/steps/tasks_repo/test_tasks_qa_task.py
================================================
import behave
from .. import fixtures


@behave.when(u'I create a qa Task')
def step_impl(context):
    context.params = {param.split('=')[0]: fixtures.get_value(params=param.split('='), context=context) for param in
                      context.table.headings if
                      fixtures.get_value(params=param.split('='), context=context) is not None}
    context.params['task'] = context.task
    context.qa_task = context.dataset.tasks.create_qa_task(**context.params)


@behave.then(u'I receive a qa task object')
def step_impl(context):
    assert isinstance(context.qa_task, context.dl.entities.Task)


@behave.then(u'Qa task is properly made')
def step_impl(context):
    assert context.qa_task.name == '{}_qa'.format(context.task.name)
    assert context.qa_task.spec['type'] == 'qa'


================================================
File: tests/features/steps/test_cache/test_cache.py
================================================
import os

import behave
import dtlpy as dl


@behave.then(u'i make project get and i get hit')
def step_impl(context):
    context.project = dl.projects.get(project_id=context.project.id)
    assert context.project.metadata['cache'] == 'hit'


@behave.then(u'i make dataset get and i get hit')
def step_impl(context):
    context.dataset = dl.datasets.get(dataset_id=context.dataset.id)
    assert context.dataset.metadata['cache'] == 'hit'


@behave.then(u'i make item get and i get hit')
def step_impl(context):
    context.item = dl.items.get(item_id=context.item.id)
    assert context.dataset._client_api.last_request.method != 'GET'


@behave.then(u'i make all item get and i get hit')
def step_impl(context):
    for item in context.item:
        item_get = dl.items.get(item_id=item.id)
        assert context.dataset._client_api.last_request.method != 'GET'


@behave.then(u'the lru is work')
def step_impl(context):
    path = os.path.join(dl.sdk_cache.cache_path_bin, 'items')
    files = os.listdir(path)
    assert len(files) < 100


@behave.when(u'set binary cache size to 10')
def step_impl(context):
    dl.sdk_cache.bin_size = 10


@behave.Given(u'cache is on')
def step_impl(context):
    dl.sdk_cache.use_cache = True


@behave.then(u'cache is off')
def step_impl(context):
    dl.sdk_cache.use_cache = False


@behave.then(u'i make annotation get and i get hit')
def step_impl(context):
    annotation = dl.annotations.get(annotation_id=context.annotation.id)
    assert annotation.metadata['cache'] == 'hit'


@behave.then(u'I upload a annotation for the item')
def step_impl(context):
    context.builder = context.item.annotations.builder()
    context.builder.add(annotation_definition=dl.Classification(label='test'))
    context.item.annotations.upload(context.builder)
    for ann in context.item.annotations.list():
        context.annotation = ann


@behave.then(u'i make dataset get and i get miss')
def step_impl(context):
    try:
        context.dataset = dl.datasets.get(dataset_id=context.dataset.id)
        assert False
    except:
        assert True


@behave.then(u'i make all item get and i get miss')
def step_impl(context):
    try:
        for item in context.item:
            item_get = dl.items.get(item_id=item.id)
        assert False
    except:
        assert True


@behave.then(u'i make item get and i get miss')
def step_impl(context):
    try:
        context.item = dl.items.get(item_id=context.item.id)
        assert False
    except:
        assert True


@behave.then(u'i make annotation get and i get miss')
def step_impl(context):
    try:
        context.annotation = dl.annotations.get(annotation_id=context.annotation.id)
        assert False
    except:
        assert True


@behave.then(u'I download the item')
def step_impl(context):
    context.download_path = context.item.download()


@behave.then(u'I git cache bin hit')
def step_impl(context):
    assert 'stream' not in str(context.dataset._client_api.last_request)
    assert os.path.isfile(context.download_path)


@behave.then(u'Item was updated')
def step_impl(context):
    assert context.item.metadata["system"]["modified"] == "True"


================================================
File: tests/features/steps/triggers_repo/test_pipeline_triggers_create.py
================================================
import dtlpy as dl
import behave
import time


@behave.when(u'I create a pipeline with 2 dataset nodes and trigger with filters')
def step_impl(context):
    params = dict()
    for row in context.table:
        params[row['key']] = row['value']

    t = time.localtime()
    current_time = time.strftime("%H-%M-%S", t)
    context.pipeline_name = 'pipeline-{}'.format(current_time)

    context.pipeline = context.project.pipelines.create(name=context.pipeline_name, project_id=context.project.id)

    dataset_node_1 = dl.DatasetNode(
        name="datsset-1",
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        dataset_folder="/pipeline",
        position=(1, 1)
    )

    dataset_node_2 = dl.DatasetNode(
        name="dataset-2",
        project_id=context.project.id,
        dataset_id=context.dataset.id,
        position=(2, 2),
        dataset_folder='/'
    )

    context.pipeline.nodes.add(dataset_node_1).connect(node=dataset_node_2)
    if params:  # Test delivered filters for trigger
        filters = dl.Filters()
        for key, val in params.items():
            filters.add(field=key, values=val)

        dataset_node_1.add_trigger(filters=filters)
    else:
        dataset_node_1.add_trigger()

    context.pipeline = context.pipeline.update()
    context.pipeline.install()

    time.sleep(5)


@behave.then(u'pipeline trigger created with filter params')
def step_impl(context):
    context.triggers = context.pipeline.triggers.list()[0][0].filters.get("$and")  # Get list of dictionaries
    context.triggers_keys = dict((key, dict_object[key]) for dict_object in context.triggers for key in dict_object).keys()

    row = context.table.headings[0]
    for val in row.split(', '):
        assert val.strip() in context.triggers_keys, "TEST FAILED: Missing {} in trigger filters\n{}".format(val.strip(), context.triggers_keys)


================================================
File: tests/features/steps/triggers_repo/test_triggers_annotation_update.py
================================================
import behave


@behave.when(u'I update annotation label with new name "{label_name}"')
@behave.then(u'I update annotation label with new name "{label_name}"')
def step_impl(context, label_name):
    context.annotation.label = label_name
    context.annotation.update(system_metadata=True)
    assert context.annotation.item.annotations.get(annotation_id=context.annotation.id).label in [label_name, "Edited"]



================================================
File: tests/features/steps/triggers_repo/test_triggers_context.py
================================================
import behave


@behave.when(u'I get the trigger from project number {project_index}')
def step_impl(context, project_index):
    context.trigger = context.projects[int(project_index) - 1].triggers.get(trigger_id=context.trigger.id)


@behave.when(u'I get the trigger from service number {project_index}')
def step_impl(context, project_index):
    context.trigger = context.services[int(project_index) - 1].triggers.get(trigger_id=context.trigger.id)


@behave.then(u'Trigger Project_id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.trigger.project_id == context.projects[int(project_index)-1].id


@behave.then(u'Trigger Project.id is equal to project {project_index} id')
def step_impl(context, project_index):
    assert context.trigger.project.id == context.projects[int(project_index)-1].id


@behave.then(u'Trigger Service_id is equal to service {service_index} id')
def step_impl(context, service_index):
    assert context.trigger.service_id == context.services[int(service_index) - 1].id


@behave.then(u'Trigger Service.id is equal to service {service_index} id')
def step_impl(context, service_index):
    assert context.trigger.service.id == context.services[int(service_index) - 1].id


================================================
File: tests/features/steps/triggers_repo/test_triggers_create.py
================================================
import datetime
import behave
import time
import os
import random


@behave.when(u"I create a trigger")
def step_impl(context):
    name = None
    filters = None
    resource = None
    actions = None
    active = None
    execution_mode = None
    function_name = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "name":
            if param[1] != "None":
                name = '{}-{}'.format(param[1], random.randrange(1000, 10000))
        elif param[0] == "filters":
            if param[1] != "None":
                filters = context.filters
        elif param[0] == "resource":
            if param[1] != "None":
                resource = param[1]
        elif param[0] == "action":
            if param[1] != "None":
                actions = param[1]
        elif param[0] == "active":
            active = param[1] == "True"
        elif param[0] == "executionMode":
            if param[1] != "None":
                execution_mode = param[1]
        elif param[0] == "function_name":
            if param[1] != "None":
                function_name = param[1]

    params_temp = {'name': name, 'filters': filters, 'resource': resource, 'actions': actions, 'active': active,
                   'execution_mode': execution_mode, 'function_name': function_name, 'service_id': context.service.id}

    params = {k: v for k, v in params_temp.items() if v is not None}
    try:
        context.trigger = context.service.triggers.create(**params)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u"I create a cron trigger")
def step_impl(context):
    function_name = None
    name = None
    cron = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "name":
            if param[1] != "None":
                name = '{}-{}'.format(param[1], random.randrange(1000, 10000))
        elif param[0] == "function_name":
            if param[1] != "None":
                function_name = param[1]
        elif param[0] == "cron":
            if param[1] != "None":
                cron = param[1]
            else:
                cron = "0 5 * * *"

    context.trigger = context.service.triggers.create(function_name=function_name,
                                                      trigger_type="Cron",
                                                      name=name,
                                                      start_at=datetime.datetime.now().isoformat(),
                                                      end_at=datetime.datetime(2024, 8, 23).isoformat(),
                                                      cron=cron)


@behave.given(u"I create a cron trigger")
def step_impl(context):
    function_name = None
    name = None
    cron = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "name":
            if param[1] != "None":
                name = '{}-{}'.format(param[1], random.randrange(1000, 10000))
        elif param[0] == "function_name":
            if param[1] != "None":
                function_name = param[1]
        elif param[0] == "cron":
            if param[1] != "None":
                cron = param[1]
            else:
                cron = "* 5 * * *"

    context.trigger = context.service.triggers.create(function_name=function_name,
                                                      trigger_type="Cron",
                                                      name=name,
                                                      start_at=datetime.datetime.now().isoformat(),
                                                      cron=cron)


@behave.then(u'I receive a Trigger entity')
def step_impl(context):
    assert isinstance(context.trigger, context.dl.entities.Trigger)


@behave.then(u'I receive a CronTrigger entity')
def step_impl(context):
    assert isinstance(context.trigger, context.dl.entities.CronTrigger)


@behave.when(u'I set the trigger in the context')
def step_impl(context):
    composition = context.project.compositions.get(composition_id=context.app.composition_id)
    context.trigger = context.project.triggers.get(trigger_id=composition["triggers"][0]["triggerId"])
    context.service = context.trigger.service


@behave.when(u'I set the execution in the context')
def step_impl(context):
    num_try = 6
    interval = 2
    for i in range(num_try):
        time.sleep(interval)
        if context.service.executions.list().items_count > 0:
            context.execution = context.service.executions.list().items[0]
            break


@behave.when(u'I set code path "{path}" to context')
def step_impl(context, path):
    context.codebase_local_dir = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], path)


@behave.then(u'I receive a Trigger entity with function "{function_name}"')
def step_impl(context, function_name):
    assert isinstance(context.trigger, context.dl.entities.Trigger)
    assert context.trigger.function_name == function_name


@behave.then(u'Service was triggered on "{resource_type}"')
def step_impl(context, resource_type):
    context.trigger_type = resource_type
    num_try = 36
    interval = 5
    triggered = False

    for i in range(num_try):
        time.sleep(interval)
        if resource_type == 'item':
            filters = context.dl.Filters(resource=context.dl.FiltersResource.EXECUTION)
            filters.add(field='serviceId', values=context.service.id)
            filters.add(field='resources.id', values=context.uploaded_item_with_trigger.id)
            execution_page = context.service.executions.list(filters=filters)
            if execution_page.items_count == 1:
                triggered = True
                break
        elif resource_type == 'itemclone':
            filters = context.dl.Filters(resource=context.dl.FiltersResource.EXECUTION)
            filters.add(field='serviceId', values=context.service.id)
            filters.add(field='resources.id', values=context.uploaded_item_with_trigger.id)
            execution_page = context.service.executions.list(filters=filters)
            if execution_page.items_count > 0:
                triggered = True
                break
        elif resource_type == 'annotation':
            annotation = context.annotation.item.annotations.get(annotation_id=context.annotation.id)
            if annotation.label == "Edited":
                triggered = True
                break
        elif resource_type == 'dataset':
            if context.service.executions.list().items_count == 1:
                execution = context.service.executions.list()[0][0]
                assert resource_type in execution.input.keys()
                triggered = True
                break
        elif resource_type == 'task':
            context.task = context.project.tasks.get(task_id=context.task.id)
            if context.task.name == "name updated by trigger":
                triggered = True
                break
        elif resource_type == 'assignment':
            context.assignment = context.project.assignments.get(assignment_id=context.assignment.id)
            if "name-updated" in context.assignment.name:
                triggered = True
                break
        elif resource_type == 'hidden-item':
            item = context.dataset.items.get(item_id=context.uploaded_item_with_trigger.id)
            if len(item.resource_executions.list()) > 0 and item.resource_executions.list()[0][0].service_name == context.service.name:
                triggered = True
                break
        elif resource_type == 'string':
            execution = None
            pages = context.service.executions.list()
            executions = pages.items
            if len(executions) > 0:
                execution = executions[0]
            if execution is not None and len(execution.input) > 0:
                context.execution = context.service.executions.list()[0][0]
                triggered = True
                break
        context.dl.logger.debug("Step is running for {:.2f}[s] and now Going to sleep {:.2f}[s]".format((i + 1) * interval,
                                                                                                        interval))

    assert triggered, f"TEST FAILED: After {round(num_try * interval / 60, 1)} minutes"


@behave.given(u'There is a package (pushed from "{package_path}") by the name of "{package_name}"')
def step_impl(context, package_name, package_path):
    package_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], package_path)
    package_name = '{}-{}'.format(package_name, random.randrange(1000, 10000))
    context.package = context.project.packages.push(src_path=package_path, package_name=package_name)
    context.to_delete_packages_ids.append(context.package.id)
    assert isinstance(context.package, context.dl.entities.Package)


@behave.given(u'There is a package (pushed from "{package_path}") with function "{function_name}"')
def step_impl(context, function_name, package_path):
    package_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], package_path)
    package_name = '{}-{}'.format(function_name, random.randrange(1000, 10000))

    function_io = [context.dl.FunctionIO(name='item', type=context.dl.PackageInputType.ITEM)]
    function = context.dl.PackageFunction(name=function_name, inputs=function_io)
    module = context.dl.PackageModule(name='default_module', functions=[function], entry_point='main.py')

    context.package = context.project.packages.push(src_path=package_path,
                                                    package_name=package_name,
                                                    modules=module)
    context.to_delete_packages_ids.append(context.package.id)
    assert isinstance(context.package, context.dl.entities.Package)


@behave.given(
    u'There is a service by the name of "{service_name}" with module name "{module_name}" saved to context "{service_attr_name}"')
def step_impl(context, service_name, module_name, service_attr_name):
    service_name = '{}-{}'.format(service_name, random.randrange(1000, 10000))
    runtime = context.dl.KubernetesRuntime(autoscaler=context.dl.KubernetesAutoscaler(min_replicas=1)).to_json()
    setattr(context, service_attr_name, context.package.services.deploy(service_name=service_name,
                                                                        bot=context.bot_user,
                                                                        runtime=runtime,
                                                                        module_name=module_name))
    context.to_delete_services_ids.append(getattr(context, service_attr_name).id)
    assert isinstance(getattr(context, service_attr_name), context.dl.entities.Service)


@behave.given(
    u'There is service by the name of "{service_name}" with module name "{module_name}" and execution_timeout of "{execution_timeout}" saved to context "{service_attr_name}"')
def step_impl(context, service_name, module_name, execution_timeout, service_attr_name):
    service_name = '{}-{}'.format(service_name, random.randrange(1000, 10000))
    runtime = context.dl.KubernetesRuntime(autoscaler=context.dl.KubernetesAutoscaler(min_replicas=1)).to_json()

    setattr(context, service_attr_name, context.package.services.deploy(service_name=service_name,
                                                                        bot=context.bot_user,
                                                                        runtime=runtime,
                                                                        on_reset='rerun',
                                                                        execution_timeout=int(execution_timeout),
                                                                        module_name=module_name))
    context.to_delete_services_ids.append(getattr(context, service_attr_name).id)
    assert isinstance(getattr(context, service_attr_name), context.dl.entities.Service)



@behave.given(
    u'There is a service with max_attempts of "{max_attempts}" by the name of "{service_name}" with module name "{module_name}" saved to context "{service_attr_name}"')
def step_impl(context, service_name, module_name, service_attr_name, max_attempts):
    max_attempts = int(max_attempts)
    service_name = '{}-{}'.format(service_name, random.randrange(1000, 10000))
    runtime = {"gpu": False, "numReplicas": 1, 'concurrency': 1}
    setattr(
        context, service_attr_name, context.package.services.deploy(
            service_name=service_name,
            bot=context.bot_user,
            runtime=runtime,
            execution_timeout=30,
            module_name=module_name,
            max_attempts=max_attempts
        )
    )
    context.to_delete_services_ids.append(getattr(context, service_attr_name).id)
    assert isinstance(getattr(context, service_attr_name), context.dl.entities.Service)


@behave.when(u"I edit item user metadata")
def step_impl(context):
    time.sleep(3)
    if 'user' not in context.uploaded_item_with_trigger.metadata:
        context.uploaded_item_with_trigger.metadata['user'] = dict()
    context.uploaded_item_with_trigger.metadata['user']['edited'] = True
    context.uploaded_item_with_trigger.update()


@behave.when(u"I annotate item")
def step_impl(context):
    time.sleep(3)
    if hasattr(context, 'uploaded_item_with_trigger'):
        annotation = context.dl.Annotation.new(annotation_definition=context.dl.Point(y=200, x=200, label='dog'),
                                               item=context.uploaded_item_with_trigger)
    else:
        annotation = context.dl.Annotation.new(annotation_definition=context.dl.Point(y=200, x=200, label='dog'),
                                               item=context.item)
    context.annotation = annotation.upload()
    assert isinstance(context.annotation, context.dl.entities.Annotation)


================================================
File: tests/features/steps/triggers_repo/test_triggers_delete.py
================================================
import behave, time


@behave.when(u'I delete trigger by "{deletion_format}"')
def step_impl(context, deletion_format):
    if deletion_format == 'id':
        assert context.service.triggers.delete(trigger_id=context.trigger.id)
    elif deletion_format == 'name':
        assert context.service.triggers.delete(trigger_name=context.trigger.name)
    else:
        assert context.trigger.delete()
    time.sleep(3)


@behave.then(u"There are no triggers")
def step_impl(context):
    assert context.service.triggers.list().items_count == 0


@behave.then(u'I validate deleted action trigger on "{resource_type}"')
def step_impl(context, resource_type):

    assert context.service.executions.list().items_count == 1, "No executions in service"

    context.trigger_type = resource_type
    num_try = 60
    interval = 10
    triggered = False

    for i in range(num_try):
        time.sleep(interval)
        if resource_type == 'item':
            if context.service.executions.list()[0][0].input['item']['item_id'] == context.item.id:
                triggered = True
                break
        elif resource_type == 'annotation':
            if context.service.executions.list()[0][0].input['annotation']['annotation_id'] == context.annotation.id:
                triggered = True
                break

    assert triggered, f"TEST FAILED: After {round(num_try * interval / 60, 1)} minutes"


================================================
File: tests/features/steps/triggers_repo/test_triggers_get.py
================================================
import behave
import random


@behave.given(u"I create a trigger")
def step_impl(context):
    name = None
    filters = None
    resource = None
    actions = None
    active = None
    execution_mode = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "name":
            if param[1] != "None":
                name = '{}-{}'.format(param[1], random.randrange(1000, 10000))
        elif param[0] == "filters":
            if param[1] != "None":
                filters = context.filters
        elif param[0] == "resource":
            if param[1] != "None":
                resource = param[1]
        elif param[0] == "action":
            if param[1] != "None":
                actions = param[1]
        elif param[0] == "active":
            active = param[1] == "True"
        elif param[0] == "executionMode":
            if param[1] != "None":
                execution_mode = param[1]

    context.trigger = context.service.triggers.create(
        service_id=context.service.id,
        name=name,
        filters=filters,
        resource=resource,
        actions=actions,
        active=active,
        execution_mode=execution_mode,
    )


@behave.when(u"I get trigger by id")
def step_impl(context):
    context.trigger_get = context.service.triggers.get(trigger_id=context.trigger.id)


@behave.when(u"I get trigger by name")
def step_impl(context):
    context.trigger_get = context.service.triggers.get(trigger_name=context.trigger.name)


@behave.then(u"I receive a Trigger object")
def step_impl(context):
    assert isinstance(context.trigger_get, context.dl.entities.Trigger)


@behave.then(u"I receive a Cron Trigger object")
def step_impl(context):
    assert isinstance(context.trigger_get, context.dl.entities.trigger.CronTrigger)


@behave.then(u"Trigger received equals to trigger created")
def step_impl(context):
    assert context.trigger_get.to_json() == context.trigger.to_json()


================================================
File: tests/features/steps/triggers_repo/test_triggers_item_update.py
================================================
import behave
import time


@behave.then(u'Service was triggered on "{resource_type}" again')
def step_impl(context, resource_type):
    context.trigger_type = resource_type
    num_try = 30
    interval = 10
    triggered = False

    for i in range(num_try):
        time.sleep(interval)
        if resource_type == 'item':
            filters = context.dl.Filters(resource=context.dl.FiltersResource.EXECUTION)
            filters.add(field='serviceId', values=context.service.id)
            filters.add(field='resources.id', values=context.uploaded_item_with_trigger.id)
            execution_page = context.service.executions.list(filters=filters)
            if execution_page.items_count == 2:
                triggered = True
                break
        elif resource_type == 'annotation':
            item = context.annotation.item.annotations.get(annotation_id=context.annotation.id)
            if item.label == "Edited":
                triggered = True
                break
        elif resource_type == 'dataset':
            if context.service.executions.list().items_count == 2:
                execution = context.service.executions.list()[0][0]
                assert resource_type in execution.input.keys()
                execution = context.service.executions.list()[0][1]
                assert resource_type in execution.input.keys()
                triggered = True
                break
        elif resource_type == 'task':
            if context.service.executions.list().items_count == 2:
                execution = context.service.executions.list()[0][0]
                assert resource_type in execution.input.keys()
                execution = context.service.executions.list()[0][1]
                assert resource_type in execution.input.keys()
                triggered = True
                break
        elif resource_type == 'assignment':
            if context.service.executions.list().items_count == 2:
                execution = context.service.executions.list()[0][0]
                assert resource_type in execution.input.keys()
                execution = context.service.executions.list()[0][1]
                assert resource_type in execution.input.keys()
                triggered = True
                break

    assert triggered, f"TEST FAILED: After {round(num_try * interval / 60, 1)} minutes"


================================================
File: tests/features/steps/triggers_repo/test_triggers_list.py
================================================
import behave


@behave.when(u"I list triggers")
def step_impl(context):
    context.trigger_list = context.service.triggers.list()


@behave.then(u'I receive a Trigger list of "{count}" objects')
def step_impl(context, count):
    assert context.trigger_list.items_count == int(count)
    if int(count) > 0:
        for page in context.trigger_list:
            for trigger in page:
                assert isinstance(trigger, context.dl.entities.Trigger) or \
                       isinstance(trigger, context.dl.entities.trigger.CronTrigger)


================================================
File: tests/features/steps/triggers_repo/test_triggers_update.py
================================================
import io
import time

import behave
import json
import datetime
from PIL import Image
import os


def update_trigger(context):
    filters = None
    resource = None
    actions = None
    active = None
    execution_mode = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "filters":
            if param[1] != "None":
                filters = json.loads(param[1])
        elif param[0] == "resource":
            if param[1] != "None":
                resource = param[1]
        elif param[0] == "action":
            if param[1] != "None":
                actions = [param[1]]
        elif param[0] == "active":
            active = param[1] == "True"
        elif param[0] == "executionMode":
            if param[1] != "None":
                execution_mode = param[1]

    if filters is not None:
        context.trigger.filters = filters
    if resource is not None:
        context.trigger.resource = resource
    if actions is not None:
        context.trigger.actions = actions
    if active is not None:
        context.trigger.active = active
    if execution_mode is not None:
        context.trigger.execution_mode = execution_mode

    context.trigger_update = context.trigger.update()


@behave.when(u"I try to update trigger")
def step_impl(context):
    try:
        update_trigger(context=context)
        context.error = None
    except Exception as e:
        context.error = e


@behave.when(u"I update trigger")
def step_impl(context):
    update_trigger(context=context)


@behave.then(u"Trigger attributes are modified")
def step_impl(context):
    filters = None
    resource = None
    actions = None
    active = None

    params = context.table.headings
    for param in params:
        param = param.split("=")
        if param[0] == "filters":
            if param[1] != "None":
                filters = json.loads(param[1])
        elif param[0] == "resource":
            if param[1] != "None":
                resource = param[1]
        elif param[0] == "action":
            if param[1] != "None":
                actions = [param[1]]
        elif param[0] == "active":
            active = param[1] == "True"

    assert context.trigger_update.filters == filters if filters is not None else True
    assert context.trigger_update.resource == resource if resource is not None else True
    assert context.trigger_update.actions == actions if actions is not None else True
    assert context.trigger_update.active == active if active is not None else True


@behave.then(u"I receive an updated Trigger object")
def step_impl(context):
    assert isinstance(context.trigger_update, context.dl.entities.Trigger)


def upload_item(context, directory):
    item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], '0000000162.jpg')
    image = Image.open(item_local_path)
    buffer = io.BytesIO()
    image.save(buffer, format='jpeg')
    buffer.name = 'item-{}'.format(datetime.datetime.now().isoformat().replace('.', '-').replace(':', '-'))
    return context.dataset.items.upload(
        local_path=buffer,
        remote_path=directory
    )


def executed_on_item(e, i):
    return e.input['item'] == i.id if isinstance(e.input['item'], str) else e.input['item']['item_id'] == i.id


@behave.then(u'Trigger works only on items in "{directory}"')
def step_impl(context, directory: str):

    item_should_not_work = upload_item(context=context, directory='/some_other_dir')
    item_should_work = upload_item(context=context, directory=directory)

    time.sleep(15)

    executions = context.service.executions.list()
    success = False
    for execution in executions.items:
        if executed_on_item(execution, item_should_not_work):
            assert False, 'Trigger created execution for item that does not pass filter'
        elif executed_on_item(execution, item_should_work):
            success = True

    assert success, "Service was not executed on item that passes filter" if not success else ''


@behave.then(u'Trigger is inactive')
def step_impl(context):
    item = upload_item(context=context, directory='/some_other_dir')
    time.sleep(30)
    executions = context.service.executions.list()
    for execution in executions.all():
        if executed_on_item(execution, item):
            assert False, 'Inactive trigger created execution for item that does not pass filter'


@behave.then(u"Trigger works only on items updated")
def step_impl(context):
    # important - this should be a json item that does not trigger global functions
    item_local_path = os.path.join(os.environ["DATALOOP_TEST_ASSETS"], 'classes_new.json')
    item = context.dataset.items.upload(local_path=item_local_path)

    time.sleep(20)
    executions = context.service.executions.list()
    for execution in executions.all():
        if executed_on_item(execution, item):
            assert False, 'Service was executed on item created'

    item.metadata['user'] = {'key': 'val'}
    item.update()

    time.sleep(20)
    executions = context.service.executions.list()
    success = False
    for execution in executions.all():
        if executed_on_item(execution, item):
            success = True

    assert success, 'Updating item did not trigger service' if not success else ''


================================================
File: tests/features/steps/utilities/platform_interface_steps.py
================================================
import attr
import behave
import time
import jwt
import os
import shutil
import dtlpy as dl
import numpy as np
from behave_testrail_reporter import TestrailReporter
import filecmp
import json
from .. import fixtures

dl.verbose.disable_progress_bar = True

try:
    # for local import
    from tests.env_from_git_branch import get_env_from_git_branch
except ImportError:
    # for remote import
    from env_from_git_branch import get_env_from_git_branch


@attr.s
class TimeKey:
    # For TestRail test-run
    _key = None

    @property
    def key(self):
        if self._key is None:
            self._key = time.strftime("%d-%m %H:%M")
        return self._key


time_key = TimeKey()


@behave.given('Platform Interface is initialized as dlp and Environment is set according to git branch')
def before_all(context):
    # set up lists to delete
    if not hasattr(context, 'to_delete_packages_ids'):
        context.to_delete_packages_ids = list()
    if not hasattr(context, 'to_delete_services_ids'):
        context.to_delete_services_ids = list()
    if not hasattr(context, 'to_delete_projects_ids'):
        context.to_delete_projects_ids = list()
    if not hasattr(context, 'to_delete_pipelines_ids'):
        context.to_delete_pipelines_ids = list()
    if not hasattr(context, 'to_delete_feature_set_ids'):
        context.to_delete_feature_set_ids = list()
    if not hasattr(context, 'to_delete_feature_ids'):
        context.to_delete_feature_ids = list()
    if not hasattr(context.feature, 'to_delete_integrations_ids'):
        context.feature.to_delete_integrations_ids = list()
    if not hasattr(context, 'to_delete_drivers_ids'):
        context.to_delete_drivers_ids = list()
    if not hasattr(context, 'to_delete_datasets_ids'):
        context.to_delete_datasets_ids = list()
    if not hasattr(context, 'to_delete_computes_ids'):
        context.to_delete_computes_ids = list()
    if not hasattr(context, 'to_delete_service_drivers_ids'):
        context.to_delete_service_drivers_ids = list()
    if not hasattr(context, 'nodes'):
        context.nodes = list()
    if hasattr(context.feature, 'dataloop_feature_dl'):
        context.dl = context.feature.dataloop_feature_dl
    else:
        # get cookie name
        # feature_name = context.feature.name.replace(' ', '_')
        # api_counter_name = 'api_counter_{}.json'.format(feature_name)
        # api_counter_filepath = os.path.join(os.path.dirname(dl.client_api.cookie_io.COOKIE), api_counter_name)
        # # set counter
        # dl.client_api.set_api_counter(api_counter_filepath)

        # set context for run
        context.dl = dl

        # reset api counter
        # context.dl.client_api.calls_counter.on()
        # context.dl.client_api.calls_counter.reset()

        # set env to dev
        _, base_env = get_env_from_git_branch()
        if base_env != dl.client_api.environments[dl.client_api.environment]['alias']:
            if base_env not in ['rc', 'prod', 'env1', 'piper']:
                context.dl.setenv('custom')
            else:
                context.dl.setenv(base_env)

        # check token
        payload = None
        for i in range(10):
            try:
                payload = jwt.decode(context.dl.token(), algorithms=['HS256'],
                                     verify=False, options={'verify_signature': False})
                break
            except jwt.exceptions.DecodeError:
                time.sleep(np.random.rand())
                pass

        allow_locally_with_user = os.environ.get('ALLOW_RUN_TESTS_LOCALLY_WITH_USER', 'false') == 'true'

        if not allow_locally_with_user and payload['email'] not in ['oa-test-4@dataloop.ai', 'oa-test-1@dataloop.ai',
                                                                    'oa-test-2@dataloop.ai',
                                                                    'oa-test-3@dataloop.ai']:
            assert False, 'Cannot run test on user: "{}". only test users'.format(payload['email'])

        # save to feature level
        context.feature.dataloop_feature_dl = context.dl

        avoid_testrail = os.environ.get('AVOID_TESTRAIL', 'true') == 'true'

        if not avoid_testrail and len(context.config.reporters) == 1:
            import sys
            build_number = os.environ.get('BITBUCKET_BUILD_NUMBER')
            _, base_env = get_env_from_git_branch()
            current_branch = "{} - #{} Python {}".format(base_env, str(build_number), sys.version.split(" ")[0])  # Get the current build branch
            testrail_reporter = TestrailReporter(current_branch)
            context.config.reporters.append(testrail_reporter)


@behave.given(u'I create the dir path "{directory}"')
def step_impl(context, directory):
    context.new_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], directory)
    try:
        if not os.path.exists(context.new_path):
            os.mkdir(context.new_path)
    except Exception as e:
        assert 'File exists' in e.args


@behave.then(u'I delete content in path path "{directory}"')
def step_impl(context, directory):
    context.content_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], directory)

    for filename in os.listdir(context.content_path):
        file_path = os.path.join(context.content_path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print('Failed to delete %s. Reason: %s' % (file_path, e))


def compare_directory_files(dir1, dir2):
    """
    Compare two directories recursively. Files in each directory are
    assumed to be equal if their names and contents are equal.

    @param dir1: First directory path
    @param dir2: Second directory path

    @return: True if the directory trees are the same and
        there were no errors while accessing the directories or files,
        False otherwise.
   """

    dirs_cmp = filecmp.dircmp(dir1, dir2)
    if len(dirs_cmp.left_only) > 0 or len(dirs_cmp.right_only) > 0 or len(dirs_cmp.funny_files) > 0:
        return False
    (_, mismatch, errors) = filecmp.cmpfiles(
        dir1, dir2, dirs_cmp.common_files, shallow=False)
    if len(mismatch) > 0 or len(errors) > 0:
        return False
    for common_dir in dirs_cmp.common_dirs:
        new_dir1 = os.path.join(dir1, common_dir)
        new_dir2 = os.path.join(dir2, common_dir)
        if not compare_directory_files(new_dir1, new_dir2):
            return False
    return True


@behave.then(u'I compare between the dirs')
def step_impl(context):
    context.dir1 = None
    context.dir2 = None

    for parameter in context.table.rows:
        if parameter.cells[0] == "dir1":
            context.dir1 = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

        if parameter.cells[0] == "dir2":
            context.dir2 = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], parameter.cells[1])

    assert compare_directory_files(context.dir1, context.dir2)


@behave.when(u'I send "{type}" gen_request with "{key}" params')
@behave.when(u'I send "{type}" gen_request with "{key}" params and interval: "{interval}" tries: "{num_try}"')
@behave.then(u'I validate gen_request "{type}" with "{key}" params and interval: "{interval}" tries: "{num_try}" response "{response}"')
def step_impl(context, type, key, interval=0, num_try=1, response=None):
    file_path = os.path.join(os.environ['DATALOOP_TEST_ASSETS'], "api", "api_assets.json")
    with open(file_path, 'r') as file:
        json_obj = json.load(file)

    context.req = json_obj.get(key)

    context.response = fixtures.gen_request(context=context, method=type, req=context.req, num_try=int(num_try), interval=int(interval), expected_response=response)
    if response:
        assert response in context.response.text, "TEST FAILED: Expected response to include {}. Actual got {}".format(response, context.response.text)


@behave.then(u'I validate no error in context')
def step_impl(context):
    assert context.error is None, f"TEST FAILED: Expected no error, Actual got {context.error}"


================================================
File: tests/features/steps/webm_converter/test_failed_video_message.py
================================================
from behave import when, then
import time
import dtlpy as dl
import json


@when(u'I wait for video services to finish')
def step_impl(context):
    context.item = context.dataset.items.get(item_id=context.item.id)
    num_try = 60
    interval = 10
    finished = False

    context.filters = context.dl.Filters(resource=dl.FiltersResource.EXECUTION)
    context.filters.add(field="resources.type", values="Item")
    context.filters.add(field="resources.id", values=context.item.id)

    for i in range(num_try):
        time.sleep(interval)
        executions_list = context.project.executions.list(filters=context.filters).items
        if {"success"} == set([exec.latest_status['status'] for exec in executions_list]):
            finished = True
            break

    assert finished, f"TEST FAILED: After {round(num_try * interval / 60, 1)} minutes"


@then(u'I validate execution status item metadata have the right message')
def step_impl(context):
    context.item = context.dataset.items.get(item_id=context.item.id)
    num_try = 6
    interval = 10
    has_error = False

    for i in range(num_try):
        time.sleep(interval)
        if 'errors' in context.item.metadata['system'].keys():
            error_message = [{"type": "origExpectedFrames",
                              "message": "Frames is not equal to FPS * (Duration - StartTime)", "value": 3,
                              "service": "VideoPreprocess"}]
            error = json.dumps(context.item.metadata['system']['errors'])
            err_message = "TEST FAILED: Wrong error message.\nExpected: {}\nGot: {}".format(
                error_message,
                context.item.metadata['system']['errors']
            )
            assert 'origExpectedFrames' in error, err_message
            assert 'Frames is not equal to FPS * (Duration - StartTime)' in error, err_message

            has_error = True
            break

    return has_error


================================================
File: tests/features/steps/xray/test_xray.py
================================================
import behave


@behave.given(u'Some initial setup')
def before_all(context):
    return None


@behave.when(u'creating foo')
def creating_foo(context):
    return None


@behave.when(u'creating bar')
def creating_bar(context):
    return None


@behave.then(u'return foo')
def return_foo(context):
    return None


@behave.then(u'return bar')
def return_bar(context):
    return None


@behave.then(u'foo')
def foo(context):
    assert True == True


@behave.given(u'existing foo')
def existing_foo(context):
    return None


================================================
File: tests/features/tasks_repo/task_consensus_create.feature
================================================
Feature: Tasks repository create consensus task

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "tasks_consensus"
    And I create a dataset with a random name
    And There are "10" items
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @testrail-C4533547
  @DAT-46620
  Scenario: Create consensus task with 100 percentage should success
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto | consensus_percentage=auto | consensus_assignees=auto |
    And I get app by name "scoring-and-metrics"
    And I add app to context.feature.apps
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"
    And I expect Task created with "30" items

  @testrail-C4533547
  @DAT-46620
  Scenario: Create consensus task with 50 percentage should success
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto | consensus_percentage=50 | consensus_assignees=auto |
    And I get app by name "scoring-and-metrics"
    And I add app to context.feature.apps
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"
    And I expect Task created with "20" items

  @DAT-51555
  Scenario: Create qualification task with 100 percentage should success
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto | consensus_task_type=qualification | consensus_percentage=auto | consensus_assignees=auto |
    And I get app by name "scoring-and-metrics"
    And I add app to context.feature.apps
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"
    And I expect Task created with "30" items


================================================
File: tests/features/tasks_repo/task_qualification_create.feature
================================================
Feature: Tasks repository create qualification task

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "tasks_consensus"
    And I create a dataset with a random name
    And There are "10" items
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @DAT-50209
  Scenario: Create qualification task for 10 items - Should create task with 30 items (20 clones, 10 originals)
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto | metadata={"system":{"consensusTaskType":"qualification"}} |
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"
    And I expect Task created with "30" items

================================================
File: tests/features/tasks_repo/test_task_context.feature
================================================
Feature: Task repository Context testing

    Background: Initiate Platform Interface and create a projects and datasets
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "project1 project2"
        And I create datasets by the name of "dataset1 dataset2"
        And I upload items in "0000000162.jpg" to datasets
        When Add Members "annotator1@dataloop.ai" as "annotator" to project 1
        And Add Members "annotator2@dataloop.ai" as "annotator" to project 1
        Given I create task belong to dataset 1
            | task_name=context_task_test | due_date=auto | assignee_ids=auto |

    @testrail-C4523172
    @DAT-46627
    Scenario: Get Task from the Project it belong to
        When I get the task from project number 1
        Then task Project_id is equal to project 1 id
        And task Project.id is equal to project 1 id
        And task dataset_id is equal to dataset 1 id
        And task dataset.id is equal to dataset 1 id

    @testrail-C4523172
    @DAT-46627
    Scenario: Get Task from the Project it is not belong to
        When I get the task from project number 2
        Then task Project_id is equal to project 1 id
        And task Project.id is equal to project 1 id
        And task dataset_id is equal to dataset 1 id
        And task dataset.id is equal to dataset 1 id

    @testrail-C4523172
    @DAT-46627
    Scenario: Get Task from the Dataset it belong to
        When I get the task from dataset number 1
        Then task Project_id is equal to project 1 id
        And task Project.id is equal to project 1 id
        And task dataset_id is equal to dataset 1 id
        And task dataset.id is equal to dataset 1 id

        @testrail-C4523172
        @DAT-46627
        Scenario: Get Task from the Dataset it not belong to
          When I get the task from dataset number 2
          Then task Project_id is equal to project 1 id
          And task Project.id is equal to project 1 id
          And task dataset_id is equal to dataset 1 id
          And task dataset.id is equal to dataset 1 id


================================================
File: tests/features/tasks_repo/test_task_priority.feature
================================================
Feature: Tasks repository priorities

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "tasks_create"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
        And I save dataset items to context

    @testrail-C4532755
    @DAT-46628
    Scenario: Create - LOW priority task
        When I create Task with priority "TaskPriority.LOW"
        Then Task with priority "LOW" got created
        When I create Task with priority "TASK_PRIORITY_LOW"
        Then Task with priority "LOW" got created

    @testrail-C4532755
    @DAT-46628
    Scenario: Create - MEDIUM priority task
        When I create Task with priority "TaskPriority.MEDIUM"
        Then Task with priority "MEDIUM" got created
        When I create Task with priority "TASK_PRIORITY_MEDIUM"
        Then Task with priority "MEDIUM" got created

    @testrail-C4532755
    @DAT-46628
    Scenario: Create - HIGH priority task
        When I create Task with priority "TaskPriority.HIGH"
        Then Task with priority "HIGH" got created
        When I create Task with priority "TASK_PRIORITY_HIGH"
        Then Task with priority "HIGH" got created



================================================
File: tests/features/tasks_repo/test_task_pulling.feature
================================================
Feature: Tasks repository create pulling task

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "tasks_pulling"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
    And I save dataset items to context

   @testrail-C4533548
   @DAT-46629
   Scenario: Create - pulling task
       When I create Task
           | task_name=pulling_task | due_date=auto | batch_size=3 | allowed_assignees=auto | max_batch_workload=5 |
       Then I receive a task entity
       And Task has the correct attributes for type "annotation"


================================================
File: tests/features/tasks_repo/test_tasks_add_and_get_items.feature
================================================
Feature: Tasks repository add/get items method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "tasks_get_add_items"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"

    @testrail-C4523166
    @DAT-46621
    Scenario: Get items - by name
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        And I get Task items by "name"
        Then I receive task items list of "12" items

    @testrail-C4523166
    @DAT-46621
    Scenario: Get items - by id
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        And I get Task items by "id"
        Then I receive task items list of "12" items

    @testrail-C4523166
    @DAT-46621
    Scenario: Add items
      When I create Task
          | task_name=min_params | due_date=auto | assignee_ids=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": true}]}} |
      And I add items to task
          | task_id=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": false}]}} | assignee_ids=auto |
      And I get Task items by "id"
      Then I receive task items list of "12" items


================================================
File: tests/features/tasks_repo/test_tasks_assignments.feature
================================================
Feature: Tasks assignments

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "tasks_test"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | metadata={"user.good": 3, "user.bad": 3} |
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @DAT-69267
  Scenario: task without batchNode - Should delete empty assignments
    When I create Task
      | task_name=batchNode | assignee_ids=auto | items=None |
    Then I receive a task entity
    When I add items to task
      | task_id=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": false}]}} | assignee_ids=auto |
    And I remove items from task
      | task_id=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": false}]}} |
    Then Task has "0" assignments

  @DAT-57964
  Scenario: task with batchNode - Shouldn't delete empty assignments
    When I create Task
      | task_name=batchNode | assignee_ids=auto | metadata={"type": "batchNode"} | items=None |
    Then I receive a task entity
    When I add items to task
      | task_id=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": false}]}} | assignee_ids=auto |
    And I remove items from task
      | task_id=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": false}]}} |
    Then Task has "2" assignments

================================================
File: tests/features/tasks_repo/test_tasks_create.feature
================================================
Feature: Tasks repository create method testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "tasks_create"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
    And I save dataset items to context
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @testrail-C4523167
  @DAT-46622
  Scenario: Create - minimum params
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto |
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"

  @testrail-C4523167
  @DAT-46622
  Scenario: Create - maximum params - filters
    When I create Task
      | task_name=filters_task | due_date=auto | assignee_ids=auto | workload=None | dataset=auto | task_owner=auto | task_type=annotation | task_parent_id=None | project_id=auto | recipe_id=auto | assignments_ids=None | metadata={"key": "value"} | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}],"$or": [{"metadata": {"user.good": true}},{"metadata": {"user.bad": true}}]}} | items=None |
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"

  @testrail-C4523167
  @DAT-46622
  Scenario: Create - maximum params - items
    When I create Task
      | task_name=items_task | due_date=auto | assignee_ids=None | workload=auto | dataset=auto | task_owner=auto | task_type=annotation | task_parent_id=None | project_id=auto | recipe_id=auto | assignments_ids=None | metadata={"key": "value"} | filters=None | items=3 |
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"


  @testrail-C4523167
  @DAT-46622
  Scenario: Create - task with metadata
    When I create Task
      | task_name=metadata_task | assignee_ids=auto | metadata={"key": "value"} | items=3 |
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"

  @testrail-C4523167
  @DAT-50650
  Scenario: Create - Assignees with capital letters - Should work as expected
    When I create Task
      | task_name=metadata_task | assignee_ids=["aNnotator1@dataloop.ai"] | metadata={"key": "value"} | items=3 |
    Then I receive a task entity
    And Task has the correct attributes for type "annotation"

================================================
File: tests/features/tasks_repo/test_tasks_delete.feature
================================================
Feature: Tasks repository delete method testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "tasks_delete"
    And I create a dataset with a random name
    And There are items, path = "filters/image.jpg"
      | annotated_type={"box": 3, "polygon": 3} | metadata={"user.good": 3, "user.bad": 3} |
    When Add Members "annotator1@dataloop.ai" as "annotator"
    And Add Members "annotator2@dataloop.ai" as "annotator"

  @testrail-C4523168
  @DAT-46623
  Scenario: Delete - by name
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto |
    And I delete task by "name"
    Then Task has been deleted

  @testrail-C4523168
  @DAT-46623
  Scenario: Delete - by id
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto |
    And I delete task by "id"
    Then Task has been deleted


  @testrail-C4523168
  @DAT-46623
  Scenario: Delete - by object
    When I create Task
      | task_name=min_params | due_date=auto | assignee_ids=auto |
    And I delete task by "object"
    Then Task has been deleted



================================================
File: tests/features/tasks_repo/test_tasks_get.feature
================================================
Feature: Tasks repository get method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "tasks_get"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"

    @testrail-C4523169
    @DAT-46624
    Scenario: Get - name
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        And I get task by "name"
        Then I receive a Task entity
        And Task received equals task created

    @testrail-C4523169
    @DAT-46624
    Scenario: Get - id
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        And I get task by "id"
        Then I receive a Task entity
        And Task received equals task created

    @testrail-C4523169
    @DAT-46624
    Scenario: Get - not existing - name
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        And I get task by wrong "name"
        Then "NotFound" exception should be raised

    @testrail-C4523169
    @DAT-46624
    Scenario: Get - not existing - id
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        And I get task by wrong "id"
        Then "NotFound" exception should be raised



================================================
File: tests/features/tasks_repo/test_tasks_list.feature
================================================
Feature: Tasks repository list method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "tasks_list"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"


    @second_project.delete
    @DAT-47055
    Scenario: list
        Given There is a second project and dataset
        When Add Members "annotator1@dataloop.ai" as "annotator" to second_project
        And Add Members "annotator2@dataloop.ai" as "annotator" to second_project
        Given There are items for another dataset, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        When I create Task
            | task_name=list_tasks | due_date=auto | project_id=auto | recipe_id=auto | assignee_ids=auto |
        And I create Task in second project
            | task_name=list_tasks | due_date=auto | project_id=second |  recipe_id=second | assignee_ids=auto |
        And I create Task
            | task_name=list_tasks | due_date=auto | recipe_id=auto | project_id=auto | assignee_ids=auto |
        And I create Task
            | task_name=list_tasks_different_name | due_date=next_week | project_id=auto | recipe_id=auto | assignee_ids=auto |

        When I list Tasks by param "project_ids" value "current_project"
        Then I receive a list of "3" tasks

        When I list Tasks by param "project_ids" value "second_project"
        Then I receive a list of "1" tasks

        When I list Tasks by param "project_ids" value "both"
        Then I receive a list of "4" tasks

        When I list Tasks by param "task_name" value "list_tasks"
        Then I receive a list of "2" tasks

        When I list Tasks by param "task_name" value "list_tasks_different_name"
        Then I receive a list of "1" tasks

        When I list Tasks by param "task_name" value "random_name"
        Then I receive a list of "0" tasks

        When I list Tasks by param "recipe" value "current_project"
        Then I receive a list of "3" tasks

        When I list Tasks by param "recipe" value "second_project"
        Then I receive a list of "1" tasks

        When I list Tasks by param "creator" value "self"
        Then I receive a list of "4" tasks

        When I list Tasks by param "min_date" value "2_days_from_now"
        Then I receive a list of "1" tasks

        When I list Tasks by param "min_date" value "2_weeks_from_now"
        Then I receive a list of "0" tasks

        When I list Tasks by param "max_date" value "2_days_from_now"
        Then I receive a list of "3" tasks

        When I list Tasks by param "max_date" value "2_weeks_from_now"
        Then I receive a list of "4" tasks

        When I list Tasks by param "max_date" value "today"
        Then I receive a list of "0" tasks


================================================
File: tests/features/tasks_repo/test_tasks_qa_task.feature
================================================
Feature: Tasks repository create qa task method testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "tasks_create_qa"
        And I create a dataset with a random name
        And There are items, path = "filters/image.jpg"
            |annotated_type={"box": 3, "polygon": 3}|metadata={"user.good": 3, "user.bad": 3}|
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"

    @testrail-C4523171
    @DAT-46626
    Scenario: Create
        When I create Task
            | task_name=qa_test | due_date=auto | assignee_ids=auto | filters={"filter": {"$and": [{"hidden": false}, {"type": "file"}, {"annotated": true}]}} |
        And I update items status to default task actions
        And I create a qa task
            | due_date=auto | assignee_ids=auto |
        Then I receive a qa task object
        And Qa task is properly made
        And Task has the correct attributes for type "qa"

    @testrail-C4523171
    @DAT-46626
    Scenario: Create - qa task with metadata
        Given I save dataset items to context
        When I create Task
            | task_name=metadata_task | assignee_ids=auto | metadata={"key": "value"} | items=3 |
        And I update items status to default task actions
        And I create a qa task
            | assignee_ids=auto | metadata={"key": "qa_value"} |
        Then I receive a qa task object
        And Qa task is properly made
        And Task has the correct attributes for type "qa"



================================================
File: tests/features/triggers_repo/test_cron_trigger_create.feature
================================================
@bot.create
Feature: Cron Triggers repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "triggers_cron_create"
    And I create a dataset with a random name


  @services.delete
  @packages.delete
  @testrail-C4530457
  @DAT-46633
  Scenario: Get cron execution
    Given There is a package (pushed from "triggers/item") by the name of "triggers-cron"
    And There is a service by the name of "triggers-cron" with module name "default_module" saved to context "service"
    And I create a cron trigger
      | name=triggers-create | function_name=run | cron=* * * * * |
    When I list service executions
    Then I receive a list of "0" executions
    And I wait "63"
    When I list service executions
    Then I receive a list of "1" executions
    And I wait "63"
    When I list service executions
    Then I receive a list of "2" executions
    When I delete trigger by "id"




================================================
File: tests/features/triggers_repo/test_cron_triggers_delete.feature
================================================
@bot.create
Feature: Triggers repository delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_delete"
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-delete"
        And There is a service by the name of "triggers-delete" with module name "default_module" saved to context "service"
        And I create a cron trigger
            |name=triggers-create|function_name=run| cron=None |

    @services.delete
    @packages.delete
    @testrail-C4523173
    @DAT-46630
    Scenario: Delete by id
        When I delete trigger by "id"
        Then There are no triggers

    @services.delete
    @packages.delete
    @testrail-C4523173
    @DAT-46630
    Scenario: Delete by name
        When I delete trigger by "name"
        Then There are no triggers

    @services.delete
    @packages.delete
    @testrail-C4523173
    @DAT-46630
    Scenario: Delete by entity
        When I delete trigger by "entity"
        Then There are no triggers


================================================
File: tests/features/triggers_repo/test_cron_triggers_get.feature
================================================
@bot.create
Feature: Triggers repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_get"
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-get"
        And There is a service by the name of "triggers-get" with module name "default_module" saved to context "service"
        And I create a cron trigger
            |name=triggers-create|function_name=run| cron=None |

    @services.delete
    @packages.delete
    @testrail-C4523174
    @DAT-46631
    Scenario: Get by id
        When I get trigger by id
        Then I receive a Cron Trigger object
        And Trigger received equals to trigger created

    @services.delete
    @packages.delete
    @testrail-C4523174
    @DAT-46631
    Scenario: Get by name
        When I get trigger by name
        Then I receive a Cron Trigger object
        And Trigger received equals to trigger created




================================================
File: tests/features/triggers_repo/test_cron_triggers_list.feature
================================================
@bot.create
Feature: Triggers repository list service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_list"
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-list"
        And There is a service by the name of "triggers-list" with module name "default_module" saved to context "service"

    @services.delete
    @packages.delete
    @testrail-C4523175
    @DAT-46632
    Scenario: List when none exist
        When I list triggers
        Then I receive a Trigger list of "0" objects

    @services.delete
    @packages.delete
    @testrail-C4523175
    @DAT-46632
    Scenario: List when 1 exist
        Given I create a cron trigger
            |name=triggers-list|function_name=run| cron=None |
        When I list triggers
        Then I receive a Trigger list of "1" objects

    @services.delete
    @packages.delete
    @testrail-C4523175
    @DAT-46632
    Scenario: List when 2 exist
        Given I create a cron trigger
            |name=triggers-list1|function_name=run| cron=None |
        Given I create a cron trigger
            |name=triggers-list2|function_name=run| cron=None |
        When I list triggers
        Then I receive a Trigger list of "2" objects


================================================
File: tests/features/triggers_repo/test_pipeline_trigger_update.feature
================================================
Feature: Pipeline entity method testing

    Background: Initiate Platform Interface and create a pipeline
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "test_pipeline_flow"
        And I create a dataset with a random name
        When I create a new plain recipe
        And I update dataset recipe to the new recipe

    @pipelines.delete
    @testrail-C4532242
    @DAT-46635
    Scenario: pipeline flow
        When I create a package and service to pipeline
        And I create a pipeline from json
        And I update pipeline trigger action
        Then valid trigger updated
        Then "trigger" has updatedBy field

    @pipelines.delete
    @testrail-C4532242
    @DAT-46635
    Scenario: pipeline flow
        When I create a package and service to pipeline
        And I create a pipeline from json
        And I add trigger to the node and check installed with param keep_triggers_active equal to "False"


================================================
File: tests/features/triggers_repo/test_pipeline_triggers_create.feature
================================================
Feature: Pipeline entity DatasetNode triggers method testing

  Background: Initiate Platform Interface and create a pipeline
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "test_pipeline_datasetNode"
    And I create a dataset with a random name
    And I add folder "pipeline" to context.dataset

  @pipelines.delete
  @testrail-C4533426
  @DAT-46634
  Scenario: Create pipeline with dataset node > Should build trigger with default and custom filter - Executions should success
    When I create a pipeline with 2 dataset nodes and trigger with filters
      | key                      | value   |
      | metadata.system.mimetype | *image* |
    Then pipeline trigger created with filter params
      | datasetId, dir, metadata.system.mimetype |
    When I upload file in path "0000000162.jpg" to remote path "/pipeline"
    Then I expect that pipeline execution has "2" success executions
    When I get all items
    Then I receive a list of "2" items


================================================
File: tests/features/triggers_repo/test_trigger_filters.feature
================================================
@bot.create
Feature: Triggers repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "triggers_filters"
    And I create a dataset with a random name
    And I add folder ".hidden-folder" to context.dataset

  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-49912
  Scenario: Service - create Item Trigger with filter hidden true - Should execute on hidden items
    Given There is a package (pushed from "triggers/item") by the name of "triggers-create"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I create filters
    And I add field "hidden" with values "True" and operator "None"
    And I create a trigger
      | name=triggers-create | filters=context | resource=Item | action=Created | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I upload file in path "0000000162.jpg" to remote path "/.hidden-folder"
    Then Service was triggered on "hidden-item"
    When I list service executions
    Then I receive a list of "1" executions


  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-49912
  Scenario: Pipeline - create Item Trigger with filter hidden true - Should execute on hidden items
    Given I create pipeline with the name "pipeline"
    And I create "code" node with params
      | key      | value |
      | position | (1,1) |
    When I add and connect all nodes in list to pipeline entity
    And I add trigger to first node with params
      | key    | value |
      | hidden | True  |
    And I install pipeline in context
    And I get service in index "0"
    When I upload file in path "0000000162.png" to remote path "/.hidden-folder"
    Then Service was triggered on "hidden-item"
    When I list service executions
    Then I receive a list of "1" executions


================================================
File: tests/features/triggers_repo/test_triggers_annotation_deleted.feature
================================================
@bot.create
Feature: Triggers repository types - Annotation

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_deleted"
        And I create a dataset with a random name
        And I upload item in "0000000162.jpg" to dataset


    @services.delete
    @packages.delete
    @testrail-C4525049
    @DAT-46636
    Scenario: Updated Annotation Trigger Once
        Given There is a package (pushed from "triggers/annotation") by the name of "triggers-delete"
        And There is a service by the name of "triggers-delete" with module name "default_module" saved to context "service"
        When I annotate item
        And I create a trigger
            |name=triggers-delete|filters=None|resource=Annotation|action=Deleted|active=True|executionMode=Once|
        Then I receive a Trigger entity
        And I wait "5"
        When I delete annotation from type "point" using "item" entity
        Then I validate deleted action trigger on "annotation"


================================================
File: tests/features/triggers_repo/test_triggers_annotation_update_always.feature
================================================
@bot.create
Feature: Triggers repository types - Annotation

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_updated"
        And I create a dataset with a random name


    @services.delete
    @packages.delete
    @testrail-C4525048
    @DAT-46637
    Scenario: Updated Annotation Trigger Always
        Given There is a package (pushed from "triggers/annotation") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I upload item in "0000000162.jpg" to dataset
        When I annotate item
        When I create a trigger
            |name=triggers-update|filters=None|resource=Annotation|action=Updated|active=True|executionMode=Always|
        Then I receive a Trigger entity
        And I wait "5"
        And I update annotation label with new name "label_name"
        Then Service was triggered on "annotation"
        And I wait "5"
        And I update annotation label with new name "label_name"
        Then Service was triggered on "annotation"


================================================
File: tests/features/triggers_repo/test_triggers_annotation_update_once.feature
================================================
@bot.create
Feature: Triggers repository types - Annotation

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_updated"
        And I create a dataset with a random name


    @services.delete
    @packages.delete
    @testrail-C4525048
    @DAT-46637
    Scenario: Updated Annotation Trigger Once
        Given There is a package (pushed from "triggers/annotation") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I upload item in "0000000162.jpg" to dataset
        And I annotate item
        And I create a trigger
            |name=triggers-update|filters=None|resource=Annotation|action=Updated|active=True|executionMode=Once|
        Then I receive a Trigger entity
        And I wait "5"
        And I update annotation label with new name "label_name"
        Then Service was triggered on "annotation"


================================================
File: tests/features/triggers_repo/test_triggers_assignment.feature
================================================
@bot.create
Feature: Triggers repository - Assignment input resource

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "assignment_triggers"
    And I create a dataset with a random name
    When Add Members "annotator1@dataloop.ai" as "annotator"

  @services.delete
  @packages.delete
  @testrail-C4536678
  @DAT-46638
  Scenario: Created Assignment Trigger action created once
    Given There is a package (pushed from "triggers/assignment") by the name of "triggers-assignment-once"
    And There is a service by the name of "triggers-assignment-once" with module name "default_module" saved to context "service"
    When I upload item in "0000000162.jpg" to dataset
    And I create a trigger
      | name=triggers-create | filters=None | resource=Assignment | action=Created | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I create Task
      | task_name=default_name | due_date=auto | assignee_ids=annotator1@dataloop.ai |
    Then I wait "5"
    When I get assignment from task
    And I update assignment name "name-updated"
    Then Service was triggered on "assignment"

  @services.delete
  @packages.delete
  @testrail-C4536678
  @DAT-46638
  Scenario: Created Assignment Trigger action updated mode always
    Given There is a package (pushed from "triggers/assignment") by the name of "triggers-assignment-always"
    And There is a service by the name of "triggers-assignment-always" with module name "default_module" saved to context "service"
    When I upload item in "0000000162.jpg" to dataset
    And I create Task
      | task_name=default_name | due_date=auto | assignee_ids=annotator1@dataloop.ai |
    And I create a trigger
      | name=triggers-update | filters=None | resource=Assignment | action=Updated | active=True | executionMode=Always |
    Then I wait "5"
    When I get assignment from task
    And I update assignment name "assignment-updated"
    Then Service was triggered on "assignment"
    When I update assignment name "assignment-updated-again"
    Then Service was triggered on "assignment" again



================================================
File: tests/features/triggers_repo/test_triggers_context.feature
================================================
@bot.create
Feature: Triggers repository context testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create projects by the name of "trigger-context services-context2"
        And I set Project to Project 1
        And There are no datasets
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-context"
        And There is a service by the name of "triggers-context" with module name "default_module" saved to context "service"
        And I append service to services
        And I create a trigger
            |name=triggers-create|filters=None|resource=Item|action=Created|active=True|executionMode=Once|

    @services.delete
    @packages.delete
    @testrail-C4523176
    @DAT-46639
    Scenario: Get trigger from the project it belong to
        When I get the trigger from project number 1
        Then Trigger Project_id is equal to project 1 id
        And Trigger Project.id is equal to project 1 id
        And Trigger Service_id is equal to service 1 id
        And Trigger Service.id is equal to service 1 id

    @services.delete
    @packages.delete
    @testrail-C4523176
    @DAT-46639
    Scenario: Get trigger from the project it belong to
        When I get the trigger from project number 2
        Then Trigger Project_id is equal to project 1 id
        And Trigger Project.id is equal to project 1 id
        And Trigger Service_id is equal to service 1 id
        And Trigger Service.id is equal to service 1 id

    @services.delete
    @packages.delete
    @testrail-C4523176
    @DAT-46639
    Scenario: Get trigger from the service it belong to
        When I get the trigger from project number 1
        Then Trigger Project_id is equal to project 1 id
        And Trigger Project.id is equal to project 1 id
        And Trigger Service_id is equal to service 1 id
        And Trigger Service.id is equal to service 1 id

    @services.delete
    @packages.delete
    @testrail-C4523176
    @DAT-46639
    Scenario: Get trigger from the service it belong to
        Given There is a service by the name of "executions-context2" with module name "default_module" saved to context "service"
        And I append service to services
        When I get the trigger from project number 2
        Then Trigger Project_id is equal to project 1 id
        And Trigger Project.id is equal to project 1 id
        And Trigger Service_id is equal to service 1 id
        And Trigger Service.id is equal to service 1 id


================================================
File: tests/features/triggers_repo/test_triggers_create_1.feature
================================================
@bot.create
Feature: Triggers repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "triggers_create"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-46640
  Scenario: Created Item Trigger
    Given There is a package (pushed from "triggers/item") by the name of "triggers-create"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I create a trigger
      | name=triggers-create | filters=None | resource=Item | action=Created | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I upload item in "0000000162.jpg" to dataset
    Then Service was triggered on "item"
    When I list service executions
    Then I receive a list of "1" executions

  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-46640
  Scenario: Created Item Trigger - specified function name
    Given There is a package (pushed from "triggers/function_name") with function "train"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I create a trigger
      | name=triggers-create | filters=None | resource=Item | action=Created | active=True | executionMode=Once | function_name=train |
    Then I receive a Trigger entity
    When I upload item in "0000000162.jpg" to dataset
    Then Service was triggered on "item"

  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-46640
  Scenario: Updated Item Trigger
    Given There is a package (pushed from "triggers/item") by the name of "triggers-create"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I upload item in "0000000162.jpg" to dataset
    Then I wait "7"
    When I create a trigger
      | name=triggers-create | filters=None | resource=Item | action=Updated | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I edit item user metadata
    Then Service was triggered on "item"


================================================
File: tests/features/triggers_repo/test_triggers_create_2.feature
================================================
@bot.create
Feature: Triggers repository create service testing

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "triggers_create"
    And I create a dataset with a random name

  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-46640
  Scenario: Created Annotation Trigger
    Given There is a package (pushed from "triggers/annotation") by the name of "triggers-create"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I upload item in "0000000162.jpg" to dataset
    And I create a trigger
      | name=triggers-create | filters=None | resource=Annotation | action=Created | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I annotate item
    Then Service was triggered on "annotation"

  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-46640
  Scenario: Created Item clone Trigger
    Given There is a package (pushed from "triggers/item") by the name of "triggers-create"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I create a trigger
      | name=triggers-clone | filters=None | resource=Item | action=Clone | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I upload item in "0000000162.jpg" to dataset
    And  I clone item to dataset
    Then Service was triggered on "itemclone"


  @services.delete
  @packages.delete
  @testrail-C4523177
  @DAT-50229
  Scenario: Created Item Trigger with wrong action - Should raise error
    Given There is a package (pushed from "triggers/item") by the name of "triggers-create"
    And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
    When I create a trigger
      | name=triggers-clone | filters=None | resource=Item | action=statusChanged | active=True | executionMode=Always |
    Then "BadRequest" exception should be raised
    And "Received an unsupported action type as input, type: statusChanged for resource type: Item" in error message

================================================
File: tests/features/triggers_repo/test_triggers_dataset_create.feature
================================================
@bot.create
Feature: Triggers repository types - dataset

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_created"
        And I create a dataset with a random name


    @services.delete
    @packages.delete
    @testrail-C4525044
    @DAT-46641
    Scenario: Created Dataset Trigger
        Given There is a package (pushed from "triggers/dataset") by the name of "triggers-create"
        And There is a service by the name of "triggers-create" with module name "default_module" saved to context "service"
        When I create a trigger
            |name=triggers-create|filters=None|resource=Dataset|action=Created|active=True|executionMode=Once|
        Then I receive a Trigger entity
        Given I create a dataset with a random name
        Then Service was triggered on "dataset"


================================================
File: tests/features/triggers_repo/test_triggers_dataset_delete.feature
================================================
@bot.create
Feature: Triggers repository types - dataset

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_deleted"
        And I create a dataset with a random name


    @services.delete
    @packages.delete
    @testrail-C4525046
    @DAT-46642
    Scenario: Deleted Dataset Trigger
        Given There is a package (pushed from "triggers/dataset") by the name of "triggers-delete"
        And There is a service by the name of "triggers-delete" with module name "default_module" saved to context "service"
        When I create a trigger
            |name=triggers-delete|filters=None|resource=Dataset|action=Deleted|active=True|executionMode=Once|
        Then I receive a Trigger entity
        When I delete the dataset that was created by name
        Then Service was triggered on "dataset"


================================================
File: tests/features/triggers_repo/test_triggers_dataset_update.feature
================================================
@bot.create
Feature: Triggers repository types - dataset

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_updated"
        And I create a dataset with a random name


    @services.delete
    @packages.delete
    @testrail-C4525045
    @DAT-46643
    Scenario: Updated Dataset Trigger Once
        Given There is a package (pushed from "triggers/dataset") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I create a trigger
            |name=triggers-update|filters=None|resource=Dataset|action=Updated|active=True|executionMode=Once|
        Then I receive a Trigger entity
        When I update dataset name to new random name
        Then Service was triggered on "dataset"


    @services.delete
    @packages.delete
    @testrail-C4525045
    @DAT-46643
    Scenario: Updated Dataset Trigger Always
        Given There is a package (pushed from "triggers/dataset") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I create a trigger
            |name=triggers-update|filters=None|resource=Dataset|action=Updated|active=True|executionMode=Always|
        Then I receive a Trigger entity
        When I update dataset name to new random name
        Then Service was triggered on "dataset"
        When I update dataset name to new random name
        Then Service was triggered on "dataset" again






================================================
File: tests/features/triggers_repo/test_triggers_delete.feature
================================================
@bot.create
Feature: Triggers repository delete service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_delete"
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-delete"
        And There is a service by the name of "triggers-delete" with module name "default_module" saved to context "service"
        And I create a trigger
            |name=triggers-create|filters=None|resource=Item|action=Created|active=True|executionMode=Once|

    @services.delete
    @packages.delete
    @testrail-C4523178
    @DAT-46644
    Scenario: Delete by id
        When I delete trigger by "id"
        Then There are no triggers

    @services.delete
    @packages.delete
    @testrail-C4523178
    @DAT-46644
    Scenario: Delete by name
        When I delete trigger by "name"
        Then There are no triggers

    @services.delete
    @packages.delete
    @testrail-C4523178
    @DAT-46644
    Scenario: Delete by entity
        When I delete trigger by "entity"
        Then There are no triggers


================================================
File: tests/features/triggers_repo/test_triggers_get.feature
================================================
@bot.create
Feature: Triggers repository get service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_get"
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-get"
        And There is a service by the name of "triggers-get" with module name "default_module" saved to context "service"
        And I create a trigger
            |name=triggers-create|filters=None|resource=Item|action=Created|active=True|executionMode=Once|

    @services.delete
    @packages.delete
    @testrail-C4523179
    @DAT-46645
    Scenario: Get by id
        When I get trigger by id
        Then I receive a Trigger object
        And Trigger received equals to trigger created

    @services.delete
    @packages.delete
    @testrail-C4523179
    @DAT-46645
    Scenario: Get by name
        When I get trigger by name
        Then I receive a Trigger object
        And Trigger received equals to trigger created




================================================
File: tests/features/triggers_repo/test_triggers_item_deleted.feature
================================================
@bot.create
Feature: Triggers repository types - item

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_deleted"
        And I create a dataset with a random name
        And I upload item in "0000000162.jpg" to dataset


    @services.delete
    @packages.delete
    @testrail-C4525043
    @DAT-46646
    Scenario: Deleted Item Trigger with once
        Given There is a package (pushed from "triggers/item") by the name of "triggers-delete"
        And There is a service by the name of "triggers-delete" with module name "default_module" saved to context "service"
        When I create a trigger
            |name=triggers-deleted|filters=None|resource=Item|action=Deleted|active=True|executionMode=Once|
        Then I receive a Trigger entity
        And I wait "5"
        When I delete the item by id
        Then I wait "5"
        And I validate deleted action trigger on "item"


================================================
File: tests/features/triggers_repo/test_triggers_item_status.feature
================================================
@bot.create
Feature: Triggers repository types - itemStatus

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_update"
        And I create a dataset with a random name
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"

    @services.delete
    @packages.delete
    @testrail-C4525051
    @DAT-46647
    Scenario: Update ItemStatus Trigger once
        Given There is a package (pushed from "triggers/item") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I upload item in "0000000162.jpg" to dataset
        When I create a trigger
            |name=triggers-update|filters=None|resource=ItemStatus|action=Updated|active=True|executionMode=Once|
        Then I receive a Trigger entity
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        Then I receive a task entity
        When I update items status to default task actions
        Then Service was triggered on "item"

    @services.delete
    @packages.delete
    @testrail-C4525051
    @DAT-46647
    Scenario: Update ItemStatus Trigger once
        Given There is a package (pushed from "triggers/item") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I upload item in "0000000162.jpg" to dataset
        When I create a trigger
            |name=triggers-update|filters=None|resource=ItemStatus|action=Updated|active=True|executionMode=Always|
       Then I receive a Trigger entity
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        Then I receive a task entity
        When I update items status to default task actions
        Then I receive a Trigger entity
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        Then I receive a task entity
        When I update items status to default task actions with task id
        Then Service was triggered on "item" again



================================================
File: tests/features/triggers_repo/test_triggers_item_update.feature
================================================
@bot.create
Feature: Triggers repository types - item

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_update"
        And I create a dataset with a random name


    @services.delete
    @packages.delete
    @testrail-C4525052
    @DAT-46648
    Scenario: Updated Item Trigger with always
        Given There is a package (pushed from "triggers/item") by the name of "triggers-update"
        And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
        When I upload item in "0000000162.jpg" to dataset
        Then I wait "7"
        When I create a trigger
            |name=triggers-update|filters=None|resource=Item|action=Updated|active=True|executionMode=Always|
        Then I receive a Trigger entity
        When I edit item user metadata
        Then Service was triggered on "item"
        And I wait "7"
        When I edit item user metadata
        Then Service was triggered on "item" again







================================================
File: tests/features/triggers_repo/test_triggers_list.feature
================================================
@bot.create
Feature: Triggers repository list service testing

    Background: Initiate Platform Interface and create a project
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "triggers_list"
        And I create a dataset with a random name
        And There is a package (pushed from "triggers/item") by the name of "triggers-list"
        And There is a service by the name of "triggers-list" with module name "default_module" saved to context "service"

    @services.delete
    @packages.delete
    @testrail-C4523180
    @DAT-46649
    Scenario: List when none exist
        When I list triggers
        Then I receive a Trigger list of "0" objects

    @services.delete
    @packages.delete
    @testrail-C4523180
    @DAT-46649
    Scenario: List when 1 exist
        Given I create a trigger
            |name=triggers-list|filters=None|resource=Item|action=Created|active=True|executionMode=Once|
        When I list triggers
        Then I receive a Trigger list of "1" objects

    @services.delete
    @packages.delete
    @testrail-C4523180
    @DAT-46649
    Scenario: List when 2 exist
        Given I create a trigger
            |name=triggers-list-1|filters=None|resource=Item|action=Created|active=True|executionMode=Once|
        And I create a trigger
            |name=triggers-list-2|filters=None|resource=Item|action=Created|active=True|executionMode=Once|
        When I list triggers
        Then I receive a Trigger list of "2" objects


================================================
File: tests/features/triggers_repo/test_triggers_task_create.feature
================================================
@bot.create
Feature: Triggers repository create service testing - Task resource

  Background: Initiate Platform Interface and create a project
    Given Platform Interface is initialized as dlp and Environment is set according to git branch
    And I create a project by the name of "task_triggers"
    And I create a dataset with a random name
    When Add Members "annotator1@dataloop.ai" as "annotator"

  @services.delete
  @packages.delete
  @testrail-C4533676
  @DAT-46650
  Scenario: Created Task Trigger action created once
    Given There is a package (pushed from "triggers/task") by the name of "triggers-task-once"
    And There is a service by the name of "triggers-task-once" with module name "default_module" saved to context "service"
    When I upload item in "0000000162.jpg" to dataset
    And I create a trigger
      | name=triggers-create | filters=None | resource=Task | action=Created | active=True | executionMode=Once |
    Then I receive a Trigger entity
    When I create Task
      | task_name=default_name | due_date=auto | assignee_ids=annotator1@dataloop.ai |
    Then I wait "20"
    And Service was triggered on "task"


  @services.delete
  @packages.delete
  @testrail-C4533676
  @DAT-46650
  Scenario: Created Task Trigger action updated mode always
    Given There is a package (pushed from "triggers/task") by the name of "triggers-task-always"
    And There is a service by the name of "triggers-task-always" with module name "default_module" saved to context "service"
    When I create Task
      | task_name=default_name | due_date=auto | assignee_ids=annotator1@dataloop.ai |
    And I create a trigger
      | name=triggers-update | filters=None | resource=Task | action=Updated | active=True | executionMode=Always |
    And I update task name "Task-updated"
    Then Service was triggered on "task"
    When I update task name "Task-updated-again"
    Then Service was triggered on "task" again



================================================
File: tests/features/triggers_repo/test_triggers_update.feature
================================================
# @bot.create
# Feature: Triggers repository update service testing

# Background: Initiate Platform Interface and create a project
#     Given Platform Interface is initialized as dlp and Environment is set according to git branch
#     And I create a project by the name of "triggers_update"
#     And I create a dataset with a random name
#     And There is a package (pushed from "triggers/item") by the name of "triggers-update"
#     And There is a service by the name of "triggers-update" with module name "default_module" saved to context "service"
#     And I create a trigger
#         | name=triggers-update | filters=None | resource=Item | action=Created | active=True | executionMode=Once |

# @services.delete
# @packages.delete
# Scenario: Update trigger

#     When I update trigger
#         | filters={"$and": [{"dir": {"$in": ["/trigger_dir"]}}]} |
#     Then I receive an updated Trigger object
#     And Trigger attributes are modified
#         | filters={"$and": [{"dir": {"$in": ["/trigger_dir"]}}]} |
#     And Trigger works only on items in "/trigger_dir"

#     When I update trigger
#         | active=False |
#     Then I receive an updated Trigger object
#     And Trigger attributes are modified
#         | active=False  |
#     Then Trigger is inactive

#     When I update trigger
#         | active=True |
#     Then I receive an updated Trigger object
#     And Trigger attributes are modified
#         | active=True  |
#     Then Trigger works only on items in "/trigger_dir"

#     When I update trigger
#         | filters={} | action=Updated |
#     Then I receive an updated Trigger object
#     And Trigger attributes are modified
#         | filters={} | action=Updated |
#     And Trigger works only on items updated

#     When I try to update trigger
#         | executionMode=Always |
#     Then "BadRequest" exception should be raised


================================================
File: tests/features/webm_converter/test_failed_video_message.feature
================================================
Feature: Webm Converter service testing - failed message

    Background: Initiate Platform Interface
        Given Platform Interface is initialized as dlp and Environment is set according to git branch
        And I create a project by the name of "project_failed_video"
        And I create a dataset with a random name
        When Add Members "annotator1@dataloop.ai" as "annotator"
        And Add Members "annotator2@dataloop.ai" as "annotator"

    @testrail-C4532774
    @DAT-46652
    Scenario: Video with wrong FPS should display message and have success execution status
        Given Item in path "webm-converter/failed_video.mp4" is uploaded to "Dataset"
        When I create Task
            | task_name=min_params | due_date=auto | assignee_ids=auto |
        When I wait for video services to finish
        Then I validate execution status item metadata have the right message



================================================
File: tests/features/xray/test_xray.feature
================================================
Feature: XRay integration

    Background: Background name
        Given Some initial setup

    @DAT-44223
    Scenario: Foo
        When creating foo
        Then return foo

    @DAT-44181
    Scenario: Bar
        Given existing foo
        When creating bar
        Then return bar
        And foo

================================================
File: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.


